#ifndef WJR_BIGINTEGER_HPP__
#define WJR_BIGINTEGER_HPP__

#ifndef WJR_BIGINTEGER_BIGINTEGER_HPP__
#define WJR_BIGINTEGER_BIGINTEGER_HPP__

#include <optional>

#ifndef WJR_FORMAT_OSTREAM_INSERT_HPP__
#define WJR_FORMAT_OSTREAM_INSERT_HPP__

#include <ostream>

namespace wjr {

/// @private
template <typename CharT, typename Tratis>
void __ostream_write_unchecked(std::basic_ostream<CharT, Tratis> &os, const CharT *str,
                               std::streamsize n) {
    const auto __put = os.rdbuf()->sputn(str, n);
    if (__put != n) {
        os.setstate(std::ios_base::badbit);
    }
}

/// @private
template <typename CharT, typename Tratis>
void __ostream_fill_unchecked(std::basic_ostream<CharT, Tratis> &os, std::streamsize n) {
    const auto ch = os.fill();
    while (n--) {
        const auto __put = os.rdbuf()->sputc(ch);
        if (Tratis::eq_int_type(__put, Tratis::eof())) {
            os.setstate(std::ios_base::badbit);
            break;
        }
    }
}

/// @private
template <typename CharT, typename Tratis>
void __ostream_insert_unchecked(std::basic_ostream<CharT, Tratis> &os, const CharT *str,
                                std::streamsize n) {
    const std::streamsize __w = os.width();
    if (__w > n) {
        const std::streamsize __pad = __w - n;
        const bool __left =
            ((os.flags() & std::ios_base::adjustfield) == std::ios_base::left);

        if (!__left) {
            __ostream_fill_unchecked(os, __pad);
        }
        if (os.good()) {
            __ostream_write_unchecked(os, str, n);
        }
        if (__left && os.good()) {
            __ostream_fill_unchecked(os, __pad);
        }
    } else {
        __ostream_write_unchecked(os, str, n);
    }

    os.width(0);
}

/**
 * @brief Fast output a string to the output stream.
 *
 */
template <typename CharT, typename Tratis>
std::basic_ostream<CharT, Tratis> &__ostream_insert(std::basic_ostream<CharT, Tratis> &os,
                                                    const CharT *str, std::streamsize n) {
    const std::ostream::sentry ok(os);
    if (ok) {
        __ostream_insert_unchecked(os, str, n);
    }

    return os;
}

} // namespace wjr

#endif // WJR_FORMAT_OSTREAM_INSERT_HPP__
#ifndef WJR_MATH_HPP__
#define WJR_MATH_HPP__

#ifndef WJR_MATH_CONVERT_HPP__
#define WJR_MATH_CONVERT_HPP__

#include <array>
#include <system_error>

#ifndef WJR_ASSERT_HPP__
#define WJR_ASSERT_HPP__

#include <cstdio>
#include <cstdlib>
#include <iostream>
#include <tuple>
#include <type_traits>
#include <utility>

#ifndef WJR_PREPROCESSOR_HPP__
#define WJR_PREPROCESSOR_HPP__

#ifndef WJR_PREPROCESSOR_PREVIEW_HPP__
#define WJR_PREPROCESSOR_PREVIEW_HPP__

// testing ...

#ifndef WJR_PREPROCESSOR_ARITHMATIC_HPP__
#define WJR_PREPROCESSOR_ARITHMATIC_HPP__

#ifndef WJR_PREPROCESSOR_ARITHMATIC_INC_HPP__
#define WJR_PREPROCESSOR_ARITHMATIC_INC_HPP__

#define WJR_PP_INC(x) WJR_PP_INC_I(x)
#define WJR_PP_INC_I(x) WJR_PP_INC_##x

#define WJR_PP_INC_0 1
#define WJR_PP_INC_1 2
#define WJR_PP_INC_2 3
#define WJR_PP_INC_3 4
#define WJR_PP_INC_4 5
#define WJR_PP_INC_5 6
#define WJR_PP_INC_6 7
#define WJR_PP_INC_7 8
#define WJR_PP_INC_8 9
#define WJR_PP_INC_9 10
#define WJR_PP_INC_10 11
#define WJR_PP_INC_11 12
#define WJR_PP_INC_12 13
#define WJR_PP_INC_13 14
#define WJR_PP_INC_14 15
#define WJR_PP_INC_15 16
#define WJR_PP_INC_16 17
#define WJR_PP_INC_17 18
#define WJR_PP_INC_18 19
#define WJR_PP_INC_19 20
#define WJR_PP_INC_20 21
#define WJR_PP_INC_21 22
#define WJR_PP_INC_22 23
#define WJR_PP_INC_23 24
#define WJR_PP_INC_24 25
#define WJR_PP_INC_25 26
#define WJR_PP_INC_26 27
#define WJR_PP_INC_27 28
#define WJR_PP_INC_28 29
#define WJR_PP_INC_29 30
#define WJR_PP_INC_30 31
#define WJR_PP_INC_31 32
#define WJR_PP_INC_32 33
#define WJR_PP_INC_33 34
#define WJR_PP_INC_34 35
#define WJR_PP_INC_35 36
#define WJR_PP_INC_36 37
#define WJR_PP_INC_37 38
#define WJR_PP_INC_38 39
#define WJR_PP_INC_39 40
#define WJR_PP_INC_40 41
#define WJR_PP_INC_41 42
#define WJR_PP_INC_42 43
#define WJR_PP_INC_43 44
#define WJR_PP_INC_44 45
#define WJR_PP_INC_45 46
#define WJR_PP_INC_46 47
#define WJR_PP_INC_47 48
#define WJR_PP_INC_48 49
#define WJR_PP_INC_49 50
#define WJR_PP_INC_50 51
#define WJR_PP_INC_51 52
#define WJR_PP_INC_52 53
#define WJR_PP_INC_53 54
#define WJR_PP_INC_54 55
#define WJR_PP_INC_55 56
#define WJR_PP_INC_56 57
#define WJR_PP_INC_57 58
#define WJR_PP_INC_58 59
#define WJR_PP_INC_59 60
#define WJR_PP_INC_60 61
#define WJR_PP_INC_61 62
#define WJR_PP_INC_62 63
#define WJR_PP_INC_63 0

#endif // ! WJR_PREPROCESSOR_ARITHMATIC_INC_HPP__
#ifndef WJR_PREPROCESSOR_ARITHMATIC_DEC_HPP__
#define WJR_PREPROCESSOR_ARITHMATIC_DEC_HPP__

#define WJR_PP_DEC(x) WJR_PP_DEC_I(x)

#define WJR_PP_DEC_I(x) WJR_PP_DEC_##x

#define WJR_PP_DEC_0 63
#define WJR_PP_DEC_1 0
#define WJR_PP_DEC_2 1
#define WJR_PP_DEC_3 2
#define WJR_PP_DEC_4 3
#define WJR_PP_DEC_5 4
#define WJR_PP_DEC_6 5
#define WJR_PP_DEC_7 6
#define WJR_PP_DEC_8 7
#define WJR_PP_DEC_9 8
#define WJR_PP_DEC_10 9
#define WJR_PP_DEC_11 10
#define WJR_PP_DEC_12 11
#define WJR_PP_DEC_13 12
#define WJR_PP_DEC_14 13
#define WJR_PP_DEC_15 14
#define WJR_PP_DEC_16 15
#define WJR_PP_DEC_17 16
#define WJR_PP_DEC_18 17
#define WJR_PP_DEC_19 18
#define WJR_PP_DEC_20 19
#define WJR_PP_DEC_21 20
#define WJR_PP_DEC_22 21
#define WJR_PP_DEC_23 22
#define WJR_PP_DEC_24 23
#define WJR_PP_DEC_25 24
#define WJR_PP_DEC_26 25
#define WJR_PP_DEC_27 26
#define WJR_PP_DEC_28 27
#define WJR_PP_DEC_29 28
#define WJR_PP_DEC_30 29
#define WJR_PP_DEC_31 30
#define WJR_PP_DEC_32 31
#define WJR_PP_DEC_33 32
#define WJR_PP_DEC_34 33
#define WJR_PP_DEC_35 34
#define WJR_PP_DEC_36 35
#define WJR_PP_DEC_37 36
#define WJR_PP_DEC_38 37
#define WJR_PP_DEC_39 38
#define WJR_PP_DEC_40 39
#define WJR_PP_DEC_41 40
#define WJR_PP_DEC_42 41
#define WJR_PP_DEC_43 42
#define WJR_PP_DEC_44 43
#define WJR_PP_DEC_45 44
#define WJR_PP_DEC_46 45
#define WJR_PP_DEC_47 46
#define WJR_PP_DEC_48 47
#define WJR_PP_DEC_49 48
#define WJR_PP_DEC_50 49
#define WJR_PP_DEC_51 50
#define WJR_PP_DEC_52 51
#define WJR_PP_DEC_53 52
#define WJR_PP_DEC_54 53
#define WJR_PP_DEC_55 54
#define WJR_PP_DEC_56 55
#define WJR_PP_DEC_57 56
#define WJR_PP_DEC_58 57
#define WJR_PP_DEC_59 58
#define WJR_PP_DEC_60 59
#define WJR_PP_DEC_61 60
#define WJR_PP_DEC_62 61
#define WJR_PP_DEC_63 62

#endif // WJR_PREPROCESSOR_ARITHMATIC_DEC_HPP__
#ifndef WJR_PREPROCESSOR_ARITHMATIC_CMP_HPP__
#define WJR_PREPROCESSOR_ARITHMATIC_CMP_HPP__

#ifndef WJR_PREPROCESSOR_ARITHMATIC_BASIC_HPP__
#define WJR_PREPROCESSOR_ARITHMATIC_BASIC_HPP__

#define WJR_PP_ARITHMATIC_FROM_NUMBER(x) WJR_PP_ARITHMATIC_FROM_NUMBER_I(x)
#define WJR_PP_ARITHMATIC_FROM_NUMBER_I(x) WJR_PP_ARITHMATIC_FROM_NUMBER_##x

#define WJR_PP_ARITHMATIC_TO_NUMBER(x) WJR_PP_ARITHMATIC_TO_NUMBER_I(x)
#define WJR_PP_ARITHMATIC_TO_NUMBER_I(x) __wjr_pp_arithmatic_from_number_##x

#define WJR_PP_ARITHMATIC_FROM_NUMBER_0
#define WJR_PP_ARITHMATIC_FROM_NUMBER_1 x
#define WJR_PP_ARITHMATIC_FROM_NUMBER_2 xx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_3 xxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_4 xxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_5 xxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_6 xxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_7 xxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_8 xxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_9 xxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_10 xxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_11 xxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_12 xxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_13 xxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_14 xxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_15 xxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_16 xxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_17 xxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_18 xxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_19 xxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_20 xxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_21 xxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_22 xxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_23 xxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_24 xxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_25 xxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_26 xxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_27 xxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_28 xxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_29 xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_30 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_31 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_32 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_33 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_34 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_35 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_36 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_37 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_38 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_39 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_40 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_41 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_42 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_43 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_44 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_45 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_46 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_47 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_48 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_49 xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_50                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_51                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_52                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_53                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_54                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_55                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_56                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_57                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_58                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_59                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_60                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_61                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_62                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#define WJR_PP_ARITHMATIC_FROM_NUMBER_63                                                 \
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

#define __wjr_pp_arithmatic_from_number_ 0
#define __wjr_pp_arithmatic_from_number_x 1
#define __wjr_pp_arithmatic_from_number_xx 2
#define __wjr_pp_arithmatic_from_number_xxx 3
#define __wjr_pp_arithmatic_from_number_xxxx 4
#define __wjr_pp_arithmatic_from_number_xxxxx 5
#define __wjr_pp_arithmatic_from_number_xxxxxx 6
#define __wjr_pp_arithmatic_from_number_xxxxxxx 7
#define __wjr_pp_arithmatic_from_number_xxxxxxxx 8
#define __wjr_pp_arithmatic_from_number_xxxxxxxxx 9
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxx 10
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxx 11
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxx 12
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxx 13
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxx 14
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxx 15
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxx 16
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxx 17
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxx 18
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxx 19
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxx 20
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxx 21
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxx 22
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxx 23
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxx 24
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxx 25
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxx 26
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxx 27
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxx 28
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx 29
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 30
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 31
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 32
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 33
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 34
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 35
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 36
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 37
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 38
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 39
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 40
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 41
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 42
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 43
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 44
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 45
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 46
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 47
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    48
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    49
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    50
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    51
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    52
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    53
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    54
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    55
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    56
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    57
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    58
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    59
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    60
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    61
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    62
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    63
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    2
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    3
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    4
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    5
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    6
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    7
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    8
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    9
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    10
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    11
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    12
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    13
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    14
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    15
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    16
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    17
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    18
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    19
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    20
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    21
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    22
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    23
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    24
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    25
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    26
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    27
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    28
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    29
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    30
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    31
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    32
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    33
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    34
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    35
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    36
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    37
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    38
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    39
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    40
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    41
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    42
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    43
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    44
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    45
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    46
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    47
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    48
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    49
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    50
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    51
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    52
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    53
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    54
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    55
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    56
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    57
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    58
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    59
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    60
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    61
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    62
#define __wjr_pp_arithmatic_from_number_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    63

#endif // ! WJR_PREPROCESSOR_ARITHMATIC_BASIC_HPP__
// Already included
#ifndef WJR_PREPROCESSOR_ARITHMATIC_NEG_HPP__
#define WJR_PREPROCESSOR_ARITHMATIC_NEG_HPP__

#define WJR_PP_NEG(x) WJR_PP_NEG_I(x)
#define WJR_PP_NEG_I(x) WJR_PP_NEG_##x

#define WJR_PP_NEG_0 63
#define WJR_PP_NEG_1 62
#define WJR_PP_NEG_2 61
#define WJR_PP_NEG_3 60
#define WJR_PP_NEG_4 59
#define WJR_PP_NEG_5 58
#define WJR_PP_NEG_6 57
#define WJR_PP_NEG_7 56
#define WJR_PP_NEG_8 55
#define WJR_PP_NEG_9 54
#define WJR_PP_NEG_10 53
#define WJR_PP_NEG_11 52
#define WJR_PP_NEG_12 51
#define WJR_PP_NEG_13 50
#define WJR_PP_NEG_14 49
#define WJR_PP_NEG_15 48
#define WJR_PP_NEG_16 47
#define WJR_PP_NEG_17 46
#define WJR_PP_NEG_18 45
#define WJR_PP_NEG_19 44
#define WJR_PP_NEG_20 43
#define WJR_PP_NEG_21 42
#define WJR_PP_NEG_22 41
#define WJR_PP_NEG_23 40
#define WJR_PP_NEG_24 39
#define WJR_PP_NEG_25 38
#define WJR_PP_NEG_26 37
#define WJR_PP_NEG_27 36
#define WJR_PP_NEG_28 35
#define WJR_PP_NEG_29 34
#define WJR_PP_NEG_30 33
#define WJR_PP_NEG_31 32
#define WJR_PP_NEG_32 31
#define WJR_PP_NEG_33 30
#define WJR_PP_NEG_34 29
#define WJR_PP_NEG_35 28
#define WJR_PP_NEG_36 27
#define WJR_PP_NEG_37 26
#define WJR_PP_NEG_38 25
#define WJR_PP_NEG_39 24
#define WJR_PP_NEG_40 23
#define WJR_PP_NEG_41 22
#define WJR_PP_NEG_42 21
#define WJR_PP_NEG_43 20
#define WJR_PP_NEG_44 19
#define WJR_PP_NEG_45 18
#define WJR_PP_NEG_46 17
#define WJR_PP_NEG_47 16
#define WJR_PP_NEG_48 15
#define WJR_PP_NEG_49 14
#define WJR_PP_NEG_50 13
#define WJR_PP_NEG_51 12
#define WJR_PP_NEG_52 11
#define WJR_PP_NEG_53 10
#define WJR_PP_NEG_54 9
#define WJR_PP_NEG_55 8
#define WJR_PP_NEG_56 7
#define WJR_PP_NEG_57 6
#define WJR_PP_NEG_58 5
#define WJR_PP_NEG_59 4
#define WJR_PP_NEG_60 3
#define WJR_PP_NEG_61 2
#define WJR_PP_NEG_62 1
#define WJR_PP_NEG_63 0

#endif // ! WJR_PREPROCESSOR_ARITHMATIC_NEG_HPP__
#ifndef WJR_PREPROCESSOR_DETAILS_BASIC_HPP__
#define WJR_PREPROCESSOR_DETAILS_BASIC_HPP__

#define WJR_PP_EMPTY(...)

#define WJR_PP_CONCAT(x, y) WJR_PP_CONCAT_I(x, y)
#define WJR_PP_CONCAT_I(x, y) x##y

#define WJR_PP_EXPAND(...) WJR_PP_EXPAND_I(__VA_ARGS__)
#define WJR_PP_EXPAND_I(...) __VA_ARGS__

#define WJR_PP_STR(x) WJR_PP_STR_I(x)
#define WJR_PP_STR_I(x) #x

#define WJR_PP_STRS(...) WJR_PP_STRS_I(__VA_ARGS__)
#define WJR_PP_STRS_I(...) # __VA_ARGS__

#define WJR_PP_ESC(x) WJR_PP_ESC_(WJR_PP_ESC_I, x)
#define WJR_PP_ESC_(M, x) M x
#define WJR_PP_ESC_I(...) __VA_ARGS__

// don't support 0 agument
#define WJR_PP_ARGS_LEN(...) WJR_PP_ARGS_LEN_I(__VA_ARGS__)

#define WJR_PP_ARGS_LEN_I(...)                                                           \
    WJR_PP_EXPAND(WJR_PP_ARGS_LEN_(0, ##__VA_ARGS__, WJR_PP_ARGS_LEN_RSEQ_N()))

#define WJR_PP_ARGS_LEN_RSEQ_N()                                                         \
    64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44,  \
        43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24,  \
        23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2,  \
        1, 0
#define WJR_PP_ARGS_LEN_(...) WJR_PP_EXPAND(WJR_PP_ARGS_LEN_N(__VA_ARGS__))
#define WJR_PP_ARGS_LEN_N(                                                               \
    _1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13, _14, _15, _16, _17, _18,     \
    _19, _20, _21, _22, _23, _24, _25, _26, _27, _28, _29, _30, _31, _32, _33, _34, _35, \
    _36, _37, _38, _39, _40, _41, _42, _43, _44, _45, _46, _47, _48, _49, _50, _51, _52, \
    _53, _54, _55, _56, _57, _58, _59, _60, _61, _62, _63, _64, _65, N, ...)             \
    N

#define WJR_PP_IS_NULLPTR(VAL)                                                           \
    WJR_PP_IS_NULLPTR_I(WJR_PP_CONCAT(WJR_PP_IS_NULLPTR_, VAL), 0)
#define WJR_PP_IS_NULLPTR_I(...) WJR_PP_IS_NULLPTR_II(__VA_ARGS__)
#define WJR_PP_IS_NULLPTR_II(HOLDER, VAL, ...) VAL
#define WJR_PP_IS_NULLPTR_WJR_PP_NULLPTR WJR_PP_HOLDER, 1

#define WJR_PP_MAP_DEF(VAL) WJR_PP_HOLDER, VAL

// if MAP ## KEY is defined as WJR_PP_MAP_DEF, then return VAL
// else return WJR_PP_NULLPTR
#define WJR_PP_MAP_FIND(MAP, KEY) WJR_PP_MAP_FIND_I(MAP, KEY)
#define WJR_PP_MAP_FIND_I(MAP, KEY)                                                      \
    WJR_PP_MAP_FIND_II(WJR_PP_CONCAT(MAP, KEY), WJR_PP_NULLPTR)
#define WJR_PP_MAP_FIND_II(...) WJR_PP_MAP_FIND_III(__VA_ARGS__)
#define WJR_PP_MAP_FIND_III(HOLDER, VAL, ...) VAL

#endif // ! WJR_PREPROCESSOR_DETAILS_BASIC_HPP__
#ifndef WJR_PREPROCESSOR_LOGICAL_BASIC_HPP__
#define WJR_PREPROCESSOR_LOGICAL_BASIC_HPP__

#define WJR_PP_BOOL_IF(cond, t, f) WJR_PP_BOOL_IF_I(cond, t, f)
#define WJR_PP_BOOL_IF_I(cond, t, f) WJR_PP_BOOL_IF_I##cond(t, f)
#define WJR_PP_BOOL_IF_I0(t, f) f
#define WJR_PP_BOOL_IF_I1(t, f) t

#define WJR_PP_BOOL_NOT(x) WJR_PP_BOOL_IF(x, 0, 1)

#define WJR_PP_BOOL_AND(x, y) WJR_PP_BOOL_IF(x, WJR_PP_BOOL_IF(y, 1, 0), 0)
#define WJR_PP_BOOL_OR(x, y) WJR_PP_BOOL_IF(x, 1, WJR_PP_BOOL_IF(y, 1, 0))
#define WJR_PP_BOOL_XOR(x, y) WJR_PP_BOOL_IF(x, WJR_PP_BOOL_NOT(y), y)

#endif // ! WJR_PREPROCESSOR_LOGICAL_BASIC_HPP__
#ifndef WJR_PREPROCESSOR_LOGICAL_BOOL_HPP__
#define WJR_PREPROCESSOR_LOGICAL_BOOL_HPP__

#define WJR_PP_BOOL(x) WJR_PP_BOOL_I(x)

#define WJR_PP_BOOL_I(x) WJR_PP_BOOL_##x

#define WJR_PP_BOOL_0 0
#define WJR_PP_BOOL_1 1
#define WJR_PP_BOOL_2 1
#define WJR_PP_BOOL_3 1
#define WJR_PP_BOOL_4 1
#define WJR_PP_BOOL_5 1
#define WJR_PP_BOOL_6 1
#define WJR_PP_BOOL_7 1
#define WJR_PP_BOOL_8 1
#define WJR_PP_BOOL_9 1
#define WJR_PP_BOOL_10 1
#define WJR_PP_BOOL_11 1
#define WJR_PP_BOOL_12 1
#define WJR_PP_BOOL_13 1
#define WJR_PP_BOOL_14 1
#define WJR_PP_BOOL_15 1
#define WJR_PP_BOOL_16 1
#define WJR_PP_BOOL_17 1
#define WJR_PP_BOOL_18 1
#define WJR_PP_BOOL_19 1
#define WJR_PP_BOOL_20 1
#define WJR_PP_BOOL_21 1
#define WJR_PP_BOOL_22 1
#define WJR_PP_BOOL_23 1
#define WJR_PP_BOOL_24 1
#define WJR_PP_BOOL_25 1
#define WJR_PP_BOOL_26 1
#define WJR_PP_BOOL_27 1
#define WJR_PP_BOOL_28 1
#define WJR_PP_BOOL_29 1
#define WJR_PP_BOOL_30 1
#define WJR_PP_BOOL_31 1
#define WJR_PP_BOOL_32 1
#define WJR_PP_BOOL_33 1
#define WJR_PP_BOOL_34 1
#define WJR_PP_BOOL_35 1
#define WJR_PP_BOOL_36 1
#define WJR_PP_BOOL_37 1
#define WJR_PP_BOOL_38 1
#define WJR_PP_BOOL_39 1
#define WJR_PP_BOOL_40 1
#define WJR_PP_BOOL_41 1
#define WJR_PP_BOOL_42 1
#define WJR_PP_BOOL_43 1
#define WJR_PP_BOOL_44 1
#define WJR_PP_BOOL_45 1
#define WJR_PP_BOOL_46 1
#define WJR_PP_BOOL_47 1
#define WJR_PP_BOOL_48 1
#define WJR_PP_BOOL_49 1
#define WJR_PP_BOOL_50 1
#define WJR_PP_BOOL_51 1
#define WJR_PP_BOOL_52 1
#define WJR_PP_BOOL_53 1
#define WJR_PP_BOOL_54 1
#define WJR_PP_BOOL_55 1
#define WJR_PP_BOOL_56 1
#define WJR_PP_BOOL_57 1
#define WJR_PP_BOOL_58 1
#define WJR_PP_BOOL_59 1
#define WJR_PP_BOOL_60 1
#define WJR_PP_BOOL_61 1
#define WJR_PP_BOOL_62 1
#define WJR_PP_BOOL_63 1

#endif // WJR_PREPROCESSOR_LOGICAL_BOOL_HPP__

#define WJR_PP_LT(x, y) WJR_PP_BOOL_IF(WJR_PP_BOOL(y), WJR_PP_ADD_OVERFLOW(x, y), 0)
#define WJR_PP_GT(x, y) WJR_PP_LT(y, x)
#define WJR_PP_LE(x, y) WJR_PP_BOOL_NOT(WJR_PP_GT(x, y))
#define WJR_PP_GE(x, y) WJR_PP_LE(y, x)
#define WJR_PP_NE(x, y) WJR_PP_BOOL_OR(WJR_PP_LT(x, y), WJR_PP_LT(y, x))
#define WJR_PP_EQ(x, y) WJR_PP_BOOL_NOT(WJR_PP_NE(x, y))

#define WJR_PP_ADD_OVERFLOW(x, y)                                                        \
    WJR_PP_ADD_OVERFLOW_I(                                                               \
        WJR_PP_CONCAT(WJR_PP_ARITHMATIC_FROM_NUMBER(x),                                  \
                      WJR_PP_ARITHMATIC_FROM_NUMBER(WJR_PP_INC(WJR_PP_NEG(y)))))
#define WJR_PP_ADD_OVERFLOW_I(x) WJR_PP_ADD_OVERFLOW_II(x)
#define WJR_PP_ADD_OVERFLOW_II(x) __wjr_arithmatic_add_overflow_##x

#define __wjr_arithmatic_add_overflow_ 1
#define __wjr_arithmatic_add_overflow_x 1
#define __wjr_arithmatic_add_overflow_xx 1
#define __wjr_arithmatic_add_overflow_xxx 1
#define __wjr_arithmatic_add_overflow_xxxx 1
#define __wjr_arithmatic_add_overflow_xxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    1
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0
#define __wjr_arithmatic_add_overflow_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \
    0

#endif // ! WJR_PREPROCESSOR_ARITHMATIC_CMP_HPP__
#ifndef WJR_PREPROCESSOR_ARITHMATIC_SUB_HPP__
#define WJR_PREPROCESSOR_ARITHMATIC_SUB_HPP__

#ifndef WJR_PREPROCESSOR_ARITHMATIC_ADD_HPP__
#define WJR_PREPROCESSOR_ARITHMATIC_ADD_HPP__

// Already included
// Already included

#define WJR_PP_ADD(x, y) WJR_PP_ADD_I(x, y)
#define WJR_PP_ADD_I(x, y)                                                               \
    WJR_PP_ARITHMATIC_TO_NUMBER(WJR_PP_CONCAT(WJR_PP_ARITHMATIC_FROM_NUMBER(x),          \
                                              WJR_PP_ARITHMATIC_FROM_NUMBER(y)))

#endif // ! WJR_PREPROCESSOR_ARITHMATIC_ADD_HPP__
// Already included
// Already included

#define WJR_PP_SUB(x, y) WJR_PP_ADD(x, WJR_PP_INC(WJR_PP_NEG(y)))

#endif // ! WJR_PREPROCESSOR_ARITHMATIC_SUB_HPP__

#endif // ! WJR_PREPROCESSOR_ARITHMATIC_HPP__
#ifndef WJR_PREPROCESSOR_COMPILER_HPP__
#define WJR_PREPROCESSOR_COMPILER_HPP__

#ifndef WJR_PREPROCESSOR_COMPILER_ATTRIBUTE_HPP__
#define WJR_PREPROCESSOR_COMPILER_ATTRIBUTE_HPP__

#ifndef WJR_PREPROCESSOR_COMPILER_HAS_HPP__
#define WJR_PREPROCESSOR_COMPILER_HAS_HPP__

#ifndef WJR_PREPROCESSOR_COMPILER_ARCH_HPP__
#define WJR_PREPROCESSOR_COMPILER_ARCH_HPP__

#if defined(__pnacl__) || defined(__CLR_VER)
#define WJR_VM
#endif

#if (defined(_M_IX86) || defined(__i386__)) && !defined(WJR_VM)
#define WJR_X86_32
#endif

#if (defined(_M_X64) || defined(__x86_64__)) && !defined(WJR_VM)
#define WJR_X86_64
#endif

#if defined(WJR_X86_32) || defined(WJR_X86_64)
#define WJR_X86
#endif

#if (defined(__arm__) || defined(_M_ARM))
#define WJR_ARM
#endif

#if defined(__aarch64__)
#define WJR_AARCH64
#endif

#if defined(__powerpc64__)
#define WJR_PPC64
#endif

#if defined(WJR_X86_64)
#if defined(__i386__) || defined(_M_IX86) || defined(_X86_)
#define CPU_INTEL
#elif defined(_M_AMD64)
#define CPU_AMD
#endif
#else
#define CPU_UNKNOWN
#endif

#endif // !WJR_PREPROCESSOR_COMPILER_ARCH_HPP__
#ifndef WJR_PREPROCESSOR_COMPILER_COMPILER_HPP__
#define WJR_PREPROCESSOR_COMPILER_COMPILER_HPP__

#if defined(__clang__)
#define WJR_COMPILER_CLANG
#elif defined(__GNUC__)
#define WJR_COMPILER_GCC
#elif defined(_MSC_VER)
#define WJR_COMPILER_MSVC
#endif

#if defined(_MSC_VER)
#define WJR_MSVC
#endif // _MSC_VER

#if defined(__GNUC__)
#define WJR_HAS_GCC(major, minor, patchlevel)                                            \
    ((__GNUC__ > (major)) || (__GNUC__ == (major) && __GNUC_MINOR__ > (minor)) ||        \
     (__GNUC__ == (major) && __GNUC_MINOR__ == (minor) &&                                \
      __GNUC_PATCHLEVEL__ >= (patchlevel)))
#else
#define WJR_HAS_GCC(major, minor, patchlevel) 0
#endif // __GNUC__

#if defined(__clang__)
#define WJR_HAS_CLANG(major, minor, patchlevel)                                          \
    ((__clang_major__ > (major)) ||                                                      \
     (__clang_major__ == (major) && __clang_minor__ > (minor)) ||                        \
     (__clang_major__ == (major) && __clang_minor__ == (minor) &&                        \
      __clang_patchlevel__ >= (patchlevel)))
#else
#define WJR_HAS_CLANG(major, minor, patchlevel) 0
#endif

#if defined(_MSC_VER)
#define WJR_HAS_MSVC(minor, level) (_MSC_VER >= (minor)*100 + (level))
#else
#define WJR_HAS_MSVC(minor, level) 0
#endif

#if (defined(WJR_COMPILER_GCC) && !WJR_HAS_GCC(7, 1, 0)) ||                              \
    (defined(WJR_COMPILER_CLANG) && !WJR_HAS_CLANG(5, 0, 0))
#error "GCC 7.1.0 or Clang 5.0.0 or later is required"
#endif

#if defined(WJR_COMPILER_CLANG) || defined(WJR_COMPILER_GCC)
#define WJR_CXX_STANDARD __cplusplus
#elif defined(WJR_COMPILER_MSVC)
#define WJR_CXX_STANDARD _MSVC_LANG
#endif

#if WJR_CXX_STANDARD >= 199711L
#define WJR_CXX_03
#endif
#if WJR_CXX_STANDARD >= 201103L
#define WJR_CXX_11
#endif
#if WJR_CXX_STANDARD >= 201402L
#define WJR_CXX_14
#endif
#if WJR_CXX_STANDARD >= 201703L
#define WJR_CXX_17
#endif
#if WJR_CXX_STANDARD >= 202002L
#define WJR_CXX_20
#endif

#ifndef WJR_CXX_17
#error "required C++17 or later"
#endif // c++17

#if defined(__cpp_char8_t)
#define WJR_CHAR8_T
#endif // __cpp_char8_t

#if defined(__LINE__)
#define WJR_LINE __LINE__
#elif defined(__COUNTER__)
#define WJR_LINE __COUNTER__
#else
#define WJR_LINE -1
#endif

#ifdef __FILE__
#define WJR_FILE __FILE__
#else
#define WJR_FILE ""
#endif

// reference: boost BOOST_CURRENT_FUNCTION
#if defined(BOOST_DISABLE_CURRENT_FUNCTION)
#define WJR_CURRENT_FUNCTION "(unknown)"
#elif defined(__GNUC__) || (defined(__MWERKS__) && (__MWERKS__ >= 0x3000)) ||            \
    (defined(__ICC) && (__ICC >= 600)) || defined(__ghs__)
#define WJR_CURRENT_FUNCTION __PRETTY_FUNCTION__
#elif defined(__DMC__) && (__DMC__ >= 0x810)
#define WJR_CURRENT_FUNCTION __PRETTY_FUNCTION__
#elif defined(__FUNCSIG__)
#define WJR_CURRENT_FUNCTION __FUNCSIG__
#elif (defined(__INTEL_COMPILER) && (__INTEL_COMPILER >= 600)) ||                        \
    (defined(__IBMCPP__) && (__IBMCPP__ >= 500))
#define WJR_CURRENT_FUNCTION __FUNCTION__
#elif defined(__BORLANDC__) && (__BORLANDC__ >= 0x550)
#define WJR_CURRENT_FUNCTION __FUNC__
#elif defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901)
#define WJR_CURRENT_FUNCTION __func__
#elif defined(__cplusplus) && (__cplusplus >= 201103)
#define WJR_CURRENT_FUNCTION __func__
#else
#define WJR_CURRENT_FUNCTION "(unknown)"
#endif

#endif // !WJR_PREPROCESSOR_COMPILER_COMPILER_HPP__
// Already included
// Already included

#define WJR_HAS_DEF_VAR(var) WJR_PP_MAP_DEF(var)
#define WJR_HAS_DEF WJR_HAS_DEF_VAR(1)

#define WJR_HAS_FIND(MAP, KEY) WJR_HAS_FIND_I(WJR_PP_MAP_FIND(MAP, KEY))
#define WJR_HAS_FIND_I(VAL) WJR_PP_BOOL_IF(WJR_PP_IS_NULLPTR(VAL), 0, VAL)

// Currently only has_builtin, has_attribute, has_feature are supported.
#define WJR_HAS_BUILTIN_FIND(KEY) WJR_HAS_FIND(WJR_HAS_BUILTIN_, KEY)
#define WJR_HAS_ATTRIBUTE_FIND(KEY) WJR_HAS_FIND(WJR_HAS_ATTRIBUTE_, KEY)
#define WJR_HAS_FEATURE_FIND(KEY) WJR_HAS_FIND(WJR_HAS_FEATURE_, KEY)
#define WJR_HAS_SIMD_FIND(KEY) WJR_HAS_FIND(WJR_HAS_SIMD_, KEY)
#define WJR_HAS_DEBUG_FIND(KEY) WJR_HAS_FIND(WJR_HAS_DEBUG_, KEY)

//

#if (defined(WJR_COMPILER_GCC) && WJR_HAS_GCC(10, 1, 0)) ||                              \
    (defined(WJR_COMPILER_CLANG) && WJR_HAS_CLANG(10, 0, 0)) ||                          \
    (!defined(WJR_COMPILER_GCC) && !defined(WJR_COMPILER_CLANG) &&                       \
     defined(__has_builtin))
#define WJR_HAS_BUILTIN(x) WJR_PP_BOOL_IF(WJR_HAS_BUILTIN_FIND(x), 1, __has_builtin(x))
#else
#define WJR_HAS_BUILTIN(x) WJR_HAS_BUILTIN_FIND(x)
#endif

#if defined(__has_include)
#define WJR_HAS_INCLUDE(x) __has_include(x)
#else
#define WJR_HAS_INCLUDE(x) 0
#endif // __has_includeF

#if defined(__has_attribute)
#define WJR_HAS_ATTRIBUTE(x)                                                             \
    WJR_PP_BOOL_IF(WJR_HAS_ATTRIBUTE_FIND(x), 1, __has_attribute(x))
#else
#define WJR_HAS_ATTRIBUTE(x) WJR_HAS_ATTRIBUTE_FIND(x)
#endif

#if defined(__has_cpp_attribute)
#define WJR_HAS_CPP_ATTRIBUTE(x) __has_cpp_attribute(x)
#else
#define WJR_HAS_CPP_ATTRIBUTE(x) 0
#endif

#define WJR_HAS_FEATURE(x) WJR_HAS_FEATURE_FIND(x)

#define WJR_HAS_SIMD(x) WJR_HAS_SIMD_FIND(x)

#define WJR_HAS_DEBUG(x) WJR_HAS_DEBUG_FIND(x)

// WJR_HAS_BUILTIN

#if WJR_HAS_GCC(7, 1, 0) || WJR_HAS_CLANG(5, 0, 0)
#define WJR_HAS_BUILTIN___builtin_unreachable WJR_HAS_DEF
#define WJR_HAS_BUILTIN___builtin_expect WJR_HAS_DEF
#define WJR_HAS_BUILTIN___builtin_constant_p WJR_HAS_DEF
#define WJR_HAS_BUILTIN___builtin_clz WJR_HAS_DEF
#define WJR_HAS_BUILTIN___builtin_ctz WJR_HAS_DEF
#define WJR_HAS_BUILTIN___builtin_popcount WJR_HAS_DEF
#endif

#if WJR_HAS_GCC(9, 1, 0) || WJR_HAS_CLANG(9, 0, 0)
#define WJR_HAS_BUILTIN___builtin_is_constant_evaluated WJR_HAS_DEF
#endif

#if WJR_HAS_CLANG(5, 0, 0)
#define WJR_HAS_BUILTIN___builtin_addc WJR_HAS_DEF
#define WJR_HAS_BUILTIN___builtin_subc WJR_HAS_DEF
#endif

#if WJR_HAS_GCC(9, 1, 0) || WJR_HAS_CLANG(11, 0, 0)
#define WJR_HAS_BUILTIN___builtin_expect_with_probability WJR_HAS_DEF
#endif

// WJR_HAS_FEATURE

#if WJR_HAS_GCC(7, 1, 0) || WJR_HAS_CLANG(5, 0, 0)
#define WJR_HAS_FEATURE_PRAGMA_UNROLL WJR_HAS_DEF
#endif

#if defined(WJR_COMPILER_GCC) || defined(WJR_COMPILER_CLANG) || defined(WJR_COMPILER_MSVC)
#define WJR_HAS_FEATURE_INLINE_ASM WJR_HAS_DEF
#ifndef WJR_COMPILER_MSVC
#define WJR_HAS_FEATURE_GCC_STYLE_INLINE_ASM WJR_HAS_DEF
#endif
#endif

#if WJR_HAS_FEATURE(GCC_STYLE_INLINE_ASM)

#if WJR_HAS_GCC(7, 1, 0) || WJR_HAS_CLANG(9, 0, 0)
#define WJR_HAS_FEATURE_INLINE_ASM_GOTO WJR_HAS_DEF
#endif

#if WJR_HAS_GCC(11, 1, 0) || WJR_HAS_CLANG(11, 0, 0)
#define WJR_HAS_FEATURE_INLINE_ASM_GOTO_OUTPUT WJR_HAS_DEF
#endif

#if defined(WJR_COMPILER_GCC) || WJR_HAS_CLANG(9, 0, 0)
#define WJR_HAS_FEATURE_INLINE_ASM_CCCOND WJR_HAS_DEF
#endif

#endif

#if defined(__SIZEOF_INT128__)
#define WJR_HAS_FEATURE_INT128 WJR_HAS_DEF
#if !(defined(__clang__) && defined(LIBDIVIDE_VC))
#define WJR_HAS_FEATURE_INT128_DIV WJR_HAS_DEF
#endif
#endif

// There are some issues with the optimization of int128 in both lower and higher versions
// (13.1/13.2) of GCC.
#if WJR_HAS_FEATURE(INT128) &&                                                           \
    (defined(WJR_COMPILER_CLANG) ||                                                      \
     (defined(WJR_COMPILER_GCC) && WJR_HAS_GCC(8, 1, 0) && !WJR_HAS_GCC(13, 1, 0)))
#define WJR_HAS_FEATURE_FAST_INT128_COMPARE WJR_HAS_DEF
#endif

// performance bug
#if WJR_HAS_FEATURE(INT128) && defined(WJR_COMPILER_CLANG)
// #define WJR_HAS_FEATURE_FAST_INT128_ADDSUB WJR_HAS_DEF
#endif

#if WJR_HAS_GCC(11, 1, 0) || WJR_HAS_CLANG(5, 0, 0)
#define WJR_HAS_ATTRIBUTE_FORCEINLINE_LAMBDA WJR_HAS_DEF
#endif

#if defined(__AVX512VL__)
#define WJR_HAS_SIMD_AVX512VL WJR_HAS_DEF
#endif

#if defined(__AVX512BW__)
#define WJR_HAS_SIMD_AVX512BW WJR_HAS_DEF
#endif

#if defined(__AVX512DQ__)
#define WJR_HAS_SIMD_AVX512DQ WJR_HAS_DEF
#endif

#if defined(__AVX512F__) ||                                                              \
    (WJR_HAS_SIMD(AVX512VL) && WJR_HAS_SIMD(AVX512BW) && WJR_HAS_SIMD(AVX512DQ))
#define WJR_HAS_SIMD_AVX512F WJR_HAS_DEF
#endif

#if defined(__AVX512__) ||                                                               \
    (WJR_HAS_SIMD(AVX512F) && WJR_HAS_SIMD(AVX512BW) && WJR_HAS_SIMD(AVX512DQ))
#define WJR_HAS_SIMD_AVX512 WJR_HAS_DEF
#endif

#if defined(__AVX2__) || (WJR_HAS_SIMD(AVX512) || WJR_HAS_SIMD(AVX512F))
#define WJR_HAS_SIMD_AVX2 WJR_HAS_DEF
#endif

#if defined(__AVX__) || WJR_HAS_SIMD(AVX2)
#define WJR_HAS_SIMD_AVX WJR_HAS_DEF
#endif

#if defined(__SSE4_2__) || WJR_HAS_SIMD(AVX)
#define WJR_HAS_SIMD_SSE4_2 WJR_HAS_DEF
#endif

#if defined(__SSE4_1__) || WJR_HAS_SIMD(SSE4_2)
#define WJR_HAS_SIMD_SSE4_1 WJR_HAS_DEF
#endif

#if defined(__SSSE3__) || WJR_HAS_SIMD(SSE4_1)
#define WJR_HAS_SIMD_SSSE3 WJR_HAS_DEF
#endif

#if defined(__SSE3__) || WJR_HAS_SIMD(SSSE3)
#define WJR_HAS_SIMD_SSE3 WJR_HAS_DEF
#endif

#if defined(__SSE2__) || WJR_HAS_SIMD(SSE3) || _M_IX86_FP >= 2 ||                        \
    (defined(_MSC_VER) && (defined(_M_AMD64) || defined(_M_X64)))
#define WJR_HAS_SIMD_SSE2 WJR_HAS_DEF
#endif

#if defined(__SSE__) || WJR_HAS_SIMD(SSE2) || _M_IX86_FP >= 1
#define WJR_HAS_SIMD_SSE WJR_HAS_DEF
#endif

#if defined(__MMX__) || WJR_HAS_SIMD(SSE)
#define WJR_HAS_SIMD_MMX WJR_HAS_DEF
#endif

#if defined(__XOP__)
#define WJR_HAS_SIMD_XOP WJR_HAS_DEF
#endif

#if defined(__POPCNT__)
#define WJR_HAS_SIMD_POPCNT WJR_HAS_DEF
#endif

#if defined(__PCLMUL__)
#define WJR_HAS_SIMD_PCLMUL WJR_HAS_DEF
#endif

#endif // WJR_PREPROCESSOR_COMPILER_HAS_HPP__

#if WJR_HAS_CPP_ATTRIBUTE(fallthrough)
#define WJR_FALLTHROUGH [[fallthrough]]
#elif WJR_HAS_ATTRIBUTE(fallthrough)
#define WJR_FALLTHROUGH __attribute__((fallthrough))
#elif defined(_MSC_VER) && defined(__fallthrough)
#define WJR_FALLTHROUGH __fallthrough
#else
#define WJR_FALLTHROUGH
#endif

#if WJR_HAS_CPP_ATTRIBUTE(noreturn)
#define WJR_NORETURN [[noreturn]]
#elif WJR_HAS_ATTRIBUTE(noreturn)
#define WJR_NORETURN __attribute__((noreturn))
#elif defined(_MSC_VER)
#define WJR_NORETURN __declspec(noreturn)
#else
#define WJR_NORETURN
#endif

#if WJR_HAS_CPP_ATTRIBUTE(nodiscard)
#define WJR_NODISCARD [[nodiscard]]
#elif WJR_HAS_ATTRIBUTE(nodiscard)
#define WJR_NODISCARD __attribute__((nodiscard))
#elif defined(_MSC_VER)
#define WJR_NODISCARD _Check_return_
#else
#define WJR_NODISCARD
#endif

#if WJR_HAS_CPP_ATTRIBUTE(deprecated)
#define WJR_DEPRECATED [[deprecated]]
#elif WJR_HAS_ATTRIBUTE(deprecated)
#define WJR_DEPRECATED __attribute__((deprecated))
#elif defined(_MSC_VER)
#define WJR_DEPRECATED __declspec(deprecated)
#else
#define WJR_DEPRECATED
#endif

#if WJR_HAS_CPP_ATTRIBUTE(maybe_unused)
#define WJR_MAYBE_UNUSED [[maybe_unused]]
#elif WJR_HAS_ATTRIBUTE(maybe_unused)
#define WJR_MAYBE_UNUSED __attribute__((maybe_unused))
#elif defined(_MSC_VER)
#define WJR_MAYBE_UNUSED
#else
#define WJR_MAYBE_UNUSED
#endif

#if WJR_HAS_ATTRIBUTE(always_inline)
#define WJR_FORCEINLINE __attribute__((always_inline))
#elif defined(_MSC_VER)
#define WJR_FORCEINLINE __forceinline
#else
#define WJR_FORCEINLINE
#endif

#if WJR_HAS_ATTRIBUTE(FORCEINLINE_LAMBDA)
#define WJR_FORCEINLINE_LAMBDA WJR_FORCEINLINE
#else
#define WJR_FORCEINLINE_LAMBDA
#endif

// NOINLINE for MSVC/GCC/CLANG ...
#if WJR_HAS_ATTRIBUTE(noinline)
#define WJR_NOINLINE __attribute__((noinline))
#elif defined(_MSC_VER)
#define WJR_NOINLINE __declspec(noinline)
#else
#define WJR_NOINLINE
#endif

#if WJR_HAS_ATTRIBUTE(hot)
#define WJR_HOT __attribute__((hot))
#elif defined(_MSC_VER)
#define WJR_HOT
#else
#define WJR_HOT
#endif

#if WJR_HAS_ATTRIBUTE(cold)
#define WJR_COLD __attribute__((cold))
#elif defined(_MSC_VER)
#define WJR_COLD
#else
#define WJR_COLD
#endif

#if WJR_HAS_ATTRIBUTE(aligned)
#define WJR_ALIGNED(size) __attribute__((aligned(size)))
#elif defined(_MSC_VER)
#define WJR_ALIGNED(size)
#else
#define WJR_ALIGNED(size)
#endif

#if defined(__cpp_lib_unreachable)
#define WJR_UNREACHABLE() std::unreachable()
#elif WJR_HAS_BUILTIN(__builtin_unreachable)
#define WJR_UNREACHABLE() __builtin_unreachable()
#elif defined(WJR_COMPILER_MSVC)
#define WJR_UNREACHABLE() __assume(0)
#else
#define WJR_UNREACHABLE()
#endif

#if defined(WJR_COMPILER_CLANG) || defined(WJR_COMPILER_GCC)
#define WJR_RESTRICT __restrict
#else
#define WJR_RESTRICT
#endif

#define WJR_ASSUME_MAY_NOT_PURE(expr)                                                    \
    do {                                                                                 \
        if (!(expr)) {                                                                   \
            WJR_UNREACHABLE();                                                           \
        }                                                                                \
    } while (0)

#if WJR_HAS_BUILTIN(__builtin_assume)
#define WJR_ASSUME(expr) __builtin_assume(expr)
#elif defined(WJR_COMPILER_MSVC)
#define WJR_ASSUME(expr) __assume(expr)
#elif WJR_HAS_CPP_ATTRIBUTE(assume)
#define WJR_ASSUME(expr) [[assume(expr)]]
#else
#define WJR_ASSUME(expr) WJR_ASSUME_MAY_NOT_PURE(expr)
#endif

#define WJR_BOOL_EXPR(expr) (!!(expr))

#if WJR_HAS_BUILTIN(__builtin_expect)
#define WJR_EXPECT(expr, expect) __builtin_expect((expr), (expect))
#else
#define WJR_EXPECT(expr, expect) (expr)
#endif

#define WJR_LIKELY(expr) WJR_EXPECT(WJR_BOOL_EXPR(expr), true)
#define WJR_UNLIKELY(expr) WJR_EXPECT(WJR_BOOL_EXPR(expr), false)

#define WJR_HAS_FEATURE_IS_CONSTANT_EVALUATED WJR_HAS_DEF

#if WJR_HAS_BUILTIN(__builtin_expect_with_probability)
#define WJR_EXPECT_WITH_PROBABILITY(exp, c, probability)                                 \
    __builtin_expect_with_probability(exp, c, probability)
#else
#define WJR_EXPECT_WITH_PROBABILITY(exp, c, probability) (expr)
#endif

#if WJR_HAS_BUILTIN(__builtin_expect_with_probability)
#define WJR_VERY_LIKELY(exp, probability)                                                \
    WJR_EXPECT_WITH_PROBABILITY(exp, true, probability)
#define WJR_VERY_UNLIKELY(exp, probability)                                              \
    WJR_EXPECT_WITH_PROBABILITY(exp, false, probability)
#else
#define WJR_VERY_LIKELY(exp, probability) WJR_LIKELY((exp))
#define WJR_VERY_UNLIKELY(exp, probability) WJR_UNLIKELY((exp))
#endif

#if defined(__cpp_lib_is_constant_evaluated)
#define WJR_IS_CONSTANT_EVALUATED() std::is_constant_evaluated()
#elif WJR_HAS_BUILTIN(__builtin_is_constant_evaluated)
#define WJR_IS_CONSTANT_EVALUATED() __builtin_is_constant_evaluated()
#else
#define WJR_IS_CONSTANT_EVALUATED() false
#undef WJR_HAS_FEATURE_IS_CONSTANT_EVALUATED
#endif

#if WJR_HAS_BUILTIN(__builtin_constant_p)
#define WJR_BUILTIN_CONSTANT_P(expr) __builtin_constant_p(expr)
#else
#define WJR_BUILTIN_CONSTANT_P(expr) false
#endif

#if WJR_HAS_FEATURE(GCC_STYLE_INLINE_ASM)
#define WJR_COMPILER_BARRIER() asm volatile("" ::: "memory")
#define WJR_COMPILER_EMPTY_ASM() asm("")
#else
#define WJR_COMPILER_BARRIER()
#define WJR_COMPILER_EMPTY_ASM()
#endif

#define WJR_CONSTEXPR_COMPILER_BARRIER()                                                 \
    do {                                                                                 \
        if (!(WJR_IS_CONSTANT_EVALUATED())) {                                            \
            WJR_COMPILER_BARRIER();                                                      \
        }                                                                                \
    } while (0)

#if defined(WJR_FORCEINLINE)
#define WJR_INTRINSIC_INLINE inline WJR_FORCEINLINE
#else
#define WJR_INTRINSIC_INLINE inline
#endif

// pure attribute
#if WJR_HAS_ATTRIBUTE(pure)
#define WJR_PURE __attribute__((pure))
#else
#define WJR_PURE
#endif

// const attribute
#if WJR_HAS_ATTRIBUTE(const)
#define WJR_CONST __attribute__((const))
#else
#define WJR_CONST
#endif

#if WJR_HAS_ATTRIBUTE(malloc)
#define WJR_MALLOC __attribute__((malloc))
#else
#define WJR_MALLOC
#endif

#define WJR_INLINE inline
#define WJR_CONSTEXPR constexpr

#if defined(WJR_CXX_20)
#define WJR_CONSTEXPR20 constexpr
#else
#define WJR_CONSTEXPR20
#endif

#if WJR_HAS_FEATURE(IS_CONSTANT_EVALUATED)
#define WJR_CONSTEXPR_E constexpr
#else
#define WJR_CONSTEXPR_E
#endif

#define WJR_INTRINSIC_CONSTEXPR WJR_INTRINSIC_INLINE constexpr
#define WJR_INTRINSIC_CONSTEXPR20 WJR_INTRINSIC_INLINE WJR_CONSTEXPR20
#define WJR_INTRINSIC_CONSTEXPR_E WJR_INTRINSIC_INLINE WJR_CONSTEXPR_E

#define WJR_INLINE_CONSTEXPR inline constexpr
#define WJR_INLINE_CONSTEXPR20 inline WJR_CONSTEXPR20
#define WJR_INLINE_CONSTEXPR_E inline WJR_CONSTEXPR_E

#define WJR_ATTRIBUTE(attribute) WJR_ATTRIBUTE_I(attribute)
#define WJR_ATTRIBUTE_I(attribute) WJR_##attribute

#if defined(_MSC_VER)
#define WJR_EMPTY_BASES __declspec(empty_bases)
#else
#define WJR_EMPTY_BASES
#endif

#endif // WJR_PREPROCESSOR_COMPILER_ATTRIBUTE_HPP__

#endif // ! WJR_PREPROCESSOR_COMPILER_HPP__
#ifndef WJR_PREPROCESSOR_DETAILS_HPP__
#define WJR_PREPROCESSOR_DETAILS_HPP__

// Already included
#ifndef WJR_PREPROCESSOR_DETAILS_IOTA_HPP__
#define WJR_PREPROCESSOR_DETAILS_IOTA_HPP__

#define WJR_PP_IOTA(n) WJR_PP_IOTA_I(n)
#define WJR_PP_IOTA_I(n) WJR_PP_IOTA_##n

#define WJR_PP_IOTA_0
#define WJR_PP_IOTA_1 0
#define WJR_PP_IOTA_2 WJR_PP_IOTA_1, 1
#define WJR_PP_IOTA_3 WJR_PP_IOTA_2, 2
#define WJR_PP_IOTA_4 WJR_PP_IOTA_3, 3
#define WJR_PP_IOTA_5 WJR_PP_IOTA_4, 4
#define WJR_PP_IOTA_6 WJR_PP_IOTA_5, 5
#define WJR_PP_IOTA_7 WJR_PP_IOTA_6, 6
#define WJR_PP_IOTA_8 WJR_PP_IOTA_7, 7
#define WJR_PP_IOTA_9 WJR_PP_IOTA_8, 8
#define WJR_PP_IOTA_10 WJR_PP_IOTA_9, 9
#define WJR_PP_IOTA_11 WJR_PP_IOTA_10, 10
#define WJR_PP_IOTA_12 WJR_PP_IOTA_11, 11
#define WJR_PP_IOTA_13 WJR_PP_IOTA_12, 12
#define WJR_PP_IOTA_14 WJR_PP_IOTA_13, 13
#define WJR_PP_IOTA_15 WJR_PP_IOTA_14, 14
#define WJR_PP_IOTA_16 WJR_PP_IOTA_15, 15
#define WJR_PP_IOTA_17 WJR_PP_IOTA_16, 16
#define WJR_PP_IOTA_18 WJR_PP_IOTA_17, 17
#define WJR_PP_IOTA_19 WJR_PP_IOTA_18, 18
#define WJR_PP_IOTA_20 WJR_PP_IOTA_19, 19
#define WJR_PP_IOTA_21 WJR_PP_IOTA_20, 20
#define WJR_PP_IOTA_22 WJR_PP_IOTA_21, 21
#define WJR_PP_IOTA_23 WJR_PP_IOTA_22, 22
#define WJR_PP_IOTA_24 WJR_PP_IOTA_23, 23
#define WJR_PP_IOTA_25 WJR_PP_IOTA_24, 24
#define WJR_PP_IOTA_26 WJR_PP_IOTA_25, 25
#define WJR_PP_IOTA_27 WJR_PP_IOTA_26, 26
#define WJR_PP_IOTA_28 WJR_PP_IOTA_27, 27
#define WJR_PP_IOTA_29 WJR_PP_IOTA_28, 28
#define WJR_PP_IOTA_30 WJR_PP_IOTA_29, 29
#define WJR_PP_IOTA_31 WJR_PP_IOTA_30, 30
#define WJR_PP_IOTA_32 WJR_PP_IOTA_31, 31
#define WJR_PP_IOTA_33 WJR_PP_IOTA_32, 32
#define WJR_PP_IOTA_34 WJR_PP_IOTA_33, 33
#define WJR_PP_IOTA_35 WJR_PP_IOTA_34, 34
#define WJR_PP_IOTA_36 WJR_PP_IOTA_35, 35
#define WJR_PP_IOTA_37 WJR_PP_IOTA_36, 36
#define WJR_PP_IOTA_38 WJR_PP_IOTA_37, 37
#define WJR_PP_IOTA_39 WJR_PP_IOTA_38, 38
#define WJR_PP_IOTA_40 WJR_PP_IOTA_39, 39
#define WJR_PP_IOTA_41 WJR_PP_IOTA_40, 40
#define WJR_PP_IOTA_42 WJR_PP_IOTA_41, 41
#define WJR_PP_IOTA_43 WJR_PP_IOTA_42, 42
#define WJR_PP_IOTA_44 WJR_PP_IOTA_43, 43
#define WJR_PP_IOTA_45 WJR_PP_IOTA_44, 44
#define WJR_PP_IOTA_46 WJR_PP_IOTA_45, 45
#define WJR_PP_IOTA_47 WJR_PP_IOTA_46, 46
#define WJR_PP_IOTA_48 WJR_PP_IOTA_47, 47
#define WJR_PP_IOTA_49 WJR_PP_IOTA_48, 48
#define WJR_PP_IOTA_50 WJR_PP_IOTA_49, 49
#define WJR_PP_IOTA_51 WJR_PP_IOTA_50, 50
#define WJR_PP_IOTA_52 WJR_PP_IOTA_51, 51
#define WJR_PP_IOTA_53 WJR_PP_IOTA_52, 52
#define WJR_PP_IOTA_54 WJR_PP_IOTA_53, 53
#define WJR_PP_IOTA_55 WJR_PP_IOTA_54, 54
#define WJR_PP_IOTA_56 WJR_PP_IOTA_55, 55
#define WJR_PP_IOTA_57 WJR_PP_IOTA_56, 56
#define WJR_PP_IOTA_58 WJR_PP_IOTA_57, 57
#define WJR_PP_IOTA_59 WJR_PP_IOTA_58, 58
#define WJR_PP_IOTA_60 WJR_PP_IOTA_59, 59
#define WJR_PP_IOTA_61 WJR_PP_IOTA_60, 60
#define WJR_PP_IOTA_62 WJR_PP_IOTA_61, 61
#define WJR_PP_IOTA_63 WJR_PP_IOTA_62, 62
#define WJR_PP_IOTA_64 WJR_PP_IOTA_63, 63

#endif // ! WJR_PREPROCESSOR_DETAILS_IOTA_HPP__
#ifndef WJR_PREPROCESSOR_DETAILS_REPEAT_HPP__
#define WJR_PREPROCESSOR_DETAILS_REPEAT_HPP__

#define WJR_PP_REPEAT(x, n) WJR_PP_REPEAT_I(x, n)
#define WJR_PP_REPEAT_I(x, n) WJR_PP_REPEAT_##n(x)

#define WJR_PP_REPEAT_0(x)
#define WJR_PP_REPEAT_1(x) x
#define WJR_PP_REPEAT_2(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_1(x)
#define WJR_PP_REPEAT_3(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x)
#define WJR_PP_REPEAT_4(x) WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_2(x)
#define WJR_PP_REPEAT_5(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_4(x)
#define WJR_PP_REPEAT_6(x) WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x)
#define WJR_PP_REPEAT_7(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x)
#define WJR_PP_REPEAT_8(x) WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_4(x)
#define WJR_PP_REPEAT_9(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_8(x)
#define WJR_PP_REPEAT_10(x) WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_8(x)
#define WJR_PP_REPEAT_11(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_8(x)
#define WJR_PP_REPEAT_12(x) WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x)
#define WJR_PP_REPEAT_13(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x)
#define WJR_PP_REPEAT_14(x) WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x)
#define WJR_PP_REPEAT_15(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x)
#define WJR_PP_REPEAT_16(x) WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_8(x)
#define WJR_PP_REPEAT_17(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_18(x) WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_19(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_20(x) WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_21(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_22(x) WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_23(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_24(x) WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_25(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_26(x) WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_27(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_28(x) WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_29(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_30(x)                                                              \
    WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_31(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x),      \
        WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_32(x) WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_16(x)
#define WJR_PP_REPEAT_33(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_34(x) WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_35(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_36(x) WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_37(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_38(x) WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_39(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_40(x) WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_41(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_42(x) WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_43(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_44(x) WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_45(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_46(x)                                                              \
    WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_47(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x),      \
        WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_48(x) WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_49(x) WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_50(x) WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_51(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_52(x) WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_53(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_54(x)                                                              \
    WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_55(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_16(x),     \
        WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_56(x) WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_57(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_58(x)                                                              \
    WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_59(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x),     \
        WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_60(x)                                                              \
    WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_61(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x),     \
        WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_62(x)                                                              \
    WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x), WJR_PP_REPEAT_16(x),     \
        WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_63(x)                                                              \
    WJR_PP_REPEAT_1(x), WJR_PP_REPEAT_2(x), WJR_PP_REPEAT_4(x), WJR_PP_REPEAT_8(x),      \
        WJR_PP_REPEAT_16(x), WJR_PP_REPEAT_32(x)
#define WJR_PP_REPEAT_64(x) WJR_PP_REPEAT_32(x), WJR_PP_REPEAT_32(x)

#endif // ! WJR_PREPROCESSOR_DETAILS_REPEAT_HPP__

#endif // ! WJR_PREPROCESSOR_DETAILS_HPP__
#ifndef WJR_PREPROCESSOR_LOGICAL_HPP__
#define WJR_PREPROCESSOR_LOGICAL_HPP__

// Already included
// Already included

#endif // ! WJR_PREPROCESSOR_LOGICAL_HPP__

// Due to the fact that call is not a simple expansion, but takes the previous output as
// the next input, the difficulty of implementing recursion is also high.
#ifndef WJR_PREPROCESSOR_QUEUE_ALGORITHM_HPP__
#define WJR_PREPROCESSOR_QUEUE_ALGORITHM_HPP__

// Already included
// Already included
#ifndef WJR_PREPROCESSOR_QUEUE_CALL_HPP__
#define WJR_PREPROCESSOR_QUEUE_CALL_HPP__

// Already included
// Already included
// Already included
#ifndef WJR_PREPROCCESSOR_QUEUE_BASIC_HPP__
#define WJR_PREPROCCESSOR_QUEUE_BASIC_HPP__

// Already included

#define WJR_PP_QUEUE_EXPAND(queue) WJR_PP_QUEUE_EXPAND_I queue
#define WJR_PP_QUEUE_EXPAND_I(...) __VA_ARGS__

#define WJR_PP_QUEUE_FRONT(queue) WJR_PP_QUEUE_FRONT_I queue
#define WJR_PP_QUEUE_FRONT_I(x, ...) x

#define WJR_PP_QUEUE_POP_FRONT(queue) WJR_PP_QUEUE_POP_FRONT_I queue
#define WJR_PP_QUEUE_POP_FRONT_I(x, ...) (__VA_ARGS__)

#define WJR_PP_QUEUE_PUSH_FRONT(queue, x) WJR_PP_QUEUE_PUSH_FRONT_I(queue, x)
#define WJR_PP_QUEUE_PUSH_FRONT_I(queue, x) (x, WJR_PP_QUEUE_EXPAND(queue))

#define WJR_PP_QUEUE_PUSH_BACK(queue, x) WJR_PP_QUEUE_PUSH_BACK_I(queue, x)
#define WJR_PP_QUEUE_PUSH_BACK_I(queue, x) (WJR_PP_QUEUE_EXPAND(queue), x)

#define WJR_PP_QUEUE_SIZE(queue) WJR_PP_QUEUE_SIZE_I(queue)
#define WJR_PP_QUEUE_SIZE_I(queue) WJR_PP_ARGS_LEN queue

#endif // ! WJR_PREPROCCESSOR_QUEUE_BASIC_HPP__

#define WJR_PP_QUEUE_CALL(args, ops)                                                     \
    WJR_PP_QUEUE_CALL_N_I(args, ops, WJR_PP_QUEUE_SIZE(args))
#define WJR_PP_QUEUE_CALL_N(args, ops, N) WJR_PP_QUEUE_CALL_N_I(args, ops, WJR_PP_INC(N))

#define WJR_PP_QUEUE_CALL_N_I(args, ops, N) WJR_PP_QUEUE_CALL_N_II(args, ops, N)
#define WJR_PP_QUEUE_CALL_N_II(args, ops, N) WJR_PP_QUEUE_CALL_##N(args, ops)

#define WJR_PP_QUEUE_CALL_GEN(args, ops)                                                 \
    WJR_PP_QUEUE_CALL_GEN_I(WJR_PP_QUEUE_FRONT(ops), WJR_PP_QUEUE_FRONT(args),           \
                            WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_POP_FRONT(args)))
#define WJR_PP_QUEUE_CALL_GEN_I(op, arg1, arg2) op(arg1, arg2)

#define WJR_PP_QUEUE_CALL_NEW_ARGS_EQ(args, ops) (WJR_PP_QUEUE_CALL_GEN(args, ops))

#define WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)                                         \
    WJR_PP_QUEUE_CALL_NEW_ARGS_NE_I(                                                     \
        WJR_PP_QUEUE_POP_FRONT(WJR_PP_QUEUE_POP_FRONT(args)),                            \
        WJR_PP_QUEUE_CALL_GEN(args, ops))
#define WJR_PP_QUEUE_CALL_NEW_ARGS_NE_I(arg1, arg2) WJR_PP_QUEUE_PUSH_FRONT(arg1, arg2)

#define WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops)                                            \
    WJR_PP_QUEUE_CALL_NEW_ARGS_I(args, ops, WJR_PP_QUEUE_SIZE(args))
#define WJR_PP_QUEUE_CALL_NEW_ARGS_I(args, ops, N)                                       \
    WJR_PP_QUEUE_CALL_NEW_ARGS_II(args, ops, N)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_II(args, ops, N)                                      \
    WJR_PP_QUEUE_CALL_NEW_ARGS_##N(args, ops)

#define WJR_PP_QUEUE_CALL_1(args, ops) args

#define WJR_PP_QUEUE_CALL_2(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops)

#define WJR_PP_QUEUE_CALL_3(args, ops)                                                   \
    WJR_PP_QUEUE_CALL_2(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                           \
                        WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_4(args, ops)                                                   \
    WJR_PP_QUEUE_CALL_3(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                           \
                        WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_5(args, ops)                                                   \
    WJR_PP_QUEUE_CALL_4(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                           \
                        WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_6(args, ops)                                                   \
    WJR_PP_QUEUE_CALL_5(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                           \
                        WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_7(args, ops)                                                   \
    WJR_PP_QUEUE_CALL_6(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                           \
                        WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_8(args, ops)                                                   \
    WJR_PP_QUEUE_CALL_7(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                           \
                        WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_9(args, ops)                                                   \
    WJR_PP_QUEUE_CALL_8(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                           \
                        WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_10(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_9(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                           \
                        WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_11(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_10(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_12(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_11(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_13(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_12(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_14(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_13(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_15(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_14(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_16(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_15(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_17(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_16(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_18(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_17(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_19(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_18(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_20(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_19(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_21(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_20(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_22(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_21(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_23(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_22(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_24(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_23(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_25(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_24(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_26(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_25(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_27(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_26(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_28(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_27(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_29(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_28(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_30(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_29(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_31(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_30(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_32(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_31(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_33(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_32(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_34(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_33(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_35(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_34(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_36(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_35(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_37(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_36(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_38(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_37(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_39(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_38(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_40(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_39(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_41(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_40(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_42(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_41(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_43(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_42(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_44(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_43(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_45(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_44(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_46(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_45(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_47(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_46(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_48(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_47(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_49(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_48(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_50(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_49(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_51(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_50(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_52(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_51(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_53(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_52(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_54(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_53(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_55(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_54(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_56(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_55(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_57(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_56(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_58(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_57(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_59(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_58(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_60(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_59(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_61(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_60(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_62(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_61(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_63(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_62(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))
#define WJR_PP_QUEUE_CALL_64(args, ops)                                                  \
    WJR_PP_QUEUE_CALL_63(WJR_PP_QUEUE_CALL_NEW_ARGS(args, ops),                          \
                         WJR_PP_QUEUE_POP_FRONT(ops))

#define WJR_PP_QUEUE_CALL_NEW_ARGS_2(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_EQ(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_3(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_4(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_5(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_6(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_7(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_8(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_9(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_10(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_11(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_12(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_13(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_14(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_15(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_16(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_17(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_18(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_19(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_20(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_21(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_22(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_23(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_24(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_25(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_26(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_27(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_28(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_29(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_30(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_31(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_32(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_33(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_34(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_35(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_36(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_37(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_38(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_39(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_40(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_41(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_42(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_43(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_44(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_45(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_46(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_47(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_48(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_49(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_50(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_51(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_52(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_53(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_54(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_55(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_56(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_57(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_58(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_59(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_60(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_61(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_62(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_63(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)
#define WJR_PP_QUEUE_CALL_NEW_ARGS_64(args, ops) WJR_PP_QUEUE_CALL_NEW_ARGS_NE(args, ops)

#endif // ! WJR_PREPROCESSOR_QUEUE_CALL_HPP__

#define WJR_PP_QUEUE_INIT_N(x, N) WJR_PP_QUEUE_INIT_N_I(x, N)
#define WJR_PP_QUEUE_INIT_N_I(x, N) (WJR_PP_REPEAT(x, N))

#define WJR_PP_QUEUE_CALL_N_SAME(args, op, N)                                            \
    WJR_PP_QUEUE_CALL_N(args, WJR_PP_QUEUE_INIT_N(op, N), N)

#define WJR_PP_QUEUE_CALL_SAME(args, op)                                                 \
    WJR_PP_QUEUE_CALL_N_SAME(args, op, WJR_PP_QUEUE_SIZE(args))

// (1,2,3), (f, g, h) -> (f(1), g(2), h(3))
#define WJR_PP_QUEUE_TRANSFORMS(queue, ops)                                              \
    WJR_PP_QUEUE_POP_FRONT(WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_CALL_N_SAME(                  \
        WJR_PP_QUEUE_PUSH_FRONT(queue, WJR_PP_QUEUE_PUSH_BACK(ops, 0)),                  \
        WJR_PP_QUEUE_TRANSFORMS_CALLER, WJR_PP_QUEUE_SIZE(queue))))

#define WJR_PP_QUEUE_TRANSFORMS_CALLER(x, y)                                             \
    WJR_PP_QUEUE_PUSH_BACK(WJR_PP_QUEUE_POP_FRONT(x), WJR_PP_QUEUE_FRONT(x)(y))

// (1,2,3), f -> (f(1), f(2), f(3))
#define WJR_PP_QUEUE_TRANSFORM(queue, op)                                                \
    WJR_PP_QUEUE_TRANSFORMS(queue, WJR_PP_QUEUE_INIT_N(op, WJR_PP_QUEUE_SIZE(queue)))

// 0, (1, 2, 3), (f, g, h) -> h(g(f(0, 1), 2), 3)
#define WJR_PP_QUEUE_ACCUMULATES(init, queue, ops)                                       \
    WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_CALL_N_SAME(                      \
        WJR_PP_QUEUE_PUSH_FRONT(                                                         \
            queue, WJR_PP_QUEUE_PUSH_BACK(WJR_PP_QUEUE_PUSH_FRONT(ops, init), 0)),       \
        WJR_PP_QUEUE_ACCUMULATES_CALLER, WJR_PP_QUEUE_SIZE(queue))))

#define WJR_PP_QUEUE_ACCUMULATES_CALLER(x, y)                                            \
    WJR_PP_QUEUE_PUSH_FRONT(                                                             \
        WJR_PP_QUEUE_POP_FRONT(WJR_PP_QUEUE_POP_FRONT(x)),                               \
        WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_POP_FRONT(x))(WJR_PP_QUEUE_FRONT(x), y))

// 0, (1, 2, 3), f
#define WJR_PP_QUEUE_ACCUMULATE(init, queue, op)                                         \
    WJR_PP_QUEUE_ACCUMULATES(init, queue,                                                \
                             WJR_PP_QUEUE_INIT_N(op, WJR_PP_QUEUE_SIZE(queue)))

// (1, 2, 3) -> 3
#define WJR_PP_QUEUE_BACK(queue)                                                         \
    WJR_PP_QUEUE_ACCUMULATE(0, queue, WJR_PP_QUEUE_BACK_CALLER)

#define WJR_PP_QUEUE_BACK_CALLER(x, y) y

// (1, 2, 3, 4, 5), 2 -> (3, 4, 5)
#define WJR_PP_QUEUE_POP_FRONT_N(queue, N)                                               \
    WJR_PP_QUEUE_POP_FRONT(WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_CALL_N(                       \
        WJR_PP_QUEUE_PUSH_FRONT(WJR_PP_QUEUE_PUSH_FRONT(queue, holder), (0)),            \
        (WJR_PP_REPEAT(WJR_PP_QUEUE_POP_FRONT_N_HEADER_CALLER, WJR_PP_INC(N)),           \
         WJR_PP_REPEAT(WJR_PP_QUEUE_POP_FRONT_N_TAILER_CALLER,                           \
                       WJR_PP_QUEUE_SIZE(queue))),                                       \
        WJR_PP_INC(WJR_PP_QUEUE_SIZE(queue)))))

#define WJR_PP_QUEUE_POP_FRONT_N_HEADER_CALLER(x, y) x
#define WJR_PP_QUEUE_POP_FRONT_N_TAILER_CALLER(x, y) WJR_PP_QUEUE_PUSH_BACK(x, y)

// (1, 2, 3, 4, 5), 2 -> (1, 2, 3)
#define WJR_PP_QUEUE_POP_BACK_N(queue, N)                                                \
    WJR_PP_QUEUE_POP_FRONT(WJR_PP_QUEUE_FRONT(                                           \
        WJR_PP_QUEUE_CALL(WJR_PP_QUEUE_PUSH_FRONT(queue, (0)),                           \
                          WJR_PP_QUEUE_POP_FRONT_N(                                      \
                              (WJR_PP_REPEAT(WJR_PP_QUEUE_POP_BACK_N_HEADER_CALLER,      \
                                             WJR_PP_QUEUE_SIZE(queue)),                  \
                               WJR_PP_REPEAT(WJR_PP_QUEUE_POP_BACK_N_TAILER_CALLER, N)), \
                              N))))

#define WJR_PP_QUEUE_POP_BACK_N_HEADER_CALLER(x, y) WJR_PP_QUEUE_PUSH_BACK(x, y)
#define WJR_PP_QUEUE_POP_BACK_N_TAILER_CALLER(x, y) x

#define WJR_PP_QUEUE_POP_BACK(queue) WJR_PP_QUEUE_POP_BACK_N(queue, 1)

#define WJR_PP_QUEUE_AT(queue, N) WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_POP_FRONT_N(queue, N))

#define WJR_PP_QUEUE_REVERSE(queue)                                                      \
    WJR_PP_QUEUE_POP_BACK(WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_CALL_N_SAME(                   \
        WJR_PP_QUEUE_PUSH_FRONT(queue, (0)), WJR_PP_QUEUE_REVERSE_CALLER,                \
        WJR_PP_QUEUE_SIZE(queue))))

#define WJR_PP_QUEUE_REVERSE_CALLER(x, y) WJR_PP_QUEUE_PUSH_FRONT(x, y)

// (a, b, c) -> a b c
#define WJR_PP_QUEUE_PUT(queue) WJR_PP_QUEUE_ACCUMULATE(, queue, WJR_PP_QUEUE_PUT_CALLER)

#define WJR_PP_QUEUE_PUT_CALLER(x, y) x y

// ((A), (B), (C)) -> (A, B, C)
#define WJR_PP_QUEUE_UNWRAP(queue)                                                       \
    WJR_PP_QUEUE_TRANSFORM(queue, WJR_PP_QUEUE_UNWRAP_CALLER)

#define WJR_PP_QUEUE_UNWRAP_CALLER(x) WJR_PP_QUEUE_UNWRAP_CALLER_I x
#define WJR_PP_QUEUE_UNWRAP_CALLER_I(x) x

// ((A), (B), (C)) -> A B C
#define WJR_PP_QUEUE_UNWRAP_PUT(queue)                                                   \
    WJR_PP_QUEUE_EXPAND(WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_CALL_N_SAME(                     \
        WJR_PP_QUEUE_PUSH_FRONT(queue, ()), WJR_PP_QUEUE_UNWRAP_PUT_CALLER,              \
        WJR_PP_QUEUE_SIZE(queue))))

#define WJR_PP_QUEUE_UNWRAP_PUT_CALLER(x, y)                                             \
    (WJR_PP_QUEUE_EXPAND(x) WJR_PP_QUEUE_EXPAND(y))

// (A, B, C) (x, y, z) -> ((A, x), (B, y), (C, z))
#define WJR_PP_QUEUE_ZIP_2(queue1, queue2)                                               \
    WJR_PP_QUEUE_POP_FRONT(WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_CALL_N_SAME(                  \
        ((queue1), queue2), WJR_PP_QUEUE_ZIP_2_CALLER, WJR_PP_QUEUE_SIZE(queue1))))

#define WJR_PP_QUEUE_ZIP_2_CALLER(x, y)                                                  \
    WJR_PP_QUEUE_PUSH_FRONT(                                                             \
        WJR_PP_QUEUE_POP_FRONT(WJR_PP_QUEUE_PUSH_BACK(                                   \
            x, (WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_FRONT(x)), WJR_PP_QUEUE_FRONT(y)))),     \
        WJR_PP_QUEUE_POP_FRONT(WJR_PP_QUEUE_FRONT(x))),                                  \
        WJR_PP_QUEUE_POP_FRONT(y)

// ((A), (B), (C)) (x, y, z) -> ((A, x), (B, y), (C, z))
#define WJR_PP_QUEUE_ZIP_MORE(queue1, queue2)                                            \
    WJR_PP_QUEUE_POP_FRONT(WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_CALL_N_SAME(                  \
        ((queue1), queue2), WJR_PP_QUEUE_ZIP_MORE_CALLER, WJR_PP_QUEUE_SIZE(queue1))))

#define WJR_PP_QUEUE_ZIP_MORE_CALLER(x, y)                                               \
    WJR_PP_QUEUE_PUSH_FRONT(                                                             \
        WJR_PP_QUEUE_POP_FRONT(WJR_PP_QUEUE_PUSH_BACK(                                   \
            x, (WJR_PP_QUEUE_EXPAND(WJR_PP_QUEUE_FRONT(WJR_PP_QUEUE_FRONT(x))),          \
                WJR_PP_QUEUE_FRONT(y)))),                                                \
        WJR_PP_QUEUE_POP_FRONT(WJR_PP_QUEUE_FRONT(x))),                                  \
        WJR_PP_QUEUE_POP_FRONT(y)

#endif // WJR_PREPROCESSOR_QUEUE_ALGORITHM_HPP__

// (a, b, c) -> f(a) f(b) f(c)
#define WJR_PP_TRANSFORM_PUT(queue, op)                                                  \
    WJR_PP_QUEUE_PUT(WJR_PP_QUEUE_TRANSFORM(queue, op))

// (a, b, c) -> (f(a), f(b), f(c)) (note : f(x) = (g(x))) -> g(a) g(b) g(c)
#define WJR_PP_TRANSFORM_UNWRAP_PUT(queue, op)                                           \
    WJR_PP_QUEUE_UNWRAP_PUT(WJR_PP_QUEUE_TRANSFORM(queue, op))

#define WJR_ATTRIBUTES(...) WJR_PP_TRANSFORM_PUT((__VA_ARGS__), WJR_ATTRIBUTES_CALLER)
#define WJR_ATTRIBUTES_CALLER(x) WJR_ATTRIBUTE(x)

#define WJR_PRAGMA_I(expr) _Pragma(#expr)
#if defined(WJR_COMPILER_GCC) || defined(WJR_COMPILER_CLANG) || defined(WJR_COMPILER_MSVC)
#define WJR_PRAGMA(expr) WJR_PRAGMA_I(expr)
#else
#define WJR_PRAGMA(expr)
#endif

#if WJR_HAS_FEATURE(PRAGMA_UNROLL)
#if defined(WJR_COMPILER_GCC)
#define WJR_UNROLL(loop) WJR_PRAGMA(GCC unroll(loop))
#else
#define WJR_UNROLL(loop) WJR_PRAGMA(unroll(loop))
#endif
#else
#define WJR_UNROLL(loop)
#endif

#define WJR_IS_OVERLAP_P(p, pn, q, qn) ((p) + (pn) > (q) && (q) + (qn) > (p))
#define WJR_IS_SEPARATE_P(p, pn, q, qn) (!WJR_IS_OVERLAP_P(p, pn, q, qn))
#define WJR_IS_SAME_OR_SEPARATE_P(p, pn, q, qn)                                          \
    (((p) == (q)) || WJR_IS_SEPARATE_P(p, pn, q, qn))
#define WJR_IS_SAME_OR_INCR_P(p, pn, q, qn)                                              \
    (((p) <= (q)) || WJR_IS_SEPARATE_P(p, pn, q, qn))
#define WJR_IS_SAME_OR_DECR_P(p, pn, q, qn)                                              \
    (((p) >= (q)) || WJR_IS_SEPARATE_P(p, pn, q, qn))

#define WJR_ASM_PIC_JMPL(LABEL, TABLE) ".long " #LABEL "-" #TABLE
#define WJR_ASM_NOPIC_JMPL(LABEL) ".quad " #LABEL

#if WJR_HAS_FEATURE(INLINE_ASM_CCCOND)
#define WJR_ASM_CCSET(c) "/* set condition codes */\n\t"
#define WJR_ASM_CCOUT(c) "=@cc" #c
#else
#define WJR_ASM_CCSET(c) "set" #c " %[_cc_" #c "]\n\t"
#define WJR_ASM_CCOUT(c) [_cc_##c] "=r"
#endif

// WJR_EXCEPTIONS_LEVEL : 0 ~ 3
// 0 : Disable exceptions
// 1 ~ 3 : Enable exceptions
#ifndef WJR_EXCEPTIONS_LEVEL
#define WJR_EXCEPTIONS_LEVEL 1 // enable exceptions by default
#endif

#define WJR_EXCEPTIONS_IF(level, expr0, expr1)                                           \
    WJR_PP_BOOL_IF(WJR_PP_GT(WJR_EXCEPTIONS_LEVEL, level), expr0, expr1)

#define WJR_EXCEPTIONS_EXPR_L(level, expr) WJR_EXCEPTIONS_IF(level, expr, )
#define WJR_EXCEPTIONS_EXPR(expr) WJR_EXCEPTIONS_EXPR_L(0, expr)

#define WJR_ENABLE_EXCEPTIONS_TRY_I try
#define WJR_ENABLE_EXCEPTIONS_CATCH_I(X) catch (X)
#define WJR_ENABLE_EXCEPTIONS_THROW_I(X) throw X

#define WJR_DISABLE_EXCEPTIONS_TRY_I if (true)
#define WJR_DISABLE_EXCEPTIONS_CATCH_I(X) if (false)
#define WJR_DISABLE_EXCEPTIONS_THROW_I(X)

#define WJR_TRY_L(level)                                                                 \
    WJR_EXCEPTIONS_IF(level, WJR_ENABLE_EXCEPTIONS_TRY_I, WJR_DISABLE_EXCEPTIONS_TRY_I)
#define WJR_CATCH_L(level, X)                                                            \
    WJR_EXCEPTIONS_IF(level, WJR_ENABLE_EXCEPTIONS_CATCH_I(X),                           \
                      WJR_DISABLE_EXCEPTIONS_CATCH_I(X))
#define WJR_THROW_L(level, X)                                                            \
    WJR_EXCEPTIONS_IF(level, WJR_ENABLE_EXCEPTIONS_THROW_I(X),                           \
                      WJR_DISABLE_EXCEPTIONS_THROW_I(X))

#define WJR_TRY WJR_TRY_L(0)
#define WJR_CATCH(X) WJR_CATCH_L(0, X)
#define WJR_THROW(X) WJR_THROW_L(0, X)

#define WJR_TRY_L1 WJR_TRY_L(1)
#define WJR_CATCH_L1(X) WJR_CATCH_L(1, X)
#define WJR_THROW_L1(X) WJR_THROW_L(1, X)

#define WJR_TRY_L2 WJR_TRY_L(2)
#define WJR_CATCH_L2(X) WJR_CATCH_L(2, X)
#define WJR_THROW_L2(X) WJR_THROW_L(2, X)

#define WJR_TRY_L3 WJR_TRY_L(3)
#define WJR_CATCH_L3(X) WJR_CATCH_L(3, X)
#define WJR_THROW_L3(X) WJR_THROW_L(3, X)

#define WJR_REQUIRES(...) std::enable_if_t<(__VA_ARGS__), int> = 0
#define WJR_REQUIRES_I(...) std::enable_if_t<(__VA_ARGS__), int>

#endif // ! WJR_PREPROCESSOR_PREVIEW_HPP__

#endif // WJR_PREPROCESSOR_HPP__

namespace wjr {

// ASSERT_LEVEL : 0 ~ 3
// 0 : Release (defined(NDEBUG) && ! defined(WJR_DEBUG_LEVEL))
// 1 : Some simple runtime checks, such as boundary checks (default)
// 2 : Most runtime checks
// 3 : Maximize runtime checks
#ifndef WJR_DEBUG_LEVEL
#if defined(NDEBUG)
#define WJR_DEBUG_LEVEL 0
#else
#define WJR_DEBUG_LEVEL 1
#endif
#endif

#define WJR_DEBUG_IF(level, expr0, expr1)                                                \
    WJR_PP_BOOL_IF(WJR_PP_GT(WJR_DEBUG_LEVEL, level), expr0, expr1)

#define WJR_DEBUG_EXPR_L(level, expr) WJR_DEBUG_IF(level, expr, )
#define WJR_DEBUG_EXPR(expr) WJR_DEBUG_EXPR_L(0, expr)

class __assert_handler_t {
private:
    template <typename Output>
    static Output &handler(Output &out) noexcept {
        return out;
    }

    template <typename Output, typename... Args>
    static Output &handler(Output &out, Args &&...args) noexcept {
        out << "Additional information: ";
        (void)(out << ... << std::forward<Args>(args));
        out << '\n';
        return out;
    }

    template <typename... Args>
    WJR_NORETURN WJR_NOINLINE static void fn(const char *expr, const char *file,
                                             const char *func, int line,
                                             Args &&...args) noexcept {
        auto &output = std::cerr;
        if (file[0] != '\0') {
            output << file << ':';
        }
        if (line != -1) {
            output << line << ':';
        }
        output << func << ": Assertion `" << expr << "' failed.\n";
        handler(output, std::forward<Args>(args)...);

        std::abort();
    }

public:
    template <typename... Args>
    void operator()(const char *expr, const char *file, const char *func, int line,
                    Args &&...args) const noexcept {
        fn(expr, file, func, line, std::forward<Args>(args)...);
    }
};

inline constexpr __assert_handler_t __assert_handler{};

#define WJR_ASSERT_CHECK_I_HANDLER(handler, expr, ...)                                   \
    do {                                                                                 \
        if (WJR_UNLIKELY(!(expr))) {                                                     \
            handler(#expr, WJR_FILE, WJR_CURRENT_FUNCTION, WJR_LINE, ##__VA_ARGS__);     \
        }                                                                                \
    } while (0)

#define WJR_ASSERT_CHECK_I(...)                                                          \
    WJR_ASSERT_CHECK_I_HANDLER(::wjr::__assert_handler, __VA_ARGS__)

// do nothing
#define WJR_ASSERT_UNCHECK_I(expr, ...)

// level = [0, 2]
// The higher the level, the less likely it is to be detected
// Runtime detect  : 1
// Maximize detect : 2
#define WJR_ASSERT_L(level, ...)                                                         \
    WJR_DEBUG_IF(level, WJR_ASSERT_CHECK_I, WJR_ASSERT_UNCHECK_I)                        \
    (__VA_ARGS__)

// level of assert is zero at default.
#define WJR_ASSERT(...) WJR_ASSERT_L(0, __VA_ARGS__)
#define WJR_ASSERT_L1(...) WJR_ASSERT_L(1, __VA_ARGS__)
#define WJR_ASSERT_L2(...) WJR_ASSERT_L(2, __VA_ARGS__)
#define WJR_ASSERT_L3(...) WJR_ASSERT_L(3, __VA_ARGS__)

// always detect
#define WJR_ASSERT_LX(...) WJR_ASSERT_CHECK_I(__VA_ARGS__)

#define WJR_ASSERT_ASSUME_L(level, ...)                                                  \
    WJR_ASSERT_L(level, __VA_ARGS__);                                                    \
    __WJR_ASSERT_ASSUME_L_ASSUME(__VA_ARGS__)
#define __WJR_ASSERT_ASSUME_L_ASSUME(expr, ...) WJR_ASSUME(expr)

#define WJR_ASSERT_ASSUME(...) WJR_ASSERT_ASSUME_L(0, __VA_ARGS__)
#define WJR_ASSERT_ASSUME_L1(...) WJR_ASSERT_ASSUME_L(1, __VA_ARGS__)
#define WJR_ASSERT_ASSUME_L2(...) WJR_ASSERT_ASSUME_L(2, __VA_ARGS__)
#define WJR_ASSERT_ASSUME_L3(...) WJR_ASSERT_ASSUME_L(3, __VA_ARGS__)

#define WJR_ASSERT_ASSUME_LX(...)                                                        \
    WJR_ASSERT_LX(__VA_ARGS__);                                                          \
    WJR_ASSUME(__VA_ARGS__)

} // namespace wjr

#endif // WJR_ASSERT_HPP__
#ifndef WJR_CONTAINER_GENERIC_TYPE_TRAITS_HPP__
#define WJR_CONTAINER_GENERIC_TYPE_TRAITS_HPP__

#ifndef WJR_VECTOR_HPP__
#define WJR_VECTOR_HPP__

#ifndef WJR_CONTAINER_GENERIC_CONTAINER_VECTOR_HPP__
#define WJR_CONTAINER_GENERIC_CONTAINER_VECTOR_HPP__

/**
 * @file vector.hpp
 * @brief Vector container with definable internal structure
 *
 * @details
 * Customized internal structure needs to follow the following function signature: \n
 * -# storage() noexcept
 * -# ~storage() noexcept
 * -# void destroy(_Alty& al) noexcept(optional)
 * -# void destroy_and_deallocate(_Alty& al) noexcept(optional)
 * -# void uninitialized_construct(storage_type &other, size_type size, size_type
 * capacity, _Alty& al) noexcept
 * -# void take_storage(storage& other, _Alty& al) noexcept(optional)
 * -# void swap_storage(storage& other, _Alty& al) noexcept(optional)
 * -# decltype(auto) size() noexcept
 * -# size_type capacity() const noexcept
 * -# pointer data() noexcept
 * -# const_pointer data() const noexcept
 *
 * 1 : should not allocate memory. \n
 * 2 : don't need to destroy or deallocate. \n
 * 3 : destroy all elements. don't change ptr, size and capacity. \n
 * 4 : destroy and deallocate. \n
 * 5 : uninitialized construct the storage. allocate memory and set the size and
 * capacity. \n
 * 6 : take the storage from other. set other to empty. \n
 * 7 : swap the storage with other. \n
 * 8 : get the size. the return type must be reference,
 * such as size_type&, std::reference_wrapper<size_type> and so on. \n
 * 9 : get the capacity. \n
 * 10-11 : get the pointer. \n
 *
 * the size type of 8 need to implement the following function signature: \n
 * -# auto& operator=(size_type) noexcept
 * -# operator size_type() const noexcept
 * -# size_type operator++() noexcept
 * -# size_type operator--() noexcept
 * -# size_type operator+=(size_type) noexcept
 * -# size_type operator-=(size_type) noexcept
 *
 * @version 0.2
 * @date 2024-04-29
 *
 */

// Already included
#ifndef WJR_COMPRESSED_PAIR_HPP__
#define WJR_COMPRESSED_PAIR_HPP__

#include <tuple>

#ifndef WJR_CAPTURE_LEAF_HPP__
#define WJR_CAPTURE_LEAF_HPP__

#include <tuple>

#ifndef WJR_CRTP_CLASS_BASE_HPP__
#define WJR_CRTP_CLASS_BASE_HPP__

#include <cstddef>
#include <type_traits>

// Already included

namespace wjr {

struct enable_default_constructor_t {
    constexpr explicit enable_default_constructor_t() noexcept = default;
};

inline constexpr enable_default_constructor_t enable_default_constructor{};

template <bool Enable, typename = void>
struct enable_default_constructor_base {
    constexpr enable_default_constructor_base() noexcept = default;
    constexpr enable_default_constructor_base(
        const enable_default_constructor_base &) noexcept = default;
    constexpr enable_default_constructor_base(
        enable_default_constructor_base &&) noexcept = default;
    constexpr enable_default_constructor_base &
    operator=(const enable_default_constructor_base &) noexcept = default;
    constexpr enable_default_constructor_base &
    operator=(enable_default_constructor_base &&) noexcept = default;

protected:
    constexpr explicit enable_default_constructor_base(
        enable_default_constructor_t) noexcept {}
};

template <typename Tag>
struct enable_default_constructor_base<false, Tag> {
    constexpr enable_default_constructor_base() noexcept = delete;
    constexpr enable_default_constructor_base(
        const enable_default_constructor_base &) noexcept = default;
    constexpr enable_default_constructor_base(
        enable_default_constructor_base &&) noexcept = default;
    constexpr enable_default_constructor_base &
    operator=(const enable_default_constructor_base &) noexcept = default;
    constexpr enable_default_constructor_base &
    operator=(enable_default_constructor_base &&) noexcept = default;

protected:
    constexpr explicit enable_default_constructor_base(
        enable_default_constructor_t) noexcept {}
};

template <bool Enable, typename Tag = void>
struct enable_destructor_base {
    constexpr enable_destructor_base() noexcept = default;
    constexpr enable_destructor_base(const enable_destructor_base &) noexcept = default;
    constexpr enable_destructor_base(enable_destructor_base &&) noexcept = default;
    constexpr enable_destructor_base &
    operator=(const enable_destructor_base &) noexcept = default;
    constexpr enable_destructor_base &
    operator=(enable_destructor_base &&) noexcept = default;
    ~enable_destructor_base() noexcept = default;

protected:
    constexpr explicit enable_destructor_base(enable_default_constructor_t) noexcept {}
};

template <typename Tag>
struct enable_destructor_base<false, Tag> {
    constexpr enable_destructor_base() noexcept = default;
    constexpr enable_destructor_base(const enable_destructor_base &) noexcept = default;
    constexpr enable_destructor_base(enable_destructor_base &&) noexcept = default;
    constexpr enable_destructor_base &
    operator=(const enable_destructor_base &) noexcept = default;
    constexpr enable_destructor_base &
    operator=(enable_destructor_base &&) noexcept = default;
    ~enable_destructor_base() noexcept = delete;

protected:
    constexpr explicit enable_destructor_base(enable_default_constructor_t) noexcept {}
};

template <bool Copy, bool Move, bool CopyAssign, bool MoveAssign, typename Tag = void>
struct enable_copy_move_base {
    constexpr enable_copy_move_base() noexcept = default;
    constexpr enable_copy_move_base(const enable_copy_move_base &) noexcept = default;
    constexpr enable_copy_move_base(enable_copy_move_base &&) noexcept = default;
    constexpr enable_copy_move_base &
    operator=(const enable_copy_move_base &) noexcept = default;
    constexpr enable_copy_move_base &
    operator=(enable_copy_move_base &&) noexcept = default;
    ~enable_copy_move_base() noexcept = default;

protected:
    constexpr explicit enable_copy_move_base(enable_default_constructor_t) noexcept {}
};

template <bool Default, bool Destructor, bool Copy, bool Move, bool CopyAssign,
          bool MoveAssign, typename Tag = void>
struct enable_special_members_base
    : public enable_copy_move_base<Copy, Move, CopyAssign, MoveAssign, Tag> {
    constexpr enable_special_members_base() noexcept = default;
    constexpr enable_special_members_base(const enable_special_members_base &) noexcept =
        default;
    constexpr enable_special_members_base(enable_special_members_base &&) noexcept =
        default;
    constexpr enable_special_members_base &
    operator=(const enable_special_members_base &) noexcept = default;
    constexpr enable_special_members_base &
    operator=(enable_special_members_base &&) noexcept = default;
    ~enable_special_members_base() noexcept = default;

protected:
    constexpr explicit enable_special_members_base(
        enable_default_constructor_t) noexcept {}
};

#define __WJR_ENABLE_BSAE_true default
#define __WJR_ENABLE_BSAE_false delete

#define WJR_REGISTER_ENABLE_COPY_MOVE_BASE(Copy, Move, CopyAssign, MoveAssign)           \
    template <typename Tag>                                                              \
    struct enable_copy_move_base<Copy, Move, CopyAssign, MoveAssign, Tag> {              \
        constexpr enable_copy_move_base() noexcept = default;                            \
        constexpr enable_copy_move_base(const enable_copy_move_base &) noexcept =        \
            WJR_PP_CONCAT(__WJR_ENABLE_BSAE_, Copy);                                     \
        constexpr enable_copy_move_base(enable_copy_move_base &&) noexcept =             \
            WJR_PP_CONCAT(__WJR_ENABLE_BSAE_, Move);                                     \
        constexpr enable_copy_move_base &operator=(                                      \
            const enable_copy_move_base &) noexcept = WJR_PP_CONCAT(__WJR_ENABLE_BSAE_,  \
                                                                    CopyAssign);         \
        constexpr enable_copy_move_base &operator=(enable_copy_move_base &&) noexcept =  \
            WJR_PP_CONCAT(__WJR_ENABLE_BSAE_, MoveAssign);                               \
    }

WJR_REGISTER_ENABLE_COPY_MOVE_BASE(false, true, true, true);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(true, false, true, true);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(false, false, true, true);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(true, true, false, true);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(false, true, false, true);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(true, false, false, true);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(false, false, false, true);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(true, true, true, false);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(false, true, true, false);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(true, false, true, false);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(false, false, true, false);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(true, true, false, false);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(false, true, false, false);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(true, false, false, false);
WJR_REGISTER_ENABLE_COPY_MOVE_BASE(false, false, false, false);

#undef WJR_REGISTER_ENABLE_COPY_MOVE_BASE

#define WJR_REGISTER_ENABLE_SPECIAL_MEMBERS_BASE(Default, Destructor)                    \
    template <bool Copy, bool Move, bool CopyAssign, bool MoveAssign, typename Tag>      \
    struct enable_special_members_base<Default, Destructor, Copy, Move, CopyAssign,      \
                                       MoveAssign, Tag>                                  \
        : public enable_copy_move_base<Copy, Move, CopyAssign, MoveAssign, Tag> {        \
        constexpr enable_special_members_base() noexcept =                               \
            WJR_PP_CONCAT(__WJR_ENABLE_BSAE_, Default);                                  \
        constexpr enable_special_members_base(                                           \
            const enable_special_members_base &) noexcept = default;                     \
        constexpr enable_special_members_base(enable_special_members_base &&) noexcept = \
            default;                                                                     \
        constexpr enable_special_members_base &                                          \
        operator=(const enable_special_members_base &) noexcept = default;               \
        constexpr enable_special_members_base &                                          \
        operator=(enable_special_members_base &&) noexcept = default;                    \
        ~enable_special_members_base() noexcept = WJR_PP_CONCAT(__WJR_ENABLE_BSAE_,      \
                                                                Destructor);             \
                                                                                         \
    protected:                                                                           \
        constexpr explicit enable_special_members_base(                                  \
            enable_default_constructor_t) noexcept {}                                    \
    }

WJR_REGISTER_ENABLE_SPECIAL_MEMBERS_BASE(false, true);
WJR_REGISTER_ENABLE_SPECIAL_MEMBERS_BASE(true, false);
WJR_REGISTER_ENABLE_SPECIAL_MEMBERS_BASE(false, false);

#undef WJR_REGISTER_ENABLE_SPECIAL_MEMBERS_BASE

#undef __WJR_ENABLE_BSAE_false
#undef __WJR_ENABLE_BSAE_true

template <typename Tag = void>
using noncopyable = enable_copy_move_base<false, true, false, true, Tag>;

template <typename Tag = void>
using nonmoveable = enable_copy_move_base<false, true, false, true, Tag>;

template <typename Tag = void, typename... Args>
using enable_special_members_of_args_base = enable_special_members_base<
    std::conjunction_v<std::is_default_constructible<Args>...>,
    std::conjunction_v<std::is_destructible<Args>...>,
    std::conjunction_v<std::is_copy_constructible<Args>...>,
    std::conjunction_v<std::is_move_constructible<Args>...>,
    std::conjunction_v<std::is_copy_assignable<Args>...>,
    std::conjunction_v<std::is_move_assignable<Args>...>, Tag>;

template <typename Tag = void, typename... Args>
using enable_trivially_special_members_of_args_base = enable_special_members_base<
    std::conjunction_v<std::is_trivially_default_constructible<Args>...>,
    std::conjunction_v<std::is_destructible<Args>...>,
    std::conjunction_v<std::is_trivially_copy_constructible<Args>...>,
    std::conjunction_v<std::is_trivially_move_constructible<Args>...>,
    std::conjunction_v<std::is_trivially_copy_assignable<Args>...>,
    std::conjunction_v<std::is_trivially_move_assignable<Args>...>, Tag>;

template <size_t I, typename T>
struct enable_base_identity_t {};

} // namespace wjr

#endif // WJR_CRTP_CLASS_BASE_HPP__
#ifndef WJR_TP_LIST_HPP__
#define WJR_TP_LIST_HPP__

#ifndef WJR_TYPE_TRAITS_HPP__
#define WJR_TYPE_TRAITS_HPP__

#include <cstddef>
#include <cstdint>
#include <functional>
#include <limits>
#include <type_traits>
#include <utility>

// Already included

namespace wjr {

struct in_place_empty_t {};

inline constexpr in_place_empty_t in_place_empty = {};

/**
 * @brief Tag of default constructor.
 *
 * @details Use dctor to indicate default constructor. \n
 * Used to avoid value initialization.  \n
 * For example : \n
 * @code
 * wjr::vector<int> vec(10, dctor); // default construct with 10 elements.
 * wjr::vector<int> vec2(10); // value construct with 10 elements.
 * wjr::vector<int> vec3(10, 0); // value construct with 10 elements.
 * wjr::vector<int> vec4(10, vctor); // value construct with 10 elements.
 * @endcode
 * elements of vec are not initialized. \n
 * elements of vec2, vec3, vec4 are initialized with 0.
 */
struct dctor_t {};

/**
 * @see dctor_t
 */
inline constexpr dctor_t dctor = {};

/**
 * @brief Tag of value constructor.
 *
 */
struct vctor_t {};

inline constexpr vctor_t vctor = {};

struct in_place_reallocate_t {};

inline constexpr in_place_reallocate_t in_place_reallocate = {};

struct in_place_reserve_t {};

inline constexpr in_place_reserve_t in_place_reserve = {};

struct in_place_move_t {};

inline constexpr in_place_move_t in_place_move = {};

struct in_place_max_t {
    template <typename T>
    WJR_CONST constexpr operator T() const noexcept {
        return std::numeric_limits<T>::max();
    }
};

inline constexpr in_place_max_t in_place_max = {};

struct in_place_min_t {
    template <typename T>
    WJR_CONST constexpr operator T() const noexcept {
        return std::numeric_limits<T>::min();
    }
};

inline constexpr in_place_min_t in_place_min = {};

struct self_init_t {};

inline constexpr self_init_t self_init = {};

inline constexpr std::size_t dynamic_extent = in_place_max;

template <typename... Args>
struct multi_conditional;

template <typename... Args>
using multi_conditional_t = typename multi_conditional<Args...>::type;

template <bool f, typename T, typename... Args>
struct multi_conditional_impl {
    using type = T;
};

template <typename T, typename... Args>
struct multi_conditional_impl<false, T, Args...> {
    using type = multi_conditional_t<Args...>;
};

template <typename F, typename T, typename U>
struct multi_conditional<F, T, U> {
    using type = std::conditional_t<F::value, T, U>;
};

template <typename F, typename T, typename U, typename V, typename... Args>
struct multi_conditional<F, T, U, V, Args...> {
    using type = typename multi_conditional_impl<F::value, T, U, V, Args...>::type;
};

template <typename T, typename... Args>
struct is_any_of : std::disjunction<std::is_same<T, Args>...> {};

template <typename T, typename... Args>
inline constexpr bool is_any_of_v = is_any_of<T, Args...>::value;

template <typename T>
struct remove_cvref {
    using type = std::remove_cv_t<std::remove_reference_t<T>>;
};

template <typename T>
using remove_cvref_t = typename remove_cvref<T>::type;

template <size_t n>
struct __uint_selector {};

template <>
struct __uint_selector<8> {
    using type = std::uint8_t;
};

template <>
struct __uint_selector<16> {
    using type = std::uint16_t;
};

template <>
struct __uint_selector<32> {
    using type = std::uint32_t;
};

template <>
struct __uint_selector<64> {
    using type = std::uint64_t;
};

template <size_t n>
struct __int_selector {
    using type = std::make_signed_t<typename __uint_selector<n>::type>;
};

#if WJR_HAS_FEATURE(INT128)
template <>
struct __uint_selector<128> {
    using type = __uint128_t;
};

template <>
struct __int_selector<128> {
    using type = __int128_t;
};
#endif

template <size_t n>
using uint_t = typename __uint_selector<n>::type;

template <size_t n>
using int_t = typename __int_selector<n>::type;

#if WJR_HAS_FEATURE(INT128)
using int128_t = int_t<128>;
#endif

#if WJR_HAS_FEATURE(INT128)
using uint128_t = uint_t<128>;
#endif

template <size_t n, bool __s>
using usint_t = std::conditional_t<__s, int_t<n>, uint_t<n>>;

using ssize_t = std::make_signed_t<size_t>;

template <typename T>
struct is_nonbool_integral
    : std::conjunction<std::is_integral<T>, std::negation<std::is_same<T, bool>>> {};

template <typename T>
inline constexpr bool is_nonbool_integral_v = is_nonbool_integral<T>::value;

template <typename T>
struct is_unsigned_integral : std::conjunction<std::is_integral<T>, std::is_unsigned<T>> {
};

template <typename T>
inline constexpr bool is_unsigned_integral_v = is_unsigned_integral<T>::value;

template <typename T>
struct is_signed_integral : std::conjunction<std::is_integral<T>, std::is_signed<T>> {};

template <typename T>
inline constexpr bool is_signed_integral_v = is_signed_integral<T>::value;

template <typename T>
struct is_nonbool_unsigned_integral
    : std::conjunction<is_unsigned_integral<T>, std::negation<std::is_same<T, bool>>> {};

template <typename T>
inline constexpr bool is_nonbool_unsigned_integral_v =
    is_nonbool_unsigned_integral<T>::value;

template <typename T>
struct is_nonbool_signed_integral
    : std::conjunction<is_signed_integral<T>, std::negation<std::is_same<T, bool>>> {};

template <typename T>
inline constexpr bool is_nonbool_signed_integral_v = is_nonbool_signed_integral<T>::value;

// type identity
template <typename T>
struct type_identity {
    using type = T;
};

template <typename T>
using type_identity_t = typename type_identity<T>::type;

template <typename T>
struct add_restrict {
    using type = T;
};

template <typename T>
struct add_restrict<T *> {
    using type = T *WJR_RESTRICT;
};

template <typename T>
using add_restrict_t = typename add_restrict<T>::type;

/**
 * @brief Return if is constant evaluated.
 *
 * @details Use macro WJR_IS_CONSTANT_EVALUATED(). \n
 * Use std::is_constant_evaluated() if C++ 20 is supported. \n
 * Otherwise, use __builtin_constant_evaluated() if
 * WJR_HAS_BUILTIN(__builtin_is_constant_evaluated). Otherwise, return false.
 *
 */
WJR_INTRINSIC_CONSTEXPR bool is_constant_evaluated() noexcept {
    return WJR_IS_CONSTANT_EVALUATED();
}

template <typename T, typename U, typename = void>
struct __is_swappable_with : std::false_type {};

template <typename T, typename U>
struct __is_swappable_with<
    T, U, std::void_t<decltype(std::swap(std::declval<T &>(), std::declval<U &>()))>>
    : std::true_type {};

template <typename T, typename U>
struct is_swappable_with
    : std::conjunction<__is_swappable_with<T, U>, __is_swappable_with<U, T>> {};

template <typename T, typename U>
inline constexpr bool is_swappable_with_v = is_swappable_with<T, U>::value;

template <typename T>
struct is_swappable
    : is_swappable_with<std::add_lvalue_reference_t<T>, std::add_lvalue_reference_t<T>> {
};

template <typename T>
inline constexpr bool is_swappable_v = is_swappable<T>::value;

template <typename T, typename U>
struct __is_nothrow_swappable_with
    : std::bool_constant<noexcept(std::swap(std::declval<T &>(), std::declval<U &>())) &&
                         noexcept(std::swap(std::declval<U &>(), std::declval<T &>()))> {
};

template <typename T, typename U>
struct is_nothrow_swappable_with
    : std::conjunction<is_swappable_with<T, U>, __is_nothrow_swappable_with<T, U>> {};

template <typename T, typename U>
inline constexpr bool is_nothrow_swappable_with_v =
    is_nothrow_swappable_with<T, U>::value;

template <typename T>
struct is_nothrow_swappable : is_nothrow_swappable_with<std::add_lvalue_reference_t<T>,
                                                        std::add_lvalue_reference_t<T>> {
};

template <typename T>
inline constexpr bool is_nothrow_swappable_v = is_nothrow_swappable<T>::value;

template <typename T>
struct __unref_wrapper_helper {
    using type = T;
};

template <typename T>
struct __unref_wrapper_helper<std::reference_wrapper<T>> {
    using type = T &;
};

template <typename T>
struct unref_wrapper {
    using type = typename __unref_wrapper_helper<std::decay_t<T>>::type;
};

template <typename T>
using unref_wrapper_t = typename unref_wrapper<T>::type;

template <typename T, typename = void>
struct __is_default_convertible : std::false_type {};

template <typename T>
void __test_default_convertible(const T &);

template <typename T>
struct __is_default_convertible<T,
                                std::void_t<decltype(__test_default_convertible<T>({}))>>
    : std::true_type {};

template <typename T>
using is_default_convertible = __is_default_convertible<T>;

template <typename T>
inline constexpr bool is_default_convertible_v = is_default_convertible<T>::value;

template <typename T>
struct get_place_index {};

template <size_t idx>
struct get_place_index<std::in_place_index_t<idx>> {
    static constexpr size_t value = idx;
};

template <typename T>
inline constexpr size_t get_place_index_v = get_place_index<T>::value;

// ....

template <class P, class M>
WJR_INTRINSIC_CONSTEXPR20 size_t container_of_offset(const M P::*member) {
    return reinterpret_cast<size_t>(&(reinterpret_cast<P *>(nullptr)->*member));
}

template <class P, class M>
WJR_INTRINSIC_CONSTEXPR20 P *container_of_offset_impl(M *ptr, const M P::*member) {
    return reinterpret_cast<P *>(reinterpret_cast<char *>(ptr) -
                                 container_of_offset(member));
}

#define WJR_CONTAINER_OF(ptr, type, member) container_of_offset_impl(ptr, &type::member)

// C++ 17 concept adapt

template <typename Derived, typename Base>
struct is_derived_from
    : std::conjunction<
          std::is_base_of<Base, Derived>,
          std::is_convertible<const volatile Derived *, const volatile Base *>> {};

template <typename Derived, typename Base>
inline constexpr bool is_derived_from_v = is_derived_from<Derived, Base>::Value;

template <typename From, typename To, typename = void>
struct __is_convertible_to_helper : std::false_type {};

template <typename From, typename To>
struct __is_convertible_to_helper<
    From, To, std::void_t<decltype(static_cast<To>(std::declval<From>()))>>
    : std::true_type {};

template <typename From, typename To>
struct is_convertible_to : std::conjunction<std::is_convertible<From, To>,
                                            __is_convertible_to_helper<From, To, void>> {
};

template <typename From, typename To>
inline constexpr bool is_convertible_to_v = is_convertible_to<From, To>::value;

// TODO : move __is_in_i32_range to other header.
WJR_CONST WJR_INTRINSIC_CONSTEXPR bool __is_in_i32_range(int64_t value) noexcept {
    return value >= (int32_t)in_place_min && value <= (int32_t)in_place_max;
}

#define __WJR_REGISTER_TYPENAMES_EXPAND(x) __WJR_REGISTER_TYPENAMES_EXPAND_I x
#define __WJR_REGISTER_TYPENAMES_EXPAND_I(...) __VA_ARGS__

#define __WJR_REGISTER_TYPENAMES(...)                                                    \
    __WJR_REGISTER_TYPENAMES_EXPAND(                                                     \
        WJR_PP_QUEUE_TRANSFORM((__VA_ARGS__), __WJR_REGISTER_TYPENAMES_CALLER))
#define __WJR_REGISTER_TYPENAMES_CALLER(x) typename x

#define WJR_REGISTER_HAS_TYPE_0(NAME, HAS_EXPR)                                          \
    template <typename Enable, typename... Args>                                         \
    struct __has_##NAME : std::false_type {};                                            \
    template <typename... Args>                                                          \
    struct __has_##NAME<std::void_t<decltype(HAS_EXPR)>, Args...> : std::true_type {};   \
    template <typename... Args>                                                          \
    struct has_##NAME : __has_##NAME<void, Args...> {};                                  \
    template <typename... Args>                                                          \
    constexpr bool has_##NAME##_v = has_##NAME<Args...>::value

#define WJR_REGISTER_HAS_TYPE_MORE(NAME, HAS_EXPR, ...)                                  \
    template <typename Enable, __WJR_REGISTER_TYPENAMES(__VA_ARGS__), typename... Args>  \
    struct __has_##NAME : std::false_type {};                                            \
    template <__WJR_REGISTER_TYPENAMES(__VA_ARGS__), typename... Args>                   \
    struct __has_##NAME<std::void_t<decltype(HAS_EXPR)>, __VA_ARGS__, Args...>           \
        : std::true_type {};                                                             \
    template <__WJR_REGISTER_TYPENAMES(__VA_ARGS__), typename... Args>                   \
    struct has_##NAME : __has_##NAME<void, __VA_ARGS__, Args...> {};                     \
    template <__WJR_REGISTER_TYPENAMES(__VA_ARGS__), typename... Args>                   \
    constexpr bool has_##NAME##_v = has_##NAME<__VA_ARGS__, Args...>::value

#define WJR_REGISTER_HAS_TYPE(NAME, ...)                                                 \
    WJR_REGISTER_HAS_TYPE_N(WJR_PP_ARGS_LEN(__VA_ARGS__), NAME, __VA_ARGS__)
#define WJR_REGISTER_HAS_TYPE_N(N, ...)                                                  \
    WJR_PP_BOOL_IF(WJR_PP_EQ(N, 1), WJR_REGISTER_HAS_TYPE_0, WJR_REGISTER_HAS_TYPE_MORE) \
    (__VA_ARGS__)

// used for SFINAE
constexpr static void allow_true_type(std::true_type) noexcept {}
constexpr static void allow_false_type(std::false_type) noexcept {}

namespace {
WJR_REGISTER_HAS_TYPE(compare, std::declval<Comp>()(std::declval<T>(), std::declval<U>()),
                      Comp, T, U);
WJR_REGISTER_HAS_TYPE(
    noexcept_compare,
    allow_true_type(std::declval<std::bool_constant<noexcept(
                        std::declval<Comp>()(std::declval<T>(), std::declval<U>()))>>()),
    Comp, T, U);

#define WJR_REGISTER_HAS_COMPARE(NAME, STD)                                              \
    template <typename T, typename U>                                                    \
    struct has_##NAME : has_compare<STD, T, U> {};                                       \
    template <typename T, typename U>                                                    \
    inline constexpr bool has_##NAME##_v = has_##NAME<T, U>::value;                      \
    template <typename T, typename U>                                                    \
    struct has_noexcept_##NAME : has_noexcept_compare<STD, T, U> {};                     \
    template <typename T, typename U>                                                    \
    inline constexpr bool has_noexcept_##NAME##_v = has_noexcept_##NAME<T, U>::value;

WJR_REGISTER_HAS_COMPARE(equal_to, std::equal_to<>);
WJR_REGISTER_HAS_COMPARE(not_equal_to, std::not_equal_to<>);
WJR_REGISTER_HAS_COMPARE(less, std::less<>);
WJR_REGISTER_HAS_COMPARE(less_equal, std::less_equal<>);
WJR_REGISTER_HAS_COMPARE(greater, std::greater<>);
WJR_REGISTER_HAS_COMPARE(greater_equal, std::greater_equal<>);

#undef WJR_REGISTER_HAS_COMPARE

WJR_REGISTER_HAS_TYPE(invocable,
                      std::invoke(std::declval<Func>(), std::declval<Args>()...), Func);
} // namespace

template <typename T>
struct get_integral_constant {
    using type = T;
};

template <typename T, T v>
struct get_integral_constant<std::integral_constant<T, v>> {
    using type = T;
};

template <typename T>
using get_integral_constant_t = typename get_integral_constant<T>::type;

template <typename T, typename U>
struct is_integral_constant_same : std::false_type {};

template <typename T, T v>
struct is_integral_constant_same<std::integral_constant<T, v>, T> : std::true_type {};

template <typename T, typename U>
inline constexpr bool is_integral_constant_same_v =
    is_integral_constant_same<T, U>::value;

} // namespace wjr

#endif // ! WJR_TYPE_TRAITS_HPP__

namespace wjr {

template <typename... Args>
struct tp_list {};

template <typename T>
struct tp_is_list : std::false_type {};

template <typename... Args>
struct tp_is_list<tp_list<Args...>> : std::true_type {};

// check if is tp_list
template <typename T>
inline constexpr bool tp_is_list_v = tp_is_list<T>::value;

template <typename T>
struct tp_is_container : std::false_type {};

template <template <typename...> typename C, typename... Args>
struct tp_is_container<C<Args...>> : std::true_type {};

template <typename T>
inline constexpr bool tp_is_container_v = tp_is_container<T>::value;

template <typename T>
struct tp_size;

template <template <typename...> typename C, typename... Args>
struct tp_size<C<Args...>> : std::integral_constant<size_t, sizeof...(Args)> {};

// get size of C<Args...>
template <typename T>
inline constexpr size_t tp_size_v = tp_size<T>::value;

template <typename T>
struct tp_is_fn : std::false_type {};

template <typename T>
inline constexpr bool tp_is_fn_v = tp_is_fn<T>::value;

template <typename _Enable, template <typename...> typename F, typename... Args>
struct __tp_is_valid_helper : std::false_type {};

template <template <typename...> typename F, typename... Args>
struct __tp_is_valid_helper<std::void_t<F<Args...>>, F, Args...> : std::true_type {};

template <template <typename...> typename F, typename... Args>
struct tp_is_valid : __tp_is_valid_helper<void, F, Args...> {};

template <template <typename...> typename F, typename... Args>
inline constexpr bool tp_is_valid_v = tp_is_valid<F, Args...>::value;

template <typename F, typename... Args>
inline constexpr bool tp_is_valid_f = tp_is_valid_v<F::template fn, Args...>;

template <template <typename...> typename F, typename... Args>
struct __tp_defer_helper {
    using type = F<Args...>;
};

template <template <typename...> typename F, typename... Args>
struct tp_defer {
    using type = std::enable_if_t<tp_is_valid_v<F, Args...>,
                                  typename __tp_defer_helper<F, Args...>::type>;
};

// use std::enable_if_t to defer the instantiation of F<Args...>
template <template <typename...> typename F, typename... Args>
using tp_defer_t = typename tp_defer<F, Args...>::type;

template <typename F, typename... Args>
using tp_defer_f = tp_defer_t<F::template fn, Args...>;

template <typename T>
struct tp_type_identity {
    using type = T;
};

// tp_type_identity_t<T> is T
template <typename T>
using tp_type_identity_t = typename tp_type_identity<T>::type;

// F1<F2<Args...>>
template <template <typename...> typename F1, template <typename...> typename F2>
struct tp_bind_fn {
    template <typename... Args>
    using fn = tp_defer_t<F1, tp_defer_t<F2, Args...>>;
};

// make F can be used as fn
template <template <typename...> typename F>
struct tp_make_fn {
    template <typename... Args>
    using fn = tp_defer_t<F, Args...>;
};

// std::negation<F<Args...>>
template <template <typename...> typename F>
struct tp_not_fn {
    template <typename... Args>
    using fn = typename tp_bind_fn<std::negation, F>::template fn<Args...>;
};

template <typename... Args>
using tp_true_type = std::true_type;

template <typename... Args>
using tp_false_type = std::false_type;

template <typename T>
struct tp_is_empty : std::bool_constant<tp_size_v<T> == 0> {};

template <typename T>
inline constexpr bool tp_is_empty_v = tp_is_empty<T>::value;

template <typename T, typename U>
struct tp_assign;

template <typename... Args1, template <typename...> typename T1, typename... Args2,
          template <typename...> typename T2>
struct tp_assign<T1<Args1...>, T2<Args2...>> {
    using type = T1<Args2...>;
};

// f(L1<Args1...>, L2<Args2...>) -> L1<Args2...>
template <typename T, typename U>
using tp_assign_t = typename tp_assign<T, U>::type;

template <typename T>
struct tp_clear;

template <template <typename...> typename T, typename... Args>
struct tp_clear<T<Args...>> {
    using type = T<>;
};

// f(L<Args...>) -> L<>
template <typename T>
using tp_clear_t = typename tp_clear<T>::type;

template <typename T, typename... Args>
struct tp_push_front;

template <template <typename...> typename C, typename... Args1, typename... Args2>
struct tp_push_front<C<Args1...>, Args2...> {
    using type = C<Args2..., Args1...>;
};

// f(L<Args1...>, Args2...) -> L<Args1..., Args2...)
template <typename T, typename... Args>
using tp_push_front_t = typename tp_push_front<T, Args...>::type;

template <typename T, typename... Args>
struct tp_push_back;

template <template <typename...> typename C, typename... Args1, typename... Args2>
struct tp_push_back<C<Args1...>, Args2...> {
    using type = C<Args1..., Args2...>;
};

// f(L<Args1...>, Args2...) -> L<Args2..., Args1...)
template <typename T, typename... Args>
using tp_push_back_t = typename tp_push_back<T, Args...>::type;

template <typename _Enable, size_t I, size_t N, typename... Args>
struct __tp_cut_helper;

template <size_t I, size_t N, typename T, typename... Args>
struct __tp_cut_helper<std::enable_if_t<N != 0, void>, I, N, T, Args...> {
    using type = typename __tp_cut_helper<void, I - 1, N, Args...>::type;
};

template <size_t I, size_t N, typename T, typename... Args>
struct __tp_cut_helper<std::enable_if_t<N == 0, void>, I, N, T, Args...> {
    using type = tp_list<>;
};

template <size_t N, typename... Args2>
struct __tp_cut_helper2;

template <size_t N, typename T, typename... Args>
struct __tp_cut_helper2<N, T, Args...> {
    using type = tp_push_front_t<typename __tp_cut_helper2<N - 1, Args...>::type, T>;
};

template <typename... Args>
struct __tp_cut_helper2<0, Args...> {
    using type = tp_list<>;
};

template <typename T, typename... Args>
struct __tp_cut_helper2<0, T, Args...> {
    using type = tp_list<>;
};

template <size_t N, typename... Args>
struct __tp_cut_helper<std::enable_if_t<N != 0>, 0, N, Args...> {
    using type = typename __tp_cut_helper2<N, Args...>::type;
};

template <size_t N, typename T, typename... Args>
struct __tp_cut_helper<std::enable_if_t<N != 0>, 0, N, T, Args...> {
    using type = typename __tp_cut_helper2<N, T, Args...>::type;
};

template <typename T, template <typename...> typename U>
struct tp_rename;

template <template <typename...> typename C, typename... Args,
          template <typename...> typename U>
struct tp_rename<C<Args...>, U> {
    using type = U<Args...>;
};

// f(L1<Args1...>, L2) -> L2<Args1...>
template <typename T, template <typename...> typename U>
using tp_rename_t = typename tp_rename<T, U>::type;

template <typename T, size_t I, size_t N>
struct tp_cut;

template <template <typename...> typename C, typename... Args, size_t I, size_t N>
struct tp_cut<C<Args...>, I, N> {
    static_assert(N <= sizeof...(Args) && I <= (sizeof...(Args) - N),
                  "tp_cut: invalid index");
    using type = tp_rename_t<typename __tp_cut_helper<void, I, N, Args...>::type, C>;
};

// f(L<Args...>, I, N) -> L<Args[I ~ I + N - 1]>
template <typename T, size_t I, size_t N>
using tp_cut_t = typename tp_cut<T, I, N>::type;

template <typename T>
struct tp_pop_front : tp_cut<T, 1, tp_size_v<T> - 1> {};

// f(L<T, Args...>) -> L<Args...>
template <typename T>
using tp_pop_front_t = typename tp_pop_front<T>::type;

template <typename T>
struct tp_pop_back : tp_cut<T, 0, tp_size_v<T> - 1> {};

// f(L<Args..., T>) -> L<Args...>
template <typename T>
using tp_pop_back_t = typename tp_pop_back<T>::type;

template <size_t index, typename... Args>
struct __tp_at_helper;

template <size_t index, typename T, typename... Args>
struct __tp_at_helper<index, T, Args...> {
    using type = typename __tp_at_helper<index - 1, Args...>::type;
};

template <typename T, typename... Args>
struct __tp_at_helper<0, T, Args...> {
    using type = T;
};

//
template <typename T, size_t index>
struct tp_at;

template <template <typename... Args> typename C, typename... Args, size_t index>
struct tp_at<C<Args...>, index> {
    static_assert(index < sizeof...(Args), "tp_at: invalid index");
    using type = typename __tp_at_helper<index, Args...>::type;
};

// f(L<Args...>, index) - > Args(index)
template <typename T, size_t index>
using tp_at_t = typename tp_at<T, index>::type;

template <typename T>
struct tp_front {
    using type = tp_at_t<T, 0>;
};

// tp_at_t(T, 0)
template <typename T>
using tp_front_t = typename tp_front<T>::type;

template <typename T>
struct tp_back {
    using type = tp_at_t<T, tp_size_v<T> - 1>;
};

// tp_at_t(T, tp_size_v<T> - 1)
template <typename T>
using tp_back_t = typename tp_back<T>::type;

template <typename T, size_t idx>
struct tp_prefix {
    using type = tp_cut_t<T, 0, idx>;
};

// f(L<Args...>, idx) -> L<Args[0 ~ idx - 1]>
template <typename T, size_t idx>
using tp_prefix_t = typename tp_prefix<T, idx>::type;

template <typename T, size_t idx>
struct tp_suffix {
    using type = tp_cut_t<T, tp_size_v<T> - idx, idx>;
};

// f(L<Args...>, idx) -> L<Args[tp_size_v<T> - idx ~ tp_size_v<T> - 1]>
template <typename T, size_t idx>
using tp_suffix_t = typename tp_suffix<T, idx>::type;

template <typename T, size_t idx>
struct tp_remove_prefix {
    using type = tp_suffix_t<T, tp_size_v<T> - idx>;
};

template <typename T, size_t idx>
using tp_remove_prefix_t = typename tp_remove_prefix<T, idx>::type;

template <typename T, size_t idx>
struct tp_remove_suffix {
    using type = tp_prefix_t<T, tp_size_v<T> - idx>;
};

template <typename T, size_t idx>
using tp_remove_suffix_t = typename tp_remove_suffix<T, idx>::type;

template <typename... Args>
struct tp_concat;

template <typename T>
struct tp_concat<T> {
    using type = T;
};

template <template <typename...> typename C1, typename... Args1,
          template <typename...> typename C2, typename... Args2>
struct tp_concat<C1<Args1...>, C2<Args2...>> {
    using type = C1<Args1..., Args2...>;
};

template <typename T, typename U, typename... Args3>
struct tp_concat<T, U, Args3...> {
    using type = typename tp_concat<typename tp_concat<T, U>::type, Args3...>::type;
};

// f(L1<Args...>, L2<Args2...>, ... Ln<Argsn...>) -> L1<Args..., Args2..., Argsn...>
template <typename... Args>
using tp_concat_t = typename tp_concat<Args...>::type;

template <typename T, size_t idx, typename U>
struct tp_replace_at {
    using type = tp_concat_t<tp_push_back_t<tp_cut_t<T, 0, idx>, U>,
                             tp_cut_t<T, idx + 1, tp_size_v<T> - idx - 1>>;
};

template <typename T, typename U>
struct tp_replace_at<T, 0, U> {
    using type = tp_push_front_t<tp_pop_front_t<T>, U>;
};

// f(L<Args...>, idx, U) -> L<Args[0 ~ idx - 1], U, Args[idx + 1 ~ tp_size_v<T> - 1]>
template <typename T, size_t idx, typename U>
using tp_replace_at_t = typename tp_replace_at<T, idx, U>::type;

template <typename T, typename U>
struct tp_replace_front_at {
    using type = tp_replace_at_t<T, 0, U>;
};

template <typename T, typename U>
using tp_replace_front_at_t = typename tp_replace_front_at<T, U>::type;

template <typename T, typename U>
struct tp_replace_back_at {
    using type = tp_replace_at_t<T, tp_size_v<T> - 1, U>;
};

template <typename T, typename U>
using tp_replace_back_at_t = typename tp_replace_back_at<T, U>::type;

template <typename V, typename T, typename... Args>
struct tp_conditional {
    using type = std::conditional_t<V::value, T, typename tp_conditional<Args...>::type>;
};

template <typename V, typename T1, typename T2>
struct tp_conditional<V, T1, T2> {
    using type = std::conditional_t<V::value, T1, T2>;
};

// f(V, T, U) -> std::conditional_t<V::value, T, U>
// f(V, T, Args...) -> std::conditional_t<V::value, T, f(Args...)>
template <typename V, typename T, typename... Args>
using tp_conditional_t = typename tp_conditional<V, T, Args...>::type;

template <size_t idx>
struct tp_arg;

template <template <typename...> typename F, typename... Args>
struct tp_bind;

template <template <typename...> typename F, typename... Args>
struct tp_bind_front;

template <template <typename...> typename F, typename... Args>
struct tp_bind_back;

template <size_t idx>
struct tp_is_fn<tp_arg<idx>> : std::true_type {};

template <template <typename...> typename F, typename... Args>
struct tp_is_fn<tp_bind<F, Args...>> : std::true_type {};

template <template <typename...> typename F, typename... Args>
struct tp_is_fn<tp_bind_front<F, Args...>> : std::true_type {};

template <template <typename...> typename F, typename... Args>
struct tp_is_fn<tp_bind_back<F, Args...>> : std::true_type {};

template <size_t idx>
struct tp_arg {
    template <typename... Args>
    using fn = tp_at_t<tp_list<Args...>, idx>;
};

template <template <typename...> typename F, typename T>
struct tp_apply {
    using type = tp_rename_t<T, F>;
};

// f(F, L<Args...>) -> F<Args...>
// same as tp_rename_t(L<Args...>, F)
template <template <typename...> typename F, typename T>
using tp_apply_t = typename tp_apply<F, T>::type;

template <typename F, typename T>
using tp_apply_f = tp_apply_t<F::template fn, T>;

template <typename _Enable, typename T, typename... Args>
struct __tp_bind_helper {
    using type = T;
};

template <typename F, typename... Args>
struct __tp_bind_helper<std::enable_if_t<tp_is_fn_v<F>, void>, F, Args...> {
    using type = typename F::template fn<Args...>;
};

template <template <typename...> typename F, typename... Args>
struct tp_bind {
    template <typename... Args2>
    using fn = F<typename __tp_bind_helper<void, Args, Args2...>::type...>;
};

template <typename F, typename... Args>
using tp_bind_f = tp_bind<F::template fn, Args...>;

template <template <typename...> typename F, typename... Args>
struct tp_bind_front {
    template <typename... Args2>
    using fn = tp_defer_t<F, Args..., Args2...>;
};

template <typename F, typename... Args>
using tp_bind_front_f = tp_bind_front<F::template fn, Args...>;

template <template <typename...> typename F, typename... Args>
struct tp_bind_back {
    template <typename... Args2>
    using fn = tp_defer_t<F, Args2..., Args...>;
};

template <typename F, typename... Args>
using tp_bind_back_f = tp_bind_back<F::template fn, Args...>;

template <typename T, template <typename...> typename F>
struct tp_transform;

template <template <typename...> typename C, typename... Args,
          template <typename...> typename F>
struct tp_transform<C<Args...>, F> {
    using type = C<F<Args>...>;
};

// f(L<Args...>, Fn) -> L<Fn(Args)...>
// use with apply, bind, bind_front, bind_back...
// for example:
// tp_transform_f<tp_bind_front<tp_apply_f, tp_bind_front<std::is_same>>,
// tp_list<tp_list<int, float>, tp_list<float, float>, tp_list<int, double>>>
// -> tp_list<std::is_same<int, float>, std::is_same<float, float>, std::is_same<int,
// double>>
template <typename T, template <typename...> typename F>
using tp_transform_t = typename tp_transform<T, F>::type;

template <typename T, typename F>
using tp_transform_f = typename tp_transform<T, F::template fn>::type;

template <template <typename...> typename C, typename... Args>
struct tp_zip;

template <template <typename...> typename C, typename T>
struct __tp_zip_helper;

template <template <typename...> typename C, size_t... Indexs>
struct __tp_zip_helper<C, std::index_sequence<Indexs...>> {
    template <size_t I, typename... Args>
    using __type = C<tp_at_t<Args, I>...>;
    template <typename... Args>
    using type = tp_list<__type<Indexs, Args...>...>;
};

template <template <typename...> typename C>
struct tp_zip<C> {
    using type = tp_list<>;
};

template <template <typename...> typename C, typename T>
struct tp_zip<C, T> {
    using type = typename __tp_zip_helper<
        C, std::make_index_sequence<tp_size_v<T>>>::template type<T>;
};

template <template <typename...> typename C, typename T, typename... Args>
struct tp_zip<C, T, Args...> {
    constexpr static size_t size = tp_size_v<T>;
    static_assert(((size == tp_size_v<Args>)&&...),
                  "tp_zip arguments must have same size, \
		you can make all arguments have same size by tp_");
    using type = typename __tp_zip_helper<
        C, std::make_index_sequence<tp_size_v<T>>>::template type<T, Args...>;
};

// f(C, L<A1, A2, ... An>, L<B1, B2, ..., Bn> ...)
// -> L<C<A1, B1, ...>, C<A2, B2, ...>, ..., C<An, Bn, ...>>
template <template <typename...> typename C, typename... Args>
using tp_zip_t = typename tp_zip<C, Args...>::type;

template <typename... Args>
struct __tp_max_size_helper;

template <typename T>
struct __tp_max_size_helper<T> {
    constexpr static size_t value = tp_size_v<T>;
};

template <typename T, typename... Args>
struct __tp_max_size_helper<T, Args...> {
    constexpr static size_t value =
        std::max(tp_size_v<T>, __tp_max_size_helper<Args...>::value);
};

template <typename T, typename... Args>
struct tp_max_size {
    constexpr static size_t value = __tp_max_size_helper<T, Args...>::value;
};

// tp_max_size_v<T, Args...> -> size_t
template <typename T, typename... Args>
inline constexpr size_t tp_max_size_v = tp_max_size<T, Args...>::value;

template <typename T>
struct tp_unwrap {
    static_assert(tp_size_v<T> == 1, "only container that size = 1 can use unwrap");
};

template <template <typename...> typename C, typename T>
struct tp_unwrap<C<T>> {
    using type = T;
};

// f(C<T>) -> T
template <typename T>
using tp_unwrap_t = typename tp_unwrap<T>::type;

template <typename T, template <typename...> typename P, typename U>
struct tp_replace_if;

template <template <typename...> typename C, typename... Args,
          template <typename...> typename P, typename U>
struct tp_replace_if<C<Args...>, P, U> {
    using type = C<tp_conditional_t<P<Args>, U, Args>...>;
};

// f(L<Args...>, P, U) -> L<if P(Args)::value then U else Args...>
template <typename T, template <typename...> typename P, typename U>
using tp_replace_if_t = typename tp_replace_if<T, P, U>::type;

template <typename T, typename P, typename U>
using tp_replace_if_f = tp_replace_if_t<T, P::template fn, U>;

template <typename T, typename U>
struct tp_replace_if_true {
    using type = tp_replace_if_t<T, tp_type_identity_t, U>;
};

template <typename T, typename U>
using tp_replace_if_true_t = typename tp_replace_if_true<T, U>::type;

template <typename T, typename U>
struct tp_replace_if_false {
    using type = tp_replace_if_f<T, tp_not_fn<tp_type_identity_t>, U>;
};

template <typename T, typename U>
using tp_replace_if_false_t = typename tp_replace_if_false<T, U>::type;

template <typename T, typename O, typename N>
struct tp_replace {
    using type = tp_replace_if_f<T, tp_bind_front<std::is_same, O>, N>;
};

template <typename T, typename O, typename N>
using tp_replace_t = typename tp_replace<T, O, N>::type;

template <typename T, typename U>
struct tp_fill {
    using type = tp_replace_if_t<T, tp_true_type, U>;
};

// f(L<Args...>, U) -> L<U, U, ..., U>
template <typename T, typename U>
using tp_fill_t = typename tp_fill<T, U>::type;

template <typename T, template <typename...> typename P>
struct tp_count_if;

template <template <typename...> typename C, template <typename...> typename P>
struct tp_count_if<C<>, P> {
    static constexpr size_t value = 0;
};

template <template <typename...> typename C, typename... Args,
          template <typename...> typename P>
struct tp_count_if<C<Args...>, P> {
    static constexpr size_t value = (P<Args>::value + ...);
};

// f(L<Args...>, P) -> count(P(Args)::value)
template <typename T, template <typename...> typename P>
constexpr size_t tp_count_if_v = tp_count_if<T, P>::value;

template <typename T, typename P>
constexpr size_t tp_count_if_f_v = tp_count_if_v<T, P::template fn>;

template <typename T, typename V>
struct tp_count {
    static constexpr size_t value = tp_count_if_f_v<T, tp_bind_front<std::is_same, V>>;
};

template <typename T, typename V>
constexpr size_t tp_count_v = tp_count<T, V>::value;

template <typename T, typename V>
struct tp_contains {
    static constexpr bool value = tp_count_v<T, V> != 0;
};

template <typename T, typename V>
constexpr bool tp_contains_v = tp_contains<T, V>::value;

template <typename T, template <typename...> typename P>
struct tp_remove_if;

template <template <typename...> typename C, typename... Args,
          template <typename...> typename P>
struct tp_remove_if<C<Args...>, P> {
    using type = tp_concat_t<C<>, tp_conditional_t<P<Args>, C<>, C<Args>>...>;
};

// f(L<Args...>, P) -> L<if P(Args)::value then L<> else L<Args>...>
template <typename T, template <typename...> typename P>
using tp_remove_if_t = typename tp_remove_if<T, P>::type;

template <typename T, typename P>
using tp_remove_if_f = tp_remove_if_t<T, P::template fn>;

template <typename T, typename V>
struct tp_remove {
    using type = tp_remove_if_f<T, tp_bind_front<std::is_same, V>>;
};

template <typename T, typename V>
using tp_remove_t = typename tp_remove<T, V>::type;

template <typename T, template <typename...> typename P>
struct tp_filter {
    using type = tp_remove_if_f<T, tp_not_fn<P>>;
};

template <typename T, template <typename...> typename P>
using tp_filter_t = typename tp_filter<T, P>::type;

template <typename T, typename P>
using tp_filter_f = tp_filter_t<T, P::template fn>;

template <typename T, typename U>
struct tp_equal;

template <typename _Enable, typename T, typename U>
struct __tp_equal_helper : std::false_type {};

template <template <typename...> typename C, typename... Args,
          template <typename...> typename D, typename... Args2>
struct __tp_equal_helper<std::enable_if_t<sizeof...(Args) == sizeof...(Args2), void>,
                         C<Args...>, D<Args2...>>
    : std::conjunction<std::is_same<Args, Args2>...> {};

template <typename T, typename U>
struct tp_equal : __tp_equal_helper<void, T, U> {};

template <typename T, typename U>
inline constexpr bool tp_equal_v = tp_equal<T, U>::value;

template <typename T, size_t N>
struct tp_repeat {
    using type = tp_concat_t<T, typename tp_repeat<T, N - 1>::type>;
};

template <typename T>
struct tp_repeat<T, 0> {
    using type = tp_clear_t<T>;
};

template <typename C, size_t N>
using tp_repeat_t = typename tp_repeat<C, N>::type;

template <typename _Enable, typename C, size_t N, typename V>
struct __tp_resize_helper {
    using type = tp_cut_t<C, 0, N>;
};

template <typename C, size_t N, typename V>
struct __tp_resize_helper<std::enable_if_t<N >= tp_size_v<C>, void>, C, N, V> {
    using type = tp_concat_t<C, tp_repeat_t<V, N - tp_size_v<C>>>;
};

template <typename C, size_t N, typename V>
struct tp_resize {
    using tyep = typename __tp_resize_helper<void, C, N, V>::type;
};

template <typename C, size_t N, typename V>
using tp_resize_t = typename tp_resize<C, N, V>::type;

template <template <typename...> typename C, typename... Args>
struct tp_product;

template <typename _Enable, template <typename...> typename C, typename... Args>
struct __tp_product_helper {
    using type = tp_list<>;
};

template <typename _Enable, template <typename...> typename C, typename T>
struct __tp_product_helper<_Enable, C, T> {
    using type = tp_list<tp_rename_t<T, C>>;
};

template <template <typename...> typename C, typename T,
          template <typename...> typename C1, typename... Args1, typename... Args>
struct __tp_product_helper<std::enable_if_t<sizeof...(Args1) != 0, void>, C, T,
                           C1<Args1...>, Args...> {
    using type =
        tp_concat_t<typename __tp_product_helper<void, C, tp_push_back_t<T, Args1>,
                                                 Args...>::type...>;
};

template <template <typename...> typename C, typename... Args>
struct tp_product {
    using type = typename __tp_product_helper<void, C, tp_list<>, Args...>::type;
};

// for example
// f(C, L<A1, A2>, L<B1, B2, B3>) -> L<C<A1, B1>, C<A1, B2>, C<A1, B3>, C<A2, B1>, C<A2,
// B2>, C<A2, B3>>
template <template <typename...> typename C, typename... Args>
using tp_product_t = typename tp_product<C, Args...>::type;

template <typename C, size_t I, typename... Args>
struct tp_insert {
    static_assert(I <= tp_size_v<C>, "tp insert index out of range");
    using type = tp_concat_t<tp_push_back_t<tp_prefix_t<C, I>, Args...>,
                             tp_suffix_t<C, tp_size_v<C> - I>>;
};

template <typename C, size_t I, typename... Args>
using tp_insert_t = typename tp_insert<C, I, Args...>::type;

template <typename C, size_t I, size_t N>
struct tp_erase {
    static_assert(N <= tp_size_v<C> && I <= tp_size_v<C> - N,
                  "tp erase index out of range");
    using type = tp_concat_t<tp_prefix_t<C, I>, tp_suffix_t<C, tp_size_v<C> - I - N>>;
};

template <typename C, size_t I, size_t N>
using tp_erase_t = typename tp_erase<C, I, N>::type;

template <typename C>
struct tp_reverse;

template <template <typename...> typename C>
struct tp_reverse<C<>> {
    using type = C<>;
};

template <template <typename...> typename C, typename T, typename... Args>
struct tp_reverse<C<T, Args...>> {
    using type = tp_push_back_t<typename tp_reverse<C<Args...>>::type, T>;
};

template <typename C>
using tp_reverse_t = typename tp_reverse<C>::type;

template <typename _Enable, size_t idx, typename C, template <typename...> typename P>
struct __tp_find_if_helper;

template <typename _Enable, size_t idx, template <typename...> typename C, typename T,
          typename... Args, template <typename...> typename P>
struct __tp_find_if_helper<_Enable, idx, C<T, Args...>, P> {
    constexpr static size_t value =
        __tp_find_if_helper<void, idx + 1, C<Args...>, P>::value;
};

template <typename _Enable, size_t idx, template <typename...> typename C,
          template <typename...> typename P>
struct __tp_find_if_helper<_Enable, idx, C<>, P> {
    constexpr static size_t value = -1;
};

template <size_t idx, template <typename...> typename C, typename T, typename... Args,
          template <typename...> typename P>
struct __tp_find_if_helper<std::enable_if_t<P<T>::value, void>, idx, C<T, Args...>, P> {
    constexpr static size_t value = idx;
};

template <typename C, template <typename...> typename P>
struct tp_find_if {
    constexpr static size_t value = __tp_find_if_helper<void, 0, C, P>::value;
};

template <typename C, template <typename...> typename P>
inline constexpr size_t tp_find_if_v = tp_find_if<C, P>::value;

template <typename C, typename P>
inline constexpr size_t tp_find_if_f = tp_find_if<C, P::template fn>::value;

template <typename C, template <typename...> typename P>
struct tp_find_if_not {
    constexpr static size_t value = tp_find_if_f<C, tp_not_fn<P>>;
};

template <typename C, template <typename...> typename P>
inline constexpr size_t tp_find_if_not_v = tp_find_if_not<C, P>::value;

template <typename C, typename P>
inline constexpr size_t tp_find_if_not_f = tp_find_if_not<C, P::template fn>::value;

template <typename C, typename V>
struct tp_find {
    constexpr static size_t value = tp_find_if_f<C, tp_bind_front<std::is_same, V>>;
};

template <typename C, typename V>
inline constexpr size_t tp_find_v = tp_find<C, V>::value;

template <typename C, typename E, template <typename...> typename F>
struct tp_left_fold;

template <template <typename...> typename C, typename E,
          template <typename...> typename F>
struct tp_left_fold<C<>, E, F> {
    using type = E;
};

template <template <typename...> typename C, typename T, typename... Args, typename E,
          template <typename...> typename F>
struct tp_left_fold<C<T, Args...>, E, F> {
    using type = typename tp_left_fold<C<Args...>, F<E, T>, F>::type;
};

// f(L<A1, A2, ... An>, E, F) -> F<F<F...<F<E, A1>, A2>, ...>, An>
template <typename C, typename E, template <typename...> typename F>
using tp_left_fold_t = typename tp_left_fold<C, E, F>::type;

template <typename C, typename E, typename F>
using tp_left_fold_f = typename tp_left_fold<C, E, F::template fn>::type;

template <typename C, typename E, template <typename...> typename F>
struct tp_right_fold;

template <template <typename...> typename C, typename E,
          template <typename...> typename F>
struct tp_right_fold<C<>, E, F> {
    using type = E;
};

template <template <typename...> typename C, typename T, typename... Args, typename E,
          template <typename...> typename F>
struct tp_right_fold<C<T, Args...>, E, F> {
    using next_type = typename tp_right_fold<C<Args...>, E, F>::type;
    using type = F<T, next_type>;
};

// f(L<A1, A2, ... An>, E, F) -> F<A1, F<A2, ... F<An, E>...>>
template <typename C, typename E, template <typename...> typename F>
using tp_right_fold_t = typename tp_right_fold<C, E, F>::type;

template <typename C, typename E, typename F>
using tp_right_fold_f = typename tp_right_fold<C, E, F::template fn>::type;

template <typename C, template <typename...> typename P>
struct tp_unique_if {
    using type = tp_left_fold_f<C, tp_clear_t<C>,
                                tp_bind<tp_conditional_t, tp_bind_front<P>, tp_arg<0>,
                                        tp_bind_front<tp_push_back_t>>>;
};

// using NOW_LIST = tp_prefix_t<C, I + 1>;
// using PRE_LIST = tp_prefix_t<C, I>;
// using PRE_UNIQUE_IF_LIST = tp_unique_if_t<PRE_LIST>;
// then :
// tp_unique_if_t<NOW_LIST, P>
// = tp_conditonal_t<
// P<PRE_UNIQUE_IF_LIST, tp_at_t<C, I>>,
// PRE_UNIQUE_IF_LIST,
// tp_push_back_t<PRE_UNIQUE_IF_LIST, tp_at_t<C, I>>>
//
// It is equivalent to calling P every time on the results
// of the previous processing and the new value.
// If P is false, the new value is added
template <typename C, template <typename...> typename P>
using tp_unique_if_t = typename tp_unique_if<C, P>::type;

template <typename C, typename P>
using tp_unique_if_f = typename tp_unique_if<C, P::template fn>::type;

template <typename C>
struct tp_unique {
    using type = tp_unique_if_t<C, tp_contains>;
};

// same as tp_unique_if_t<C, tp_contains>
// remove the same type
template <typename C>
using tp_unique_t = typename tp_unique<C>::type;

template <typename _Enable, typename C, typename C1, typename C2,
          template <typename...> typename P>
struct __tp_merge_helper;

template <typename _Enable, template <typename...> typename C, typename... Args,
          template <typename...> typename C1, template <typename...> typename C2,
          typename... Args2, template <typename...> typename P>
struct __tp_merge_helper<_Enable, C<Args...>, C1<>, C2<Args2...>, P> {
    using type = tp_list<Args..., Args2...>;
};

template <typename _Enable, template <typename...> typename C, typename... Args,
          template <typename...> typename C1, typename... Args1,
          template <typename...> typename C2, template <typename...> typename P>
struct __tp_merge_helper<_Enable, C<Args...>, C1<Args1...>, C2<>, P> {
    using type = tp_list<Args..., Args1...>;
};

template <typename _Enable, template <typename...> typename C, typename... Args,
          template <typename...> typename C1, template <typename...> typename C2,
          template <typename...> typename P>
struct __tp_merge_helper<_Enable, C<Args...>, C1<>, C2<>, P> {
    using type = tp_list<Args...>;
};

template <template <typename...> typename C, typename... Args,
          template <typename...> typename C1, typename T1, typename... Args1,
          template <typename...> typename C2, typename T2, typename... Args2,
          template <typename...> typename P>
struct __tp_merge_helper<std::enable_if_t<P<T1, T2>::value, void>, C<Args...>,
                         C1<T1, Args1...>, C2<T2, Args2...>, P> {
    using type = typename __tp_merge_helper<void, C<Args..., T1>, C1<Args1...>,
                                            C2<T2, Args2...>, P>::type;
};

template <template <typename...> typename C, typename... Args,
          template <typename...> typename C1, typename T1, typename... Args1,
          template <typename...> typename C2, typename T2, typename... Args2,
          template <typename...> typename P>
struct __tp_merge_helper<std::enable_if_t<!P<T1, T2>::value, void>, C<Args...>,
                         C1<T1, Args1...>, C2<T2, Args2...>, P> {
    using type = typename __tp_merge_helper<void, C<Args..., T2>, C1<T1, Args1...>,
                                            C2<Args2...>, P>::type;
};

template <typename C1, typename C2, template <typename...> typename P>
struct tp_merge {
    using type = typename __tp_merge_helper<void, tp_list<>, C1, C2, P>::type;
};

// like std::merge
// merge two list with P
template <typename C1, typename C2, template <typename...> typename P>
using tp_merge_t = typename tp_merge<C1, C2, P>::type;

template <typename C1, typename C2, typename P>
using tp_merge_f = typename tp_merge<C1, C2, P::template fn>::type;

template <typename C, template <typename...> typename P>
struct tp_sort;

template <typename C, template <typename...> typename P>
struct __tp_sort_helper;

template <template <typename...> typename C, typename... Args,
          template <typename...> typename P>
struct __tp_sort_helper<C<Args...>, P> {
    using _Container = C<Args...>;
    constexpr static size_t size = tp_size_v<_Container>;
    constexpr static size_t mid = size / 2;
    using type1 = typename __tp_sort_helper<tp_prefix_t<_Container, mid>, P>::type;
    using type2 = typename __tp_sort_helper<tp_suffix_t<_Container, size - mid>, P>::type;
    using type = tp_merge_t<type1, type2, P>;
};

template <template <typename...> typename C, typename T,
          template <typename...> typename P>
struct __tp_sort_helper<C<T>, P> {
    using type = C<T>;
};

template <template <typename...> typename C, template <typename...> typename P>
struct __tp_sort_helper<C<>, P> {
    using type = C<>;
};

template <template <typename...> typename C, typename... Args,
          template <typename...> typename P>
struct tp_sort<C<Args...>, P> {
    using type = tp_rename_t<typename __tp_sort_helper<C<Args...>, P>::type, C>;
};

// list std::sort
template <typename C, template <typename...> typename P>
using tp_sort_t = typename tp_sort<C, P>::type;

template <typename C, typename P>
using tp_sort_f = typename tp_sort<C, P::template fn>::type;

template <typename T, typename S>
struct __tp_make_integer_sequence_helper;

template <typename T, T... Indexs>
struct __tp_make_integer_sequence_helper<T, std::integer_sequence<T, Indexs...>> {
    using type = tp_list<std::integral_constant<T, Indexs>...>;
};

template <typename T, T N>
using tp_make_integer_sequence =
    typename __tp_make_integer_sequence_helper<T, std::make_integer_sequence<T, N>>::type;

template <size_t N>
using tp_make_index_sequence = tp_make_integer_sequence<size_t, N>;

template <typename... Args>
using tp_index_sequence_for = tp_make_index_sequence<sizeof...(Args)>;

template <typename T, typename S>
struct __tp_make_std_integer_sequence_helper;

template <typename T, T... Indexs>
struct __tp_make_std_integer_sequence_helper<
    T, tp_list<std::integral_constant<T, Indexs>...>> {
    using type = std::integer_sequence<T, Indexs...>;
};

template <typename S>
using tp_make_std_index_sequence =
    typename __tp_make_std_integer_sequence_helper<size_t, S>::type;

} // namespace wjr

#endif // WJR_TP_LIST_HPP__
// Already included

namespace wjr {

/**
 * @class capture_leaf
 *
 * @brief Capture any type as a new type. Can be used as a class base.
 *
 */
template <typename T, typename Tag = void>
class capture_leaf : enable_special_members_of_args_base<Tag, T> {
    using Mybase = enable_special_members_of_args_base<Tag, T>;

public:
    template <typename Ty = T, WJR_REQUIRES(std::is_default_constructible_v<Ty>)>
    constexpr capture_leaf() noexcept(std::is_nothrow_constructible_v<T>)
        : Mybase(enable_default_constructor), m_value() {}

    template <typename... Args, WJR_REQUIRES(std::is_constructible_v<T, Args &&...>)>
    constexpr capture_leaf(Args &&...args) noexcept(
        std::is_constructible_v<T, Args &&...>)
        : Mybase(enable_default_constructor), m_value(std::forward<Args>(args)...) {}

    template <typename Ty = T, WJR_REQUIRES(std::is_default_constructible_v<Ty>)>
    constexpr explicit capture_leaf(dctor_t) noexcept(
        std::is_nothrow_default_constructible_v<T>)
        : Mybase(enable_default_constructor) {}

    constexpr T &get() noexcept { return m_value; }
    constexpr const T &get() const noexcept { return m_value; }

private:
    T m_value;
};

/**
 * @class compressed_capture_leaf
 *
 * @brief Compressed capture any type as a new type.
 *
 * @details Use `EBO`(empty base optimization) to compress the size of the object.
 *
 */
template <typename T, typename Tag = void>
class compressed_capture_leaf : T {
    using Mybase = T;

public:
    template <typename Ty = T, WJR_REQUIRES(std::is_default_constructible_v<Ty>)>
    constexpr compressed_capture_leaf() noexcept(std::is_nothrow_constructible_v<T>)
        : Mybase() {}

    template <typename... Args, WJR_REQUIRES(std::is_constructible_v<T, Args &&...>)>
    constexpr compressed_capture_leaf(Args &&...args) noexcept(
        std::is_constructible_v<T, Args &&...>)
        : Mybase(std::forward<Args>(args)...) {}

    template <typename Ty = T, WJR_REQUIRES(std::is_default_constructible_v<Ty>)>
    constexpr explicit compressed_capture_leaf(dctor_t) noexcept(
        std::is_nothrow_default_constructible_v<T>) {}

    constexpr T &get() noexcept { return *this; }
    constexpr const T &get() const noexcept { return *this; }
};

/**
 * @struct is_compressed
 *
 * @brief Check if a class can be compressed.
 *
 * @details A class can be compressed if it is a `empty` `class`, and it is not `final`.
 *
 */
template <typename T>
struct is_compressed : std::conjunction<std::is_class<T>, std::is_empty<T>,
                                        std::negation<std::is_final<T>>> {};

/**
 * @brief Value of @ref is_compressed.
 *
 */
template <typename T>
inline constexpr bool is_compressed_v = is_compressed<T>::value;

/// @private
template <template <typename...> typename Test, typename Seq, typename LP, typename RP,
          typename = void>
struct __is_tuple_test_impl : std::false_type {};

/// @private
template <template <typename...> typename Test, size_t... Idxs, typename LP, typename RP>
struct __is_tuple_test_impl<
    Test, std::index_sequence<Idxs...>, LP, RP,
    std::enable_if_t<!std::is_same_v<LP, remove_cvref_t<RP>> &&
                     std::tuple_size_v<LP> ==
                         tp_defer_t<std::tuple_size, remove_cvref_t<RP>>::value>>
    : std::conjunction<Test<std::tuple_element_t<Idxs, LP>,
                            decltype(std::get<Idxs>(std::declval<RP>()))>...> {};

/**
 * @brief Use template<...>typename Test to test all element of LP and RP.
 *
 * @details For example, Test is std::is_assignable, LP is std::tuple<T0, U0>, RP is
 * std::tuple<T1, U1>. \n
 * Then __is_tuple_test = std::conjunction<std::is_assignable<T0,
 * T1>, std::is_assignable<U0, U1>>.
 *
 */
template <template <typename...> typename Test, typename LP, typename RP>
struct __is_tuple_test
    : __is_tuple_test_impl<Test, std::make_index_sequence<std::tuple_size_v<LP>>, LP,
                           RP> {};

/**
 * @brief Value of @ref __is_tuple_test.
 *
 */
template <template <typename...> typename Test, typename LP, typename RP>
inline constexpr bool __is_tuple_test_v = __is_tuple_test<Test, LP, RP>::value;

/// @private
template <typename T, typename U>
struct __is_tuple_assignable : std::is_assignable<T &, U> {};

} // namespace wjr

#endif // WJR_CAPTURE_LEAF_HPP__

namespace wjr {

template <typename T, typename U>
class compressed_pair;

}

namespace std {

template <typename T, typename U>
struct tuple_size<wjr::compressed_pair<T, U>> : std::integral_constant<size_t, 2> {};

template <size_t I, typename T, typename U>
struct tuple_element<I, wjr::compressed_pair<T, U>> {
    using type = wjr::tp_at_t<wjr::compressed_pair<T, U>, I>;
};

} // namespace std

namespace wjr {

/**
 * @brief Select the base class of compressed_pair.
 *
 * @details For compressed_pair<T, U> : \n
 * If `T` is @ref is_compressed_v "compressed" and `U` is not ref is_compressed_v
 * "compressed", then the base class is
 * @ref compressed_capture_leaf \<T> and @ref capture_leaf \<U>. \n
 * If `T` is not ref is_compressed_v "compressed" and `U` is ref is_compressed_v
 * "compressed", then the base class is
 * @ref capture_leaf \<T> and @ref compressed_capture_leaf \<U>. \n
 * If `T` and `U` are both ref is_compressed_v "compressed", then the base class is
 * @ref compressed_capture_leaf \<T> and @ref capture_leaf \<U>. \n
 * Otherwise, the base class is @ref capture_leaf \<T> and @ref capture_leaf \<U>. \n
 * Notice that both `T` and `U` are ref is_compressed_v "compressed" is not allowed.
 *
 */
template <size_t index, typename T, typename U, typename Tag = void>
using compressed_pair_wrapper =
    std::conditional_t<is_compressed_v<T> && (index == 0 || !is_compressed_v<U>),
                       compressed_capture_leaf<T, enable_base_identity_t<index, Tag>>,
                       capture_leaf<T, enable_base_identity_t<index, Tag>>>;

/// @private
template <typename T, typename U>
struct __compressed_pair1 {};

/// @private
template <typename T, typename U>
struct __compressed_pair2 {};

/// @private
template <typename T, typename U>
struct __compressed_pair3 {};

/// @private
template <typename T, typename U>
using __compressed_pair_base1 =
    compressed_pair_wrapper<0, T, U, __compressed_pair1<T, U>>;

/// @private
template <typename T, typename U>
using __compressed_pair_base2 =
    compressed_pair_wrapper<1, U, T, __compressed_pair2<T, U>>;

/**
 * @class compressed_pair
 *
 * @brief A pair used empty base optimization to reduce the size of the pair.
 *
 * @details See @ref compressed_pair_wrapper for the base class of compressed_pair. \n
 * compressed_pair is final, so it can't be derived from. \n
 * For example : \n
 * @code
 * static_assert(sizeof(compressed_pair<int, double>) == sizeof(int) + sizeof(double));
 * static_assert(sizeof(compressed_pair<std::allocator<int>, int>) == sizeof(int));
 * static_assert(sizeof(compressed_pair<int, std::allocator<int>>) == sizeof(int));
 * @endcode
 */
template <typename T, typename U>
class WJR_EMPTY_BASES compressed_pair final
    : __compressed_pair_base1<T, U>,
      __compressed_pair_base2<T, U>,
      enable_special_members_of_args_base<compressed_pair<T, U>,
                                          __compressed_pair_base1<T, U>,
                                          __compressed_pair_base2<T, U>> {

    using Mybase1 = __compressed_pair_base1<T, U>;
    using Mybase2 = __compressed_pair_base2<T, U>;
    using Mybase3 =
        enable_special_members_of_args_base<compressed_pair<T, U>, Mybase1, Mybase2>;

    template <typename Ty, typename Uy>
    using __is_all_copy_constructible =
        std::conjunction<std::is_copy_constructible<Ty>, std::is_copy_constructible<Uy>>;

    template <typename Ty, typename Uy, typename Vty, typename Wuy>
    using __is_all_convertible =
        std::conjunction<std::is_convertible<Vty, Ty>, std::is_convertible<Wuy, Uy>>;

    template <typename Ty, typename Uy, typename Vty, typename Wuy>
    using __is_all_constructible =
        std::conjunction<std::is_constructible<Ty, Vty>, std::is_constructible<Uy, Wuy>>;

public:
    using first_type = T;
    using second_type = U;

    template <typename Ty = T, typename Uy = U,
              WJR_REQUIRES(std::conjunction_v<std::is_default_constructible<Ty>,
                                              std::is_default_constructible<Uy>>
                               &&std::conjunction_v<is_default_convertible<Ty>,
                                                    is_default_convertible<Uy>>)>
    constexpr compressed_pair() noexcept(
        std::conjunction_v<std::is_nothrow_constructible<Ty>,
                           std::is_nothrow_constructible<Uy>>) {}

    template <typename Ty = T, typename Uy = U,
              WJR_REQUIRES(std::conjunction_v<std::is_default_constructible<Ty>,
                                              std::is_default_constructible<Uy>> &&
                           !std::conjunction_v<is_default_convertible<Ty>,
                                               is_default_convertible<Uy>>)>
    constexpr explicit compressed_pair() noexcept(
        std::conjunction_v<std::is_nothrow_constructible<Ty>,
                           std::is_nothrow_constructible<Uy>>) {}

    template <typename Ty = T, typename Uy = U,
              WJR_REQUIRES(std::conjunction_v<
                           __is_all_copy_constructible<Ty, Uy>,
                           __is_all_convertible<Ty, Uy, const Ty &, const Uy &>>)>
    constexpr compressed_pair(const T &_First, const U &_Second) noexcept(
        std::conjunction_v<std::is_nothrow_copy_constructible<Ty>,
                           std::is_nothrow_copy_constructible<Uy>>)
        : Mybase1(_First), Mybase2(_Second), Mybase3(enable_default_constructor) {}

    template <typename Ty = T, typename Uy = U,
              WJR_REQUIRES(std::conjunction_v<__is_all_copy_constructible<Ty, Uy>,
                                              std::negation<__is_all_convertible<
                                                  Ty, Uy, const Ty &, const Uy &>>>)>
    constexpr explicit compressed_pair(const T &_First, const U &_Second) noexcept(
        std::conjunction_v<std::is_nothrow_copy_constructible<Ty>,
                           std::is_nothrow_copy_constructible<Uy>>)
        : Mybase1(_First), Mybase2(_Second), Mybase3(enable_default_constructor) {}

    template <typename Other1, typename Other2,
              WJR_REQUIRES(std::conjunction_v<
                           __is_all_constructible<Mybase1, Mybase2, Other1 &&, Other2 &&>,
                           __is_all_convertible<T, U, Other1 &&, Other2 &&>>)>
    constexpr compressed_pair(Other1 &&_First, Other2 &&_Second) noexcept(
        std::conjunction_v<std::is_nothrow_constructible<Mybase1, Other1 &&>,
                           std::is_nothrow_constructible<Mybase2, Other2 &&>>)
        : Mybase1(std::forward<Other1>(_First)), Mybase2(std::forward<Other2>(_Second)),
          Mybase3(enable_default_constructor) {}

    template <
        typename Other1, typename Other2,
        WJR_REQUIRES(std::conjunction_v<
                     __is_all_constructible<Mybase1, Mybase2, Other1 &&, Other2 &&>,
                     std::negation<__is_all_convertible<T, U, Other1 &&, Other2 &&>>>)>
    constexpr explicit compressed_pair(Other1 &&_First, Other2 &&_Second) noexcept(
        std::conjunction_v<std::is_nothrow_constructible<Mybase1, Other1 &&>,
                           std::is_nothrow_constructible<Mybase2, Other2 &&>>)
        : Mybase1(std::forward<Other1>(_First)), Mybase2(std::forward<Other2>(_Second)),
          Mybase3(enable_default_constructor) {}

    template <typename Tuple1, typename Tuple2, size_t... N1, size_t... N2>
    constexpr compressed_pair(
        Tuple1 &tp1, Tuple2 &tp2, std::index_sequence<N1...>,
        std::index_sequence<
            N2...>) noexcept(noexcept(Mybase1(std::get<N1>(std::move(tp1))...))
                                 && noexcept(Mybase2(std::get<N2>(std::move(tp2))...)))
        : Mybase1(std::get<N1>(std::move(tp1))...),
          Mybase2(std::get<N2>(std::move(tp2))...), Mybase3(enable_default_constructor) {}

    template <typename... Args1, typename... Args2>
    constexpr compressed_pair(
        std::piecewise_construct_t, std::tuple<Args1...> tp1,
        std::tuple<Args2...>
            tp2) noexcept(noexcept(compressed_pair(tp1, tp2,
                                                   std::index_sequence_for<Args1...>{},
                                                   std::index_sequence_for<Args2...>{})))
        : compressed_pair(tp1, tp2, std::index_sequence_for<Args1...>{},
                          std::index_sequence_for<Args2...>{}) {}

    template <typename PairLike,
              WJR_REQUIRES(
                  __is_tuple_test_v<std::is_constructible, compressed_pair, PairLike &&>
                      &&__is_all_convertible<T, U,
                                             decltype(std::get<0>(std::forward<PairLike>(
                                                 std::declval<PairLike>()))),
                                             decltype(std::get<1>(std::forward<PairLike>(
                                                 std::declval<PairLike>())))>::value)>
    constexpr compressed_pair(PairLike &&pr) noexcept(
        std::conjunction_v<std::is_nothrow_constructible<
                               T, decltype(std::get<0>(std::forward<PairLike>(pr)))>,
                           std::is_nothrow_constructible<
                               U, decltype(std::get<1>(std::forward<PairLike>(pr)))>>)
        : Mybase1(std::get<0>(std::forward<PairLike>(pr))),
          Mybase2(std::get<1>(std::forward<PairLike>(pr))),
          Mybase3(enable_default_constructor) {}

    template <
        typename PairLike,
        WJR_REQUIRES(
            __is_tuple_test_v<std::is_constructible, compressed_pair, PairLike &&> &&
            !__is_all_convertible<
                T, U,
                decltype(std::get<0>(std::forward<PairLike>(std::declval<PairLike>()))),
                decltype(std::get<1>(
                    std::forward<PairLike>(std::declval<PairLike>())))>::value)>
    constexpr explicit compressed_pair(PairLike &&pr) noexcept(
        std::conjunction_v<std::is_nothrow_constructible<
                               T, decltype(std::get<0>(std::forward<PairLike>(pr)))>,
                           std::is_nothrow_constructible<
                               U, decltype(std::get<1>(std::forward<PairLike>(pr)))>>)
        : Mybase1(std::get<0>(std::forward<PairLike>(pr))),
          Mybase2(std::get<1>(std::forward<PairLike>(pr))),
          Mybase3(enable_default_constructor) {}

    template <typename PairLike,
              WJR_REQUIRES(
                  __is_tuple_test_v<__is_tuple_assignable, compressed_pair, PairLike &&>)>
    constexpr compressed_pair &operator=(PairLike &&pr) noexcept(
        std::conjunction_v<std::is_nothrow_assignable<T, decltype(std::get<0>(pr))>,
                           std::is_nothrow_assignable<U, decltype(std::get<1>(pr))>>) {
        first() = std::get<0>(std::forward<PairLike>(pr));
        second() = std::get<1>(std::forward<PairLike>(pr));
        return *this;
    }

    template <typename Myself = compressed_pair, typename _T = T,
              WJR_REQUIRES(std::conjunction_v<is_swappable<_T>, is_swappable<U>>)>
    constexpr void swap(type_identity_t<compressed_pair &> other) noexcept(
        std::conjunction_v<is_nothrow_swappable<T>, is_nothrow_swappable<U>>) {
        using std::swap;
        swap(first(), other.first());
        swap(second(), other.second());
    }

    constexpr T &first() noexcept { return Mybase1::get(); }
    constexpr const T &first() const noexcept { return Mybase1::get(); }
    constexpr U &second() noexcept { return Mybase2::get(); }
    constexpr const U &second() const noexcept { return Mybase2::get(); }

    // extension

    template <size_t I>
    constexpr std::tuple_element_t<I, compressed_pair> &get() & noexcept {
        if constexpr (I == 0) {
            return first();
        } else {
            return second();
        }
    }

    template <size_t I>
    constexpr const std::tuple_element_t<I, compressed_pair> &get() const & noexcept {
        if constexpr (I == 0) {
            return first();
        } else {
            return second();
        }
    }

    template <size_t I>
    constexpr std::tuple_element_t<I, compressed_pair> &&get() && noexcept {
        if constexpr (I == 0) {
            return std::move(first());
        } else {
            return std::move(second());
        }
    }

    template <size_t I>
    constexpr const std::tuple_element_t<I, compressed_pair> &&get() const && noexcept {
        if constexpr (I == 0) {
            return std::move(first());
        } else {
            return std::move(second());
        }
    }
};

template <typename T, typename U>
compressed_pair(T, U) -> compressed_pair<T, U>;

template <typename T1, typename U1, typename T2, typename U2>
WJR_CONST constexpr bool operator==(
    const compressed_pair<T1, U1> &lhs,
    const compressed_pair<T2, U2> &
        rhs) noexcept(std::conjunction_v<has_noexcept_equal_to<const T1 &, const T2 &>,
                                         has_noexcept_equal_to<const U1 &, const U2 &>>) {
    return lhs.first() == rhs.first() && lhs.second() == rhs.second();
}

template <typename T1, typename U1, typename T2, typename U2>
WJR_CONST constexpr bool
operator!=(const compressed_pair<T1, U1> &lhs,
           const compressed_pair<T2, U2> &rhs) noexcept(noexcept(lhs == rhs)) {
    return !(lhs == rhs);
}

template <typename T1, typename U1, typename T2, typename U2>
WJR_CONST constexpr bool operator<(
    const compressed_pair<T1, U1> &lhs,
    const compressed_pair<T2, U2>
        &rhs) noexcept(std::conjunction_v<has_noexcept_less<const T1 &, const T2 &>,
                                          has_noexcept_less<const T2 &, const T1 &>,
                                          has_noexcept_less<const U1 &, const U2 &>>) {
    return lhs.first() < rhs.first() ||
           (!(rhs.first() < lhs.first()) && lhs.second() < rhs.second());
}

template <typename T1, typename U1, typename T2, typename U2>
WJR_CONST constexpr bool
operator>(const compressed_pair<T1, U1> &lhs,
          const compressed_pair<T2, U2> &rhs) noexcept(noexcept(rhs < lhs)) {
    return rhs < lhs;
}

template <typename T1, typename U1, typename T2, typename U2>
WJR_CONST constexpr bool
operator<=(const compressed_pair<T1, U1> &lhs,
           const compressed_pair<T2, U2> &rhs) noexcept(noexcept(rhs < lhs)) {
    return !(rhs < lhs);
}

template <typename T1, typename U1, typename T2, typename U2>
WJR_CONST constexpr bool
operator>=(const compressed_pair<T1, U1> &lhs,
           const compressed_pair<T2, U2> &rhs) noexcept(noexcept(lhs < rhs)) {
    return !(lhs < rhs);
}

template <typename T, typename U>
constexpr compressed_pair<unref_wrapper_t<T>, unref_wrapper_t<U>>
make_compressed_pair(T &&t, U &&u) noexcept(
    std::conjunction_v<std::is_nothrow_constructible<unref_wrapper_t<T>, T &&>,
                       std::is_nothrow_constructible<unref_wrapper_t<U>, U &&>>) {
    return compressed_pair<unref_wrapper_t<T>, unref_wrapper_t<U>>(std::forward<T>(t),
                                                                   std::forward<U>(u));
}

} // namespace wjr

namespace std {

template <typename T, typename U,
          WJR_REQUIRES(std::conjunction_v<wjr::is_swappable<T>, wjr::is_swappable<U>>)>
constexpr void swap(wjr::compressed_pair<T, U> &lhs,
                    wjr::compressed_pair<T, U> &rhs) noexcept(noexcept(lhs.swap(rhs))) {
    lhs.swap(rhs);
}

template <size_t I, typename T, typename U>
WJR_NODISCARD constexpr tuple_element_t<I, wjr::compressed_pair<T, U>> &
get(wjr::compressed_pair<T, U> &pr) noexcept {
    if constexpr (I == 0) {
        return pr.first();
    } else {
        return pr.second();
    }
}

template <size_t I, typename T, typename U>
WJR_NODISCARD constexpr const tuple_element_t<I, wjr::compressed_pair<T, U>> &
get(const wjr::compressed_pair<T, U> &pr) noexcept {
    if constexpr (I == 0) {
        return pr.first();
    } else {
        return pr.second();
    }
}

template <size_t I, typename T, typename U>
WJR_NODISCARD constexpr tuple_element_t<I, wjr::compressed_pair<T, U>> &&
get(wjr::compressed_pair<T, U> &&pr) noexcept {
    if constexpr (I == 0) {
        return std::forward<T>(pr.first());
    } else {
        return std::forward<U>(pr.second());
    }
}

template <size_t I, typename T, typename U>
WJR_NODISCARD constexpr const tuple_element_t<I, wjr::compressed_pair<T, U>> &&
get(const wjr::compressed_pair<T, U> &&pr) noexcept {
    if constexpr (I == 0) {
        return std::forward<T>(pr.first());
    } else {
        return std::forward<U>(pr.second());
    }
}

template <typename T, typename U>
WJR_NODISCARD constexpr T &get(wjr::compressed_pair<T, U> &pr) noexcept {
    return std::get<0>(pr);
}

template <typename T, typename U>
WJR_NODISCARD constexpr const T &get(const wjr::compressed_pair<T, U> &pr) noexcept {
    return get<0>(pr);
}

template <typename T, typename U>
WJR_NODISCARD constexpr T &&get(wjr::compressed_pair<T, U> &&pr) noexcept {
    return get<0>(std::move(pr));
}

template <typename T, typename U>
WJR_NODISCARD constexpr const T &&get(const wjr::compressed_pair<T, U> &&pr) noexcept {
    return get<0>(std::move(pr));
}

template <typename T, typename U>
WJR_NODISCARD constexpr T &get(wjr::compressed_pair<U, T> &pr) noexcept {
    return get<1>(pr);
}

template <typename T, typename U>
WJR_NODISCARD constexpr const T &get(const wjr::compressed_pair<U, T> &pr) noexcept {
    return get<1>(pr);
}

template <typename T, typename U>
WJR_NODISCARD constexpr T &&get(wjr::compressed_pair<U, T> &&pr) noexcept {
    return get<1>(std::move(pr));
}

template <typename T, typename U>
WJR_NODISCARD constexpr const T &&get(const wjr::compressed_pair<U, T> &&pr) noexcept {
    return get<1>(std::move(pr));
}

} // namespace std

#endif // WJR_COMPRESSED_PAIR_HPP__
#ifndef WJR_CONTAINER_GENERIC_CONTAINER_TRAITS_HPP__
#define WJR_CONTAINER_GENERIC_CONTAINER_TRAITS_HPP__

#include <memory>
#include <type_traits>

// Already included

namespace wjr {

/**
 * @class container_fn<Alloc>
 * @brief The same characteristics and behavior of all allocator containers
 *
 * @details container must have the following member functions:
 * -# auto& __get_allocator() noexcept
 * -# void __destroy() noexcept
 * -# void __destroy_and_deallocate() noexcept
 * -# void __copy_element(const container& other)
 * -# void __take_storage(container&& other)
 * -# void __move_element(container&& other)
 * -# void __swap_storage(container& other)
 *
 * 1 : is used to manage the allocator of the container. \n
 * 2-3 : is used to destroy the container and deallocate the memory. \n
 * 4-7 : is used to assign the container data. Shouldn't change the allocator.
 *
 */
template <typename Alloc>
class container_fn {
private:
    using allocator_type = Alloc;
    using allocator_traits = std::allocator_traits<allocator_type>;
    using is_always_equal = typename allocator_traits::is_always_equal;
    using propagate_on_container_copy_assignment =
        typename allocator_traits::propagate_on_container_copy_assignment;
    using propagate_on_container_move_assignment =
        typename allocator_traits::propagate_on_container_move_assignment;
    using propagate_on_container_swap =
        typename allocator_traits::propagate_on_container_swap;

public:
    template <typename Container>
    WJR_CONSTEXPR20 static void
    copy_assign(Container &lhs, const Container &rhs) noexcept(
        noexcept(lhs.__copy_element(rhs)) &&
                !propagate_on_container_copy_assignment::value
            ? true
            : (noexcept(lhs.__get_allocator() = rhs.__get_allocator()) &&
                       is_always_equal::value
                   ? true
                   : noexcept(lhs.__destroy_and_deallocate()))) {
        if constexpr (propagate_on_container_copy_assignment::value) {
            auto &lhs_allocator = lhs.__get_allocator();
            auto &rhs_allocator = rhs.__get_allocator();
            if constexpr (!is_always_equal::value) {
                if (lhs_allocator != rhs_allocator) {
                    lhs.__destroy_and_deallocate();
                }
            }

            lhs_allocator = rhs_allocator;
        }

        lhs.__copy_element(rhs);
    }

    template <typename Container>
    WJR_CONSTEXPR20 static void move_assign(Container &lhs, Container &&rhs) noexcept(
        noexcept(lhs.__destroy_and_deallocate()) && noexcept(
            lhs.__take_storage(std::move(rhs))) &&
                std::disjunction_v<propagate_on_container_move_assignment,
                                   is_always_equal>
            ? (!propagate_on_container_move_assignment::value
                   ? true
                   : noexcept(lhs.__get_allocator() = std::move(rhs.__get_allocator())))
            : (noexcept(lhs.__destroy()) && noexcept(
                  lhs.__move_element(std::move(rhs))))) {
        if constexpr (std::disjunction_v<propagate_on_container_move_assignment,
                                         is_always_equal>) {
            lhs.__destroy_and_deallocate();
            if constexpr (propagate_on_container_move_assignment::value) {
                lhs.__get_allocator() = std::move(rhs.__get_allocator());
            }
            lhs.__take_storage(std::move(rhs));
        } else {
            if (lhs.__get_allocator() != rhs.__get_allocator()) {
                lhs.__destroy();
                lhs.__move_element(std::move(rhs));
            } else {
                lhs.__destroy_and_deallocate();
                lhs.__take_storage(std::move(rhs));
            }
        }
    }

    template <typename Container>
    WJR_CONSTEXPR20 static void swap(Container &lhs, Container &rhs) noexcept(
        noexcept(lhs.__swap_storage(rhs)) &&
                !std::conjunction_v<propagate_on_container_swap,
                                    std::negation<is_always_equal>>
            ? true
            : noexcept(std::swap(lhs.__get_allocator(), rhs.__get_allocator()))) {
        if constexpr (std::conjunction_v<propagate_on_container_swap,
                                         std::negation<is_always_equal>>) {
            auto &lhs_allocator = lhs.__get_allocator();
            auto &rhs_allocator = rhs.__get_allocator();
            if (lhs_allocator != rhs_allocator) {
                std::swap(lhs_allocator, rhs_allocator);
            }
        }

        lhs.__swap_storage(rhs);
    }
};

} // namespace wjr

#endif // WJR_CONTAINER_GENERIC_CONTAINER_TRAITS_HPP__
#ifndef WJR_ITERATOR_CONTIGUOUS_ITERATOR_ADAPTER_HPP__
#define WJR_ITERATOR_CONTIGUOUS_ITERATOR_ADAPTER_HPP__

// Already included
#ifndef WJR_MEMORY_TO_ADDRESS_HPP__
#define WJR_MEMORY_TO_ADDRESS_HPP__

#include <cstring>

#ifndef WJR_ITERATOR_DETAILS_HPP__
#define WJR_ITERATOR_DETAILS_HPP__

#include <array>
#include <string>
#include <vector>

// Already included

namespace wjr {

template <typename Iter>
using iterator_difference_t = typename std::iterator_traits<Iter>::difference_type;

template <typename Iter>
using iterator_value_t = typename std::iterator_traits<Iter>::value_type;

template <typename Iter>
using iterator_reference_t = typename std::iterator_traits<Iter>::reference;

template <typename Iter>
using iterator_pointer_t = typename std::iterator_traits<Iter>::pointer;

template <typename Iter>
using iterator_category_t = typename std::iterator_traits<Iter>::iterator_category;

template <typename Iter, typename = void>
struct __is_iterator_impl : std::false_type {};

template <typename Iter>
struct __is_iterator_impl<
    Iter, std::void_t<typename std::iterator_traits<Iter>::iterator_category>>
    : std::true_type {};

template <typename Iter>
struct is_iterator : __is_iterator_impl<Iter> {};

template <typename Iter>
inline constexpr bool is_iterator_v = is_iterator<Iter>::value;

/// @private
template <typename Iter, typename Category, typename = void>
struct __is_category_iterator_impl : std::false_type {};

/// @private
template <typename Iter, typename Category>
struct __is_category_iterator_impl<
    Iter, Category, std::void_t<typename std::iterator_traits<Iter>::iterator_category>>
    : std::is_base_of<Category, iterator_category_t<Iter>> {};

template <typename Iter>
struct is_input_iterator : __is_category_iterator_impl<Iter, std::input_iterator_tag> {};

template <typename Iter>
inline constexpr bool is_input_iterator_v = is_input_iterator<Iter>::value;

template <typename Iter>
struct is_output_iterator : __is_category_iterator_impl<Iter, std::output_iterator_tag> {
};

template <typename Iter>
inline constexpr bool is_output_iterator_v = is_output_iterator<Iter>::value;

template <typename Iter>
struct is_forward_iterator
    : __is_category_iterator_impl<Iter, std::forward_iterator_tag> {};

template <typename Iter>
inline constexpr bool is_forward_iterator_v = is_forward_iterator<Iter>::value;

template <typename Iter>
struct is_bidirectional_iterator
    : __is_category_iterator_impl<Iter, std::bidirectional_iterator_tag> {};

template <typename Iter>
inline constexpr bool is_bidirectional_iterator_v =
    is_bidirectional_iterator<Iter>::value;

template <typename Iter>
struct is_random_access_iterator
    : __is_category_iterator_impl<Iter, std::random_access_iterator_tag> {};

template <typename Iter>
inline constexpr bool is_random_access_iterator_v =
    is_random_access_iterator<Iter>::value;

/// @private
template <typename Iter, typename = void>
struct __is_contiguous_iterator_impl
    : std::disjunction<std::is_pointer<Iter>, std::is_array<Iter>> {};

/// @private
template <typename Iter>
struct __is_contiguous_iterator_impl<std::move_iterator<Iter>, void>
    : std::conjunction<__is_contiguous_iterator_impl<Iter>,
                       std::is_trivial<iterator_value_t<Iter>>> {};

#if defined(WJR_CXX_20)
template <typename Iter>
struct is_contiguous_iterator : __is_contiguous_iterator_impl<Iter> {};

template <std::contiguous_iterator Iter>
struct is_contiguous_iterator<Iter> : std::true_type {};
#else
template <typename Iter>
struct is_contiguous_iterator : __is_contiguous_iterator_impl<Iter> {};
#endif

template <typename Iter>
inline constexpr bool is_contiguous_iterator_v = is_contiguous_iterator<Iter>::value;

template <typename Iter, WJR_REQUIRES(is_contiguous_iterator_v<Iter>)>
using iterator_contiguous_value_t = std::remove_reference_t<iterator_reference_t<Iter>>;

template <typename Iter, WJR_REQUIRES(is_contiguous_iterator_v<Iter>)>
using iterator_contiguous_pointer_t =
    std::add_pointer_t<iterator_contiguous_value_t<Iter>>;

#if WJR_DEBUG_LEVEL > 1
#define WJR_HAS_DEBUG_CONTIGUOUS_ITERATOR_CHECK WJR_HAS_DEF
#endif

} // namespace wjr

#endif // WJR_ITERATOR_DETAILS_HPP__

namespace wjr {

namespace {
WJR_REGISTER_HAS_TYPE(pointer_traits_to_address,
                      std::pointer_traits<Ptr>::to_address(std::declval<const Ptr &>()),
                      Ptr);
} // namespace

template <typename T>
constexpr T *to_address(T *p) noexcept {
    static_assert(!std::is_function_v<T>, "T cannot be a function.");
    return p;
}

/**
 * @details If std::pointer_traits<remove_cvref_t<Ptr>>::to_address(p) is valid, return
 * std::pointer_traits<remove_cvref_t<Ptr>>::to_address(p), otherwise return
 * to_address(p.operator->()).
 *
 * @tparam Ptr remove_cvref_t<Ptr> must be a @ref is_contiguous_iterator_v "continuous
 * iterator".
 */
template <typename Ptr, WJR_REQUIRES(is_contiguous_iterator_v<remove_cvref_t<Ptr>>)>
constexpr auto to_address(const Ptr &p) noexcept {
    if constexpr (has_pointer_traits_to_address_v<remove_cvref_t<Ptr>>) {
        return std::pointer_traits<remove_cvref_t<Ptr>>::to_address(p);
    } else {
        return (to_address)(p.operator->());
    }
}

/**
 * @tparam Iter std::move_iterator<Iter> must be a @ref is_contiguous_iterator_v
 * "continuous iterator".
 *
 * @return to_address(p.base()).
 *
 */
template <typename Iter, WJR_REQUIRES(is_contiguous_iterator_v<std::move_iterator<Iter>>)>
constexpr auto to_address(const std::move_iterator<Iter> &p) noexcept {
    return (to_address)(p.base());
}

/**
 * @brief Return to_address(p) if p is a contiguous iterator and contiguouse iterato check
 * is disabled, otherwise return p.
 *
 */
template <typename T>
constexpr decltype(auto) try_to_address(T &&t) noexcept {
#if !WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECK)
    if constexpr (is_contiguous_iterator_v<remove_cvref_t<T>>) {
        return (to_address)(std::forward<T>(t));
    } else {
#endif
        return std::forward<T>(t);
#if !WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECK)
    }
#endif
}

class __is_little_endian_helper {
    constexpr static std::uint32_t u4 = 1;
    constexpr static std::uint8_t u1 = static_cast<const std::uint8_t &>(u4);

public:
    constexpr static bool value = u1 != 0;
};

// constexpr endian
enum class endian {
    little = 0,
    big = 1,
    native = __is_little_endian_helper::value ? little : big
};

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR T fallback_byteswap(T x) noexcept {
    constexpr auto digits = std::numeric_limits<T>::digits;
    auto val = static_cast<uint_t<digits>>(x);
    if constexpr (digits == 8) {
        return val;
    } else if constexpr (digits == 16) {
        return (val >> 8) | (val << 8);
    } else if constexpr (digits == 32) {
        return ((val >> 24) & 0xff) | ((val >> 8) & 0xff00) | ((val << 8) & 0xff0000) |
               ((val << 24));
    } else if constexpr (digits == 64) {
        return ((val >> 56) & 0xff) | ((val >> 40) & 0xff00) | ((val >> 24) & 0xff0000) |
               ((val >> 8) & 0xff000000) | ((val << 8) & 0xff00000000) |
               ((val << 24) & 0xff0000000000) | ((val << 40) & 0xff000000000000) |
               ((val << 56));
    } else {
        static_assert(digits <= 64, "Unsupported bit width");
    }
}

#if WJR_HAS_BUILTIN(__builtin_bswap16)
#define WJR_HAS_BUILTIN_BYTESWAP WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(BYTESWAP)

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E T builtin_byteswap(T x) noexcept {
    constexpr auto digits = std::numeric_limits<T>::digits;
    auto val = static_cast<uint_t<digits>>(x);
    if constexpr (digits == 8) {
        return val;
    } else if constexpr (digits == 16) {
        return __builtin_bswap16(val);
    } else if constexpr (digits == 32) {
        return __builtin_bswap32(val);
    } else if constexpr (digits == 64) {
        return __builtin_bswap64(val);
    } else {
        static_assert(digits <= 64, "Unsupported bit width");
    }
}

#endif

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E T byteswap(T x, endian to = endian::little) noexcept {
    if (to == endian::native) {
        return x;
    }

#if WJR_HAS_BUILTIN(BYTESWAP)
    if (is_constant_evaluated() || WJR_BUILTIN_CONSTANT_P(x)) {
        return fallback_byteswap(x);
    }

    return builtin_byteswap(x);
#else
    return fallback_byteswap(x);
#endif
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_PURE WJR_INTRINSIC_INLINE T read_memory(const void *ptr,
                                            endian to = endian::little) noexcept {
    T x;
    std::memcpy(&x, ptr, sizeof(T));

    if (to != endian::native) {
        x = byteswap(x);
    }

    return x;
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_INLINE void write_memory(void *ptr, T x,
                                       endian to = endian::little) noexcept {
    if (to != endian::native) {
        x = byteswap(x);
    }

    std::memcpy(ptr, &x, sizeof(T));
}

template <class Pointer, class SizeType = std::size_t>
struct allocation_result {
    Pointer ptr;
    SizeType count;
};

namespace {
WJR_REGISTER_HAS_TYPE(
    allocate_at_least,
    std::declval<Allocator>().allocate_at_least(std::declval<SizeType>()), Allocator,
    SizeType);
}

template <typename Allocator, typename SizeType,
          typename Pointer = typename std::allocator_traits<Allocator>::pointer>
WJR_NODISCARD auto allocate_at_least(Allocator &alloc, SizeType count) {
    if constexpr (has_allocate_at_least_v<Allocator, SizeType>) {
        return alloc.allocate_at_least(count);
    } else {
        const auto ptr = std::allocator_traits<Allocator>::allocate(alloc, count);
        return allocation_result<decltype(ptr), SizeType>{ptr, count};
    }
}

} // namespace wjr

#endif // WJR_MEMORY_TO_ADDRESS_HPP__

namespace wjr {

template <typename Container, typename Traits>
struct contiguous_const_iterator_adapter {
    using __pointer = typename Traits::pointer;

public:
#if defined(WJR_CXX_20)
    using iterator_concept = std::contiguous_iterator_tag;
#endif
    using iterator_category = std::random_access_iterator_tag;
    using value_type = typename Traits::value_type;
    using difference_type = typename Traits::difference_type;
    using pointer = typename Traits::const_pointer;
    using reference = typename Traits::const_reference;

    WJR_CONSTEXPR20 contiguous_const_iterator_adapter() noexcept(
        std::is_nothrow_default_constructible_v<__pointer>) = default;

    WJR_CONSTEXPR20
    contiguous_const_iterator_adapter(__pointer ptr, const Container *container) noexcept(
        std::is_nothrow_copy_constructible_v<__pointer>)
        : m_ptr(ptr) {
        __set_container(container);
    }

    WJR_CONSTEXPR20
    contiguous_const_iterator_adapter(const contiguous_const_iterator_adapter &) noexcept(
        std::is_nothrow_copy_constructible_v<__pointer>) = default;
    WJR_CONSTEXPR20
    contiguous_const_iterator_adapter(contiguous_const_iterator_adapter &&) noexcept(
        std::is_nothrow_move_constructible_v<__pointer>) = default;
    WJR_CONSTEXPR20 contiguous_const_iterator_adapter &
    operator=(const contiguous_const_iterator_adapter &) noexcept(
        std::is_nothrow_copy_assignable_v<__pointer>) = default;
    WJR_CONSTEXPR20 contiguous_const_iterator_adapter &
    operator=(contiguous_const_iterator_adapter &&) noexcept(
        std::is_nothrow_move_assignable_v<__pointer>) = default;

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 pointer operator->() const noexcept {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECK)
        WJR_ASSERT_LX(m_container != nullptr,
                      "Can't dereference an value-initialized iterator.");
        WJR_ASSERT_LX(m_ptr != nullptr, "Can't dereference an invalid iterator.");
        WJR_ASSERT_LX(m_ptr >= __begin() && m_ptr < __end(),
                      "Can't dereference an out-of-range iterator.");
#endif
        return const_cast<pointer>(m_ptr);
    }

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 reference operator*() const noexcept {
        return *operator->();
    }

    WJR_CONSTEXPR20 contiguous_const_iterator_adapter &operator++() noexcept {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECK)
        WJR_ASSERT_LX(m_container != nullptr,
                      "Can't increment an value-initialized iterator.");
        WJR_ASSERT_LX(m_ptr != nullptr, "Can't increment an invalid iterator.");
        WJR_ASSERT_LX(m_ptr < __end(),
                      "Can't increment an iterator that is already at/after the end.");
#endif
        ++m_ptr;
        return *this;
    }

    WJR_CONSTEXPR20 contiguous_const_iterator_adapter operator++(int) noexcept {
        auto tmp = *this;
        ++*this;
        return tmp;
    }

    WJR_CONSTEXPR20 contiguous_const_iterator_adapter &operator--() noexcept {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECK)
        WJR_ASSERT_LX(m_container != nullptr,
                      "Can't decrement an value-initialized iterator.");
        WJR_ASSERT_LX(m_ptr != nullptr, "Can't decrement an invalid iterator.");
        WJR_ASSERT_LX(
            m_ptr > __begin(),
            "Can't decrement an iterator that is already at/before the beginning.");
#endif
        --m_ptr;
        return *this;
    }

    WJR_CONSTEXPR20 contiguous_const_iterator_adapter operator--(int) noexcept {
        auto tmp = *this;
        --*this;
        return tmp;
    }

    WJR_CONSTEXPR20 contiguous_const_iterator_adapter &
    operator+=(difference_type n) noexcept {
        __check_offset(n);
        m_ptr += n;
        return *this;
    }

    WJR_NODISCARD WJR_CONSTEXPR20 contiguous_const_iterator_adapter
    operator+(difference_type n) const noexcept {
        auto tmp = *this;
        return tmp += n;
    }

    WJR_NODISCARD friend WJR_CONSTEXPR20 contiguous_const_iterator_adapter
    operator+(difference_type n, const contiguous_const_iterator_adapter &rhs) noexcept {
        return rhs + n;
    }

    WJR_CONSTEXPR20 contiguous_const_iterator_adapter &
    operator-=(difference_type n) noexcept {
        __check_offset(-n);
        m_ptr -= n;
        return *this;
    }

    WJR_NODISCARD WJR_CONSTEXPR20 contiguous_const_iterator_adapter
    operator-(difference_type n) const noexcept {
        auto tmp = *this;
        return tmp -= n;
    }

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 difference_type
    operator-(const contiguous_const_iterator_adapter &rhs) const noexcept {
        __check_same_container(rhs);
        return m_ptr - rhs.m_ptr;
    }

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 reference
    operator[](difference_type n) const noexcept {
        return *(*this + n);
    }

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 bool
    operator==(const contiguous_const_iterator_adapter &rhs) const noexcept {
        __check_same_container(rhs);
        return m_ptr == rhs.m_ptr;
    }

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 bool
    operator!=(const contiguous_const_iterator_adapter &rhs) const noexcept {
        return !(*this == rhs);
    }

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 bool
    operator<(const contiguous_const_iterator_adapter &rhs) const noexcept {
        __check_same_container(rhs);
        return m_ptr < rhs.m_ptr;
    }

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 bool
    operator>(const contiguous_const_iterator_adapter &rhs) const noexcept {
        return rhs < *this;
    }

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 bool
    operator<=(const contiguous_const_iterator_adapter &rhs) const noexcept {
        return !(rhs < *this);
    }

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 bool
    operator>=(const contiguous_const_iterator_adapter &rhs) const noexcept {
        return !(*this < rhs);
    }

    WJR_CONSTEXPR20 void
    check_same_container(WJR_MAYBE_UNUSED const Container *cont) const noexcept {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECK)
        WJR_ASSERT_LX(m_container == cont,
                      "Can't compare iterators from different containers.");
#else
        (void)(cont);
#endif
    }

#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECK)
    /// @private
    WJR_CONSTEXPR20 void __set_container(const Container *container) noexcept {
        m_container = container;
    }

    /// @private
    WJR_CONSTEXPR20 void __check_offset(difference_type offset) const noexcept {
        if (offset == 0) {
            return;
        }
        WJR_ASSERT_LX(m_container != nullptr,
                      "Can't seek an value-initialized iterator.");
        WJR_ASSERT_LX(m_ptr != nullptr, "Can't seek an invalid iterator.");
        if (offset < 0) {
            WJR_ASSERT_LX(offset >= __begin() - m_ptr,
                          "Can't seek an iterator that before the beginning.");
        } else {
            WJR_ASSERT_LX(offset <= __end() - m_ptr,
                          "Can't seek an iterator that after the end.");
        }
    }

    /// @private
    WJR_CONSTEXPR20 void
    __check_same_container(const contiguous_const_iterator_adapter &rhs) const noexcept {
        WJR_ASSERT_LX(m_container == rhs.m_container,
                      "Can't compare iterators from different containers.");
    }

    /// @private
    WJR_PURE WJR_CONSTEXPR20 pointer __begin() const noexcept {
        return m_container->data();
    }

    /// @private
    WJR_PURE WJR_CONSTEXPR20 pointer __end() const noexcept {
        return m_container->data() + m_container->size();
    }
#else
    /// @private
    constexpr static void __set_container(const Container *) noexcept {}

    /// @private
    constexpr static void __check_offset(difference_type) noexcept {}

    /// @private
    constexpr static void
    __check_same_container(const contiguous_const_iterator_adapter &) noexcept {}
#endif

    __pointer m_ptr;
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECK)
    const Container *m_container;
#endif
};

template <typename Container, typename Traits>
class contiguous_iterator_adapter
    : public contiguous_const_iterator_adapter<Container, Traits> {
    using Mybase = contiguous_const_iterator_adapter<Container, Traits>;
    using __pointer = typename Traits::pointer;

public:
#if defined(WJR_CXX_20)
    using iterator_concept = typename Mybase::iterator_concept;
#endif
    using iterator_category = typename Mybase::iterator_category;
    using value_type = typename Mybase::value_type;
    using difference_type = typename Mybase::difference_type;
    using pointer = typename Traits::pointer;
    using reference = typename Traits::reference;

    using Mybase::Mybase;

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 pointer operator->() const noexcept {
        return const_cast<pointer>(Mybase::operator->());
    }

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 reference operator*() const noexcept {
        return *operator->();
    }

    WJR_CONSTEXPR20 contiguous_iterator_adapter &operator++() noexcept {
        Mybase::operator++();
        return *this;
    }

    WJR_CONSTEXPR20 contiguous_iterator_adapter operator++(int) noexcept {
        auto tmp = *this;
        ++*this;
        return tmp;
    }

    WJR_CONSTEXPR20 contiguous_iterator_adapter &operator--() noexcept {
        Mybase::operator--();
        return *this;
    }

    WJR_CONSTEXPR20 contiguous_iterator_adapter operator--(int) noexcept {
        auto tmp = *this;
        --*this;
        return tmp;
    }

    WJR_CONSTEXPR20 contiguous_iterator_adapter &operator+=(difference_type n) noexcept {
        Mybase::operator+=(n);
        return *this;
    }

    WJR_NODISCARD WJR_CONSTEXPR20 contiguous_iterator_adapter
    operator+(difference_type n) const noexcept {
        auto tmp = *this;
        return tmp += n;
    }

    WJR_NODISCARD friend WJR_CONSTEXPR20 contiguous_iterator_adapter
    operator+(difference_type n, const contiguous_iterator_adapter &rhs) noexcept {
        return rhs + n;
    }

    WJR_CONSTEXPR20 contiguous_iterator_adapter &operator-=(difference_type n) noexcept {
        Mybase::operator-=(n);
        return *this;
    }

    WJR_NODISCARD WJR_CONSTEXPR20 contiguous_iterator_adapter
    operator-(difference_type n) const noexcept {
        auto tmp = *this;
        return tmp -= n;
    }

    using Mybase::operator-;

    WJR_NODISCARD WJR_PURE WJR_CONSTEXPR20 reference
    operator[](difference_type n) const noexcept {
        return *(*this + n);
    }

    using Mybase::check_same_container;
};

template <typename Container, typename Traits>
struct __is_contiguous_iterator_impl<contiguous_const_iterator_adapter<Container, Traits>>
    : std::true_type {};

template <typename Container, typename Traits>
struct __is_contiguous_iterator_impl<contiguous_iterator_adapter<Container, Traits>>
    : std::true_type {};

} // namespace wjr

namespace std {

template <typename Container, typename Traits>
struct pointer_traits<wjr::contiguous_const_iterator_adapter<Container, Traits>> {
    using pointer = wjr::contiguous_const_iterator_adapter<Container, Traits>;
    using element_type = const typename pointer::value_type;
    using difference_type = typename pointer::difference_type;

    WJR_NODISCARD constexpr static element_type *to_address(const pointer &ptr) noexcept {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECK)
        auto cont = ptr.m_container;
        if (cont) {
            WJR_ASSERT_LX(ptr.m_ptr >= ptr.__begin() && ptr.m_ptr <= ptr.__end(),
                          "can't convert out-of-range vector iterator to pointer");
        } else {
            WJR_ASSERT_LX(ptr.m_ptr == nullptr,
                          "can't convert invalid vector iterator to pointer");
        }
#endif
        return wjr::to_address(ptr.m_ptr);
    }
};

template <typename Container, typename Traits>
struct pointer_traits<wjr::contiguous_iterator_adapter<Container, Traits>> {
    using pointer = wjr::contiguous_iterator_adapter<Container, Traits>;
    using element_type = typename pointer::value_type;
    using difference_type = typename pointer::difference_type;

    WJR_NODISCARD constexpr static element_type *to_address(const pointer &ptr) noexcept {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECK)
        auto cont = ptr.m_container;
        if (cont) {
            WJR_ASSERT_LX(ptr.m_ptr >= ptr.__begin() && ptr.m_ptr <= ptr.__end(),
                          "can't convert out-of-range vector iterator to pointer");
        } else {
            WJR_ASSERT_LX(ptr.m_ptr == nullptr,
                          "can't convert invalid vector iterator to pointer");
        }
#endif
        return wjr::to_address(ptr.m_ptr);
    }
};

} // namespace std

#endif // WJR_ITERATOR_CONTIGUOUS_ITERATOR_ADAPTER_HPP__
#ifndef WJR_MATH_DETAILS_HPP__
#define WJR_MATH_DETAILS_HPP__

// Already included

namespace wjr {

namespace math_details {

template <typename T, T seed>
class de_bruijn {
public:
    constexpr static uint8_t digits = std::numeric_limits<T>::digits;
    constexpr static uint8_t mv = digits == 32 ? 27 : 58;
    constexpr de_bruijn() : lookup(), lookupr() { initialize(); }

    WJR_INTRINSIC_CONSTEXPR int get(T idx) const { return lookup[(idx * seed) >> mv]; }
    WJR_INTRINSIC_CONSTEXPR int getr(T idx) const { return lookupr[(idx * seed) >> mv]; }

private:
    constexpr void initialize() {
        for (uint8_t i = 0; i < digits; ++i) {
            auto idx = (seed << i) >> mv;
            lookup[idx] = i;
            lookupr[idx] = i == 0 ? 0 : digits - i;
        }
    }

    uint8_t lookup[digits];
    uint8_t lookupr[digits];
};

inline constexpr de_bruijn<uint32_t, 0x077C'B531> de_bruijn32 = {};
inline constexpr de_bruijn<uint64_t, 0x03f7'9d71'b4ca'8b09> de_bruijn64 = {};

} // namespace math_details

// preview ...

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR bool is_zero_or_single_bit(T n) noexcept {
    return (n & (n - 1)) == 0;
}

/**
 * @brief
 *
 * @note `n & -n` is the lowest bit of n.
 */
template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR T lowbit(T n) noexcept {
    return n & -n;
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR T clear_lowbit(T n) noexcept {
    return n & (n - 1);
}

template <typename Value, WJR_REQUIRES(is_nonbool_integral_v<Value>)>
WJR_CONST constexpr decltype(auto) to_signed(Value value) noexcept {
    return static_cast<std::make_signed_t<Value>>(value);
}

template <typename Value, WJR_REQUIRES(is_nonbool_integral_v<Value>)>
WJR_CONST constexpr decltype(auto) to_unsigned(Value value) noexcept {
    return static_cast<std::make_unsigned_t<Value>>(value);
}

// preview :

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR bool __has_high_bit(T n) noexcept {
    return n >> (std::numeric_limits<T>::digits - 1);
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR T __ceil_div(T n, type_identity_t<T> div) {
    return (n + div - 1) / div;
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR T __align_down(T n, type_identity_t<T> alignment) {
    WJR_ASSERT_ASSUME_L1(is_zero_or_single_bit(alignment));
    return n & (-alignment);
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR T __align_down_offset(T n,
                                                        type_identity_t<T> alignment) {
    WJR_ASSERT_ASSUME_L1(is_zero_or_single_bit(alignment));
    return n & (alignment - 1);
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR T __align_up(T n, type_identity_t<T> alignment) {
    WJR_ASSERT_ASSUME_L1(is_zero_or_single_bit(alignment));
    return (n + alignment - 1) & (-alignment);
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR T __align_up_offset(T n, type_identity_t<T> alignment) {
    WJR_ASSERT_ASSUME_L1(is_zero_or_single_bit(alignment));
    return (-n) & (alignment - 1);
}

template <typename T, typename U = std::make_unsigned_t<T>,
          WJR_REQUIRES(is_nonbool_integral_v<T>)>
WJR_CONST constexpr U __fasts_sign_mask() {
    return (U)(1) << (std::numeric_limits<U>::digits - 1);
}

template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
WJR_CONST constexpr T __fasts_get_sign_mask(T x) {
    return x & __fasts_sign_mask<T>();
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST constexpr std::make_signed_t<T> __fasts_from_unsigned(T x) {
    std::make_signed_t<T> ret = x;
    WJR_ASSERT_ASSUME_L1(ret >= 0, "overflow");
    return ret;
}

template <typename T, typename U = std::make_unsigned_t<T>,
          WJR_REQUIRES(is_nonbool_signed_integral_v<T>)>
WJR_CONST constexpr U __fasts_abs(T x) {
    return static_cast<U>(x < 0 ? -x : x);
}

template <typename T, WJR_REQUIRES(is_nonbool_signed_integral_v<T>)>
WJR_CONST constexpr T __fasts_negate(T x) {
    return -x;
}

template <typename T, typename U = std::make_unsigned_t<T>,
          WJR_REQUIRES(is_nonbool_signed_integral_v<T>)>
WJR_CONST constexpr T __fasts_conditional_negate(bool condition, T x) {
    return condition ? -x : x;
}

template <typename T, typename U = std::make_unsigned_t<T>,
          WJR_REQUIRES(is_nonbool_signed_integral_v<T>)>
WJR_CONST constexpr T __fasts_negate_with(T condition, T x) {
    return __fasts_conditional_negate(condition < 0, x);
}

template <typename T, WJR_REQUIRES(is_nonbool_signed_integral_v<T>)>
WJR_CONST constexpr T __fasts_increment(T x) {
    WJR_ASSERT_L1(x != std::numeric_limits<T>::min() &&
                      x != std::numeric_limits<T>::max(),
                  "overflow");

    return x < 0 ? x - 1 : x + 1;
}

template <typename T, WJR_REQUIRES(is_nonbool_signed_integral_v<T>)>
WJR_CONST constexpr T __fasts_decrement(T x) {
    WJR_ASSERT_L1(x != 0 && x + 1 != T(0), "overflow");

    return x < 0 ? x + 1 : x - 1;
}

template <typename T, WJR_REQUIRES(is_nonbool_signed_integral_v<T>)>
WJR_CONST constexpr T __fasts_add(T x, std::make_unsigned_t<T> y) {
    return x < 0 ? x - y : x + y;
}

template <typename T, WJR_REQUIRES(is_nonbool_signed_integral_v<T>)>
WJR_CONST constexpr T __fasts_sub(T x, std::make_unsigned_t<T> y) {
    return x < 0 ? x + y : x - y;
}

} // namespace wjr

#endif // WJR_MATH_DETAILS_HPP__
#ifndef WJR_MEMORY_COPY_HPP__
#define WJR_MEMORY_COPY_HPP__

#ifndef WJR_CONTAINER_GENERIC_DETAILS_HPP__
#define WJR_CONTAINER_GENERIC_DETAILS_HPP__

#include <string>

// Already included

namespace wjr {

namespace {

WJR_REGISTER_HAS_TYPE(container_begin,
                      std::begin(std::declval<std::add_lvalue_reference_t<Container>>()),
                      Container);
WJR_REGISTER_HAS_TYPE(container_cbegin,
                      std::cbegin(std::declval<std::add_lvalue_reference_t<Container>>()),
                      Container);
WJR_REGISTER_HAS_TYPE(container_end,
                      std::end(std::declval<std::add_lvalue_reference_t<Container>>()),
                      Container);
WJR_REGISTER_HAS_TYPE(container_cend,
                      std::cend(std::declval<std::add_lvalue_reference_t<Container>>()),
                      Container);
WJR_REGISTER_HAS_TYPE(container_size,
                      std::size(std::declval<std::add_lvalue_reference_t<Container>>()),
                      Container);

WJR_REGISTER_HAS_TYPE(__container_resize,
                      std::declval<std::add_lvalue_reference_t<Container>>().resize(
                          std::declval<Size>(), std::declval<Args>()...),
                      Container, Size);
WJR_REGISTER_HAS_TYPE(__container_append,
                      std::declval<Container>().append(std::declval<Args>()...),
                      Container);

} // namespace

template <typename Container>
struct resize_fn_impl_base {
    template <typename... Args,
              WJR_REQUIRES(has___container_resize_v<Container, Args...>)>
    WJR_INTRINSIC_INLINE static void
    resize(Container &cont,
           Args &&...args) noexcept(noexcept(cont.resize(std::forward<Args>(args)...))) {
        cont.resize(std::forward<Args>(args)...);
    }
};

template <typename Container>
struct resize_fn_impl : resize_fn_impl_base<Container> {};

struct resize_fn {
    template <typename Container, typename... Args>
    void operator()(Container &cont, Args &&...args) const noexcept(
        noexcept(resize_fn_impl<Container>::resize(cont, std::forward<Args>(args)...))) {
        resize_fn_impl<Container>::resize(cont, std::forward<Args>(args)...);
    }
};

inline constexpr resize_fn resize{};

template <typename Container>
struct append_fn_impl_base {
    template <typename... Args,
              WJR_REQUIRES(has___container_append_v<Container, Args...>)>
    WJR_INTRINSIC_INLINE static void
    append(Container &cont,
           Args &&...args) noexcept(noexcept(cont.append(std::forward<Args>(args)...))) {
        cont.append(std::forward<Args>(args)...);
    }
};

template <typename Container>
struct append_fn_impl : append_fn_impl_base<Container> {};

struct append_fn {
    template <typename Container, typename... Args>
    void operator()(Container &cont, Args &&...args) const noexcept(
        noexcept(append_fn_impl<Container>::append(cont, std::forward<Args>(args)...))) {
        append_fn_impl<Container>::append(cont, std::forward<Args>(args)...);
    }
};

inline constexpr append_fn append{};

#define WJR_HAS_FEATURE_STRING_UNINITIALIZED_RESIZE WJR_HAS_DEF

#if __cpp_lib_string_resize_and_overwrite >= 202110L
template <typename CharT, typename Traits, typename Alloc>
WJR_INTRINSIC_INLINE void
__uninitialized_resize(std::basic_string<CharT, Traits, Alloc> &str,
                       typename std::basic_string<CharT, Traits, Alloc>::size_type sz) {
    str.resize_and_overwrite(sz, [](char *, Size sz) { return sz; });
}

#define __WJR_REGISTER_STRING_UNINITIALIZED_RESIZE_TEMPLATE(NAME, Container)

#elif (defined(__clang_major__) && __clang_major__ <= 11) ||                             \
    (defined(_MSC_VER) && _MSC_VER <= 1920)
#undef WJR_HAS_FEATURE_STRING_UNINITIALIZED_RESIZE
#elif defined(__GLIBCXX__) || defined(_LIBCPP_VERSION) || defined(_MSVC_STL_VERSION)

template <typename Container>
void string_set_length_hacker(Container &bank, typename Container::size_type sz);

#if defined(__GLIBCXX__) || defined(_LIBCPP_VERSION)
#define __WJR_REGISTER_STRING_UNINITIALIZED_RESIZE_CLASS(NAME, Container)                \
    inline void WJR_PP_CONCAT(string_set_length_hacker_of_,                              \
                              NAME)(Container & bank, typename Container::size_type sz); \
    template <typename Money_t, Money_t Container::*p>                                   \
    class WJR_PP_CONCAT(string_thief_of_, NAME) {                                        \
    public:                                                                              \
        friend void WJR_PP_CONCAT(string_set_length_hacker_of_,                          \
                                  NAME)(Container & bank,                                \
                                        typename Container::size_type sz) {              \
            (bank.*p)(sz);                                                               \
        }                                                                                \
    };                                                                                   \
    template <>                                                                          \
    inline void string_set_length_hacker<Container>(Container & bank,                    \
                                                    typename Container::size_type sz) {  \
        WJR_PP_CONCAT(string_set_length_hacker_of_, NAME)(bank, sz);                     \
    }
#else
#define __WJR_REGISTER_STRING_UNINITIALIZED_RESIZE_CLASS(NAME, Container)                \
    inline void WJR_PP_CONCAT(string_set_length_hacker_of_,                              \
                              NAME)(Container & bank, typename Container::size_type sz); \
    template <typename Money_t, Money_t Container::*p>                                   \
    class WJR_PP_CONCAT(string_thief_of_, NAME) {                                        \
    public:                                                                              \
        friend void WJR_PP_CONCAT(string_set_length_hacker_of_,                          \
                                  NAME)(Container & bank,                                \
                                        typename Container::size_type sz) {              \
            (bank.*p)._Myval2._Mysize = sz;                                              \
        }                                                                                \
    };                                                                                   \
    template <>                                                                          \
    inline void string_set_length_hacker<Container>(Container & bank,                    \
                                                    typename Container::size_type sz) {  \
        WJR_PP_CONCAT(string_set_length_hacker_of_, NAME)(bank, sz);                     \
    }
#endif

#if defined(__GLIBCXX__)
#define __WJR_REGISTER_STRING_UNINITIALIZED_RESIZE_TEMPLATE(NAME, Container)             \
    __WJR_REGISTER_STRING_UNINITIALIZED_RESIZE_CLASS(NAME, Container);                   \
    template class WJR_PP_CONCAT(                                                        \
        string_thief_of_, NAME)<void(Container::size_type), &Container::_M_set_length>
#elif defined(_LIBCPP_VERSION)
#define __WJR_REGISTER_STRING_UNINITIALIZED_RESIZE_TEMPLATE(NAME, Container)             \
    __WJR_REGISTER_STRING_UNINITIALIZED_RESIZE_CLASS(NAME, Container);                   \
    template class WJR_PP_CONCAT(                                                        \
        string_thief_of_, NAME)<void(Container::size_type), &Container::__set_size>
#else
#define __WJR_REGISTER_STRING_UNINITIALIZED_RESIZE_TEMPLATE(NAME, Container)             \
    __WJR_REGISTER_STRING_UNINITIALIZED_RESIZE_CLASS(NAME, Container);                   \
    template class WJR_PP_CONCAT(                                                        \
        string_thief_of_, NAME)<decltype(Container::_Mypair), &Container::_Mypair>
#endif

template <typename CharT, typename Traits, typename Alloc>
WJR_INTRINSIC_INLINE void
__uninitialized_resize(std::basic_string<CharT, Traits, Alloc> &str,
                       typename std::basic_string<CharT, Traits, Alloc>::size_type sz) {
    str.reserve(sz);
    string_set_length_hacker(str, sz);
    WJR_ASSERT_L1(str.size() == sz);
    str[sz] = '\0';
}

#else
#undef WJR_HAS_FEATURE_STRING_UNINITIALIZED_RESIZE
#define WJR_REGISTER_STRING_UNINITIALIZED_RESIZE(Container)
#endif

#if WJR_HAS_FEATURE(STRING_UNINITIALIZED_RESIZE)

#define WJR_REGISTER_STRING_UNINITIALIZED_RESIZE(Name, Container)                        \
    __WJR_REGISTER_STRING_UNINITIALIZED_RESIZE_TEMPLATE(Name, Container);                \
    template <>                                                                          \
    struct resize_fn_impl<Container> : resize_fn_impl_base<Container> {                  \
        using resize_fn_impl_base<Container>::resize;                                    \
        WJR_INTRINSIC_INLINE static void                                                 \
        resize(Container &cont, typename Container::size_type sz, dctor_t) {             \
            __uninitialized_resize(cont, sz);                                            \
        }                                                                                \
    };                                                                                   \
    template <>                                                                          \
    struct append_fn_impl<Container> : append_fn_impl_base<Container> {                  \
        using append_fn_impl_base<Container>::append;                                    \
        WJR_INTRINSIC_INLINE static void                                                 \
        append(Container &cont, typename Container::size_type sz, dctor_t) {             \
            __uninitialized_resize(cont, cont.size() + sz);                              \
        }                                                                                \
    }
#else
#define WJR_REGISTER_STRING_UNINITIALIZED_RESIZE(Container)
#endif

WJR_REGISTER_STRING_UNINITIALIZED_RESIZE(string, std::string);

namespace {

WJR_REGISTER_HAS_TYPE(container_resize,
                      resize_fn_impl<Container>::resize(std::declval<Container &>(),
                                                        std::declval<Size>(),
                                                        std::declval<Args>()...),
                      Container, Size);
WJR_REGISTER_HAS_TYPE(container_reserve,
                      std::declval<Container>().reserve(std::declval<Size>()), Container,
                      Size);
WJR_REGISTER_HAS_TYPE(container_append,
                      append_fn_impl<Container>::append(std::declval<Container &>(),
                                                        std::declval<Args>()...),
                      Container);
WJR_REGISTER_HAS_TYPE(container_insert,
                      (std::declval<Container>().insert(
                           std::declval<Container>().cbegin(), std::declval<Args>()...),
                       std::declval<Container>().insert(std::declval<Container>().cend(),
                                                        std::declval<Args>()...)),
                      Container);

} // namespace

template <typename Container, typename Size,
          WJR_REQUIRES(has_container_resize_v<Container, Size>)>
WJR_INTRINSIC_INLINE void try_uninitialized_resize(Container &cont, Size sz) {
    if constexpr (has_container_resize_v<Container, Size, dctor_t>) {
        resize(cont, sz, dctor);
    } else {
        resize(cont, sz);
    }
}

template <typename T, typename = void>
struct __container_traits_base_iterator_helper {
    using iterator = T;
};

template <typename T>
struct __container_traits_base_iterator_helper<T, std::void_t<typename T::iterator>> {
    using iterator = typename T::iterator;
};

template <typename T, typename = void>
struct __container_traits_base_size_type_helper {
    using size_type = size_t;
};

template <typename T>
struct __container_traits_base_size_type_helper<T, std::void_t<typename T::size_type>> {
    using size_type = typename T::size_type;
};

template <typename Container>
struct __container_traits_base {
private:
    using iterator =
        typename __container_traits_base_iterator_helper<Container>::iterator;
    using size_type =
        typename __container_traits_base_size_type_helper<Container>::size_type;

public:
    constexpr static bool is_contiguous_v = is_contiguous_iterator_v<iterator>;

    /**
     * @details Trivially contiguous means that the container can be resized and then
     * filled, and the result should be consistent with the element by element push_back
     * result. It does not verify whether the element is trial. Because different
     * containers may have different ways of constructing elements. The main purpose is
     * for types like std::basic_string<CharT, Traits, Alloc>, and for unknown
     * Traits, it should not be assumed that filling after resizing yields the same
     * result as using Traits::copy.
     *
     */
    constexpr static bool is_trivially_contiguous_v = false;
};

template <typename Container>
struct container_traits;

} // namespace wjr

#endif // WJR_CONTAINER_GENERIC_DETAILS_HPP__
#ifndef WJR_ITERATOR_INSERTER_HPP__
#define WJR_ITERATOR_INSERTER_HPP__

// Already included

namespace wjr {

template <typename T>
struct is_insert_iterator : std::false_type {};

template <typename Container>
struct is_insert_iterator<std::insert_iterator<Container>> : std::true_type {};

template <typename T>
inline constexpr bool is_insert_iterator_v = is_insert_iterator<T>::value;

template <typename T>
struct is_back_insert_iterator : std::false_type {};

template <typename Container>
struct is_back_insert_iterator<std::back_insert_iterator<Container>> : std::true_type {};

template <typename T>
inline constexpr bool is_back_insert_iterator_v = is_back_insert_iterator<T>::value;

template <typename T>
struct is_front_insert_iterator : std::false_type {};

template <typename Container>
struct is_front_insert_iterator<std::front_insert_iterator<Container>> : std::true_type {
};

template <typename T>
inline constexpr bool is_front_insert_iterator_v = is_front_insert_iterator<T>::value;

template <typename T>
struct is_any_insert_iterator
    : std::bool_constant<is_insert_iterator_v<T> || is_back_insert_iterator_v<T> ||
                         is_front_insert_iterator_v<T>> {};

template <typename T>
inline constexpr bool is_any_insert_iterator_v = is_any_insert_iterator<T>::value;

/// @private
template <typename Iter>
struct __inserter_container_accessor : Iter {
    __inserter_container_accessor(Iter it) noexcept(
        std::is_nothrow_copy_constructible_v<Iter>)
        : Iter(it) {}
    using Iter::container;
};

/// @private
template <typename Iter>
struct __inserter_iterator_accessor : Iter {
    __inserter_iterator_accessor(Iter it) noexcept(
        std::is_nothrow_copy_constructible_v<Iter>)
        : Iter(it) {}
    using Iter::iter;
};

template <typename Container>
Container &get_inserter_container(std::insert_iterator<Container> it) noexcept(
    std::is_nothrow_copy_constructible_v<std::insert_iterator<Container>>) {
    return *(__inserter_container_accessor(it).container);
}

template <typename Container>
Container &get_inserter_container(std::back_insert_iterator<Container> it) noexcept(
    std::is_nothrow_copy_constructible_v<std::back_insert_iterator<Container>>) {
    return *(__inserter_container_accessor(it).container);
}

template <typename Container>
Container &get_inserter_container(std::front_insert_iterator<Container> it) noexcept(
    std::is_nothrow_copy_constructible_v<std::front_insert_iterator<Container>>) {
    return *(__inserter_container_accessor(it).container);
}

template <typename Container>
typename Container::iterator
get_inserter_iterator(std::insert_iterator<Container> it) noexcept(
    std::is_nothrow_copy_constructible_v<std::insert_iterator<Container>>) {
    return __inserter_iterator_accessor(it).iter;
}

} // namespace wjr

#endif // WJR_ITERATOR_INSERTER_HPP__
// Already included

namespace wjr {

/**
 * @fn copy
 *
 * @details Optimized for back_insert_iterator and insert_iterator.
 *
 */
template <typename InputIt, typename OutputIt>
constexpr OutputIt copy(InputIt first, InputIt last, OutputIt d_first) {
    using Out = remove_cvref_t<OutputIt>;

    if constexpr (is_back_insert_iterator_v<Out> || is_insert_iterator_v<Out>) {
        using Container = typename Out::container_type;

        if constexpr (is_back_insert_iterator_v<Out>) {
            if constexpr (has_container_append_v<Container, InputIt, InputIt>) {
                append(get_inserter_container(d_first), first, last);
                return d_first;
            } else if constexpr (has_container_insert_v<Container, InputIt, InputIt>) {
                auto &cont = get_inserter_container(d_first);
                cont.insert(cont.cend(), first, last);
                return d_first;
            } else {
                return std::copy(first, last, d_first);
            }
        } else {
            if constexpr (has_container_insert_v<Container, InputIt, InputIt>) {
                auto &cont = get_inserter_container(d_first);
                const auto pos = get_inserter_iterator(d_first);
                cont.insert(pos, first, last);
                return d_first;
            } else {
                return std::copy(first, last, d_first);
            }
        }
    } else {
        return std::copy(first, last, d_first);
    }
}

/// @private
template <typename InputIt, typename OutputIt>
constexpr OutputIt __copy_restrict_impl_aux(add_restrict_t<InputIt> first,
                                            add_restrict_t<InputIt> last,
                                            add_restrict_t<OutputIt> d_first) {
    return wjr::copy(first, last, d_first);
}

/// @private
template <typename InputIt, typename OutputIt>
constexpr OutputIt __copy_restrict_impl(InputIt first, InputIt last, OutputIt d_first) {
    return __copy_restrict_impl_aux<InputIt, OutputIt>(first, last, d_first);
}

/**
 * @brief Copy elements from a range to another range with restricted pointers.
 *
 * @details Use @ref wjr::copy. \n
 * If iterator is contiguouse, then get restricted pointer
 * by iterator to optimize.
 */
template <typename InputIt, typename OutputIt>
constexpr OutputIt copy_restrict(InputIt first, InputIt last, OutputIt d_first) {
    const auto __first = try_to_address(std::move(first));
    const auto __last = try_to_address(std::move(last));
    if constexpr (is_contiguous_iterator_v<OutputIt>) {
        const auto __d_first = (to_address)(d_first);
        const auto __d_last = __copy_restrict_impl(__first, __last, __d_first);
        return std::next(d_first, std::distance(__d_first, __d_last));
    } else {
        return __copy_restrict_impl(__first, __last, d_first);
    }
}

/**
 * @fn wjr::copy_n
 *
 * @details Optimized for back_insert_iterator and insert_iterator.
 *
 */
template <typename InputIt, typename Size, typename OutputIt>
constexpr OutputIt copy_n(InputIt first, Size count, OutputIt d_first) {
    using Out = remove_cvref_t<OutputIt>;

    if constexpr (is_random_access_iterator_v<InputIt> &&
                  (is_back_insert_iterator_v<Out> || is_insert_iterator_v<Out>)) {
        using Container = typename Out::container_type;

        if constexpr (is_back_insert_iterator_v<Out>) {
            if constexpr (has_container_append_v<Container, InputIt, InputIt>) {
                append(get_inserter_container(d_first), first, std::next(first, count));
                return d_first;
            } else if constexpr (has_container_insert_v<Container, InputIt, InputIt>) {
                auto &cont = get_inserter_container(d_first);
                cont.insert(cont.cend(), first, std::next(first, count));
                return d_first;
            } else {
                return std::copy_n(first, count, d_first);
            }
        } else {
            if constexpr (has_container_insert_v<Container, InputIt, InputIt>) {
                auto &cont = get_inserter_container(d_first);
                auto pos = get_inserter_iterator(d_first);
                cont.insert(pos, first, std::next(first, count));
                return d_first;
            } else {
                return std::copy_n(first, std::next(first, count), d_first);
            }
        }
    } else {
        return std::copy_n(first, count, d_first);
    }
}

/// @private
template <typename InputIt, typename Size, typename OutputIt>
constexpr OutputIt __copy_n_restrict_impl_aux(add_restrict_t<InputIt> first, Size count,
                                              add_restrict_t<OutputIt> d_first) {
    return wjr::copy_n(first, count, d_first);
}

/// @private
template <typename InputIt, typename Size, typename OutputIt>
constexpr OutputIt __copy_n_restrict_impl(InputIt first, Size count, OutputIt d_first) {
    return __copy_n_restrict_impl_aux<InputIt, Size, OutputIt>(first, count, d_first);
}

/**
 * @brief Copy elements from a range to another range with restricted pointers.
 *
 * @details @see wjr::copy_restrict. \n
 *
 */
template <typename InputIt, typename Size, typename OutputIt>
constexpr OutputIt copy_n_restrict(InputIt first, Size count, OutputIt d_first) {
    const auto __first = try_to_address(std::move(first));
    if constexpr (is_contiguous_iterator_v<OutputIt>) {
        const auto __d_first = (to_address)(d_first);
        const auto __d_last = __copy_n_restrict_impl(__first, count, __d_first);
        return std::next(d_first, std::distance(__d_first, __d_last));
    } else {
        return __copy_n_restrict_impl(__first, count, d_first);
    }
}

template <typename InputIt, typename OutputIt>
constexpr OutputIt move(InputIt first, InputIt last, OutputIt d_first) {
    return wjr::copy(std::make_move_iterator(first), std::make_move_iterator(last),
                     d_first);
}

template <typename InputIt, typename OutputIt>
constexpr OutputIt move_restrict(InputIt first, InputIt last, OutputIt d_first) {
    return wjr::copy_restrict(std::make_move_iterator(first),
                              std::make_move_iterator(last), d_first);
}

template <typename InputIt, typename Size, typename OutputIt>
constexpr OutputIt move_n(InputIt first, Size count, OutputIt d_first) {
    return wjr::copy_n(std::make_move_iterator(first), count, d_first);
}

template <typename InputIt, typename Size, typename OutputIt>
constexpr OutputIt move_n_restrict(InputIt first, Size count, OutputIt d_first) {
    return wjr::copy_n_restrict(std::make_move_iterator(first), count, d_first);
}

} // namespace wjr

#endif // WJR_MEMORY_COPY_HPP__
#ifndef WJR_MEMORY_TEMPORARY_VALUE_ALLOCATOR_HPP__
#define WJR_MEMORY_TEMPORARY_VALUE_ALLOCATOR_HPP__

#ifndef WJR_MEMORY_UNINITIALIZED_HPP__
#define WJR_MEMORY_UNINITIALIZED_HPP__

/**
 * @file uninitialized.hpp
 * @brief The header file for uninitialized memory operations using allocator.
 *
 * @version 0.0.1
 * @date 2024-03-18
 *
 */

// Already included
#ifndef WJR_CRTP_TRIVIALLY_ALLOCATOR_BASE_HPP__
#define WJR_CRTP_TRIVIALLY_ALLOCATOR_BASE_HPP__

// Already included

namespace wjr {

namespace {
WJR_REGISTER_HAS_TYPE(is_trivially_allocator,
                      std::declval<typename Alloc::is_trivially_allocator>(), Alloc);
WJR_REGISTER_HAS_TYPE(
    is_trivially_allocator_constructible,
    std::declval<typename Alloc::is_trivially_allocator_constructible>(), Alloc);
WJR_REGISTER_HAS_TYPE(is_trivially_allocator_destructible,
                      std::declval<typename Alloc::is_trivially_allocator_destructible>(),
                      Alloc);
} // namespace

/// @private
template <typename Alloc, typename = void>
struct __is_trivially_allocator_impl : std::false_type {};

/// @private
template <typename Alloc>
struct __is_trivially_allocator_impl<
    Alloc, std::enable_if_t<has_is_trivially_allocator_v<Alloc>>>
    : Alloc::is_trivially_allocator {};

/**
 * @brief Default construct, destruct allocator.
 *
 * @details If `Alloc::is_trivially_allocator` is not defined or
 * `Alloc::is_trivially_allocator` is `std::false_type`, derive from `std::false_type`. \n
 * If is_trivially_allocator_v is true, then `construct_at_using_allocator` and
 * `destroy_at_using_allocator` are same as `construct_at` and `destroy_at`.
 *
 */
template <typename Alloc>
struct is_trivially_allocator : __is_trivially_allocator_impl<Alloc> {};

template <typename T>
struct is_trivially_allocator<std::allocator<T>> : std::true_type {};

template <typename Alloc>
inline constexpr bool is_trivially_allocator_v = is_trivially_allocator<Alloc>::value;

/// @private
template <typename Alloc, typename = void>
struct __is_trivially_allocator_constructible_impl : std::false_type {};

/// @private
template <typename Alloc>
struct __is_trivially_allocator_constructible_impl<
    Alloc, std::enable_if_t<has_is_trivially_allocator_constructible_v<Alloc>>>
    : Alloc::is_trivially_allocator_constructible {};

template <typename Alloc>
struct is_trivially_allocator_constructible
    : std::disjunction<__is_trivially_allocator_constructible_impl<Alloc>,
                       is_trivially_allocator<Alloc>> {};

template <typename Alloc>
inline constexpr bool is_trivially_allocator_constructible_v =
    is_trivially_allocator_constructible<Alloc>::value;

/// @private
template <typename Alloc, typename = void>
struct __is_trivially_allocator_destructible_impl : std::false_type {};

/// @private
template <typename Alloc>
struct __is_trivially_allocator_destructible_impl<
    Alloc, std::enable_if_t<has_is_trivially_allocator_destructible_v<Alloc>>>
    : Alloc::is_trivially_allocator_destructible {};

template <typename Alloc>
struct is_trivially_allocator_destructible
    : std::disjunction<__is_trivially_allocator_destructible_impl<Alloc>,
                       is_trivially_allocator<Alloc>> {};

template <typename Alloc>
inline constexpr bool is_trivially_allocator_destructible_v =
    is_trivially_allocator_destructible<Alloc>::value;

template <typename Alloc>
struct trivially_allocator_traits {
    using is_trivially = is_trivially_allocator<Alloc>;
    using is_trivially_constructible = is_trivially_allocator_constructible<Alloc>;
    using is_trivially_destructible = is_trivially_allocator_destructible<Alloc>;
};

} // namespace wjr

#endif // WJR_CRTP_TRIVIALLY_ALLOCATOR_BASE_HPP__
// Already included

namespace wjr {

template <typename Iter, typename... Args>
WJR_CONSTEXPR20 void construct_at(Iter iter, Args &&...args) {
    ::new (static_cast<void *>((to_address)(iter)))
        iterator_value_t<Iter>(std::forward<Args>(args)...);
}

template <typename Iter>
WJR_CONSTEXPR20 void construct_at(Iter iter, dctor_t) {
    ::new (static_cast<void *>((to_address)(iter))) iterator_value_t<Iter>;
}

template <typename Iter, typename Alloc, typename... Args>
WJR_CONSTEXPR20 void uninitialized_construct_using_allocator(Iter iter, Alloc &alloc,
                                                             Args &&...args) {
    if constexpr (is_trivially_allocator_constructible_v<Alloc>) {
        (construct_at)(iter, std::forward<Args>(args)...);
    } else {
        std::allocator_traits<Alloc>::construct(alloc, (to_address)(iter),
                                                std::forward<Args>(args)...);
    }
}

template <typename Iter, typename Alloc>
WJR_CONSTEXPR20 void uninitialized_construct_using_allocator(Iter iter, Alloc &alloc,
                                                             dctor_t) {
    if constexpr (is_trivially_allocator_constructible_v<Alloc>) {
        (construct_at)(iter, dctor_t{});
    } else {
        std::allocator_traits<Alloc>::construct(alloc, (to_address)(iter));
    }
}

template <typename InputIt, typename OutputIt, typename Alloc>
WJR_CONSTEXPR20 OutputIt uninitialized_copy_using_allocator(InputIt first, InputIt last,
                                                            OutputIt d_first,
                                                            Alloc &alloc) {
    if constexpr (is_trivially_allocator_constructible_v<Alloc>) {
        return std::uninitialized_copy(first, last, d_first);
    } else {
        for (; first != last; ++first, ++d_first) {
            std::allocator_traits<Alloc>::construct(alloc, (to_address)(d_first), *first);
        }
        return d_first;
    }
}

/// @private
template <typename InputIt, typename OutputIt, typename Alloc>
WJR_CONSTEXPR20 OutputIt __uninitialized_copy_restrict_using_allocator_impl_aux(
    add_restrict_t<InputIt> first, add_restrict_t<InputIt> last,
    add_restrict_t<OutputIt> d_first, Alloc &alloc) {
    return uninitialized_copy_using_allocator(first, last, d_first, alloc);
}

/// @private
template <typename InputIt, typename OutputIt, typename Alloc>
WJR_CONSTEXPR20 OutputIt __uninitialized_copy_restrict_using_allocator_impl(
    InputIt first, InputIt last, OutputIt d_first, Alloc &alloc) {
    return __uninitialized_copy_restrict_using_allocator_impl_aux<InputIt, OutputIt,
                                                                  Alloc>(first, last,
                                                                         d_first, alloc);
}

template <typename InputIt, typename OutputIt, typename Alloc>
WJR_CONSTEXPR20 OutputIt uninitialized_copy_restrict_using_allocator(InputIt first,
                                                                     InputIt last,
                                                                     OutputIt d_first,
                                                                     Alloc &alloc) {
    const auto __first = try_to_address(first);
    const auto __last = try_to_address(last);
    if constexpr (is_contiguous_iterator_v<OutputIt>) {
        const auto __d_first = (to_address)(d_first);
        const auto __d_last = __uninitialized_copy_restrict_using_allocator_impl(
            __first, __last, __d_first, alloc);
        return std::next(d_first, std::distance(__d_first, __d_last));
    } else {
        return __uninitialized_copy_restrict_using_allocator_impl(__first, __last,
                                                                  d_first, alloc);
    }
}

template <typename InputIt, typename Size, typename OutputIt, typename Alloc>
WJR_CONSTEXPR20 OutputIt uninitialized_copy_n_using_allocator(InputIt first, Size n,
                                                              OutputIt d_first,
                                                              Alloc &alloc) {
    if constexpr (is_trivially_allocator_constructible_v<Alloc>) {
        return std::uninitialized_copy_n(first, n, d_first);
    } else {
        for (; n > 0; ++first, ++d_first, --n) {
            std::allocator_traits<Alloc>::construct(alloc, (to_address)(d_first), *first);
        }
        return d_first;
    }
}

/// @private
template <typename InputIt, typename Size, typename OutputIt, typename Alloc>
WJR_CONSTEXPR20 OutputIt __uninitialized_copy_n_restrict_using_allocator_impl_aux(
    add_restrict_t<InputIt> first, Size n, add_restrict_t<OutputIt> d_first,
    Alloc &alloc) {
    return uninitialized_copy_n_using_allocator(first, n, d_first, alloc);
}

/// @private
template <typename InputIt, typename Size, typename OutputIt, typename Alloc>
WJR_CONSTEXPR20 OutputIt __uninitialized_copy_n_restrict_using_allocator_impl(
    InputIt first, Size n, OutputIt d_first, Alloc &alloc) {
    return __uninitialized_copy_n_restrict_using_allocator_impl_aux<InputIt, Size,
                                                                    OutputIt, Alloc>(
        first, n, d_first, alloc);
}

template <typename InputIt, typename Size, typename OutputIt, typename Alloc>
WJR_CONSTEXPR20 OutputIt uninitialized_copy_n_restrict_using_allocator(InputIt first,
                                                                       Size n,
                                                                       OutputIt d_first,
                                                                       Alloc &alloc) {
    const auto __first = try_to_address(first);
    if constexpr (is_contiguous_iterator_v<OutputIt>) {
        const auto __d_first = (to_address)(d_first);
        const auto __d_last = __uninitialized_copy_n_restrict_using_allocator_impl(
            __first, n, __d_first, alloc);
        return std::next(d_first, std::distance(__d_first, __d_last));
    } else {
        return __uninitialized_copy_n_restrict_using_allocator_impl(__first, n, d_first,
                                                                    alloc);
    }
}

template <typename InputIt, typename OutputIt, typename Alloc>
WJR_CONSTEXPR20 OutputIt uninitialized_move_using_allocator(InputIt first, InputIt last,
                                                            OutputIt d_first,
                                                            Alloc &alloc) {
    return uninitialized_copy_using_allocator(
        std::make_move_iterator(first), std::make_move_iterator(last), d_first, alloc);
}

template <typename InputIt, typename OutputIt, typename Alloc>
WJR_CONSTEXPR20 OutputIt uninitialized_move_restrict_using_allocator(InputIt first,
                                                                     InputIt last,
                                                                     OutputIt d_first,
                                                                     Alloc &alloc) {
    return uninitialized_copy_restrict_using_allocator(
        std::make_move_iterator(first), std::make_move_iterator(last), d_first, alloc);
}

template <typename InputIt, typename Size, typename OutputIt, typename Alloc>
WJR_CONSTEXPR20 OutputIt uninitialized_move_n_using_allocator(InputIt first, Size n,
                                                              OutputIt d_first,
                                                              Alloc &alloc) {
    return uninitialized_copy_n_using_allocator(std::make_move_iterator(first), n,
                                                d_first, alloc);
}

template <typename InputIt, typename Size, typename OutputIt, typename Alloc>
WJR_CONSTEXPR20 OutputIt uninitialized_move_n_restrict_using_allocator(InputIt first,
                                                                       Size n,
                                                                       OutputIt d_first,
                                                                       Alloc &alloc) {
    return uninitialized_copy_n_restrict_using_allocator(std::make_move_iterator(first),
                                                         n, d_first, alloc);
}

template <typename Iter, typename Alloc>
WJR_CONSTEXPR20 void
uninitialized_default_construct_using_allocator(Iter first, Iter last, Alloc &alloc) {
    if constexpr (is_trivially_allocator_constructible_v<Alloc>) {
        std::uninitialized_default_construct(first, last);
    } else {
        using value_type = iterator_value_t<Iter>;
        if constexpr (!std::is_trivially_default_constructible_v<value_type>) {
            for (; first != last; ++first) {
                std::allocator_traits<Alloc>::construct(alloc, (to_address)(first));
            }
        }
    }
}

template <typename Iter, typename Size, typename Alloc>
WJR_CONSTEXPR20 void uninitialized_default_construct_n_using_allocator(Iter first, Size n,
                                                                       Alloc &alloc) {
    if constexpr (is_trivially_allocator_constructible_v<Alloc>) {
        std::uninitialized_default_construct_n(first, n);
    } else {
        using value_type = iterator_value_t<Iter>;
        if constexpr (!std::is_trivially_default_constructible_v<value_type>) {
            for (; n > 0; ++first, --n) {
                std::allocator_traits<Alloc>::construct(alloc, (to_address)(first),
                                                        value_type());
            }
        }
    }
}

template <typename Iter, typename Alloc>
WJR_CONSTEXPR20 void uninitialized_value_construct_using_allocator(Iter first, Iter last,
                                                                   Alloc &alloc) {
    if constexpr (is_trivially_allocator_constructible_v<Alloc>) {
        std::uninitialized_value_construct(first, last);
    } else {
        for (; first != last; ++first) {
            std::allocator_traits<Alloc>::construct(alloc, (to_address)(first));
        }
    }
}

template <typename Iter, typename Size, typename Alloc>
WJR_CONSTEXPR20 void uninitialized_value_construct_n_using_allocator(Iter first, Size n,
                                                                     Alloc &alloc) {
    if constexpr (is_trivially_allocator_constructible_v<Alloc>) {
        std::uninitialized_value_construct_n(first, n);
    } else {
        for (; n > 0; ++first, --n) {
            std::allocator_traits<Alloc>::construct(alloc, (to_address)(first));
        }
    }
}

/**
 * @brief Fill the range [first, last) with value using allocator.
 *
 * @tparam T The value type. Use `dctor_t` to default construct the
 * elements. Use `vctor_t` to value construct the elements.
 */
template <typename Iter, typename Alloc, typename T>
WJR_CONSTEXPR20 void uninitialized_fill_using_allocator(Iter first, Iter last,
                                                        Alloc &alloc, const T &value) {
    if constexpr (std::is_same_v<T, dctor_t>) {
        uninitialized_default_construct_using_allocator(first, last, alloc);
    } else if constexpr (std::is_same_v<T, vctor_t>) {
        uninitialized_value_construct_using_allocator(first, last, alloc);
    } else {
        if constexpr (is_trivially_allocator_constructible_v<Alloc>) {
            std::uninitialized_fill(first, last, value);
        } else {
            for (; first != last; ++first) {
                std::allocator_traits<Alloc>::construct(alloc, (to_address)(first),
                                                        value);
            }
        }
    }
}

/**
 * @brief Fill the range [first, first + n) with value using allocator.
 *
 * @tparam T The value type. Use `dctor_t` to default construct the
 * elements. Use `vctor_t` to value construct the elements.
 */
template <typename Iter, typename Size, typename Alloc, typename T>
WJR_CONSTEXPR20 void uninitialized_fill_n_using_allocator(Iter first, Size n,
                                                          Alloc &alloc, const T &value) {
    if constexpr (std::is_same_v<T, dctor_t>) {
        uninitialized_default_construct_n_using_allocator(first, n, alloc);
    } else if constexpr (std::is_same_v<T, vctor_t>) {
        uninitialized_value_construct_n_using_allocator(first, n, alloc);
    } else {
        if constexpr (is_trivially_allocator_constructible_v<Alloc>) {
            std::uninitialized_fill_n(first, n, value);
        } else {
            for (; n > 0; ++first, --n) {
                std::allocator_traits<Alloc>::construct(alloc, (to_address)(first),
                                                        value);
            }
        }
    }
}

template <typename Iter, typename Alloc>
WJR_CONSTEXPR20 void destroy_at_using_allocator(Iter iter, Alloc &alloc) {
    if constexpr (is_trivially_allocator_destructible_v<Alloc>) {
        std::destroy_at((to_address)(iter));
    } else {
        std::allocator_traits<Alloc>::destroy(alloc, (to_address)(iter));
    }
}

template <typename Iter, typename Alloc>
WJR_CONSTEXPR20 void destroy_using_allocator(Iter first, Iter last, Alloc &alloc) {
    if constexpr (is_trivially_allocator_destructible_v<Alloc>) {
        std::destroy(first, last);
    } else {
        for (; first != last; ++first) {
            std::allocator_traits<Alloc>::destroy(alloc, (to_address)(first));
        }
    }
}

template <typename Iter, typename Size, typename Alloc>
WJR_CONSTEXPR20 void destroy_n_using_allocator(Iter first, Size n, Alloc &alloc) {
    if constexpr (is_trivially_allocator_v<Alloc>) {
        std::destroy_n(first, n);
    } else {
        for (; n > 0; ++first, --n) {
            std::allocator_traits<Alloc>::destroy(alloc, (to_address)(first));
        }
    }
}

/// @private
template <typename T, typename Tag>
using __uninitialized_checker_base_enabler_select =
    enable_special_members_base<true, true, std::is_trivially_copy_constructible_v<T>,
                                std::is_trivially_move_constructible_v<T>,
                                std::is_trivially_copy_assignable_v<T>,
                                std::is_trivially_move_assignable_v<T>, Tag>;

/// @private
template <bool Default, bool Destructor, typename T>
struct __uninitialized_base;

#define WJR_REGISTER_UNINITIALIZED_BASE(DEF, DES)                                        \
    template <typename T>                                                                \
    struct __uninitialized_base<DEF, DES, T>                                             \
        : __uninitialized_checker_base_enabler_select<                                   \
              T, __uninitialized_base<DEF, DES, T>> {                                    \
        using Mybase = __uninitialized_checker_base_enabler_select<                      \
            T, __uninitialized_base<DEF, DES, T>>;                                       \
                                                                                         \
        constexpr __uninitialized_base() noexcept WJR_PP_BOOL_IF(DEF, = default,         \
                                                                 : m_storage(){});       \
                                                                                         \
        template <typename... Args,                                                      \
                  WJR_REQUIRES(std::is_constructible_v<T, Args &&...>)>                  \
        constexpr __uninitialized_base(Args &&...args) noexcept(                         \
            std::is_nothrow_constructible_v<T, Args &&...>)                              \
            : m_value(std::forward<Args>(args)...) {}                                    \
                                                                                         \
        ~__uninitialized_base() noexcept WJR_PP_BOOL_IF(DES, = default, {});             \
                                                                                         \
        union {                                                                          \
            T m_value;                                                                   \
            std::aligned_storage_t<sizeof(T), alignof(T)> m_storage;                     \
        };                                                                               \
    }

WJR_REGISTER_UNINITIALIZED_BASE(1, 1);
WJR_REGISTER_UNINITIALIZED_BASE(1, 0);
WJR_REGISTER_UNINITIALIZED_BASE(0, 1);
WJR_REGISTER_UNINITIALIZED_BASE(0, 0);

#undef WJR_REGISTER_UNINITIALIZED_BASE

#if WJR_DEBUG_LEVEL > 1
#define WJR_HAS_DEBUG_UNINITIALIZED_CHECKER WJR_HAS_DEF
#endif

/// @private
template <typename T>
using __uninitialized_base_select =
    __uninitialized_base<std::is_trivially_default_constructible_v<T>,
                         std::is_trivially_destructible_v<T>, T>;

/**
 * @class uninitialized
 *
 * @details Uninitialized object. Make trivially constructible and destructible of
 * any type.+
 *
 * @details Trivially constructible and destructible uninitialized object. Copy/move
 * constructor and assignment operators are deleted if the type is not trivially
 * copy/move constructible/assignable.
 *
 */
template <typename T>
class uninitialized : __uninitialized_base_select<T> {
    using Mybase = __uninitialized_base_select<T>;

public:
    using Mybase::Mybase;

    constexpr uninitialized() noexcept = default;

    template <typename... Args, WJR_REQUIRES(std::is_constructible_v<Mybase, Args &&...>)>
    constexpr uninitialized(Args &&...args) noexcept(
        std::is_nothrow_constructible_v<Mybase, Args...>)
        : Mybase(std::forward<Args>(args)...) {
        checker_set(true);
    }

    constexpr uninitialized(dctor_t) noexcept : Mybase() {}

    constexpr T &get() & noexcept {
        check(true);
        return Mybase::m_value;
    }

    constexpr const T &get() const & noexcept {
        check(true);
        return Mybase::m_value;
    }

    constexpr T &&get() && noexcept {
        check(true);
        return std::move(Mybase::m_value);
    }

    constexpr const T &&get() const && noexcept {
        check(true);
        return std::move(Mybase::m_value);
    }

    template <typename Func, typename... Args,
              WJR_REQUIRES(std::is_invocable_v<Func, T *, Args &&...>)>
    constexpr T &emplace_by(Func &&fn, Args &&...args) noexcept(
        noexcept(fn(this->ptr_unsafe(), std::forward<Args>(args)...))) {
        if constexpr (!std::is_trivially_destructible_v<T>) {
            check(false);
        }

        fn(ptr_unsafe(), std::forward<Args>(args)...);
        checker_set(true);
        return get();
    }

    template <typename... Args, WJR_REQUIRES(std::is_constructible_v<T, Args &&...>)>
    constexpr T &
    emplace(Args &&...args) noexcept(std::is_nothrow_constructible_v<Mybase, Args...>) {
        return emplace_by(
            [](T *ptr, auto &&...args) noexcept(
                std::is_nothrow_constructible_v<Mybase, Args...>) {
                (construct_at)(ptr, std::forward<decltype(args)>(args)...);
            },
            std::forward<Args>(args)...);
    }

    template <typename Func, typename... Args,
              WJR_REQUIRES(std::is_invocable_v<Func, T *, Args &&...>)>
    constexpr void reset_by(Func &&fn, Args &&...args) noexcept(
        noexcept(fn(this->ptr_unsafe(), std::forward<Args>(args)...))) {
        if constexpr (!std::is_trivially_destructible_v<T>) {
            check(true);
        }

        fn(ptr_unsafe(), std::forward<Args>(args)...);
        checker_set(false);
    }

    constexpr void reset() noexcept(std::is_nothrow_destructible_v<T>) {
        reset_by([](T *ptr) noexcept(std::is_nothrow_destructible_v<T>) {
            if constexpr (!std::is_trivially_destructible_v<T>) {
                std::destroy_at(ptr);
            }
        });
    }

    constexpr T *operator->() noexcept { return std::addressof(get()); }

    constexpr const T *operator->() const noexcept { return std::addressof(get()); }

    constexpr T &operator*() & noexcept { return get(); }

    constexpr const T &operator*() const & noexcept { return get(); }

    constexpr T &&operator*() && noexcept { return std::move(get()); }

    constexpr const T &&operator*() const && noexcept { return std::move(get()); }

    constexpr T *ptr_unsafe() noexcept { return std::addressof(Mybase::m_value); }
    constexpr const T *ptr_unsafe() const noexcept {
        return std::addressof(Mybase::m_value);
    }

private:
#if WJR_HAS_DEBUG(UNINITIALIZED_CHECKER)
    struct __checker {
        constexpr void set(bool value) noexcept { m_initialized = value; }
        constexpr void check(bool value) const {
            WJR_ASSERT_LX(m_initialized == value, "Expected ",
                          (value ? "initialized" : "uninitialized"),
                          " value when using an uninitialized object.");
        }

        ~__checker() {
            if constexpr (!std::is_trivially_destructible_v<T>) {
                check(false);
            }
        }

        bool m_initialized = false;
    };

    __checker m_checker;

    constexpr void checker_set(bool value) noexcept { m_checker.set(value); }
    constexpr void check(bool value) const { m_checker.check(value); }
#else
    constexpr static void checker_set(bool) noexcept {}
    constexpr static void check(bool) noexcept {}
#endif
};

/// @private
template <typename T, bool = true>
class __lazy_crtp : public uninitialized<T> {
    using Mybase = uninitialized<T>;

public:
    using Mybase::Mybase;
};

/// @private
template <typename T>
class __lazy_crtp<T, false> : public uninitialized<T> {
    using Mybase = uninitialized<T>;

public:
    using Mybase::Mybase;

    ~__lazy_crtp() noexcept(noexcept(this->Mybase::reset())) { Mybase::reset(); }
};

/// @private
template <typename T>
using lazy_crtp = __lazy_crtp<T,
#if WJR_HAS_DEBUG(UNINITIALIZED_CHECKER)
                              false
#else
                              std::is_trivially_destructible_v<T>
#endif
                              >;

template <typename T>
class lazy : public lazy_crtp<T> {
    using Mybase = lazy_crtp<T>;

public:
    using Mybase::Mybase;
};

} // namespace wjr

#endif // WJR_MEMORY_UNINITIALIZED_HPP__

namespace wjr {

template <typename Alloc>
class temporary_value_allocator {
public:
    using value_type = typename std::allocator_traits<Alloc>::value_type;
    using pointer = value_type *;
    using const_pointer = const value_type *;

    template <typename... Args>
    WJR_CONSTEXPR20 temporary_value_allocator(Alloc &al, Args &&...args) noexcept(
        std::is_nothrow_constructible_v<value_type, Args &&...>)
        : al(al) {
        uninitialized_construct_using_allocator(get(), al, std::forward<Args>(args)...);
    }

    temporary_value_allocator(const temporary_value_allocator &) = delete;
    temporary_value_allocator(temporary_value_allocator &&) = delete;
    temporary_value_allocator &operator=(const temporary_value_allocator &) = delete;
    temporary_value_allocator &operator=(temporary_value_allocator &&) = delete;

    ~temporary_value_allocator() { destroy_at_using_allocator(get(), al); }

    WJR_CONSTEXPR20 pointer get() noexcept { return reinterpret_cast<pointer>(storage); }
    WJR_CONSTEXPR20 const_pointer get() const noexcept {
        return reinterpret_cast<const_pointer>(storage);
    }

private:
    Alloc &al;
    std::aligned_storage_t<sizeof(value_type), alignof(value_type)> storage[1];
};

template <typename Alloc, typename... Args>
temporary_value_allocator(Alloc &, Args &&...) -> temporary_value_allocator<Alloc>;

} // namespace wjr

#endif // WJR_MEMORY_TEMPORARY_VALUE_ALLOCATOR_HPP__

namespace wjr {

template <typename T, typename Alloc>
class vector_storage_traits {
    using _Alty = typename std::allocator_traits<Alloc>::template rebind_alloc<T>;
    using _Alty_traits = std::allocator_traits<_Alty>;

public:
    using value_type = T;
    using pointer = typename _Alty_traits::pointer;
    using const_pointer = typename _Alty_traits::const_pointer;
    using size_type = typename _Alty_traits::size_type;
    using difference_type = typename _Alty_traits::difference_type;
    using allocator_type = Alloc;
    using is_trivially_contiguous = std::true_type;

    template <typename... Args>
    WJR_CONSTEXPR20 static void
    uninitialized_construct_using_allocator(pointer ptr, _Alty &al, Args &&...args) {
        wjr::uninitialized_construct_using_allocator(ptr, al,
                                                     std::forward<Args>(args)...);
    }

    WJR_CONSTEXPR20 static void fill(pointer first, pointer last, const value_type &val) {
        std::fill(first, last, val);
    }

    WJR_CONSTEXPR20 static pointer fill_n(pointer first, size_type n,
                                          const value_type &val) {
        return std::fill_n(first, n, val);
    }

    template <typename Val>
    WJR_CONSTEXPR20 static void
    uninitialized_fill_n_using_allocator(pointer first, size_type n, _Alty &al,
                                         const Val &val) {
        wjr::uninitialized_fill_n_using_allocator(first, n, al, val);
    }

    template <typename InputIt>
    WJR_CONSTEXPR20 static pointer copy(InputIt first, InputIt last, pointer result) {
        return std::copy(first, last, result);
    }

    template <typename InputIt, typename Size>
    WJR_CONSTEXPR20 static void copy_n(InputIt first, Size size, pointer result) {
        std::copy_n(first, size, result);
    }

    template <typename InputIt>
    WJR_CONSTEXPR20 static void
    uninitialized_copy_using_allocator(InputIt first, InputIt last, pointer result,
                                       _Alty &al) {
        wjr::uninitialized_copy_using_allocator(first, last, result, al);
    }

    template <typename InputIt, typename Size>
    WJR_CONSTEXPR20 static void
    uninitialized_copy_n_using_allocator(InputIt first, Size size, pointer result,
                                         _Alty &al) {
        wjr::uninitialized_copy_n_using_allocator(first, size, result, al);
    }

    template <typename InputIt>
    WJR_CONSTEXPR20 static void move(InputIt first, InputIt last, pointer result) {
        std::move(first, last, result);
    }

    template <typename InputIt>
    WJR_CONSTEXPR20 static void
    uninitialized_move_using_allocator(InputIt first, InputIt last, pointer result,
                                       _Alty &al) {
        wjr::uninitialized_move_using_allocator(first, last, result, al);
    }

    template <typename InputIt, typename Size>
    WJR_CONSTEXPR20 static void
    uninitialized_move_n_using_allocator(InputIt first, Size size, pointer result,
                                         _Alty &al) {
        wjr::uninitialized_move_n_using_allocator(first, size, result, al);
    }

    template <typename InputIt>
    WJR_CONSTEXPR20 static void move_backward(InputIt first, InputIt last,
                                              pointer result) {
        std::move_backward(first, last, result);
    }

    template <typename InputIt>
    WJR_CONSTEXPR20 static pointer copy_restrict(InputIt first, InputIt last,
                                                 pointer result) {
        return wjr::copy_restrict(first, last, result);
    }

    template <typename InputIt, typename Size>
    WJR_CONSTEXPR20 static pointer copy_n_restrict(InputIt first, Size n,
                                                   pointer result) {
        return wjr::copy_n_restrict(first, n, result);
    }

    template <typename InputIt>
    WJR_CONSTEXPR20 static void
    uninitialized_copy_restrict_using_allocator(InputIt first, InputIt last,
                                                pointer result, _Alty &al) {
        wjr::uninitialized_copy_restrict_using_allocator(first, last, result, al);
    }

    template <typename InputIt, typename Size>
    WJR_CONSTEXPR20 static void
    uninitialized_copy_n_restrict_using_allocator(InputIt first, Size size,
                                                  pointer result, _Alty &al) {
        wjr::uninitialized_copy_n_restrict_using_allocator(first, size, result, al);
    }

    template <typename InputIt>
    WJR_CONSTEXPR20 static void
    uninitialized_move_restrict_using_allocator(InputIt first, InputIt last,
                                                pointer result, _Alty &al) {
        wjr::uninitialized_move_restrict_using_allocator(first, last, result, al);
    }

    template <typename InputIt, typename Size>
    WJR_CONSTEXPR20 static void
    uninitialized_move_n_restrict_using_allocator(InputIt first, Size size,
                                                  pointer result, _Alty &al) {
        wjr::uninitialized_move_n_restrict_using_allocator(first, size, result, al);
    }
};

template <typename pointer, typename size_type>
class default_vector_size_reference {
public:
    default_vector_size_reference() = delete;
    default_vector_size_reference(const default_vector_size_reference &) = delete;
    default_vector_size_reference(default_vector_size_reference &&) = default;
    default_vector_size_reference &
    operator=(const default_vector_size_reference &) = delete;
    default_vector_size_reference &operator=(default_vector_size_reference &&) = default;

    explicit default_vector_size_reference(pointer ptr, pointer &pos) noexcept
        : m_ptr(ptr), m_pos(pos) {}
    ~default_vector_size_reference() = default;

    default_vector_size_reference &operator=(size_type size) noexcept {
        m_pos = m_ptr + size;
        return *this;
    }

    WJR_PURE operator size_type() const noexcept {
        return static_cast<size_type>(m_pos - m_ptr);
    }

    default_vector_size_reference &operator++() noexcept {
        ++m_pos;
        return *this;
    }

    default_vector_size_reference &operator--() noexcept {
        --m_pos;
        return *this;
    }

    default_vector_size_reference &operator+=(uint32_t size) noexcept {
        m_pos += size;
        return *this;
    }

    default_vector_size_reference &operator-=(uint32_t size) noexcept {
        m_pos -= size;
        return *this;
    }

private:
    pointer m_ptr;
    pointer &m_pos;
};

template <typename pointer, typename size_type>
struct __unref_wrapper_helper<default_vector_size_reference<pointer, size_type>> {
    using type = uint32_t &;
};

/**
 * @brief Default vector storage
 *
 * @details Use one pointer ans two size_type currently.
 *
 */
template <typename T, typename Alloc, typename STraits>
class __default_vector_storage_impl {
    using _Alty = typename std::allocator_traits<Alloc>::template rebind_alloc<T>;
    using _Alty_traits = std::allocator_traits<_Alty>;

public:
    using storage_traits_type = STraits;
    using value_type = typename storage_traits_type::value_type;
    using pointer = typename storage_traits_type::pointer;
    using const_pointer = typename storage_traits_type::const_pointer;
    using size_type = typename storage_traits_type::size_type;
    using difference_type = typename storage_traits_type::difference_type;
    using allocator_type = typename storage_traits_type::allocator_type;
    using is_reallocatable = std::true_type;

private:
    struct Data {
        pointer m_data = nullptr;
        pointer m_end = nullptr;
        pointer m_buffer = nullptr;
    };

    using data_type = Data;
    using SizeRef = default_vector_size_reference<pointer, size_type>;

public:
    __default_vector_storage_impl() noexcept = default;

    __default_vector_storage_impl(const __default_vector_storage_impl &) = delete;
    __default_vector_storage_impl(__default_vector_storage_impl &&) = delete;
    __default_vector_storage_impl &
    operator=(const __default_vector_storage_impl &) = delete;
    __default_vector_storage_impl &operator=(__default_vector_storage_impl &&) = delete;

    ~__default_vector_storage_impl() noexcept = default;

    WJR_CONSTEXPR20 void
    destroy(_Alty &al) noexcept(std::is_nothrow_destructible_v<value_type>) {
        if (WJR_BUILTIN_CONSTANT_P(data() == nullptr) && data() == nullptr) {
            return;
        }

        if (WJR_BUILTIN_CONSTANT_P(size() == 0) && size() == 0) {
            return;
        }

        destroy_using_allocator(m_storage.m_data, m_storage.m_end, al);
    }

    WJR_CONSTEXPR20 void destroy_and_deallocate(_Alty &al) noexcept(
        std::is_nothrow_destructible_v<value_type>) {
        if (WJR_BUILTIN_CONSTANT_P(data() == nullptr) && data() == nullptr) {
            return;
        }

        if (WJR_BUILTIN_CONSTANT_P(capacity() == 0) && capacity() == 0) {
            return;
        }

        if (m_storage.m_data != nullptr) {
            destroy_using_allocator(m_storage.m_data, m_storage.m_end, al);
            al.deallocate(m_storage.m_data, capacity());
        }
    }

    WJR_CONSTEXPR20 static void
    uninitialized_construct(__default_vector_storage_impl &other, size_type size,
                            size_type capacity, _Alty &al) {
        const auto result = allocate_at_least(al, capacity);

        other.m_storage = {
            result.ptr,
            result.ptr + size,
            result.ptr + capacity,
        };
    }

    WJR_CONSTEXPR20 void take_storage(__default_vector_storage_impl &other,
                                      _Alty &) noexcept {
        auto &other_storage = other.m_storage;
        m_storage = std::move(other_storage);
        other_storage = {};
    }

    WJR_CONSTEXPR20 void swap_storage(__default_vector_storage_impl &other,
                                      _Alty &) noexcept {
        std::swap(m_storage, other.m_storage);
    }

    WJR_PURE WJR_CONSTEXPR20 SizeRef size() noexcept {
        return SizeRef(m_storage.m_data, m_storage.m_end);
    }

    WJR_PURE WJR_CONSTEXPR20 size_type size() const noexcept {
        return m_storage.m_end - m_storage.m_data;
    }

    WJR_PURE WJR_CONSTEXPR20 size_type capacity() const noexcept {
        return m_storage.m_buffer - m_storage.m_data;
    }

    WJR_PURE WJR_CONSTEXPR20 pointer data() noexcept { return m_storage.m_data; }
    WJR_PURE WJR_CONSTEXPR20 const_pointer data() const noexcept {
        return m_storage.m_data;
    }

private:
    data_type m_storage;
};

template <typename T, typename Alloc>
using default_vector_storage =
    __default_vector_storage_impl<T, Alloc, vector_storage_traits<T, Alloc>>;

template <typename T, size_t Capacity, typename Alloc, typename STraits>
class __static_vector_storage_impl {
    using _Alty = typename std::allocator_traits<Alloc>::template rebind_alloc<T>;
    using _Alty_traits = std::allocator_traits<_Alty>;

public:
    using storage_traits_type = STraits;
    using value_type = typename storage_traits_type::value_type;
    using pointer = typename storage_traits_type::pointer;
    using const_pointer = typename storage_traits_type::const_pointer;
    using size_type = typename storage_traits_type::size_type;
    using difference_type = typename storage_traits_type::difference_type;
    using allocator_type = typename storage_traits_type::allocator_type;
    using is_reallocatable = std::false_type;

private:
    static constexpr auto max_alignment =
        std::max<size_type>(alignof(T), alignof(size_type));
    static constexpr bool __use_memcpy = is_trivially_allocator_constructible_v<Alloc> &&
                                         std::is_trivially_copyable_v<T> &&
                                         Capacity * sizeof(T) <= 64;

    struct Data {
        size_type m_size = 0;
        alignas(max_alignment) char m_data[Capacity * sizeof(T)];
    };

    using data_type = Data;

public:
    __static_vector_storage_impl() noexcept = default;

    __static_vector_storage_impl(const __static_vector_storage_impl &) = delete;
    __static_vector_storage_impl(__static_vector_storage_impl &&) = delete;
    __static_vector_storage_impl &
    operator=(const __static_vector_storage_impl &) = delete;
    __static_vector_storage_impl &operator=(__static_vector_storage_impl &&) = delete;

    ~__static_vector_storage_impl() noexcept = default;

    WJR_CONSTEXPR20 void
    destroy(_Alty &al) noexcept(std::is_nothrow_destructible_v<value_type>) {
        if (WJR_BUILTIN_CONSTANT_P(size() == 0) && size() == 0) {
            return;
        }

        destroy_n_using_allocator(data(), size(), al);
    }

    WJR_CONSTEXPR20 void destroy_and_deallocate(_Alty &al) noexcept(
        std::is_nothrow_destructible_v<value_type>) {
        destroy(al);
    }

    WJR_CONSTEXPR20 static void
    uninitialized_construct(__static_vector_storage_impl &other, size_type size,
                            WJR_MAYBE_UNUSED size_type capacity, _Alty &) noexcept {
        WJR_ASSERT_ASSUME(capacity <= Capacity,
                          "capacity must be less than or equal to Capacity");
        other.m_storage.m_size = size;
    }

    WJR_CONSTEXPR20 void take_storage(__static_vector_storage_impl &other, _Alty &al) {
        auto &other_storage = other.m_storage;
        const auto lhs = data();
        const auto rhs = other.data();

        m_storage.m_size = other_storage.m_size;

        if constexpr (__use_memcpy) {
            if (other.size()) {
                __memcpy(lhs, rhs, Capacity);
            }
        } else {
            STraits::uninitialized_move_n_restrict_using_allocator(
                rhs, other_storage.m_size, lhs, al);
        }

        other_storage.m_size = 0;
    }

    WJR_CONSTEXPR20 void swap_storage(__static_vector_storage_impl &other, _Alty &al) {
        auto &other_storage = other.m_storage;
        auto lhs = data();
        auto lsize = size();
        auto rhs = other.data();
        auto rsize = other.size();

        if (lsize && rsize) {
            m_storage.m_size = rsize;
            other_storage.m_size = lsize;

            T tmp[Capacity];
            if constexpr (__use_memcpy) {
                __memcpy(tmp, lhs, Capacity);
                __memcpy(lhs, rhs, Capacity);
                __memcpy(rhs, tmp, Capacity);
            } else {
                if (lsize > rsize) {
                    std::swap(lhs, rhs);
                    std::swap(lsize, rsize);
                }

                STraits::uninitialized_move_n_restrict_using_allocator(lhs, lsize, tmp,
                                                                       al);
                STraits::uninitialized_move_n_restrict_using_allocator(rhs, rsize, lhs,
                                                                       al);
                STraits::uninitialized_move_n_restrict_using_allocator(tmp, lsize, rhs,
                                                                       al);
            }
            return;
        } else if (rsize) {
            if constexpr (__use_memcpy) {
                __memcpy(lhs, rhs, Capacity);
            } else {
                STraits::uninitialized_move_n_restrict_using_allocator(rhs, rsize, lhs,
                                                                       al);
            }
            m_storage.m_size = rsize;
            other_storage.m_size = 0;
            return;
        } else if (lsize) {
            if constexpr (__use_memcpy) {
                __memcpy(rhs, lhs, Capacity);
            } else {
                STraits::uninitialized_move_n_restrict_using_allocator(lhs, lsize, rhs,
                                                                       al);
            }
            other_storage.m_size = lsize;
            m_storage.m_size = 0;
            return;
        } else {
            return;
        }
    }

    WJR_PURE WJR_CONSTEXPR20 size_type &size() noexcept { return m_storage.m_size; }
    WJR_PURE WJR_CONSTEXPR20 size_type size() const noexcept { return m_storage.m_size; }
    WJR_CONST constexpr size_type capacity() const noexcept { return Capacity; }

    WJR_PURE WJR_CONSTEXPR20 pointer data() noexcept {
        return reinterpret_cast<pointer>(m_storage.m_data);
    }
    WJR_PURE WJR_CONSTEXPR20 const_pointer data() const noexcept {
        return reinterpret_cast<const_pointer>(m_storage.m_data);
    }

private:
    static void __memcpy(pointer dst, const_pointer src, size_type count) {
        std::memcpy(dst, src, count * sizeof(T));
    }

    data_type m_storage;
};

template <typename T, size_t Capacity, typename Alloc>
using static_vector_storage =
    __static_vector_storage_impl<T, Capacity, Alloc, vector_storage_traits<T, Alloc>>;

template <typename T, typename Alloc, typename STraits>
class __fixed_vector_storage_impl {
    using _Alty = typename std::allocator_traits<Alloc>::template rebind_alloc<T>;
    using _Alty_traits = std::allocator_traits<_Alty>;

public:
    using storage_traits_type = STraits;
    using value_type = typename storage_traits_type::value_type;
    using pointer = typename storage_traits_type::pointer;
    using const_pointer = typename storage_traits_type::const_pointer;
    using size_type = typename storage_traits_type::size_type;
    using difference_type = typename storage_traits_type::difference_type;
    using allocator_type = typename storage_traits_type::allocator_type;
    using is_reallocatable = std::false_type;

private:
    struct Data {
        pointer m_data = nullptr;
        pointer m_end = nullptr;
        pointer m_buffer = nullptr;
    };

    using data_type = Data;
    using SizeRef = default_vector_size_reference<pointer, size_type>;

public:
    __fixed_vector_storage_impl() noexcept = default;

    __fixed_vector_storage_impl(const __fixed_vector_storage_impl &) = delete;
    __fixed_vector_storage_impl(__fixed_vector_storage_impl &&) = delete;
    __fixed_vector_storage_impl &operator=(const __fixed_vector_storage_impl &) = delete;
    __fixed_vector_storage_impl &operator=(__fixed_vector_storage_impl &&) = delete;

    ~__fixed_vector_storage_impl() noexcept = default;

private:
    WJR_PURE WJR_INTRINSIC_INLINE bool __is_null_data() const {
        return WJR_BUILTIN_CONSTANT_P(data() == nullptr) && data() == nullptr;
    }

    WJR_PURE WJR_INTRINSIC_INLINE bool __is_zero_size() const {
        return WJR_BUILTIN_CONSTANT_P(size() == 0) && size() == 0;
    }

    WJR_PURE WJR_INTRINSIC_INLINE bool __is_zero_capacity() const {
        return WJR_BUILTIN_CONSTANT_P(capacity() == 0) && capacity() == 0;
    }

public:
    WJR_CONSTEXPR20 void
    destroy(_Alty &al) noexcept(std::is_nothrow_destructible_v<value_type>) {
        if (__is_null_data() || __is_zero_size()) {
            return;
        }

        destroy_using_allocator(m_storage.m_data, m_storage.m_end, al);
    }

    WJR_CONSTEXPR20 void destroy_and_deallocate(_Alty &al) noexcept(
        std::is_nothrow_destructible_v<value_type>) {
        if (__is_null_data() || __is_zero_capacity()) {
            return;
        }

        if (m_storage.m_data != nullptr) {
            destroy_using_allocator(m_storage.m_data, m_storage.m_end, al);
            al.deallocate(m_storage.m_data, capacity());
        }
    }

    WJR_CONSTEXPR20 static void
    uninitialized_construct(__fixed_vector_storage_impl &other, size_type size,
                            size_type capacity, _Alty &al) {
        const auto result = allocate_at_least(al, capacity);

        other.m_storage = {
            result.ptr,
            result.ptr + size,
            result.ptr + capacity,
        };
    }

    WJR_CONSTEXPR20 void take_storage(__fixed_vector_storage_impl &other,
                                      _Alty &) noexcept {
        auto &other_storage = other.m_storage;
        m_storage = std::move(other_storage);
        other_storage = {};
    }

    WJR_CONSTEXPR20 void swap_storage(__fixed_vector_storage_impl &other,
                                      _Alty &) noexcept {
        std::swap(m_storage, other.m_storage);
    }

    WJR_PURE WJR_CONSTEXPR20 SizeRef size() noexcept {
        return SizeRef(m_storage.m_data, m_storage.m_end);
    }

    WJR_PURE WJR_CONSTEXPR20 size_type size() const noexcept {
        return m_storage.m_end - m_storage.m_data;
    }

    WJR_PURE WJR_CONSTEXPR20 size_type capacity() const noexcept {
        return m_storage.m_buffer - m_storage.m_data;
    }

    WJR_PURE WJR_CONSTEXPR20 pointer data() noexcept { return m_storage.m_data; }
    WJR_PURE WJR_CONSTEXPR20 const_pointer data() const noexcept {
        return m_storage.m_data;
    }

private:
    data_type m_storage;
};

template <typename T, typename Alloc>
using fixed_vector_storage =
    __fixed_vector_storage_impl<T, Alloc, vector_storage_traits<T, Alloc>>;

template <typename T, size_t Capacity, typename Alloc, typename STraits>
class __sso_vector_storage_impl {
    using _Alty = typename std::allocator_traits<Alloc>::template rebind_alloc<T>;
    using _Alty_traits = std::allocator_traits<_Alty>;

public:
    using storage_traits_type = STraits;
    using value_type = typename storage_traits_type::value_type;
    using pointer = typename storage_traits_type::pointer;
    using const_pointer = typename storage_traits_type::const_pointer;
    using size_type = typename storage_traits_type::size_type;
    using difference_type = typename storage_traits_type::difference_type;
    using allocator_type = typename storage_traits_type::allocator_type;
    using is_reallocatable = std::true_type;

private:
    static constexpr auto max_alignment = std::max<size_type>(
        alignof(T), std::max<size_type>(alignof(pointer), alignof(size_type)));

    struct __Data_test {
        pointer m_data;
        size_type m_size;
        union {
            size_type m_capacity;
            alignas(max_alignment) T m_storage[Capacity];
        };
    };

    static constexpr auto __sizeof_data = sizeof(__Data_test);
    static constexpr auto __offsetof_storage =
        __align_up(__align_up(sizeof(pointer), alignof(pointer)) +
                       __align_up(sizeof(size_type), alignof(size_type)),
                   max_alignment);

    static constexpr auto __max_capacity =
        (__sizeof_data - __offsetof_storage) / sizeof(T);
    static constexpr bool __use_memcpy = is_trivially_allocator_constructible_v<Alloc> &&
                                         std::is_trivially_copyable_v<T> &&
                                         __max_capacity * sizeof(T) <= 64;

    struct Data {
        Data() : m_capacity() {}
        Data(const Data &) = delete;
        Data &operator=(const Data &) = delete;
        ~Data() {}

        pointer m_data = m_storage;
        size_type m_size = 0;
        union {
            size_type m_capacity;
            alignas(max_alignment) T m_storage[__max_capacity];
        };
    };

    using data_type = Data;

public:
    __sso_vector_storage_impl() noexcept = default;

    __sso_vector_storage_impl(const __sso_vector_storage_impl &) = delete;
    __sso_vector_storage_impl(__sso_vector_storage_impl &&) = delete;
    __sso_vector_storage_impl &operator=(const __sso_vector_storage_impl &) = delete;
    __sso_vector_storage_impl &operator=(__sso_vector_storage_impl &&) = delete;

    ~__sso_vector_storage_impl() noexcept = default;

    WJR_CONSTEXPR20 void
    destroy(_Alty &al) noexcept(std::is_nothrow_destructible_v<value_type>) {
        if (WJR_BUILTIN_CONSTANT_P(size() == 0) && size() == 0) {
            return;
        }

        destroy_n_using_allocator(data(), size(), al);
    }

    WJR_CONSTEXPR20 void destroy_and_deallocate(_Alty &al) noexcept(
        std::is_nothrow_destructible_v<value_type>) {

        destroy(al);
        if (!__is_sso()) {
            al.deallocate(data(), capacity());
            m_storage.m_data = m_storage.m_storage;
        }
    }

    WJR_CONSTEXPR20 static void uninitialized_construct(__sso_vector_storage_impl &other,
                                                        size_type size,
                                                        size_type capacity, _Alty &al) {
        auto &storage = other.m_storage;
        if (capacity <= __max_capacity) {
            storage.m_size = size;
        } else {
            const auto result = allocate_at_least(al, capacity);

            storage.m_data = result.ptr;
            storage.m_size = size;
            storage.m_capacity = result.count;
        }
    }

    WJR_CONSTEXPR20 void take_storage(__sso_vector_storage_impl &other, _Alty &al) {
        auto &other_storage = other.m_storage;

        WJR_ASSERT_ASSUME_L1(__is_sso());

        if (other.__is_sso()) {
            m_storage.m_size = other_storage.m_size;

            if constexpr (__use_memcpy) {
                if (other.size()) {
                    __memcpy(m_storage.m_storage, other_storage.m_storage,
                             __max_capacity);
                }
            } else {
                STraits::uninitialized_move_n_restrict_using_allocator(
                    other_storage.m_storage, other.size(), m_storage.m_storage, al);
            }
        } else {
            m_storage.m_data = other_storage.m_data;
            m_storage.m_size = other_storage.m_size;
            m_storage.m_capacity = other_storage.m_capacity;

            other_storage.m_data = other_storage.m_storage;
        }

        other_storage.m_size = 0;
        WJR_ASSUME(other.__is_sso());
    }

    WJR_CONSTEXPR20 void swap_storage(__sso_vector_storage_impl &other, _Alty &al) {
        auto &storage = m_storage;
        auto &other_storage = other.m_storage;

        if (__is_sso()) {
            if (other.__is_sso()) {
                auto lhs = storage.m_storage;
                auto lsize = size();
                auto rhs = other_storage.m_storage;
                auto rsize = other.size();

                if (lsize && rsize) {
                    T tmp[__max_capacity];
                    if constexpr (__use_memcpy) {
                        __memcpy(tmp, lhs, __max_capacity);
                        __memcpy(lhs, rhs, __max_capacity);
                        __memcpy(rhs, tmp, __max_capacity);
                    } else {
                        if (lsize > rsize) {
                            std::swap(lhs, rhs);
                            std::swap(lsize, rsize);
                        }

                        STraits::uninitialized_move_n_restrict_using_allocator(lhs, lsize,
                                                                               tmp, al);
                        STraits::uninitialized_move_n_restrict_using_allocator(rhs, rsize,
                                                                               lhs, al);
                        STraits::uninitialized_move_n_restrict_using_allocator(tmp, lsize,
                                                                               rhs, al);
                    }
                } else if (rsize) {
                    if constexpr (__use_memcpy) {
                        __memcpy(lhs, rhs, __max_capacity);
                    } else {
                        STraits::uninitialized_move_n_restrict_using_allocator(rhs, rsize,
                                                                               lhs, al);
                    }
                    storage.m_size = rsize;
                    other_storage.m_size = 0;
                    return;
                } else if (lsize) {
                    if constexpr (__use_memcpy) {
                        __memcpy(rhs, lhs, __max_capacity);
                    } else {
                        STraits::uninitialized_move_n_restrict_using_allocator(lhs, lsize,
                                                                               rhs, al);
                    }
                    other_storage.m_size = lsize;
                    storage.m_size = 0;
                    return;
                } else {
                    return;
                }
            } else {
                const size_type __tmp_capacity = other_storage.m_capacity;
                if constexpr (__use_memcpy) {
                    if (size()) {
                        __memcpy(other_storage.m_storage, storage.m_storage,
                                 __max_capacity);
                    }
                } else {
                    STraits::uninitialized_move_n_restrict_using_allocator(
                        storage.m_storage, size(), other_storage.m_storage, al);
                }

                storage.m_data = other_storage.m_data;
                other_storage.m_data = other_storage.m_storage;
                storage.m_capacity = __tmp_capacity;
            }
        } else {
            const size_type __tmp_capacity = storage.m_capacity;
            if (other.__is_sso()) {
                if constexpr (__use_memcpy) {
                    if (other.size()) {
                        __memcpy(storage.m_storage, other_storage.m_storage,
                                 __max_capacity);
                    }
                } else {
                    STraits::uninitialized_move_n_restrict_using_allocator(
                        other_storage.m_storage, other.size(), storage.m_storage, al);
                }

                other_storage.m_data = storage.m_data;
                storage.m_data = storage.m_storage;
            } else {
                const auto __tmp_data = storage.m_data;
                storage.m_data = other_storage.m_data;
                other_storage.m_data = __tmp_data;
                storage.m_capacity = other_storage.m_capacity;
            }
            other_storage.m_capacity = __tmp_capacity;
        }

        const size_type __tmp_size = size();
        storage.m_size = other.size();
        other_storage.m_size = __tmp_size;
    }

    WJR_PURE WJR_CONSTEXPR20 size_type &size() noexcept { return m_storage.m_size; }
    WJR_PURE WJR_CONSTEXPR20 size_type size() const noexcept { return m_storage.m_size; }
    WJR_PURE WJR_CONSTEXPR20 size_type capacity() const noexcept {
        const size_type ret = __is_sso() ? __max_capacity : m_storage.m_capacity;
        WJR_ASSERT_ASSUME_L1(ret >= __max_capacity);
        return ret;
    }

    WJR_PURE WJR_CONSTEXPR20 pointer data() noexcept { return m_storage.m_data; }
    WJR_PURE WJR_CONSTEXPR20 const_pointer data() const noexcept {
        return m_storage.m_data;
    }

private:
    static void __memcpy(pointer dst, const_pointer src, size_type count) {
        std::memcpy(dst, src, count * sizeof(T));
    }

    WJR_PURE bool __is_sso() const noexcept {
        return m_storage.m_data == m_storage.m_storage;
    }

    data_type m_storage;
};

template <typename T, size_t Capacity, typename Alloc>
using sso_vector_storage =
    __sso_vector_storage_impl<T, Capacity, Alloc, vector_storage_traits<T, Alloc>>;

namespace {

WJR_REGISTER_HAS_TYPE(vector_storage_shrink_to_fit,
                      std::declval<Storage>().shrink_to_fit(), Storage);

WJR_REGISTER_HAS_TYPE(vector_storage_empty, std::declval<Storage>().empty(), Storage);

WJR_REGISTER_HAS_TYPE(vector_storage_uninitialized_construct,
                      std::declval<Storage>().uninitialized_construct(
                          std::declval<Size>(), std::declval<Size>(),
                          std::declval<Alloc &>()),
                      Storage, Size, Alloc);

template <typename Storage>
struct basic_vector_traits {
    using value_type = typename Storage::value_type;
    using difference_type = typename Storage::difference_type;
    using pointer = typename Storage::pointer;
    using const_pointer = typename Storage::const_pointer;
    using reference = value_type &;
    using const_reference = const value_type &;
};

} // namespace

/**
 * @brief Customized vector by storage.
 *
 * @details Type of pointer is same as iterator.
 *
 */
template <typename Storage>
class basic_vector {
public:
    using value_type = typename Storage::value_type;
    using allocator_type = typename Storage::allocator_type;
    using storage_type = Storage;

private:
    using _Alty =
        typename std::allocator_traits<allocator_type>::template rebind_alloc<value_type>;
    using _Alty_traits = std::allocator_traits<_Alty>;

    using storage_fn_type = container_fn<_Alty>;
    using __get_size_t = decltype(std::declval<storage_type>().size());
    using IteratorTraits = basic_vector_traits<storage_type>;

    friend class container_fn<_Alty>;

public:
    static_assert(std::is_same_v<typename _Alty_traits::value_type, value_type>,
                  "allocator_type::value_type must be the same as value_type");

    using size_type = typename storage_type::size_type;
    using difference_type = typename storage_type::difference_type;
    using reference = value_type &;
    using const_reference = const value_type &;
    using pointer = typename Storage::pointer;
    using const_pointer = typename Storage::const_pointer;
    using iterator = contiguous_iterator_adapter<basic_vector, IteratorTraits>;
    using const_iterator =
        contiguous_const_iterator_adapter<basic_vector, IteratorTraits>;
    using reverse_iterator = std::reverse_iterator<iterator>;
    using const_reverse_iterator = std::reverse_iterator<const_iterator>;

    using storage_traits_type = typename storage_type::storage_traits_type;
    using is_trivially_contiguous = typename storage_traits_type::is_trivially_contiguous;
    using is_reallocatable = typename storage_type::is_reallocatable;

private:
    using STraits = storage_traits_type;

    static_assert(is_contiguous_iterator_v<iterator>, "");
    static_assert(is_contiguous_iterator_v<const_iterator>, "");

    static constexpr bool __storage_noexcept_destroy =
        noexcept(std::declval<storage_type>().destroy(std::declval<_Alty &>()));
    static constexpr bool __storage_noexcept_destroy_and_deallocate = noexcept(
        std::declval<storage_type>().destroy_and_deallocate(std::declval<_Alty &>()));
    static constexpr bool __storage_noexcept_take_storage =
        noexcept(std::declval<storage_type>().take_storage(std::declval<storage_type &>(),
                                                           std::declval<_Alty &>()));
    static constexpr bool __storage_noexcept_swap_storage =
        noexcept(std::declval<storage_type>().swap_storage(std::declval<storage_type &>(),
                                                           std::declval<_Alty &>()));

public:
    WJR_CONSTEXPR20
    basic_vector() noexcept(std::is_nothrow_default_constructible_v<_Alty>) = default;

    WJR_CONSTEXPR20 explicit basic_vector(const allocator_type &al) noexcept(
        std::is_nothrow_constructible_v<_Alty, const allocator_type &>)
        : m_pair(std::piecewise_construct, std::forward_as_tuple(al),
                 std::forward_as_tuple()) {}

    WJR_CONSTEXPR20 explicit basic_vector(const size_type n,
                                          const allocator_type &al = allocator_type())
        : m_pair(std::piecewise_construct, std::forward_as_tuple(al),
                 std::forward_as_tuple()) {
        __construct_n(n, vctor);
    }

    WJR_CONSTEXPR20 basic_vector(size_type n, const value_type &val,
                                 const allocator_type &al = allocator_type())
        : m_pair(std::piecewise_construct, std::forward_as_tuple(al),
                 std::forward_as_tuple()) {
        __construct_n(n, val);
    }

private:
    template <typename _Alloc>
    WJR_CONSTEXPR20 basic_vector(const basic_vector &other, _Alloc &&al, in_place_empty_t)
        : m_pair(std::piecewise_construct, std::forward_as_tuple(al),
                 std::forward_as_tuple()) {
        const auto size = other.size();
        uninitialized_construct(size, other.capacity());
        STraits::uninitialized_copy_n_restrict_using_allocator(other.data(), size, data(),
                                                               __get_allocator());
    }

    template <typename _Alloc>
    WJR_CONSTEXPR20
    basic_vector(basic_vector &&other, _Alloc &&al, in_place_empty_t) noexcept(
        std::is_nothrow_constructible_v<storage_type, _Alloc &&>
            &&__storage_noexcept_take_storage)
        : m_pair(std::piecewise_construct, std::forward_as_tuple(al),
                 std::forward_as_tuple()) {
        __take_storage(std::move(other));
    }

public:
    WJR_CONSTEXPR20 basic_vector(const basic_vector &other)
        : basic_vector(other,
                       _Alty_traits::select_on_container_copy_construction(
                           other.__get_allocator()),
                       in_place_empty) {}

    WJR_CONSTEXPR20 basic_vector(const basic_vector &other, const allocator_type &al)
        : basic_vector(other, al, in_place_empty) {}

    WJR_CONSTEXPR20 basic_vector(basic_vector &&other) noexcept(noexcept(basic_vector(
        std::move(other), std::move(other.__get_allocator()), in_place_empty)))
        : basic_vector(std::move(other), std::move(other.__get_allocator()),
                       in_place_empty) {}

    WJR_CONSTEXPR20 basic_vector(basic_vector &&other, const allocator_type &al) noexcept(
        noexcept(basic_vector(std::move(other), al, in_place_empty)))
        : basic_vector(std::move(other), al, in_place_empty) {}

    template <typename Iter, WJR_REQUIRES(is_iterator_v<Iter>)>
    WJR_CONSTEXPR20 basic_vector(Iter first, Iter last,
                                 const allocator_type &al = allocator_type())
        : m_pair(std::piecewise_construct, std::forward_as_tuple(al),
                 std::forward_as_tuple()) {
        __range_construct(try_to_address(first), try_to_address(last),
                          iterator_category_t<Iter>());
    }

    WJR_CONSTEXPR20 basic_vector(std::initializer_list<value_type> il,
                                 const allocator_type &al = allocator_type())
        : basic_vector(il.begin(), il.end(), al) {}

    WJR_CONSTEXPR20 ~basic_vector() noexcept(__storage_noexcept_destroy_and_deallocate) {
        __destroy_and_deallocate();
    }

    WJR_CONSTEXPR20 basic_vector &operator=(const basic_vector &other) noexcept(
        noexcept(storage_fn_type::copy_assign(*this, other))) {
        if (WJR_LIKELY(this != std::addressof(other))) {
            storage_fn_type::copy_assign(*this, other);
        }

        return *this;
    }

    WJR_CONSTEXPR20 basic_vector &operator=(basic_vector &&other) noexcept(
        noexcept(storage_fn_type::move_assign(*this, std::move(other)))) {
        if (WJR_LIKELY(this != std::addressof(other))) {
            storage_fn_type::move_assign(*this, std::move(other));
        }

        return *this;
    }

    WJR_CONSTEXPR20 basic_vector &operator=(std::initializer_list<value_type> il) {
        return assign(il);
    }

    WJR_CONSTEXPR20 basic_vector &assign(size_type n, const value_type &val) {
        __fill_assign(n, val);
        return *this;
    }

    template <typename Iter, WJR_REQUIRES(is_iterator_v<Iter>)>
    WJR_CONSTEXPR20 basic_vector &assign(Iter first, Iter last) {
        __range_assign(try_to_address(first), try_to_address(last),
                       iterator_category_t<Iter>());
        return *this;
    }

    WJR_CONSTEXPR20 basic_vector &assign(std::initializer_list<value_type> il) {
        return assign(il.begin(), il.end());
    }

    WJR_PURE WJR_CONSTEXPR20 pointer begin_unsafe() noexcept { return data(); }
    WJR_PURE WJR_CONSTEXPR20 const_pointer begin_unsafe() const noexcept {
        return data();
    }
    WJR_PURE WJR_CONSTEXPR20 const_pointer cbegin_unsafe() const noexcept {
        return data();
    }

    WJR_PURE WJR_CONSTEXPR20 pointer end_unsafe() noexcept { return data() + size(); }
    WJR_PURE WJR_CONSTEXPR20 const_pointer end_unsafe() const noexcept {
        return data() + size();
    }
    WJR_PURE WJR_CONSTEXPR20 const_pointer cend_unsafe() const noexcept {
        return end_unsafe();
    }

    WJR_PURE WJR_CONSTEXPR20 pointer buf_end_unsafe() noexcept {
        return data() + capacity();
    }
    WJR_PURE WJR_CONSTEXPR20 const_pointer buf_end_unsafe() const noexcept {
        return data() + capacity();
    }
    WJR_PURE WJR_CONSTEXPR20 const_pointer cbuf_end_unsafe() const noexcept {
        return buf_end_unsafe();
    }

private:
    WJR_PURE WJR_CONSTEXPR20 iterator __make_iterator(const_pointer ptr) const noexcept {
        return iterator(const_cast<pointer>(ptr), this);
    }

    WJR_PURE WJR_CONSTEXPR20 pointer __get_pointer(iterator ptr) const noexcept {
        ptr.check_same_container(this);
        return (to_address)(ptr);
    }

    WJR_PURE WJR_CONSTEXPR20 pointer __get_pointer(const_iterator ptr) const noexcept {
        ptr.check_same_container(this);
        return const_cast<pointer>((to_address)(ptr));
    }

public:
    WJR_PURE WJR_CONSTEXPR20 iterator begin() noexcept {
        return __make_iterator(begin_unsafe());
    }

    WJR_PURE WJR_CONSTEXPR20 const_iterator begin() const noexcept {
        return __make_iterator(begin_unsafe());
    }

    WJR_PURE WJR_CONSTEXPR20 const_iterator cbegin() const noexcept {
        return __make_iterator(cbegin_unsafe());
    }

    WJR_PURE WJR_CONSTEXPR20 iterator end() noexcept {
        return __make_iterator(end_unsafe());
    }

    WJR_PURE WJR_CONSTEXPR20 const_iterator end() const noexcept {
        return __make_iterator(end_unsafe());
    }

    WJR_PURE WJR_CONSTEXPR20 const_iterator cend() const noexcept {
        return __make_iterator(cend_unsafe());
    }

    WJR_PURE WJR_CONSTEXPR20 reverse_iterator rbegin() noexcept {
        return reverse_iterator(end());
    }

    WJR_PURE WJR_CONSTEXPR20 const_reverse_iterator rbegin() const noexcept {
        return const_reverse_iterator(end());
    }

    WJR_PURE WJR_CONSTEXPR20 const_reverse_iterator crbegin() const noexcept {
        return const_reverse_iterator(cend());
    }

    WJR_PURE WJR_CONSTEXPR20 reverse_iterator rend() noexcept {
        return reverse_iterator(begin());
    }

    WJR_PURE WJR_CONSTEXPR20 const_reverse_iterator rend() const noexcept {
        return const_reverse_iterator(begin());
    }

    WJR_PURE WJR_CONSTEXPR20 const_reverse_iterator crend() const noexcept {
        return const_reverse_iterator(cbegin());
    }

    WJR_PURE WJR_CONSTEXPR20 size_type size() const noexcept {
        return get_storage().size();
    }

    WJR_CONSTEXPR20 void resize(const size_type new_size) { __resize(new_size, vctor); }

    WJR_CONSTEXPR20 void resize(const size_type new_size, const value_type &val) {
        __resize(new_size, val);
    }

    /**
     * @todo designed shrink_to_fit for storage.
     */
    WJR_CONSTEXPR20 void shrink_to_fit() {
        if constexpr (has_vector_storage_shrink_to_fit_v<storage_type>) {
            get_storage().shrink_to_fit();
        } else if constexpr (is_reallocatable::value) {
            const size_type __size = size();
            if (__size < capacity()) {
                auto &al = __get_allocator();

                storage_type new_storage;
                uninitialized_construct(new_storage, __size, __size);

                STraits::uninitialized_move_n_restrict_using_allocator(
                    data(), __size, new_storage.data(), al);
                __destroy_and_deallocate();
                __take_storage(new_storage);
            }
        }
    }

    WJR_PURE WJR_CONSTEXPR20 size_type capacity() const noexcept {
        return get_storage().capacity();
    }

    WJR_PURE WJR_CONSTEXPR20 bool empty() const noexcept {
        if constexpr (has_vector_storage_empty_v<storage_type>) {
            return get_storage().empty();
        } else {
            return size() == 0;
        }
    }

    WJR_CONST WJR_CONSTEXPR20 static size_type
    get_growth_capacity(size_type old_capacity, size_type new_size) noexcept {
        return std::max(old_capacity + (((old_capacity + 6) >> 3) << 2), new_size);
    }

    WJR_CONSTEXPR20 void reserve(size_type n) {
        if constexpr (is_reallocatable::value) {
            const size_type old_size = size();
            const size_type old_capacity = capacity();
            if (WJR_UNLIKELY(old_capacity < n)) {
                auto &al = __get_allocator();
                const size_type new_capacity = get_growth_capacity(old_capacity, n);

                storage_type new_storage;
                uninitialized_construct(new_storage, old_size, new_capacity);

                STraits::uninitialized_move_n_restrict_using_allocator(
                    data(), old_size, new_storage.data(), al);
                __destroy_and_deallocate();
                __take_storage(new_storage);
            }
        }
    }

    WJR_CONSTEXPR20 reference operator[](size_type pos) noexcept {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECKER)
        WJR_ASSERT_LX(pos < size(), "basic_vector::operator[]: out of range");
#endif
        return data()[pos];
    }

    WJR_CONSTEXPR20 const_reference operator[](size_type pos) const noexcept {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECKER)
        WJR_ASSERT_LX(pos < size(), "basic_vector::operator[]: out of range");
#endif
        return data()[pos];
    }

    WJR_CONSTEXPR20 reference at(size_type pos) {
        if (WJR_UNLIKELY(pos >= size())) {
            WJR_THROW(std::out_of_range("basic_vector::at"));
        }

        return data()[pos];
    }

    WJR_CONSTEXPR20 const_reference at(size_type pos) const {
        if (WJR_UNLIKELY(pos >= size())) {
            WJR_THROW(std::out_of_range("basic_vector::at"));
        }

        return data()[pos];
    }

    WJR_CONSTEXPR20 reference front() noexcept {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECKER)
        WJR_ASSERT_LX(size() > 0, "basic_vector::front: empty");
#endif
        return *data();
    }

    WJR_CONSTEXPR20 const_reference front() const noexcept {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECKER)
        WJR_ASSERT_LX(size() > 0, "basic_vector::front: empty");
#endif
        return *data();
    }

    WJR_CONSTEXPR20 reference back() noexcept {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECKER)
        WJR_ASSERT_LX(size() > 0, "basic_vector::back: empty");
#endif
        return *(end_unsafe() - 1);
    }

    WJR_CONSTEXPR20 const_reference back() const noexcept {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECKER)
        WJR_ASSERT_LX(size() > 0, "basic_vector::back: empty");
#endif
        return *(end_unsafe() - 1);
    }

    WJR_PURE WJR_CONSTEXPR20 pointer data() noexcept { return get_storage().data(); }

    WJR_PURE WJR_CONSTEXPR20 const_pointer data() const noexcept {
        return get_storage().data();
    }

    WJR_PURE WJR_CONSTEXPR20 const_pointer cdata() const noexcept { return data(); }

    template <typename... Args>
    WJR_CONSTEXPR20 reference emplace_back(Args &&...args) {
        const pointer __end = end_unsafe();
        const pointer __buf_end = buf_end_unsafe();

        if (WJR_LIKELY(__end != __buf_end)) {
            STraits::uninitialized_construct_using_allocator(__end, __get_allocator(),
                                                             std::forward<Args>(args)...);
            ++__get_size();
        } else {
            __realloc_insert_at_end(std::forward<Args>(args)...);
        }

        return back();
    }

    WJR_CONSTEXPR20 void push_back(const value_type &val) { emplace_back(val); }

    WJR_CONSTEXPR20 void push_back(value_type &&val) { emplace_back(std::move(val)); }

    WJR_CONSTEXPR20 void pop_back() noexcept {
        const size_type __size = --__get_size();
        destroy_at_using_allocator(data() + __size, __get_allocator());
    }

    template <typename... Args>
    WJR_CONSTEXPR20 iterator emplace(const_iterator pos, Args &&...args) {
        return __emplace_aux(__get_pointer(pos), std::forward<Args>(args)...);
    }

    WJR_CONSTEXPR20 iterator insert(const_iterator pos, const value_type &val) {
        return emplace(pos, val);
    }

    WJR_CONSTEXPR20 iterator insert(const_iterator pos, value_type &&val) {
        return emplace(pos, std::move(val));
    }

    WJR_CONSTEXPR20 iterator insert(const_iterator pos,
                                    std::initializer_list<value_type> il) {
        return insert(pos, il.begin(), il.end());
    }

    WJR_CONSTEXPR20 iterator insert(const_iterator pos, size_type n,
                                    const value_type &val) {
        const auto old_pos = static_cast<size_type>(pos - cbegin());
        __fill_insert(data() + old_pos, n, val);
        return begin() + old_pos;
    }

    template <typename Iter, WJR_REQUIRES(is_iterator_v<Iter>)>
    WJR_CONSTEXPR20 iterator insert(const_iterator pos, Iter first, Iter last) {
        const auto old_pos = static_cast<size_type>(pos - cbegin());
        __range_insert(data() + old_pos, try_to_address(first), try_to_address(last),
                       iterator_category_t<Iter>());
        return begin() + old_pos;
    }

    WJR_CONSTEXPR20 iterator erase(const_iterator pos) {
        return __erase(__get_pointer(pos));
    }

    WJR_CONSTEXPR20 iterator erase(const_iterator first, const_iterator last) {
        return __erase(__get_pointer(first), __get_pointer(last));
    }

    WJR_CONSTEXPR20 void swap(basic_vector &other) noexcept {
        storage_fn_type::swap(*this, other);
    }

    WJR_CONSTEXPR20 void clear() {
        __erase_at_end(data());
        WJR_ASSUME(size() == 0);
    }

    WJR_PURE WJR_CONSTEXPR20 allocator_type &get_allocator() noexcept {
        return __get_allocator();
    }
    WJR_PURE WJR_CONSTEXPR20 const allocator_type &get_allocator() const noexcept {
        return __get_allocator();
    }

    // extension

    WJR_CONSTEXPR20 basic_vector(size_type n, dctor_t,
                                 const allocator_type &al = allocator_type())
        : m_pair(std::piecewise_construct, std::forward_as_tuple(al),
                 std::forward_as_tuple()) {
        __construct_n(n, dctor);
    }

    WJR_CONSTEXPR20 basic_vector(size_type n, in_place_reserve_t,
                                 const allocator_type &al = allocator_type())
        : m_pair(std::piecewise_construct, std::forward_as_tuple(al),
                 std::forward_as_tuple()) {
        uninitialized_construct(0, n);
    }

    WJR_CONSTEXPR20 void resize(const size_type new_size, dctor_t) {
        __resize(new_size, dctor);
    }

    WJR_CONSTEXPR20 void push_back(dctor_t) { emplace_back(dctor); }

    WJR_CONSTEXPR20 basic_vector &append(const value_type &val) {
        emplace_back(val);
        return *this;
    }

    WJR_CONSTEXPR20 basic_vector &append(value_type &&val) {
        emplace_back(std::move(val));
        return *this;
    }

    WJR_CONSTEXPR20 basic_vector &append(dctor_t) {
        emplace_back(dctor);
        return *this;
    }

    WJR_CONSTEXPR20 basic_vector &append(const size_type n, const value_type &val) {
        __append(n, val);
        return *this;
    }

    WJR_CONSTEXPR20 basic_vector &append(const size_type n, dctor_t) {
        __append(n, dctor);
        return *this;
    }

    template <typename Iter, WJR_REQUIRES(is_iterator_v<Iter>)>
    WJR_CONSTEXPR20 basic_vector &append(Iter first, Iter last) {
        __range_append(try_to_address(first), try_to_address(last),
                       iterator_category_t<Iter>());
        return *this;
    }

    WJR_CONSTEXPR20 basic_vector &append(std::initializer_list<value_type> il) {
        return append(il.begin(), il.end());
    }

    /**
     * @brief Pop n elements from the end
     *
     */
    WJR_CONSTEXPR20 basic_vector &chop(const size_type n) {
        __erase_at_end(end_unsafe() - n);
        return *this;
    }

    /**
     * @brief Truncate the size to n
     *
     */
    WJR_CONSTEXPR20 basic_vector &truncate(const size_type n) { return chop(size() - n); }

    template <typename Iter, WJR_REQUIRES(is_iterator_v<Iter>)>
    WJR_CONSTEXPR20 basic_vector &replace(const_iterator from, const_iterator to,
                                          Iter first, Iter last) {
        __range_replace(__get_pointer(from), __get_pointer(to), try_to_address(first),
                        try_to_address(last), iterator_category_t<Iter>());
        return *this;
    }

    WJR_CONSTEXPR20 basic_vector &replace(const_iterator from, const_iterator to,
                                          const size_type n, const value_type &val) {
        __fill_replace(__get_pointer(from), __get_pointer(to), n, val);
        return *this;
    }

    WJR_PURE WJR_CONSTEXPR20 storage_type &get_storage() noexcept {
        return m_pair.second();
    }

    WJR_PURE WJR_CONSTEXPR20 const storage_type &get_storage() const noexcept {
        return m_pair.second();
    }

    WJR_CONSTEXPR20 void take_storage(storage_type &other) noexcept {
        get_storage().take_storage(other, __get_allocator());
    }

    WJR_CONSTEXPR20 void uninitialized_construct(storage_type &other, size_type siz,
                                                 size_type cap) noexcept {
        get_storage().uninitialized_construct(other, siz, cap, __get_allocator());
    }

    WJR_CONSTEXPR20 void uninitialized_construct(size_type siz, size_type cap) noexcept {
        if constexpr (has_vector_storage_uninitialized_construct_v<storage_type,
                                                                   size_type, _Alty>) {
            get_storage().uninitialized_construct(siz, cap, __get_allocator());
        } else {
            uninitialized_construct(get_storage(), siz, cap);
        }
    }

private:
    // member function for container_fn (START)

    WJR_PURE WJR_CONSTEXPR20 _Alty &__get_allocator() noexcept { return m_pair.first(); }

    WJR_PURE WJR_CONSTEXPR20 const _Alty &__get_allocator() const noexcept {
        return m_pair.first();
    }

    WJR_CONSTEXPR20 void __destroy() noexcept(__storage_noexcept_destroy) {
        get_storage().destroy(__get_allocator());
    }

    WJR_CONSTEXPR20 void
    __destroy_and_deallocate() noexcept(__storage_noexcept_destroy_and_deallocate) {
        get_storage().destroy_and_deallocate(__get_allocator());
    }

    WJR_CONSTEXPR20 void __copy_element(const basic_vector &other) {
        assign(other.begin_unsafe(), other.end_unsafe());
    }

    WJR_CONSTEXPR20 void __take_storage(basic_vector &&other) noexcept(
        noexcept(__take_storage(other.get_storage()))) {
        __take_storage(other.get_storage());
    }

    WJR_CONSTEXPR20 void __move_element(basic_vector &&other) {
        assign(std::make_move_iterator(other.begin_unsafe()),
               std::make_move_iterator(other.end_unsafe()));
    }

    WJR_CONSTEXPR20 void
    __swap_storage(basic_vector &other) noexcept(__storage_noexcept_swap_storage) {
        get_storage().swap_storage(other.get_storage(), __get_allocator());
    }

    // member function for container_fn (END)

    WJR_PURE WJR_CONSTEXPR20 __get_size_t __get_size() noexcept {
        return get_storage().size();
    }

    WJR_CONSTEXPR20 void
    __take_storage(storage_type &other) noexcept(__storage_noexcept_take_storage) {
        take_storage(other);
    }

    WJR_NORETURN WJR_CONSTEXPR20 void
    __unreallocatable_unreachable(WJR_MAYBE_UNUSED size_type new_capacity) const {
        WJR_ASSERT(
            new_capacity <= capacity(),
            "new_capacity must be less than or equal to capacity if the storage is not reallocatable.\nnew_capacity = ",
            new_capacity, ", capacity = ", capacity());
        WJR_UNREACHABLE();
    }

    template <typename... Args,
              WJR_REQUIRES(sizeof...(Args) == 1 || sizeof...(Args) == 2)>
    WJR_CONSTEXPR20 void __construct_n(const size_type n, Args &&...args) {
        if (n != 0) {
            auto &al = __get_allocator();
            uninitialized_construct(get_storage(), n, n);
            if constexpr (sizeof...(Args) == 1) {
                STraits::uninitialized_fill_n_using_allocator(
                    data(), n, al, std::forward<Args>(args)...);
            } else if constexpr (sizeof...(Args) == 2) {
                STraits::uninitialized_copy_restrict_using_allocator(
                    std::forward<Args>(args)..., data(), al);
            }
        }
    }

    template <typename Iter>
    WJR_CONSTEXPR20 void __range_construct(Iter first, Iter last,
                                           std::input_iterator_tag) {
        for (; first != last; ++first) {
            emplace_back(*first);
        }
    }

    template <typename Iter>
    WJR_CONSTEXPR20 void __range_construct(Iter first, Iter last,
                                           std::forward_iterator_tag) {
        const auto n = static_cast<size_type>(std::distance(first, last));
        __construct_n(n, first, last);
    }

    WJR_CONSTEXPR20 void __erase_at_end(pointer pos) noexcept {
        const pointer __begin = data();
        const pointer __end = end_unsafe();
        WJR_ASSERT_L1(pos >= __begin && pos <= __end,
                      "pos must be in the range of [begin(), end()]");
        const auto new_size = static_cast<size_type>(pos - __begin);
        destroy_using_allocator(__begin + new_size, __end, __get_allocator());
        __get_size() = new_size;
    }

    WJR_CONSTEXPR20 iterator __erase(pointer pos) noexcept {
        const pointer __end = end_unsafe();
        if (pos + 1 != __end) {
            STraits::move(pos + 1, __end, pos);
        }

        destroy_at_using_allocator(__end - 1, __get_allocator());
        --__get_size();
        return __make_iterator(pos);
    }

    WJR_CONSTEXPR20 iterator __erase(pointer first, pointer last) noexcept {
        const pointer __end = end_unsafe();
        if (WJR_LIKELY(first != last)) {
            if (last != __end) {
                STraits::move(last, __end, first);
            }

            __erase_at_end(__end - (last - first));
        }

        return __make_iterator(first);
    }

    template <typename Iter>
    WJR_CONSTEXPR20 void __range_insert(pointer pos, Iter first, Iter last,
                                        std::input_iterator_tag) {
        if (pos == end_unsafe()) {
            __range_append(first, last, std::input_iterator_tag());
        } else if (first != last) {
            basic_vector tmp(first, last, __get_allocator());
            __range_insert(pos, tmp.begin_unsafe(), tmp.end_unsafe(),
                           std::forward_iterator_tag());
        }
    }

    template <typename Iter>
    WJR_CONSTEXPR20 void __range_insert(pointer pos, Iter first, Iter last,
                                        std::forward_iterator_tag) {
        if (WJR_UNLIKELY(first == last)) {
            return;
        }

        auto &al = __get_allocator();
        const pointer __begin = data();
        const pointer __end = end_unsafe();
        const pointer __buf_end = buf_end_unsafe();

        const auto n = static_cast<size_type>(std::distance(first, last));
        const auto __rest = static_cast<size_type>(__buf_end - __end);

        if (WJR_LIKELY(__rest >= n)) {
            const auto __elements_after = static_cast<size_type>(__end - pos);
            if (__elements_after > n) {
                STraits::uninitialized_move_n_restrict_using_allocator(__end - n, n,
                                                                       __end, al);
                STraits::move_backward(pos, __end - n, __end);
                STraits::copy_restrict(first, last, pos);
            } else {
                const auto mid = std::next(first, __elements_after);

                STraits::uninitialized_copy_restrict_using_allocator(mid, last, __end,
                                                                     al);
                STraits::uninitialized_move_restrict_using_allocator(pos, __end, pos + n,
                                                                     al);
                STraits::copy_restrict(first, mid, pos);
            }

            __get_size() += n;
        } else {
            if constexpr (is_reallocatable::value) {
                const auto old_size = static_cast<size_type>(__end - __begin);
                const auto old_pos = static_cast<size_type>(pos - __begin);
                const size_type new_capacity =
                    get_growth_capacity(capacity(), old_size + n);

                storage_type new_storage;
                uninitialized_construct(new_storage, old_size + n, new_capacity);

                const pointer __new_begin = new_storage.data();

                STraits::uninitialized_copy_restrict_using_allocator(
                    first, last, __new_begin + old_pos, al);
                STraits::uninitialized_move_restrict_using_allocator(__begin, pos,
                                                                     __new_begin, al);
                STraits::uninitialized_move_restrict_using_allocator(
                    pos, __end, __new_begin + old_pos + n, al);

                __destroy_and_deallocate();
                __take_storage(new_storage);
            } else {
                __unreallocatable_unreachable(size() + n);
            }
        }
    }

    template <typename Iter>
    WJR_CONSTEXPR20 void __range_append(Iter first, Iter last, std::input_iterator_tag) {
        for (; first != last; ++first) {
            emplace_back(*first);
        }
    }

    template <typename Iter>
    WJR_CONSTEXPR20 void __range_append(Iter first, Iter last,
                                        std::forward_iterator_tag) {
        if (WJR_UNLIKELY(first == last)) {
            return;
        }

        auto &al = __get_allocator();
        const pointer __begin = data();
        const pointer __end = end_unsafe();
        const pointer __buf_end = buf_end_unsafe();

        const auto n = static_cast<size_type>(std::distance(first, last));
        const auto __rest = static_cast<size_type>(__buf_end - __end);

        if (WJR_LIKELY(__rest >= n)) {
            STraits::uninitialized_copy_n_restrict_using_allocator(first, n, __end, al);
            __get_size() += n;
        } else {
            if constexpr (is_reallocatable::value) {
                const auto old_size = static_cast<size_type>(__end - __begin);
                const size_type new_capacity =
                    get_growth_capacity(capacity(), old_size + n);

                storage_type new_storage;
                uninitialized_construct(new_storage, old_size + n, new_capacity);

                const pointer __new_begin = new_storage.data();

                STraits::uninitialized_copy_restrict_using_allocator(
                    first, last, __new_begin + old_size, al);
                STraits::uninitialized_move_restrict_using_allocator(__begin, __end,
                                                                     __new_begin, al);

                __destroy_and_deallocate();
                __take_storage(new_storage);
            } else {
                __unreallocatable_unreachable(size() + n);
            }
        }
    }

    template <typename Iter>
    WJR_CONSTEXPR20 void __range_assign(Iter first, Iter last, std::input_iterator_tag) {
        pointer cur = data();
        const pointer __end = end_unsafe();

        for (; first != last && cur != __end; ++first, ++cur) {
            *cur = *first;
        }

        if (first == last) {
            __erase_at_end(cur);
        } else {
            __range_append(first, last, std::input_iterator_tag());
        }
    }

    template <typename Iter>
    WJR_CONSTEXPR20 void __range_assign(Iter first, Iter last,
                                        std::forward_iterator_tag) {
        auto n = static_cast<size_type>(std::distance(first, last));
        auto &al = __get_allocator();
        const pointer __begin = data();
        const pointer __end = end_unsafe();

        if (n <= size()) {
            STraits::copy_restrict(first, last, __begin);
            __erase_at_end(__begin + n);
        } else if (WJR_LIKELY(n <= capacity())) {
            auto mid = first;
            std::advance(mid, size());
            STraits::copy_restrict(first, mid, data());
            STraits::uninitialized_copy_restrict_using_allocator(mid, last, __end, al);
            __get_size() = n;
        } else {
            if constexpr (is_reallocatable::value) {
                size_type new_capacity = get_growth_capacity(capacity(), n);

                storage_type new_storage;
                uninitialized_construct(new_storage, n, new_capacity);

                const pointer __new_begin = new_storage.data();
                STraits::uninitialized_copy_n_restrict_using_allocator(first, n,
                                                                       __new_begin, al);

                __destroy_and_deallocate();
                __take_storage(new_storage);
            } else {
                __unreallocatable_unreachable(n);
            }
        }
    }

    WJR_CONSTEXPR20 void __fill_assign(size_type n, const value_type &val) {
        auto &al = __get_allocator();

        if (WJR_UNLIKELY(n > capacity())) {
            if constexpr (is_reallocatable::value) {
                __destroy_and_deallocate();

                storage_type new_storage;
                uninitialized_construct(new_storage, n, n);

                STraits::uninitialized_fill_n_using_allocator(new_storage.data(), n, al,
                                                              val);
                __take_storage(new_storage);
                return;
            } else {
                __unreallocatable_unreachable(n);
            }
        }

        if (n > size()) {
            STraits::fill(begin_unsafe(), end_unsafe(), val);
            STraits::uninitialized_fill_n_using_allocator(end_unsafe(), n - size(), al,
                                                          val);
            __get_size() = n;
        } else {
            __erase_at_end(STraits::fill_n(begin_unsafe(), n, val));
        }
    }

    template <typename... Args>
    WJR_CONSTEXPR20 void __realloc_insert(pointer pos, Args &&...args) {
        if constexpr (is_reallocatable::value) {
            auto &al = __get_allocator();
            const pointer __begin = data();
            const pointer __end = end_unsafe();

            const auto old_pos_size = static_cast<size_type>(pos - __begin);
            const auto old_size = static_cast<size_type>(__end - __begin);
            const size_type new_size = old_size + 1;
            const size_type new_capacity = get_growth_capacity(old_size, new_size);

            storage_type new_storage;
            uninitialized_construct(new_storage, new_size, new_capacity);

            const pointer __new_begin = new_storage.data();
            const pointer new_pos = __new_begin + old_pos_size;

            STraits::uninitialized_construct_using_allocator(new_pos, al,
                                                             std::forward<Args>(args)...);

            STraits::uninitialized_move_n_restrict_using_allocator(__begin, old_pos_size,
                                                                   __new_begin, al);
            STraits::uninitialized_move_restrict_using_allocator(pos, __end, new_pos + 1,
                                                                 al);

            __destroy_and_deallocate();
            __take_storage(new_storage);
        } else {
            __unreallocatable_unreachable(size() + 1);
        }
    }

    template <typename... Args>
    WJR_CONSTEXPR20 void __realloc_insert_at_end(Args &&...args) {
        if constexpr (is_reallocatable::value) {
            auto &al = __get_allocator();
            const pointer __begin = data();
            const pointer __end = end_unsafe();

            const auto old_size = static_cast<size_type>(__end - __begin);
            const auto new_size = old_size + 1;
            const size_type new_capacity = get_growth_capacity(old_size, new_size);

            storage_type new_storage;
            uninitialized_construct(new_storage, new_size, new_capacity);

            const pointer __new_begin = new_storage.data();

            const pointer new_pos = __new_begin + old_size;
            STraits::uninitialized_construct_using_allocator(new_pos, al,
                                                             std::forward<Args>(args)...);

            STraits::uninitialized_move_restrict_using_allocator(__begin, __end,
                                                                 __new_begin, al);

            __destroy_and_deallocate();
            __take_storage(new_storage);
        } else {
            __unreallocatable_unreachable(size() + 1);
        }
    }

    WJR_CONSTEXPR20 void __fill_insert(pointer pos, size_type n, const value_type &val) {
        if (WJR_UNLIKELY(n == 0)) {
            return;
        }

        auto &al = __get_allocator();
        const pointer __begin = data();
        const pointer __end = end_unsafe();
        const pointer __buf_end = buf_end_unsafe();

        const auto __rest = static_cast<size_type>(__buf_end - __end);

        if (WJR_LIKELY(__rest >= n)) {
            const temporary_value_allocator tmp(al, val);
            const auto &real_val = *tmp.get();

            const auto __elements_after = static_cast<size_type>(__end - pos);
            if (__elements_after > n) {
                STraits::uninitialized_move_n_restrict_using_allocator(__end - n, n,
                                                                       __end, al);
                STraits::move_backward(pos, __end - n, __end);
                STraits::fill_n(pos, n, real_val);
            } else {
                STraits::uninitialized_fill_n_using_allocator(__end, n - __elements_after,
                                                              al, real_val);
                STraits::uninitialized_move_restrict_using_allocator(pos, __end, pos + n,
                                                                     al);
                STraits::fill(pos, __end, real_val);
            }

            __get_size() += n;
        } else {
            const auto old_size = static_cast<size_type>(__end - __begin);
            if constexpr (is_reallocatable::value) {
                const auto new_capacity = get_growth_capacity(capacity(), old_size + n);

                storage_type new_storage;
                uninitialized_construct(new_storage, old_size + n, new_capacity);

                const pointer __new_begin = new_storage.data();

                const auto old_pos = static_cast<size_type>(pos - __begin);

                STraits::uninitialized_fill_n_using_allocator(__new_begin + old_pos, n,
                                                              al, val);
                STraits::uninitialized_move_restrict_using_allocator(__begin, pos,
                                                                     __new_begin, al);
                STraits::uninitialized_move_restrict_using_allocator(
                    pos, __end, __new_begin + old_pos + n, al);

                __destroy_and_deallocate();
                __take_storage(new_storage);
            } else {
                __unreallocatable_unreachable(old_size + n);
            }
        }
    }

    template <typename Ty>
    WJR_CONSTEXPR20 void __resize(const size_type new_size, const Ty &val) {
        const auto old_size = size();

        if constexpr (is_reallocatable::value) {
            if (new_size > old_size) {
                __append(new_size - old_size, val);
            } else if (new_size < old_size) {
                __erase_at_end(data() + new_size);
            }
        } else {
            auto &al = __get_allocator();

            const pointer __begin = data();
            const pointer __end = data() + old_size;

            if (WJR_UNLIKELY(new_size > capacity())) {
                __unreallocatable_unreachable(new_size);
            }

            if (new_size > old_size) {
                STraits::uninitialized_fill_n_using_allocator(__end, new_size - old_size,
                                                              al, val);
            } else if (new_size < old_size) {
                destroy_using_allocator(__begin + new_size, __end, al);
            }

            __get_size() = new_size;
        }
    }

    template <typename Ty>
    WJR_CONSTEXPR20 void __append(size_type n, const Ty &val) {
        auto &al = __get_allocator();

        const auto old_size = size();
        const auto old_capacity = capacity();

        const pointer __begin = data();
        const pointer __end = __begin + old_size;

        const auto __rest = old_capacity - old_size;
        const auto new_size = old_size + n;

        if (WJR_LIKELY(__rest >= n)) {
            STraits::uninitialized_fill_n_using_allocator(__end, n, al, val);
            __get_size() = new_size;
        } else {
            if constexpr (is_reallocatable::value) {
                auto new_capacity = get_growth_capacity(old_capacity, new_size);

                storage_type new_storage;
                uninitialized_construct(new_storage, new_size, new_capacity);

                const pointer __new_begin = new_storage.data();

                STraits::uninitialized_fill_n_using_allocator(__new_begin + old_size, n,
                                                              al, val);
                STraits::uninitialized_move_restrict_using_allocator(__begin, __end,
                                                                     __new_begin, al);

                __destroy_and_deallocate();
                __take_storage(new_storage);
            } else {
                __unreallocatable_unreachable(new_size);
            }
        }
    }

    template <typename Args>
    WJR_CONSTEXPR20 void __insert_aux(pointer pos, Args &&args) {
        auto &al = __get_allocator();
        const pointer __end = end_unsafe();

        STraits::uninitialized_construct_using_allocator(__end, al,
                                                         std::move(*(__end - 1)));

        STraits::move_backward(pos, __end - 1, __end);
        *pos = std::forward<Args>(args);

        ++__get_size();
    }

    template <typename... Args>
    WJR_CONSTEXPR20 iterator __emplace_aux(pointer pos, Args &&...args) {
        auto &al = __get_allocator();
        const pointer __end = end_unsafe();
        const pointer __buf_end = buf_end_unsafe();

        if (WJR_LIKELY(__end != __buf_end)) {
            if (pos == __end) {
                STraits::uninitialized_construct_using_allocator(
                    __end, al, std::forward<Args>(args)...);
                ++__get_size();
            } else {
                temporary_value_allocator tmp(al, std::forward<Args>(args)...);
                __insert_aux(pos, std::move(*tmp.get()));
            }

            return __make_iterator(pos);
        }

        const size_type offset = static_cast<size_type>(pos - data());
        __realloc_insert(pos, std::forward<Args>(args)...);
        return begin() + offset;
    }

    template <typename Iter>
    WJR_CONSTEXPR20 void __range_replace(pointer old_first, pointer old_last,
                                         Iter new_begin, Iter new_last,
                                         std::input_iterator_tag) {
        for (; old_first != old_last && new_begin != new_last; ++old_first, ++new_begin) {
            *old_first = *new_begin;
        }

        if (new_begin == new_last) {
            __erase_at_end(old_first, old_last);
        } else {
            __range_insert(old_last, new_begin, new_last, std::input_iterator_tag());
        }
    }

    template <typename Iter>
    WJR_CONSTEXPR20 void __range_replace(pointer old_first, pointer old_last,
                                         Iter new_begin, Iter new_last,
                                         std::forward_iterator_tag) {
        const auto n = static_cast<size_type>(old_last - old_first);
        const auto m = static_cast<size_type>(std::distance(new_begin, new_last));

        if (m <= n) {
            erase(STraits::copy_n(new_begin, m, old_first), old_last);
        } else {
            const auto __delta = m - n;

            auto &al = __get_allocator();
            const auto __begin = data();
            const auto __end = end_unsafe();
            const auto __buf_end = buf_end_unsafe();

            const auto __rest = static_cast<size_type>(__buf_end - __end);

            if (WJR_LIKLELY(__rest >= __delta)) {
                const auto __elements_after = static_cast<size_type>(__end - old_first);
                if (__elements_after > m) {
                    STraits::uninitialized_move_using_allocator(__end - __delta, __end,
                                                                __end, al);
                    STraits::move_backward(old_last, __end - __delta, __end);
                    STraits::copy(new_begin, new_last, old_first);
                } else {
                    auto mid = new_begin;
                    std::advance(mid, __elements_after);
                    STraits::uninitialized_copy_using_allocator(mid, new_last, __end, al);
                    STraits::uninitialized_move_using_allocator(old_last, __end,
                                                                old_first + m, al);
                    STraits::copy(new_begin, mid, old_first);
                }
                __get_size() += __delta;
            } else {
                if constexpr (is_reallocatable::value) {
                    const auto old_size = static_cast<size_type>(__end - __begin);
                    const auto old_pos = static_cast<size_type>(old_first - __begin);
                    const auto new_capacity =
                        get_growth_capacity(capacity(), old_size + __delta);

                    storage_type new_storage;
                    uninitialized_construct(new_storage, old_size + __delta,
                                            new_capacity);

                    const pointer __new_begin = new_storage.data();

                    STraits::uninitialized_copy_restrict_using_allocator(
                        new_begin, new_last, __new_begin + old_pos, al);
                    STraits::uninitialized_move_restrict_using_allocator(
                        __begin, old_first, __new_begin, al);
                    STraits::uninitialized_move_restrict_using_allocator(
                        old_last, __end, __new_begin + old_pos + m, al);

                    __destroy_and_deallocate();
                    __take_storage(new_storage);
                } else {
                    __unreallocatable_unreachable(size() + __delta);
                }
            }
        }
    }

    WJR_CONSTEXPR20 void __fill_replace(pointer old_first, pointer old_last, size_type m,
                                        const value_type &val) {
        const auto n = static_cast<size_type>(old_last - old_first);

        if (m <= n) {
            __erase(STraits::fill_n(old_first, m, val), old_last);
        } else {
            const auto __delta = m - n;

            auto &al = __get_allocator();
            const auto __begin = data();
            const auto __end = end_unsafe();
            const auto __buf_end = buf_end_unsafe();

            const auto __rest = static_cast<size_type>(__buf_end - __end);

            if (WJR_LIKELY(__rest >= __delta)) {
                const temporary_value_allocator tmp(al, val);
                const auto &real_value = *tmp.get();

                const auto __elements_after = static_cast<size_type>(__end - old_first);
                if (__elements_after > m) {
                    STraits::uninitialized_move_using_allocator(__end - __delta, __end,
                                                                __end, al);
                    STraits::move_backward(old_last, __end - __delta, __end);
                    STraits::fill_n(old_first, m, real_value);
                } else {
                    STraits::uninitialized_fill_n_using_allocator(
                        __end, m - __elements_after, al, real_value);
                    STraits::uninitialized_move_using_allocator(old_last, __end,
                                                                old_first + m, al);
                    STraits::fill(old_first, __end, real_value);
                }
                __get_size() += __delta;
            } else {
                if constexpr (is_reallocatable::value) {
                    const auto old_size = static_cast<size_type>(__end - __begin);
                    const auto old_pos = static_cast<size_type>(old_first - __begin);
                    const auto new_capacity =
                        get_growth_capacity(capacity(), old_size + __delta);

                    storage_type new_storage;
                    uninitialized_construct(new_storage, old_size + __delta,
                                            new_capacity);

                    const pointer __ptr = new_storage.data();

                    STraits::uninitialized_fill_n_using_allocator(__ptr + old_pos, m, al,
                                                                  val);
                    STraits::uninitialized_move_restrict_using_allocator(
                        __begin, old_first, __ptr, al);
                    STraits::uninitialized_move_restrict_using_allocator(
                        old_last, __end, __ptr + old_pos + m, al);

                    __destroy_and_deallocate();
                    __take_storage(new_storage);
                } else {
                    __unreallocatable_unreachable(size() + __delta);
                }
            }
        }
    }

private:
    compressed_pair<_Alty, storage_type> m_pair;
};

template <typename T, typename Alloc = std::allocator<T>>
using vector = basic_vector<default_vector_storage<T, Alloc>>;

/**
 * @brief A vector with elements stored on the stack.
 *
 */
template <typename T, size_t Capacity, typename Alloc = std::allocator<T>>
using static_vector = basic_vector<static_vector_storage<T, Capacity, Alloc>>;

/**
 * @brief A vector with fixed capacity by construction.
 *
 * @details Only allocate memory on construction and deallocation on destruction.
 * After construction, it cannot be expanded and can only be modified through move
 * assignment. For example, vector that using stack allocator.
 */
template <typename T, typename Alloc = std::allocator<T>>
using fixed_vector = basic_vector<fixed_vector_storage<T, Alloc>>;

template <typename T, size_t Capacity, typename Alloc = std::allocator<T>>
using sso_vector = basic_vector<sso_vector_storage<T, Capacity, Alloc>>;

template <typename Iter, typename T = iterator_value_t<Iter>,
          typename Alloc = std::allocator<T>, WJR_REQUIRES(is_iterator_v<Iter>)>
basic_vector(Iter, Iter, Alloc = Alloc())
    -> basic_vector<default_vector_storage<T, Alloc>>;

template <typename Storage>
void swap(basic_vector<Storage> &lhs, basic_vector<Storage> &rhs) noexcept {
    lhs.swap(rhs);
}

template <typename Storage>
bool operator==(const basic_vector<Storage> &lhs, const basic_vector<Storage> &rhs) {
    return std::equal(lhs.begin_unsafe(), lhs.end_unsafe(), rhs.begin_unsafe(),
                      rhs.end_unsafe());
}

template <typename Storage>
bool operator!=(const basic_vector<Storage> &lhs, const basic_vector<Storage> &rhs) {
    return !(lhs == rhs);
}

template <typename Storage>
bool operator<(const basic_vector<Storage> &lhs, const basic_vector<Storage> &rhs) {
    return std::lexicographical_compare(lhs.begin_unsafe(), lhs.end_unsafe(),
                                        rhs.begin_unsafe(), rhs.end_unsafe());
}

template <typename Storage>
bool operator>(const basic_vector<Storage> &lhs, const basic_vector<Storage> &rhs) {
    return rhs < lhs;
}

template <typename Storage>
bool operator<=(const basic_vector<Storage> &lhs, const basic_vector<Storage> &rhs) {
    return !(rhs < lhs);
}

template <typename Storage>
bool operator>=(const basic_vector<Storage> &lhs, const basic_vector<Storage> &rhs) {
    return !(lhs < rhs);
}

} // namespace wjr

#endif // WJR_CONTAINER_GENERIC_CONTAINER_VECTOR_HPP__

#endif // WJR_VECTOR_HPP__

namespace wjr {

template <typename Container>
struct container_traits : __container_traits_base<Container> {};

template <typename T, size_t N>
struct container_traits<std::array<T, N>> : __container_traits_base<std::array<T, N>> {
    constexpr static bool is_contiguous_v = true;
    constexpr static bool is_trivially_contiguous_v = true;
};

template <typename T, typename Alloc>
struct container_traits<std::vector<T, Alloc>>
    : __container_traits_base<std::vector<T, Alloc>> {
    constexpr static bool is_contiguous_v = true;
    constexpr static bool is_trivially_contiguous_v = true;
};

template <typename CharT, typename Traits, typename Alloc>
struct container_traits<std::basic_string<CharT, Traits, Alloc>>
    : __container_traits_base<std::basic_string<CharT, Traits, Alloc>> {
    constexpr static bool is_contiguous_v = true;
    constexpr static bool is_trivially_contiguous_v =
        std::is_same_v<Traits, std::char_traits<CharT>>;
};

template <typename Storage>
struct container_traits<basic_vector<Storage>>
    : __container_traits_base<basic_vector<Storage>> {
    constexpr static bool is_contiguous_v = true;
    constexpr static bool is_trivially_contiguous_v =
        basic_vector<Storage>::is_trivially_contiguous::value;
};

} // namespace wjr

#endif // WJR_CONTAINER_GENERIC_TYPE_TRAITS_HPP__
#ifndef WJR_MATH_BIT_HPP__
#define WJR_MATH_BIT_HPP__

#ifndef WJR_MATH_CLZ_HPP__
#define WJR_MATH_CLZ_HPP__

#ifndef WJR_MATH_POPCOUNT_HPP__
#define WJR_MATH_POPCOUNT_HPP__

// Already included

namespace wjr {

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR int fallback_popcount(T x) {
    constexpr auto nd = std::numeric_limits<T>::digits;
    if constexpr (nd < 32) {
        return fallback_popcount(static_cast<uint32_t>(x));
    } else {
        if constexpr (nd == 32) {
            x -= (x >> 1) & 0x5555'5555;
            x = (x & 0x3333'3333) + ((x >> 2) & 0x3333'3333);
            x = (x + (x >> 4)) & 0x0f0f'0f0f;
            return (x * 0x0101'0101) >> 24;
        } else {
            x -= (x >> 1) & 0x5555'5555'5555'5555;
            x = (x & 0x3333'3333'3333'3333) + ((x >> 2) & 0x3333'3333'3333'3333);
            x = (x + (x >> 4)) & 0x0f0f'0f0f'0f0f'0f0f;
            return (x * 0x0101'0101'0101'0101) >> 56;
        }
    }
}

#if WJR_HAS_BUILTIN(__builtin_popcount)
#define WJR_HAS_BUILTIN_POPCOUNT WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(POPCOUNT)

template <typename T>
WJR_CONST WJR_INTRINSIC_INLINE int builtin_popcount(T x) {
    constexpr auto nd = std::numeric_limits<T>::digits;
    if constexpr (nd < 32) {
        return builtin_popcount(static_cast<uint32_t>(x));
    } else {
        if constexpr (nd <= std::numeric_limits<unsigned int>::digits) {
            return __builtin_popcount(x);
        } else if constexpr (nd <= std::numeric_limits<unsigned long>::digits) {
            return __builtin_popcountl(x);
        }
        if constexpr (nd <= std::numeric_limits<unsigned long long>::digits) {
            return __builtin_popcountll(x);
        } else {
            static_assert(nd <= 64, "not support yet");
        }
    }
}

#endif // WJR_HAS_BUILTIN(POPCOUNT)

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int popcount_impl(T x) {
    if (WJR_BUILTIN_CONSTANT_P(is_zero_or_single_bit(x)) && is_zero_or_single_bit(x)) {
        return x != 0;
    }

#if WJR_HAS_BUILTIN(POPCOUNT)
    if (is_constant_evaluated() || WJR_BUILTIN_CONSTANT_P(x)) {
        return fallback_popcount(x);
    }

    return builtin_popcount(x);
#else
    return fallback_popcount(x);
#endif
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int popcount(T x) {
    int ret = popcount_impl(x);
    WJR_ASSUME(0 <= ret && ret <= std::numeric_limits<T>::digits);
    return ret;
}

} // namespace wjr

#endif // WJR_MATH_POPCOUNT_HPP__

namespace wjr {

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int fallback_clz_impl(T x) {
    constexpr auto nd = std::numeric_limits<T>::digits;

#if !(WJR_HAS_BUILTIN(POPCOUNT) && WJR_HAS_SIMD(POPCNT))
    if constexpr (nd >= 32) {
#endif
        x |= (x >> 1);
        x |= (x >> 2);
        x |= (x >> 4);

        if constexpr (nd >= 16) {
            x |= (x >> 8);
        }

        if constexpr (nd >= 32) {
            x |= (x >> 16);
        }

        if constexpr (nd >= 64) {
            x |= (x >> 32);
        }
#if !(WJR_HAS_BUILTIN(POPCOUNT) && WJR_HAS_SIMD(POPCNT))
    }
#endif

#if WJR_HAS_BUILTIN(POPCOUNT) && WJR_HAS_SIMD(POPCNT)
    return popcount<T>(~x);
#else
    if constexpr (nd < 32) {
        return fallback_clz_impl(static_cast<uint32_t>(x)) - (32 - nd);
    } else {
        ++x;

        if constexpr (nd <= 32) {
            return math_details::de_bruijn32.getr(x);
        } else if constexpr (nd <= 64) {
            return math_details::de_bruijn64.getr(x);
        } else {
            static_assert(nd <= 64, "not support yet");
        }
    }
#endif
}

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int fallback_clz(T x) {
    return fallback_clz_impl(x);
}

#if WJR_HAS_BUILTIN(__builtin_clz)
#define WJR_HAS_BUILTIN_CLZ WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(CLZ)

template <typename T>
WJR_CONST WJR_INTRINSIC_INLINE int builtin_clz_impl(T x) {
    constexpr auto nd = std::numeric_limits<T>::digits;

    if constexpr (nd < 32) {
        return builtin_clz_impl(static_cast<uint32_t>(x)) - (32 - nd);
    } else {
        if constexpr (nd <= std::numeric_limits<unsigned int>::digits) {
            constexpr auto delta = std::numeric_limits<unsigned int>::digits - nd;
            return __builtin_clz(static_cast<unsigned int>(x)) - delta;
        } else if constexpr (nd <= std::numeric_limits<unsigned long>::digits) {
            constexpr auto delta = std::numeric_limits<unsigned long>::digits - nd;
            return __builtin_clzl(static_cast<unsigned long>(x)) - delta;
        } else if constexpr (nd <= std::numeric_limits<unsigned long long>::digits) {
            constexpr auto delta = std::numeric_limits<unsigned long long>::digits - nd;
            return __builtin_clzll(static_cast<unsigned long long>(x)) - delta;
        } else {
            static_assert(nd <= 64, "not supported yet");
        }
    }
}

template <typename T>
WJR_CONST WJR_INTRINSIC_INLINE int builtin_clz(T x) {
    return builtin_clz_impl(x);
}

#endif

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int clz_impl(T x) {
#if WJR_HAS_BUILTIN(CLZ)
    if (is_constant_evaluated() || WJR_BUILTIN_CONSTANT_P(x)) {
        return fallback_clz(x);
    }

    return builtin_clz(x);
#else
    return fallback_clz(x);
#endif
}

/**
 * @brief Fast count leading zeros
 *
 * @tparam T Must be an unsigned integral type
 */
template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int clz(T x) {
    WJR_ASSERT_ASSUME_L1(x != 0);
    const int ret = clz_impl(x);
    WJR_ASSUME(0 <= ret && ret < std::numeric_limits<T>::digits);
    return ret;
}

} // namespace wjr

#endif // WJR_MATH_CLZ_HPP__
#ifndef WJR_MATH_CTZ_HPP__
#define WJR_MATH_CTZ_HPP__

// Already included

namespace wjr {

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int fallback_ctz_impl(T x) {
#if WJR_HAS_BUILTIN(POPCOUNT) && WJR_HAS_SIMD(POPCNT)
    return popcount<T>(lowbit(x) - 1);
#else
    constexpr auto nd = std::numeric_limits<T>::digits;

    if constexpr (nd < 32) {
        return fallback_ctz_impl(static_cast<uint32_t>(x));
    } else {
        x = lowbit(x);

        if constexpr (nd <= 32) {
            return math_details::de_bruijn32.get(x);
        } else if constexpr (nd <= 64) {
            return math_details::de_bruijn64.get(x);
        } else {
            static_assert(nd <= 64, "not support yet");
        }
    }
#endif //
}

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int fallback_ctz(T x) {
    return fallback_ctz_impl(x);
}

#if WJR_HAS_BUILTIN(__builtin_ctz)
#define WJR_HAS_BUILTIN_CTZ WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(CTZ)

template <typename T>
WJR_CONST WJR_INTRINSIC_INLINE int builtin_ctz_impl(T x) {
    constexpr auto nd = std::numeric_limits<T>::digits;

    if constexpr (nd < 32) {
        return builtin_ctz_impl(static_cast<uint32_t>(x));
    } else {
        if constexpr (nd <= std::numeric_limits<unsigned int>::digits) {
            return __builtin_ctz(static_cast<unsigned int>(x));
        } else if constexpr (nd <= std::numeric_limits<unsigned long>::digits) {
            return __builtin_ctzl(static_cast<unsigned long>(x));
        } else if constexpr (nd <= std::numeric_limits<unsigned long long>::digits) {
            return __builtin_ctzll(static_cast<unsigned long long>(x));
        } else {
            static_assert(nd <= 64, "not supported yet");
        }
    }
}

template <typename T>
WJR_CONST WJR_INTRINSIC_INLINE int builtin_ctz(T x) {
    return builtin_ctz_impl(x);
}

#endif

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int ctz_impl(T x) {
#if WJR_HAS_BUILTIN(CTZ)
    if (is_constant_evaluated() || WJR_BUILTIN_CONSTANT_P(x)) {
        return fallback_ctz(x);
    }

    return builtin_ctz(x);
#else
    return fallback_ctz(x);
#endif
}

/**
 * @brief Fast count trailing zeros
 *
 * @details Very fast even on non-optimized platforms by using a De Bruijn sequence. \n
 * Try __builtin_clz if available, otherwise fallback to a portable implementation. \n
 * In fallback_clz, use popcount and lowbit if POPCOUNT and POPCNT are available, make
 * sure popcount is fast. \n
 * Then use De Bruijn sequence, just a bit slower than popcount + lowbit.
 *
 * @tparam T Must be an unsigned integral type
 */
template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int ctz(T x) {
    WJR_ASSERT_ASSUME_L1(x != 0);
    const int ret = ctz_impl(x);
    WJR_ASSUME(0 <= ret && ret < std::numeric_limits<T>::digits);
    return ret;
}

} // namespace wjr

#endif // WJR_MATH_CTZ_HPP__
// Already included

namespace wjr {

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR bool has_single_bit(T n) noexcept {
    return (n != 0) && is_zero_or_single_bit(n);
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int countl_zero(T x) noexcept {
    if (WJR_UNLIKELY(x == 0)) {
        return std::numeric_limits<T>::digits;
    }

    return clz(x);
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int countr_zero(T x) noexcept {
    if (WJR_UNLIKELY(x == 0)) {
        return std::numeric_limits<T>::digits;
    }

    return ctz(x);
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int countl_one(T x) noexcept {
    return countl_zero(static_cast<T>(~x));
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int countr_one(T x) noexcept {
    return countr_zero(static_cast<T>(~x));
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int bit_width(T x) noexcept {
    return std::numeric_limits<T>::digits - countl_zero(x);
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E T bit_ceil(T x) noexcept {
    if (x <= 1) {
        return T(1);
    }
    if constexpr (std::is_same_v<T, decltype(+x)>) {
        return T(1) << bit_width(T(x - 1));
    } else {
        constexpr int offset_for_ub =
            std::numeric_limits<unsigned>::digits - std::numeric_limits<T>::digits;
        return T(1 << (bit_width(T(x - 1)) + offset_for_ub) >> offset_for_ub);
    }
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E T bit_floor(T x) noexcept {
    if (x != 0) {
        return T{1} << (bit_width(x) - 1);
    }
    return 0;
}

} // namespace wjr

#endif // WJR_MATH_BIT_HPP__
#ifndef WJR_MATH_CONVERT_IMPL_HPP__
#define WJR_MATH_CONVERT_IMPL_HPP__

namespace wjr {

class char_converter_t {
    static constexpr std::array<uint8_t, 36> to_table = {
        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b',
        'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n',
        'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'};

    static constexpr std::array<uint8_t, 256> from_table = {
        127, 127, 127, 127, 127, 127, 127, 127, 127, 64,  64,  64,  64,  64,  127, 127,
        127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
        64,  127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
        0,   1,   2,   3,   4,   5,   6,   7,   8,   9,   127, 127, 127, 127, 127, 127,
        127, 10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,
        25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  127, 127, 127, 127, 127,
        127, 10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,
        25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  127, 127, 127, 127, 127,
        127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
        127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
        127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
        127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
        127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
        127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
        127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,
        127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127};

public:
    template <uint64_t Base = 0>
    WJR_CONST static constexpr uint8_t to(uint8_t x) {
        if constexpr (Base == 0) {
            WJR_ASSERT_L2(x < 36);
        } else {
            WJR_ASSERT_L2(x < Base);
        }

        if constexpr (Base == 0 || Base > 10) {

            if (WJR_BUILTIN_CONSTANT_P(x < 10) && x < 10) {
                return x + '0';
            }

            return to_table[x];
        } else {
            return x + '0';
        }
    }

    template <uint64_t Base = 0>
    WJR_CONST static constexpr uint8_t from(uint8_t x) {
        if constexpr (Base == 0 || Base > 10) {
            return from_table[x];
        } else {
            return x - '0';
        }
    }

private:
};

inline constexpr char_converter_t char_converter;

class origin_converter_t {
public:
    template <uint64_t Base = 0>
    WJR_CONST static constexpr uint64_t to(uint64_t x) {
        return x;
    }

    template <uint64_t Base = 0>
    WJR_CONST static constexpr uint64_t from(uint64_t x) {
        return x;
    }
};

inline constexpr origin_converter_t origin_converter;

} // namespace wjr

#endif // WJR_MATH_CONVERT_IMPL_HPP__
#ifndef WJR_MATH_DIV_HPP__
#define WJR_MATH_DIV_HPP__

#ifndef WJR_MATH_CMP_HPP__
#define WJR_MATH_CMP_HPP__

#ifndef WJR_MATH_SUB_HPP__
#define WJR_MATH_SUB_HPP__

// Already included
#ifndef WJR_MATH_REPLACE_HPP__
#define WJR_MATH_REPLACE_HPP__

#ifndef WJR_MATH_FIND_HPP__
#define WJR_MATH_FIND_HPP__

// Already included

#if defined(WJR_X86)
#ifndef WJR_X86_MATH_FIND_HPP__
#define WJR_X86_MATH_FIND_HPP__

#ifndef WJR_SIMD_SIMD_HPP__
#define WJR_SIMD_SIMD_HPP__

#include <cstring>

#ifndef WJR_MATH_BROADCAST_HPP__
#define WJR_MATH_BROADCAST_HPP__

#include <cstdint>

// Already included

namespace wjr {

template <typename From, typename To>
struct broadcast_fn {};

/**
 * @brief Broadcast a value to a wider type.
 * 
 * @note From must be a smaller type than To.
 */
template <typename From, typename To>
inline constexpr broadcast_fn<From, To> broadcast{};

template <>
struct broadcast_fn<uint8_t, uint8_t> {
    WJR_INTRINSIC_CONSTEXPR uint8_t operator()(uint8_t x) const { return x; }
};
template <>
struct broadcast_fn<uint16_t, uint16_t> {
    WJR_INTRINSIC_CONSTEXPR uint16_t operator()(uint16_t x) const { return x; }
};
template <>
struct broadcast_fn<uint32_t, uint32_t> {
    WJR_INTRINSIC_CONSTEXPR uint32_t operator()(uint32_t x) const { return x; }
};
template <>
struct broadcast_fn<uint64_t, uint64_t> {
    WJR_INTRINSIC_CONSTEXPR uint64_t operator()(uint64_t x) const { return x; }
};

template <>
struct broadcast_fn<uint8_t, uint16_t> {
    WJR_INTRINSIC_CONSTEXPR uint16_t operator()(uint8_t x) const {
        return static_cast<uint16_t>(static_cast<uint32_t>(x) |
                                     (static_cast<uint16_t>(x) << 8));
    }
};

template <>
struct broadcast_fn<uint16_t, uint32_t> {
    WJR_INTRINSIC_CONSTEXPR uint32_t operator()(uint16_t x) const {
        return x | (static_cast<uint32_t>(x) << 16);
    }
};
template <>
struct broadcast_fn<uint32_t, uint64_t> {
    WJR_INTRINSIC_CONSTEXPR uint64_t operator()(uint32_t x) const {
        return static_cast<uint64_t>(x) | (static_cast<uint64_t>(x) << 32);
    }
};

template <>
struct broadcast_fn<uint8_t, uint32_t> {
    WJR_INTRINSIC_CONSTEXPR uint32_t operator()(uint8_t x) const {
        return x * static_cast<uint32_t>(0x01010101u);
    }
};
template <>
struct broadcast_fn<uint16_t, uint64_t> {
    WJR_INTRINSIC_CONSTEXPR uint64_t operator()(uint16_t x) const {
        return x * static_cast<uint64_t>(0x0001000100010001ull);
    }
};

template <>
struct broadcast_fn<uint8_t, uint64_t> {
    WJR_INTRINSIC_CONSTEXPR uint64_t operator()(uint8_t x) const {
        return x * static_cast<uint64_t>(0x0101010101010101ull);
    }
};

} // namespace wjr

#endif // WJR_MATH_BROADCAST_HPP__
// Already included
#ifndef WJR_SIMD_SIMD_CAST_HPP__
#define WJR_SIMD_SIMD_CAST_HPP__

// Already included

#define WJR_HAS_SIMD_X86_SIMD WJR_HAS_DEF

#if defined(_MSC_VER)
/* Microsoft C/C++-compatible compiler */
#include <intrin.h>
#elif defined(__GNUC__)
/* GCC-compatible compiler, targeting x86/x86-64 */
#include <x86intrin.h>
#else
#undef WJR_HAS_SIMD_X86_SIMD
#endif

#if WJR_HAS_SIMD(X86_SIMD)

namespace wjr {

template <typename From, typename To>
struct simd_cast_fn;

template <typename From, typename To>
inline constexpr simd_cast_fn<From, To> simd_cast;

// simd type can't be directly used on template
template <typename T>
struct simd_wrapper {
    using type = T;
};

template <typename T>
using simd_wrapper_t = typename simd_wrapper<T>::type;

#if WJR_HAS_SIMD(SSE)

struct __m128_t {
    using type = __m128;
};

#endif // SSE

#if WJR_HAS_SIMD(SSE2)

struct __m128i_t {
    using type = __m128i;
};
struct __m128d_t {
    using type = __m128d;
};

template <>
struct simd_cast_fn<__m128_t, __m128i_t> {
    __m128i operator()(__m128 v) const { return _mm_castps_si128(v); }
};
template <>
struct simd_cast_fn<__m128_t, __m128d_t> {
    __m128d operator()(__m128 v) const { return _mm_castps_pd(v); }
};
template <>
struct simd_cast_fn<__m128i_t, __m128_t> {
    __m128 operator()(__m128i v) const { return _mm_castsi128_ps(v); }
};
template <>
struct simd_cast_fn<__m128i_t, __m128d_t> {
    __m128d operator()(__m128i v) const { return _mm_castsi128_pd(v); }
};
template <>
struct simd_cast_fn<__m128d_t, __m128_t> {
    __m128 operator()(__m128d v) const { return _mm_castpd_ps(v); }
};
template <>
struct simd_cast_fn<__m128d_t, __m128i_t> {
    __m128i operator()(__m128d v) const { return _mm_castpd_si128(v); }
};

template <>
struct simd_cast_fn<int8_t, __m128i_t> {
    __m128i operator()(int8_t v) const { return _mm_cvtsi32_si128(v); }
};
template <>
struct simd_cast_fn<uint8_t, __m128i_t> {
    __m128i operator()(uint8_t v) const { return _mm_cvtsi32_si128(v); }
};
template <>
struct simd_cast_fn<__m128i_t, int8_t> {
    int8_t operator()(__m128i v) const { return _mm_cvtsi128_si32(v); }
};
template <>
struct simd_cast_fn<__m128i_t, uint8_t> {
    uint8_t operator()(__m128i v) const { return _mm_cvtsi128_si32(v); }
};

template <>
struct simd_cast_fn<int16_t, __m128i_t> {
    __m128i operator()(int16_t v) const { return _mm_cvtsi32_si128(v); }
};
template <>
struct simd_cast_fn<uint16_t, __m128i_t> {
    __m128i operator()(uint16_t v) const { return _mm_cvtsi32_si128(v); }
};
template <>
struct simd_cast_fn<__m128i_t, int16_t> {
    int16_t operator()(__m128i v) const { return _mm_cvtsi128_si32(v); }
};
template <>
struct simd_cast_fn<__m128i_t, uint16_t> {
    uint16_t operator()(__m128i v) const { return _mm_cvtsi128_si32(v); }
};

template <>
struct simd_cast_fn<int32_t, __m128i_t> {
    __m128i operator()(int32_t v) const { return _mm_cvtsi32_si128(v); }
};
template <>
struct simd_cast_fn<uint32_t, __m128i_t> {
    __m128i operator()(uint32_t v) const { return _mm_cvtsi32_si128(v); }
};
template <>
struct simd_cast_fn<__m128i_t, int32_t> {
    int32_t operator()(__m128i v) const { return _mm_cvtsi128_si32(v); }
};
template <>
struct simd_cast_fn<__m128i_t, uint32_t> {
    uint32_t operator()(__m128i v) const { return _mm_cvtsi128_si32(v); }
};

template <>
struct simd_cast_fn<int64_t, __m128i_t> {
    __m128i operator()(int64_t v) const { return _mm_cvtsi64_si128(v); }
};
template <>
struct simd_cast_fn<uint64_t, __m128i_t> {
    __m128i operator()(uint64_t v) const { return _mm_cvtsi64_si128(v); }
};
template <>
struct simd_cast_fn<__m128i_t, int64_t> {
    int64_t operator()(__m128i v) const { return _mm_cvtsi128_si64(v); }
};
template <>
struct simd_cast_fn<__m128i_t, uint64_t> {
    uint64_t operator()(__m128i v) const { return _mm_cvtsi128_si64(v); }
};

#endif // SSE2

#if WJR_HAS_SIMD(AVX)

struct __m256_t {
    using type = __m256;
};
struct __m256i_t {
    using type = __m256i;
};
struct __m256d_t {
    using type = __m256d;
};

template <>
struct simd_cast_fn<__m256_t, __m256i_t> {
    __m256i operator()(__m256 v) const { return _mm256_castps_si256(v); }
};
template <>
struct simd_cast_fn<__m256_t, __m256d_t> {
    __m256d operator()(__m256 v) const { return _mm256_castps_pd(v); }
};
template <>
struct simd_cast_fn<__m256i_t, __m256_t> {
    __m256 operator()(__m256i v) const { return _mm256_castsi256_ps(v); }
};
template <>
struct simd_cast_fn<__m256i_t, __m256d_t> {
    __m256d operator()(__m256i v) const { return _mm256_castsi256_pd(v); }
};
template <>
struct simd_cast_fn<__m256d_t, __m256_t> {
    __m256 operator()(__m256d v) const { return _mm256_castpd_ps(v); }
};
template <>
struct simd_cast_fn<__m256d_t, __m256i_t> {
    __m256i operator()(__m256d v) const { return _mm256_castpd_si256(v); }
};
template <>
struct simd_cast_fn<__m128i_t, __m256i_t> {
    __m256i operator()(__m128i v) const { return _mm256_castsi128_si256(v); }
};
template <>
struct simd_cast_fn<__m256i_t, __m128i_t> {
    __m128i operator()(__m256i v) const { return _mm256_castsi256_si128(v); }
};

template <>
struct simd_cast_fn<int8_t, __m256i_t> {
    __m256i operator()(int8_t v) const {
        return simd_cast<__m128i_t, __m256i_t>(simd_cast<uint32_t, __m128i_t>(v));
    }
};
template <>
struct simd_cast_fn<uint8_t, __m256i_t> {
    __m256i operator()(uint8_t v) const {
        return simd_cast<__m128i_t, __m256i_t>(simd_cast<uint32_t, __m128i_t>(v));
    }
};
template <>
struct simd_cast_fn<__m256i_t, int8_t> {
    int8_t operator()(__m256i v) const {
        return simd_cast<__m128i_t, uint32_t>(simd_cast<__m256i_t, __m128i_t>(v));
    }
};
template <>
struct simd_cast_fn<__m256i_t, uint8_t> {
    uint8_t operator()(__m256i v) const {
        return simd_cast<__m128i_t, uint32_t>(simd_cast<__m256i_t, __m128i_t>(v));
    }
};

template <>
struct simd_cast_fn<int16_t, __m256i_t> {
    __m256i operator()(int16_t v) const {
        return simd_cast<__m128i_t, __m256i_t>(simd_cast<uint32_t, __m128i_t>(v));
    }
};
template <>
struct simd_cast_fn<uint16_t, __m256i_t> {
    __m256i operator()(uint16_t v) const {
        return simd_cast<__m128i_t, __m256i_t>(simd_cast<uint32_t, __m128i_t>(v));
    }
};
template <>
struct simd_cast_fn<__m256i_t, int16_t> {
    int16_t operator()(__m256i v) const {
        return simd_cast<__m128i_t, uint32_t>(simd_cast<__m256i_t, __m128i_t>(v));
    }
};
template <>
struct simd_cast_fn<__m256i_t, uint16_t> {
    uint16_t operator()(__m256i v) const {
        return simd_cast<__m128i_t, uint32_t>(simd_cast<__m256i_t, __m128i_t>(v));
    }
};

template <>
struct simd_cast_fn<int32_t, __m256i_t> {
    __m256i operator()(int32_t v) const {
        return simd_cast<__m128i_t, __m256i_t>(simd_cast<uint32_t, __m128i_t>(v));
    }
};
template <>
struct simd_cast_fn<uint32_t, __m256i_t> {
    __m256i operator()(uint32_t v) const {
        return simd_cast<__m128i_t, __m256i_t>(simd_cast<uint32_t, __m128i_t>(v));
    }
};
template <>
struct simd_cast_fn<__m256i_t, int32_t> {
    int32_t operator()(__m256i v) const {
        return simd_cast<__m128i_t, uint32_t>(simd_cast<__m256i_t, __m128i_t>(v));
    }
};
template <>
struct simd_cast_fn<__m256i_t, uint32_t> {
    uint32_t operator()(__m256i v) const {
        return simd_cast<__m128i_t, uint32_t>(simd_cast<__m256i_t, __m128i_t>(v));
    }
};

template <>
struct simd_cast_fn<int64_t, __m256i_t> {
    __m256i operator()(int64_t v) const {
        return simd_cast<__m128i_t, __m256i_t>(simd_cast<uint64_t, __m128i_t>(v));
    }
};
template <>
struct simd_cast_fn<uint64_t, __m256i_t> {
    __m256i operator()(uint64_t v) const {
        return simd_cast<__m128i_t, __m256i_t>(simd_cast<uint64_t, __m128i_t>(v));
    }
};
template <>
struct simd_cast_fn<__m256i_t, int64_t> {
    int64_t operator()(__m256i v) const {
        return simd_cast<__m128i_t, uint64_t>(simd_cast<__m256i_t, __m128i_t>(v));
    }
};
template <>
struct simd_cast_fn<__m256i_t, uint64_t> {
    uint64_t operator()(__m256i v) const {
        return simd_cast<__m128i_t, uint64_t>(simd_cast<__m256i_t, __m128i_t>(v));
    }
};

#endif // AVX

} // namespace wjr

#endif

#endif // WJR_SIMD_SIMD_CAST_HPP__

#if WJR_HAS_SIMD(X86_SIMD)

namespace wjr {

struct sse {
    using mask_type = uint16_t;

#if WJR_HAS_SIMD(SSE)

    using float_type = __m128;
    using float_tag_type = __m128_t;

#endif // SSE

#if WJR_HAS_SIMD(SSE2)

    using int_type = __m128i;
    using int_tag_type = __m128i_t;
    using double_type = __m128d;
    using double_tag_type = __m128d_t;

#endif // SSE2

    constexpr static size_t width();

    constexpr static mask_type mask();

#if WJR_HAS_SIMD(SSE)

    WJR_INTRINSIC_INLINE static mask_type movemask_ps(__m128 v);

    WJR_INTRINSIC_INLINE static void sfence();

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128 shuffle_ps(__m128 a, __m128 b);

#endif // SSE

#if WJR_HAS_SIMD(SSE2)

    WJR_INTRINSIC_INLINE static __m128i add_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i add_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i add_epi32(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i add_epi64(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i add(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i add(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i add(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i add(__m128i a, __m128i b, int64_t);
    WJR_INTRINSIC_INLINE static __m128i add(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i add(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i add(__m128i a, __m128i b, uint32_t);
    WJR_INTRINSIC_INLINE static __m128i add(__m128i a, __m128i b, uint64_t);

    WJR_INTRINSIC_INLINE static int8_t add_epi8(__m128i a);
    WJR_INTRINSIC_INLINE static int16_t add_epi16(__m128i a);
    WJR_INTRINSIC_INLINE static int32_t add_epi32(__m128i a);
    WJR_INTRINSIC_INLINE static int64_t add_epi64(__m128i a);

    WJR_INTRINSIC_INLINE static uint8_t add_epu8(__m128i a);
    WJR_INTRINSIC_INLINE static uint16_t add_epu16(__m128i a);
    WJR_INTRINSIC_INLINE static uint32_t add_epu32(__m128i a);
    WJR_INTRINSIC_INLINE static uint64_t add_epu64(__m128i a);

    WJR_INTRINSIC_INLINE static int8_t add(__m128i a, int8_t);
    WJR_INTRINSIC_INLINE static int16_t add(__m128i a, int16_t);
    WJR_INTRINSIC_INLINE static int32_t add(__m128i a, int32_t);
    WJR_INTRINSIC_INLINE static int64_t add(__m128i a, int64_t);
    WJR_INTRINSIC_INLINE static uint8_t add(__m128i a, uint8_t);
    WJR_INTRINSIC_INLINE static uint16_t add(__m128i a, uint16_t);
    WJR_INTRINSIC_INLINE static uint32_t add(__m128i a, uint32_t);
    WJR_INTRINSIC_INLINE static uint64_t add(__m128i a, uint64_t);

    WJR_INTRINSIC_INLINE static __m128i adds_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i adds_epi16(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i adds_epu8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i adds_epu16(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i adds(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i adds(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i adds(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i adds(__m128i a, __m128i b, uint16_t);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i alignr(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i alignr_epi16(__m128i a, __m128i b, int c);
    WJR_INTRINSIC_INLINE static __m128i alignr_epi32(__m128i a, __m128i b, int c);
    WJR_INTRINSIC_INLINE static __m128i alignr_epi64(__m128i a, __m128i b, int c);

    WJR_INTRINSIC_INLINE static __m128i alignr(__m128i a, __m128i b, int c, int16_t);
    WJR_INTRINSIC_INLINE static __m128i alignr(__m128i a, __m128i b, int c, int32_t);
    WJR_INTRINSIC_INLINE static __m128i alignr(__m128i a, __m128i b, int c, int64_t);
    WJR_INTRINSIC_INLINE static __m128i alignr(__m128i a, __m128i b, int c, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i alignr(__m128i a, __m128i b, int c, uint32_t);
    WJR_INTRINSIC_INLINE static __m128i alignr(__m128i a, __m128i b, int c, uint64_t);

    WJR_INTRINSIC_INLINE static __m128i And(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i AndNot(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i avg_epu8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i avg_epu16(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i avg(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i avg(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i avg(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i avg(__m128i a, __m128i b, uint16_t);

    // notice that mask must be 0 or 255(every byte)
    WJR_INTRINSIC_INLINE static __m128i blendv_epi8(__m128i a, __m128i b, __m128i mask);
    WJR_INTRINSIC_INLINE static __m128i blendv_epi16(__m128i a, __m128i b, __m128i mask);
    WJR_INTRINSIC_INLINE static __m128i blendv_epi32(__m128i a, __m128i b, __m128i mask);

    WJR_INTRINSIC_INLINE static __m128i blendv(__m128i a, __m128i b, __m128i mask,
                                               int8_t);
    WJR_INTRINSIC_INLINE static __m128i blendv(__m128i a, __m128i b, __m128i mask,
                                               int16_t);
    WJR_INTRINSIC_INLINE static __m128i blendv(__m128i a, __m128i b, __m128i mask,
                                               int32_t);
    WJR_INTRINSIC_INLINE static __m128i blendv(__m128i a, __m128i b, __m128i mask,
                                               uint8_t);
    WJR_INTRINSIC_INLINE static __m128i blendv(__m128i a, __m128i b, __m128i mask,
                                               uint16_t);
    WJR_INTRINSIC_INLINE static __m128i blendv(__m128i a, __m128i b, __m128i mask,
                                               uint32_t);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i bslli(__m128i val);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i bsrli(__m128i val);

    WJR_INTRINSIC_INLINE static __m128i cmpeq_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmpeq_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmpeq_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmpeq(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i cmpeq(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i cmpeq(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i cmpeq(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i cmpeq(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i cmpeq(__m128i a, __m128i b, uint32_t);

    WJR_INTRINSIC_INLINE static __m128i cmpge_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmpge_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmpge_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmpge_epu8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmpge_epu16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmpge_epu32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmpge(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i cmpge(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i cmpge(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i cmpge(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i cmpge(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i cmpge(__m128i a, __m128i b, uint32_t);

    WJR_INTRINSIC_INLINE static __m128i cmpgt_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmpgt_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmpgt_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmpgt_epu8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmpgt_epu16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmpgt_epu32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmpgt(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i cmpgt(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i cmpgt(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i cmpgt(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i cmpgt(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i cmpgt(__m128i a, __m128i b, uint32_t);

    WJR_INTRINSIC_INLINE static __m128i cmple_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmple_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmple_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmple_epu8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmple_epu16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmple_epu32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmple(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i cmple(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i cmple(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i cmple(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i cmple(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i cmple(__m128i a, __m128i b, uint32_t);

    WJR_INTRINSIC_INLINE static __m128i cmplt_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmplt_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmplt_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmplt_epu8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmplt_epu16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmplt_epu32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmplt(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i cmplt(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i cmplt(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i cmplt(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i cmplt(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i cmplt(__m128i a, __m128i b, uint32_t);

    WJR_INTRINSIC_INLINE static __m128i cmpne_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmpne_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i cmpne_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmpne(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i cmpne(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i cmpne(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i cmpne(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i cmpne(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i cmpne(__m128i a, __m128i b, uint32_t);

    template <typename T>
    WJR_INTRINSIC_INLINE static __m128i cmp(__m128i a, __m128i b, std::equal_to<>, T);
    template <typename T>
    WJR_INTRINSIC_INLINE static __m128i cmp(__m128i a, __m128i b, std::not_equal_to<>, T);
    template <typename T>
    WJR_INTRINSIC_INLINE static __m128i cmp(__m128i a, __m128i b, std::greater<>, T);
    template <typename T>
    WJR_INTRINSIC_INLINE static __m128i cmp(__m128i a, __m128i b, std::greater_equal<>,
                                            T);
    template <typename T>
    WJR_INTRINSIC_INLINE static __m128i cmp(__m128i a, __m128i b, std::less<>, T);
    template <typename T>
    WJR_INTRINSIC_INLINE static __m128i cmp(__m128i a, __m128i b, std::less_equal<>, T);

    WJR_INTRINSIC_INLINE static __m128i concat(uint64_t lo, uint64_t hi);

    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract_epi8(__m128i a);

    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract_epi16(__m128i a);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract_epi32(__m128i a);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int64_t extract_epi64(__m128i a);

    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract(__m128i a, int8_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract(__m128i a, int16_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract(__m128i a, int32_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int64_t extract(__m128i a, int64_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract(__m128i a, uint8_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract(__m128i a, uint16_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract(__m128i a, uint32_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int64_t extract(__m128i a, uint64_t);

    WJR_INTRINSIC_INLINE static uint64_t getlow(__m128i v);
    WJR_INTRINSIC_INLINE static uint64_t gethigh(__m128i v);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i insert_epi16(__m128i a, int i);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i insert(__m128i a, int i, int16_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i insert(__m128i a, int i, uint16_t);

    WJR_INTRINSIC_INLINE static void lfence();

    WJR_INTRINSIC_INLINE static __m128i load(const __m128i *ptr);
    WJR_INTRINSIC_INLINE static __m128i loadu(const __m128i *ptr);
    WJR_INTRINSIC_INLINE static __m128i loadu_si16(const void *ptr);
    WJR_INTRINSIC_INLINE static __m128i loadu_si32(const void *ptr);
    WJR_INTRINSIC_INLINE static __m128i loadu_si64(const void *ptr);

    template <typename T,
              WJR_REQUIRES(is_any_of_v<T, int8_t, int16_t, int32_t, int64_t, uint8_t,
                                       uint16_t, uint32_t, uint64_t>)>
    WJR_INTRINSIC_INLINE static __m128i logical_and(__m128i a, __m128i b, T);

    template <typename T,
              WJR_REQUIRES(is_any_of_v<T, int8_t, int16_t, int32_t, int64_t, uint8_t,
                                       uint16_t, uint32_t, uint64_t>)>
    WJR_INTRINSIC_INLINE static __m128i logical_not(__m128i v, T);

    template <typename T,
              WJR_REQUIRES(is_any_of_v<T, int8_t, int16_t, int32_t, int64_t, uint8_t,
                                       uint16_t, uint32_t, uint64_t>)>
    WJR_INTRINSIC_INLINE static __m128i logical_or(__m128i a, __m128i b, T);

    WJR_INTRINSIC_INLINE static __m128i madd_epi16(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static void maskmoveu(__m128i a, __m128i mask, char *mem_addr);

    WJR_INTRINSIC_INLINE static __m128i max_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i max_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i max_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i max_epu8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i max_epu16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i max_epu32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i max(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i max(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i max(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i max(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i max(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i max(__m128i a, __m128i b, uint32_t);

    WJR_INTRINSIC_INLINE static int8_t max_epi8(__m128i a);
    WJR_INTRINSIC_INLINE static int16_t max_epi16(__m128i a);
    WJR_INTRINSIC_INLINE static int32_t max_epi32(__m128i a);

    WJR_INTRINSIC_INLINE static uint8_t max_epu8(__m128i a);
    WJR_INTRINSIC_INLINE static uint16_t max_epu16(__m128i a);
    WJR_INTRINSIC_INLINE static uint32_t max_epu32(__m128i a);

    WJR_INTRINSIC_INLINE static int8_t max(__m128i a, int8_t);
    WJR_INTRINSIC_INLINE static int16_t max(__m128i a, int16_t);
    WJR_INTRINSIC_INLINE static int32_t max(__m128i a, int32_t);
    WJR_INTRINSIC_INLINE static uint8_t max(__m128i a, uint8_t);
    WJR_INTRINSIC_INLINE static uint16_t max(__m128i a, uint16_t);
    WJR_INTRINSIC_INLINE static uint32_t max(__m128i a, uint32_t);

    WJR_INTRINSIC_INLINE static void mfence();

    WJR_INTRINSIC_INLINE static __m128i min_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i min_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i min_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i min_epu8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i min_epu16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i min_epu32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i min(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i min(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i min(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i min(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i min(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i min(__m128i a, __m128i b, uint32_t);

    WJR_INTRINSIC_INLINE static int8_t min_epi8(__m128i a);
    WJR_INTRINSIC_INLINE static int16_t min_epi16(__m128i a);
    WJR_INTRINSIC_INLINE static int32_t min_epi32(__m128i a);

    WJR_INTRINSIC_INLINE static uint8_t min_epu8(__m128i a);
    WJR_INTRINSIC_INLINE static uint16_t min_epu16(__m128i a);
    WJR_INTRINSIC_INLINE static uint32_t min_epu32(__m128i a);

    WJR_INTRINSIC_INLINE static int8_t min(__m128i a, int8_t);
    WJR_INTRINSIC_INLINE static int16_t min(__m128i a, int16_t);
    WJR_INTRINSIC_INLINE static int32_t min(__m128i a, int32_t);

    WJR_INTRINSIC_INLINE static uint8_t min(__m128i a, uint8_t);
    WJR_INTRINSIC_INLINE static uint16_t min(__m128i a, uint16_t);
    WJR_INTRINSIC_INLINE static uint32_t min(__m128i a, uint32_t);

    WJR_INTRINSIC_INLINE static __m128i move_epi64(__m128i a);

    WJR_INTRINSIC_INLINE static mask_type movemask_epi8(__m128i a);
    WJR_INTRINSIC_INLINE static mask_type movemask_pd(__m128d v);

    WJR_INTRINSIC_INLINE static mask_type movemask(__m128i v, int8_t);
    WJR_INTRINSIC_INLINE static mask_type movemask(__m128i v, int32_t);
    WJR_INTRINSIC_INLINE static mask_type movemask(__m128i v, int64_t);

    WJR_INTRINSIC_INLINE static mask_type movemask(__m128i v, uint8_t);
    WJR_INTRINSIC_INLINE static mask_type movemask(__m128i v, uint32_t);
    WJR_INTRINSIC_INLINE static mask_type movemask(__m128i v, uint64_t);

    WJR_INTRINSIC_INLINE static __m128i mul_epu32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i mulhi_epi16(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i mulhi_epu16(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i mullo_epi16(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i negate_epi8(__m128i a);
    WJR_INTRINSIC_INLINE static __m128i negate_epi16(__m128i a);
    WJR_INTRINSIC_INLINE static __m128i negate_epi32(__m128i a);
    WJR_INTRINSIC_INLINE static __m128i negate_epi64(__m128i a);

    WJR_INTRINSIC_INLINE static __m128i negate(__m128i a, int8_t);
    WJR_INTRINSIC_INLINE static __m128i negate(__m128i a, int16_t);
    WJR_INTRINSIC_INLINE static __m128i negate(__m128i a, int32_t);
    WJR_INTRINSIC_INLINE static __m128i negate(__m128i a, int64_t);
    WJR_INTRINSIC_INLINE static __m128i negate(__m128i a, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i negate(__m128i a, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i negate(__m128i a, uint32_t);
    WJR_INTRINSIC_INLINE static __m128i negate(__m128i a, uint64_t);

    WJR_INTRINSIC_INLINE static __m128i Not(__m128i v);

    WJR_INTRINSIC_INLINE static __m128i Or(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i packs_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i packs_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i packus_epi16(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i preloadu_si16(const void *ptr);
    WJR_INTRINSIC_INLINE static __m128i preloadu_si32(const void *ptr);
    WJR_INTRINSIC_INLINE static __m128i preloadu_si48(const void *ptr);
    WJR_INTRINSIC_INLINE static __m128i preloadu_si64(const void *ptr);
    WJR_INTRINSIC_INLINE static __m128i preloadu_si80(const void *ptr);
    WJR_INTRINSIC_INLINE static __m128i preloadu_si96(const void *ptr);
    WJR_INTRINSIC_INLINE static __m128i preloadu_si112(const void *ptr);
    WJR_INTRINSIC_INLINE static __m128i preloadu_si128(const void *ptr);

    WJR_INTRINSIC_INLINE static __m128i preloadu_si16x(const void *ptr, int n);

    WJR_INTRINSIC_INLINE static __m128i sad_epu8(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i zeros();
    WJR_INTRINSIC_INLINE static __m128i ones();

    WJR_INTRINSIC_INLINE static __m128i set_epi8(char e15, char e14, char e13, char e12,
                                                 char e11, char e10, char e9, char e8,
                                                 char e7, char e6, char e5, char e4,
                                                 char e3, char e2, char e1, char e0);

    WJR_INTRINSIC_INLINE static __m128i set_epi16(short e7, short e6, short e5, short e4,
                                                  short e3, short e2, short e1, short e0);
    WJR_INTRINSIC_INLINE static __m128i set_epi32(int e3, int e2, int e1, int e0);
    WJR_INTRINSIC_INLINE static __m128i set_epi64x(long long e1, long long e0);

    WJR_INTRINSIC_INLINE static __m128i setr_epi8(char e15, char e14, char e13, char e12,
                                                  char e11, char e10, char e9, char e8,
                                                  char e7, char e6, char e5, char e4,
                                                  char e3, char e2, char e1, char e0);

    WJR_INTRINSIC_INLINE static __m128i setr_epi16(short e7, short e6, short e5, short e4,
                                                   short e3, short e2, short e1,
                                                   short e0);
    WJR_INTRINSIC_INLINE static __m128i setr_epi32(int e3, int e2, int e1, int e0);

    WJR_INTRINSIC_INLINE static __m128i set1_epi8(int8_t val);
    WJR_INTRINSIC_INLINE static __m128i set1_epi16(int16_t val);
    WJR_INTRINSIC_INLINE static __m128i set1_epi32(int32_t val);
    WJR_INTRINSIC_INLINE static __m128i set1_epi64(int64_t val);

    WJR_INTRINSIC_INLINE static __m128i set1(int8_t val, int8_t);
    WJR_INTRINSIC_INLINE static __m128i set1(int16_t val, int16_t);
    WJR_INTRINSIC_INLINE static __m128i set1(int32_t val, int32_t);
    WJR_INTRINSIC_INLINE static __m128i set1(int64_t val, int64_t);
    WJR_INTRINSIC_INLINE static __m128i set1(uint8_t val, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i set1(uint16_t val, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i set1(uint32_t val, uint32_t);
    WJR_INTRINSIC_INLINE static __m128i set1(uint64_t val, uint64_t);

    WJR_INTRINSIC_INLINE static __m128i setmin_epi8();
    WJR_INTRINSIC_INLINE static __m128i setmin_epi16();
    WJR_INTRINSIC_INLINE static __m128i setmin_epi32();

    WJR_INTRINSIC_INLINE static __m128i setmin(int8_t);
    WJR_INTRINSIC_INLINE static __m128i setmin(int16_t);
    WJR_INTRINSIC_INLINE static __m128i setmin(int32_t);
    WJR_INTRINSIC_INLINE static __m128i setmin(uint8_t);
    WJR_INTRINSIC_INLINE static __m128i setmin(uint16_t);
    WJR_INTRINSIC_INLINE static __m128i setmin(uint32_t);

    WJR_INTRINSIC_INLINE static __m128i setmax_epi8();
    WJR_INTRINSIC_INLINE static __m128i setmax_epi16();
    WJR_INTRINSIC_INLINE static __m128i setmax_epi32();

    WJR_INTRINSIC_INLINE static __m128i setmax(int8_t);
    WJR_INTRINSIC_INLINE static __m128i setmax(int16_t);
    WJR_INTRINSIC_INLINE static __m128i setmax(int32_t);
    WJR_INTRINSIC_INLINE static __m128i setmax(uint8_t);
    WJR_INTRINSIC_INLINE static __m128i setmax(uint16_t);
    WJR_INTRINSIC_INLINE static __m128i setmax(uint32_t);

    template <int imm>
    WJR_INTRINSIC_INLINE static __m128i shl(__m128i a);

    template <int imm>
    WJR_INTRINSIC_INLINE static __m128i shr(__m128i b);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i shuffle_epi32(__m128i v);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i shufflehi_epi16(__m128i v);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i shufflelo_epi16(__m128i v);

    WJR_INTRINSIC_INLINE static __m128i sll_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i sll_epi32(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i sll_epi64(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i sll(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i sll(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i sll(__m128i a, __m128i b, int64_t);
    WJR_INTRINSIC_INLINE static __m128i sll(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i sll(__m128i a, __m128i b, uint32_t);
    WJR_INTRINSIC_INLINE static __m128i sll(__m128i a, __m128i b, uint64_t);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i slli(__m128i v);
    WJR_INTRINSIC_INLINE static __m128i slli_epi16(__m128i a, int imm8);
    WJR_INTRINSIC_INLINE static __m128i slli_epi32(__m128i a, int imm8);
    WJR_INTRINSIC_INLINE static __m128i slli_epi64(__m128i a, int imm8);

    WJR_INTRINSIC_INLINE static __m128i slli(__m128i a, int imm8, int16_t);
    WJR_INTRINSIC_INLINE static __m128i slli(__m128i a, int imm8, int32_t);
    WJR_INTRINSIC_INLINE static __m128i slli(__m128i a, int imm8, int64_t);
    WJR_INTRINSIC_INLINE static __m128i slli(__m128i a, int imm8, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i slli(__m128i a, int imm8, uint32_t);
    WJR_INTRINSIC_INLINE static __m128i slli(__m128i a, int imm8, uint64_t);

    WJR_INTRINSIC_INLINE static __m128i sra_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i sra_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i sra(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i sra(__m128i a, __m128i b, int32_t);

    WJR_INTRINSIC_INLINE static __m128i srai_epi16(__m128i a, int imm8);
    WJR_INTRINSIC_INLINE static __m128i srai_epi32(__m128i a, int imm8);

    WJR_INTRINSIC_INLINE static __m128i srai(__m128i a, int imm8, int16_t);
    WJR_INTRINSIC_INLINE static __m128i srai(__m128i a, int imm8, int32_t);

    WJR_INTRINSIC_INLINE static __m128i srl_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i srl_epi32(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i srl_epi64(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i srl(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i srl(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i srl(__m128i a, __m128i b, int64_t);
    WJR_INTRINSIC_INLINE static __m128i srl(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i srl(__m128i a, __m128i b, uint32_t);
    WJR_INTRINSIC_INLINE static __m128i srl(__m128i a, __m128i b, uint64_t);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i srli(__m128i v);
    WJR_INTRINSIC_INLINE static __m128i srli_epi8(__m128i a, int imm8);
    WJR_INTRINSIC_INLINE static __m128i srli_epi16(__m128i a, int imm8);
    WJR_INTRINSIC_INLINE static __m128i srli_epi32(__m128i a, int imm8);
    WJR_INTRINSIC_INLINE static __m128i srli_epi64(__m128i a, int imm8);

    WJR_INTRINSIC_INLINE static __m128i srli(__m128i a, int imm8, int8_t);
    WJR_INTRINSIC_INLINE static __m128i srli(__m128i a, int imm8, int16_t);
    WJR_INTRINSIC_INLINE static __m128i srli(__m128i a, int imm8, int32_t);
    WJR_INTRINSIC_INLINE static __m128i srli(__m128i a, int imm8, int64_t);
    WJR_INTRINSIC_INLINE static __m128i srli(__m128i a, int imm8, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i srli(__m128i a, int imm8, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i srli(__m128i a, int imm8, uint32_t);
    WJR_INTRINSIC_INLINE static __m128i srli(__m128i a, int imm8, uint64_t);

    WJR_INTRINSIC_INLINE static void stream(__m128i *ptr, __m128i v);

    WJR_INTRINSIC_INLINE static void store(__m128i *ptr, __m128i val);
    WJR_INTRINSIC_INLINE static void storeu(__m128i *ptr, __m128i val);

    WJR_INTRINSIC_INLINE static __m128i sub_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i sub_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i sub_epi32(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i sub_epi64(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i sub(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i sub(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i sub(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i sub(__m128i a, __m128i b, int64_t);
    WJR_INTRINSIC_INLINE static __m128i sub(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i sub(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i sub(__m128i a, __m128i b, uint32_t);
    WJR_INTRINSIC_INLINE static __m128i sub(__m128i a, __m128i b, uint64_t);

    WJR_INTRINSIC_INLINE static __m128i subs_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i subs_epi16(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i subs_epu8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i subs_epu16(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i subs(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i subs(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i subs(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i subs(__m128i a, __m128i b, uint16_t);

    WJR_INTRINSIC_INLINE static __m128i unpackhi_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i unpackhi_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i unpackhi_epi32(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i unpackhi_epi64(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i unpackhi(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i unpackhi(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i unpackhi(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i unpackhi(__m128i a, __m128i b, int64_t);
    WJR_INTRINSIC_INLINE static __m128i unpackhi(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i unpackhi(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i unpackhi(__m128i a, __m128i b, uint32_t);
    WJR_INTRINSIC_INLINE static __m128i unpackhi(__m128i a, __m128i b, uint64_t);

    WJR_INTRINSIC_INLINE static __m128i unpacklo_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i unpacklo_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i unpacklo_epi32(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i unpacklo_epi64(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i unpacklo(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i unpacklo(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i unpacklo(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i unpacklo(__m128i a, __m128i b, int64_t);
    WJR_INTRINSIC_INLINE static __m128i unpacklo(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i unpacklo(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i unpacklo(__m128i a, __m128i b, uint32_t);

    WJR_INTRINSIC_INLINE static __m128i Xor(__m128i a, __m128i b);

#endif // SSE2

#if WJR_HAS_SIMD(SSE3)

    WJR_INTRINSIC_INLINE static __m128i lddqu(const __m128i *ptr);

#endif // SSE3

#if WJR_HAS_SIMD(SSSE3)

    WJR_INTRINSIC_INLINE static __m128i abs_epi8(__m128i val);
    WJR_INTRINSIC_INLINE static __m128i abs_epi16(__m128i val);
    WJR_INTRINSIC_INLINE static __m128i abs_epi32(__m128i val);

    WJR_INTRINSIC_INLINE static __m128i abs(__m128i val, int8_t);
    WJR_INTRINSIC_INLINE static __m128i abs(__m128i val, int16_t);
    WJR_INTRINSIC_INLINE static __m128i abs(__m128i val, int32_t);
    WJR_INTRINSIC_INLINE static __m128i abs(__m128i val, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i abs(__m128i val, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i abs(__m128i val, uint32_t);

    WJR_INTRINSIC_INLINE static __m128i shuffle_epi8(__m128i v, __m128i imm8);

    WJR_INTRINSIC_INLINE static __m128i sign_epi8(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i sign_epi16(__m128i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m128i sign_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i sign(__m128i a, __m128i b, int8_t);
    WJR_INTRINSIC_INLINE static __m128i sign(__m128i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m128i sign(__m128i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m128i sign(__m128i a, __m128i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m128i sign(__m128i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m128i sign(__m128i a, __m128i b, uint32_t);

#endif // SSSE3

#if WJR_HAS_SIMD(SSE4_1)

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i blend_epi16(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmpeq_epi64(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmpeq(__m128i a, __m128i b, int64_t);
    WJR_INTRINSIC_INLINE static __m128i cmpeq(__m128i a, __m128i b, uint64_t);

    WJR_INTRINSIC_INLINE static __m128i cmpgt_epi64(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i cmpgt(__m128i a, __m128i b, int64_t);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i insert_epi8(__m128i a, int i);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i insert_epi32(__m128i a, int i);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i insert_epi64(__m128i a, int64_t i);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i insert(__m128i a, int i, int8_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i insert(__m128i a, int i, int32_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i insert(__m128i a, int64_t i, int64_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i insert(__m128i a, int i, uint8_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i insert(__m128i a, int i, uint32_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i insert(__m128i a, int64_t i, uint64_t);

    WJR_INTRINSIC_INLINE static __m128i minpos_epu16(__m128i a);

    WJR_INTRINSIC_INLINE static __m128i mul_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i mullo_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i packus_epi32(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m128i stream_load(__m128i *p);

    WJR_INTRINSIC_INLINE static int test_all_ones(__m128i a);

    WJR_INTRINSIC_INLINE static int test_all_zeros(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static int test_all_zeros(__m128i a);

    WJR_INTRINSIC_INLINE static int test_mix_ones_zeros(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static int testc(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static int testnzc(__m128i a, __m128i b);

    WJR_INTRINSIC_INLINE static int testz(__m128i a, __m128i b);

#endif // SSE4_1
};

struct avx {
    using mask_type = uint32_t;

#if WJR_HAS_SIMD(AVX)

    using float_type = __m256;
    using float_tag_type = __m256_t;
    using int_type = __m256i;
    using int_tag_type = __m256i_t;
    using double_type = __m256d;
    using double_tag_type = __m256d_t;

#endif // AVX

    constexpr static size_t width();

    constexpr static mask_type mask();

#if WJR_HAS_SIMD(AVX)

    WJR_INTRINSIC_INLINE static __m256i concat(__m128i a, __m128i b);

    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract_epi32(__m256i v);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int64_t extract_epi64(__m256i v);

    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract(__m256i v, int32_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int64_t extract(__m256i v, int64_t);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m128i extract_si128(__m256i v);

    WJR_INTRINSIC_INLINE static __m128i getlow(__m256i a);

    WJR_INTRINSIC_INLINE static __m128i gethigh(__m256i a);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i insert_epi8(__m256i v, int8_t i);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i insert_epi16(__m256i v, int16_t i);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i insert_epi32(__m256i v, int32_t i);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i insert_epi64(__m256i v, int64_t i);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i insert_si128(__m256i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m256i load(const __m256i *p);
    WJR_INTRINSIC_INLINE static __m256i loadu(const __m256i *p);

    WJR_INTRINSIC_INLINE static __m256i ones();

    WJR_INTRINSIC_INLINE static __m256i preloadu_si16(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si32(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si48(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si64(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si80(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si96(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si112(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si128(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si144(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si160(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si176(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si192(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si208(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si224(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si240(const void *ptr);
    WJR_INTRINSIC_INLINE static __m256i preloadu_si256(const void *ptr);

    WJR_INTRINSIC_INLINE static __m256i preloadu_si16x(const void *ptr, int n);

    WJR_INTRINSIC_INLINE static __m256i
    set_epi8(char e31, char e30, char e29, char e28, char e27, char e26, char e25,
             char e24, char e23, char e22, char e21, char e20, char e19, char e18,
             char e17, char e16, char e15, char e14, char e13, char e12, char e11,
             char e10, char e9, char e8, char e7, char e6, char e5, char e4, char e3,
             char e2, char e1, char e0);

    WJR_INTRINSIC_INLINE static __m256i set_epi16(short e15, short e14, short e13,
                                                  short e12, short e11, short e10,
                                                  short e9, short e8, short e7, short e6,
                                                  short e5, short e4, short e3, short e2,
                                                  short e1, short e0);

    WJR_INTRINSIC_INLINE static __m256i set_epi32(int e7, int e6, int e5, int e4, int e3,
                                                  int e2, int e1, int e0);

    WJR_INTRINSIC_INLINE static __m256i set_epi64x(long long e3, long long e2,
                                                   long long e1, long long e0);

    WJR_INTRINSIC_INLINE static __m256i
    setr_epi8(char e31, char e30, char e29, char e28, char e27, char e26, char e25,
              char e24, char e23, char e22, char e21, char e20, char e19, char e18,
              char e17, char e16, char e15, char e14, char e13, char e12, char e11,
              char e10, char e9, char e8, char e7, char e6, char e5, char e4, char e3,
              char e2, char e1, char e0);

    WJR_INTRINSIC_INLINE static __m256i setr_epi16(short e15, short e14, short e13,
                                                   short e12, short e11, short e10,
                                                   short e9, short e8, short e7, short e6,
                                                   short e5, short e4, short e3, short e2,
                                                   short e1, short e0);

    WJR_INTRINSIC_INLINE static __m256i setr_epi32(int e7, int e6, int e5, int e4, int e3,
                                                   int e2, int e1, int e0);

    WJR_INTRINSIC_INLINE static __m256i setr_epi64x(long long e3, long long e2,
                                                    long long e1, long long e0);

    WJR_INTRINSIC_INLINE static __m256i set1_epi8(int8_t a);
    WJR_INTRINSIC_INLINE static __m256i set1_epi16(int16_t a);
    WJR_INTRINSIC_INLINE static __m256i set1_epi32(int32_t a);
    WJR_INTRINSIC_INLINE static __m256i set1_epi64(int64_t a);

    WJR_INTRINSIC_INLINE static __m256i set1(int8_t a, int8_t);
    WJR_INTRINSIC_INLINE static __m256i set1(int16_t a, int16_t);
    WJR_INTRINSIC_INLINE static __m256i set1(int32_t a, int32_t);
    WJR_INTRINSIC_INLINE static __m256i set1(int64_t a, int64_t);
    WJR_INTRINSIC_INLINE static __m256i set1(uint8_t a, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i set1(uint16_t a, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i set1(uint32_t a, uint32_t);
    WJR_INTRINSIC_INLINE static __m256i set1(uint64_t a, uint64_t);

    WJR_INTRINSIC_INLINE static __m256i setmin_epi8();
    WJR_INTRINSIC_INLINE static __m256i setmin_epi16();
    WJR_INTRINSIC_INLINE static __m256i setmin_epi32();
    WJR_INTRINSIC_INLINE static __m256i setmin_epi64();

    WJR_INTRINSIC_INLINE static __m256i setmin(int8_t);
    WJR_INTRINSIC_INLINE static __m256i setmin(int16_t);
    WJR_INTRINSIC_INLINE static __m256i setmin(int32_t);
    WJR_INTRINSIC_INLINE static __m256i setmin(int64_t);

    WJR_INTRINSIC_INLINE static __m256i setmax_epi8();
    WJR_INTRINSIC_INLINE static __m256i setmax_epi16();
    WJR_INTRINSIC_INLINE static __m256i setmax_epi32();
    WJR_INTRINSIC_INLINE static __m256i setmax_epi64();

    WJR_INTRINSIC_INLINE static __m256i setmax(int8_t);
    WJR_INTRINSIC_INLINE static __m256i setmax(int16_t);
    WJR_INTRINSIC_INLINE static __m256i setmax(int32_t);
    WJR_INTRINSIC_INLINE static __m256i setmax(int64_t);

    WJR_INTRINSIC_INLINE static void stream(__m256i *p, __m256i a);

    WJR_INTRINSIC_INLINE static void store(__m256i *p, __m256i a);
    WJR_INTRINSIC_INLINE static void storeu(__m256i *p, __m256i a);

    WJR_INTRINSIC_INLINE static int test_all_zeros(__m256i a);

    WJR_INTRINSIC_INLINE static int testc(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static int testnzc(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static int testz(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i zeros();

#endif // AVX

#if WJR_HAS_SIMD(AVX2)

    WJR_INTRINSIC_INLINE static __m256i And(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i AndNot(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i Or(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i Xor(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i Not(__m256i v);

    WJR_INTRINSIC_INLINE static __m256i abs_epi8(__m256i v);
    WJR_INTRINSIC_INLINE static __m256i abs_epi16(__m256i v);
    WJR_INTRINSIC_INLINE static __m256i abs_epi32(__m256i v);

    WJR_INTRINSIC_INLINE static __m256i abs(__m256i v, int8_t);
    WJR_INTRINSIC_INLINE static __m256i abs(__m256i v, int16_t);
    WJR_INTRINSIC_INLINE static __m256i abs(__m256i v, int32_t);
    WJR_INTRINSIC_INLINE static __m256i abs(__m256i v, int64_t);

    WJR_INTRINSIC_INLINE static __m256i add_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i add_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i add_epi32(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i add_epi64(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i add(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i add(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i add(__m256i a, __m256i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i add(__m256i a, __m256i b, int64_t);
    WJR_INTRINSIC_INLINE static __m256i add(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i add(__m256i a, __m256i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i add(__m256i a, __m256i b, uint32_t);
    WJR_INTRINSIC_INLINE static __m256i add(__m256i a, __m256i b, uint64_t);

    WJR_INTRINSIC_INLINE static uint8_t add_epu8(__m256i v);
    WJR_INTRINSIC_INLINE static uint16_t add_epu16(__m256i v);
    WJR_INTRINSIC_INLINE static uint32_t add_epu32(__m256i v);
    WJR_INTRINSIC_INLINE static uint64_t add_epu64(__m256i v);

    WJR_INTRINSIC_INLINE static int8_t add_epi8(__m256i v);
    WJR_INTRINSIC_INLINE static int16_t add_epi16(__m256i v);
    WJR_INTRINSIC_INLINE static int32_t add_epi32(__m256i v);
    WJR_INTRINSIC_INLINE static int64_t add_epi64(__m256i v);

    WJR_INTRINSIC_INLINE static int8_t add(__m256i v, int8_t);
    WJR_INTRINSIC_INLINE static int16_t add(__m256i v, int16_t);
    WJR_INTRINSIC_INLINE static int32_t add(__m256i v, int32_t);
    WJR_INTRINSIC_INLINE static int64_t add(__m256i v, int64_t);
    WJR_INTRINSIC_INLINE static uint8_t add(__m256i v, uint8_t);
    WJR_INTRINSIC_INLINE static uint16_t add(__m256i v, uint16_t);
    WJR_INTRINSIC_INLINE static uint32_t add(__m256i v, uint32_t);
    WJR_INTRINSIC_INLINE static uint64_t add(__m256i v, uint64_t);

    WJR_INTRINSIC_INLINE static __m256i adds_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i adds_epi16(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i adds_epu8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i adds_epu16(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i adds(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i adds(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i adds(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i adds(__m256i a, __m256i b, uint16_t);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i alignr(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i alignr_epi16(__m256i a, __m256i b, int c);
    WJR_INTRINSIC_INLINE static __m256i alignr_epi32(__m256i a, __m256i b, int c);
    WJR_INTRINSIC_INLINE static __m256i alignr_epi64(__m256i a, __m256i b, int c);

    WJR_INTRINSIC_INLINE static __m256i alignr(__m256i a, __m256i b, int c, int16_t);
    WJR_INTRINSIC_INLINE static __m256i alignr(__m256i a, __m256i b, int c, int32_t);
    WJR_INTRINSIC_INLINE static __m256i alignr(__m256i a, __m256i b, int c, int64_t);
    WJR_INTRINSIC_INLINE static __m256i alignr(__m256i a, __m256i b, int c, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i alignr(__m256i a, __m256i b, int c, uint32_t);
    WJR_INTRINSIC_INLINE static __m256i alignr(__m256i a, __m256i b, int c, uint64_t);

    WJR_INTRINSIC_INLINE static __m256i avg_epu8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i avg_epu16(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i avg(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i avg(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i avg(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i avg(__m256i a, __m256i b, uint16_t);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i blend_epi16(__m256i a, __m256i b);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i blend_epi32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i blendv_epi8(__m256i a, __m256i b, __m256i mask);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i bslli_epi128(__m256i a);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i bsrli_epi128(__m256i a);

    WJR_INTRINSIC_INLINE static __m256i cmpeq_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpeq_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpeq_epi32(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpeq_epi64(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i cmpeq(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i cmpeq(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i cmpeq(__m256i a, __m256i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i cmpeq(__m256i a, __m256i b, int64_t);
    WJR_INTRINSIC_INLINE static __m256i cmpeq(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i cmpeq(__m256i a, __m256i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i cmpeq(__m256i a, __m256i b, uint32_t);
    WJR_INTRINSIC_INLINE static __m256i cmpeq(__m256i a, __m256i b, uint64_t);

    WJR_INTRINSIC_INLINE static __m256i cmpge_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpge_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpge_epi32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i cmpge_epu8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpge_epu16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpge_epu32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i cmpge(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i cmpge(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i cmpge(__m256i a, __m256i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i cmpge(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i cmpge(__m256i a, __m256i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i cmpge(__m256i a, __m256i b, uint32_t);

    WJR_INTRINSIC_INLINE static __m256i cmpgt_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpgt_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpgt_epi32(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpgt_epi64(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i cmpgt_epu8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpgt_epu16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpgt_epu32(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpgt_epu64(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i cmpgt(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i cmpgt(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i cmpgt(__m256i a, __m256i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i cmpgt(__m256i a, __m256i b, int64_t);
    WJR_INTRINSIC_INLINE static __m256i cmpgt(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i cmpgt(__m256i a, __m256i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i cmpgt(__m256i a, __m256i b, uint32_t);
    WJR_INTRINSIC_INLINE static __m256i cmpgt(__m256i a, __m256i b, uint64_t);

    WJR_INTRINSIC_INLINE static __m256i cmple_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmple_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmple_epi32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i cmple_epu8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmple_epu16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmple_epu32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i cmple(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i cmple(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i cmple(__m256i a, __m256i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i cmple(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i cmple(__m256i a, __m256i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i cmple(__m256i a, __m256i b, uint32_t);

    WJR_INTRINSIC_INLINE static __m256i cmplt_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmplt_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmplt_epi32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i cmplt_epu8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmplt_epu16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmplt_epu32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i cmplt(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i cmplt(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i cmplt(__m256i a, __m256i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i cmplt(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i cmplt(__m256i a, __m256i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i cmplt(__m256i a, __m256i b, uint32_t);

    WJR_INTRINSIC_INLINE static __m256i cmpne_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpne_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i cmpne_epi32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i cmpne(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i cmpne(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i cmpne(__m256i a, __m256i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i cmpne(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i cmpne(__m256i a, __m256i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i cmpne(__m256i a, __m256i b, uint32_t);

    template <typename T>
    WJR_INTRINSIC_INLINE static __m256i cmp(__m256i a, __m256i b, std::equal_to<>, T);
    template <typename T>
    WJR_INTRINSIC_INLINE static __m256i cmp(__m256i a, __m256i b, std::not_equal_to<>, T);
    template <typename T>
    WJR_INTRINSIC_INLINE static __m256i cmp(__m256i a, __m256i b, std::greater<>, T);
    template <typename T>
    WJR_INTRINSIC_INLINE static __m256i cmp(__m256i a, __m256i b, std::greater_equal<>,
                                            T);
    template <typename T>
    WJR_INTRINSIC_INLINE static __m256i cmp(__m256i a, __m256i b, std::less<>, T);
    template <typename T>
    WJR_INTRINSIC_INLINE static __m256i cmp(__m256i a, __m256i b, std::less_equal<>, T);

    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract_epi8(__m256i v);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract_epi16(__m256i v);

    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract(__m256i v, int8_t);
    template <int imm8>
    WJR_INTRINSIC_INLINE static int extract(__m256i v, int16_t);

    WJR_INTRINSIC_INLINE static __m256i hadd_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i hadd_epi32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i hadd(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i hadd(__m256i a, __m256i b, int32_t);

    WJR_INTRINSIC_INLINE static __m256i hadds_epi16(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i hsub_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i hsub_epi32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i hsub(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i hsub(__m256i a, __m256i b, int32_t);

    WJR_INTRINSIC_INLINE static __m256i hsubs_epi16(__m256i a, __m256i b);

    template <typename T,
              WJR_REQUIRES(is_any_of_v<T, int8_t, int16_t, int32_t, int64_t, uint8_t,
                                       uint16_t, uint32_t, uint64_t>)>
    WJR_INTRINSIC_INLINE static __m256i logical_and(__m256i a, __m256i b, T);

    template <typename T,
              WJR_REQUIRES(is_any_of_v<T, int8_t, int16_t, int32_t, int64_t, uint8_t,
                                       uint16_t, uint32_t, uint64_t>)>
    WJR_INTRINSIC_INLINE static __m256i logical_not(__m256i v, T);

    template <typename T,
              WJR_REQUIRES(is_any_of_v<T, int8_t, int16_t, int32_t, int64_t, uint8_t,
                                       uint16_t, uint32_t, uint64_t>)>
    WJR_INTRINSIC_INLINE static __m256i logical_or(__m256i a, __m256i b, T);

    WJR_INTRINSIC_INLINE static __m256i madd_epi16(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i max_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i max_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i max_epi32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i max_epu8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i max_epu16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i max_epu32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i max(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i max(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i max(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i max(__m256i a, __m256i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i max(__m256i a, __m256i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i max(__m256i a, __m256i b, uint32_t);

    WJR_INTRINSIC_INLINE static int8_t max_epi8(__m256i a);
    WJR_INTRINSIC_INLINE static int16_t max_epi16(__m256i a);
    WJR_INTRINSIC_INLINE static int32_t max_epi32(__m256i a);
    WJR_INTRINSIC_INLINE static uint8_t max_epu8(__m256i a);
    WJR_INTRINSIC_INLINE static uint16_t max_epu16(__m256i a);
    WJR_INTRINSIC_INLINE static uint32_t max_epu32(__m256i a);

    WJR_INTRINSIC_INLINE static int8_t max(__m256i a, int8_t);
    WJR_INTRINSIC_INLINE static int16_t max(__m256i a, int16_t);
    WJR_INTRINSIC_INLINE static int32_t max(__m256i a, int32_t);

    WJR_INTRINSIC_INLINE static uint8_t max(__m256i a, uint8_t);
    WJR_INTRINSIC_INLINE static uint16_t max(__m256i a, uint16_t);
    WJR_INTRINSIC_INLINE static uint32_t max(__m256i a, uint32_t);

    WJR_INTRINSIC_INLINE static __m256i min_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i min_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i min_epi32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i min_epu8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i min_epu16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i min_epu32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i min(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i min(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i min(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i min(__m256i a, __m256i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i min(__m256i a, __m256i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i min(__m256i a, __m256i b, uint32_t);

    WJR_INTRINSIC_INLINE static int8_t min_epi8(__m256i a);
    WJR_INTRINSIC_INLINE static int16_t min_epi16(__m256i a);
    WJR_INTRINSIC_INLINE static int32_t min_epi32(__m256i a);

    WJR_INTRINSIC_INLINE static uint8_t min_epu8(__m256i a);
    WJR_INTRINSIC_INLINE static uint16_t min_epu16(__m256i a);
    WJR_INTRINSIC_INLINE static uint32_t min_epu32(__m256i a);

    WJR_INTRINSIC_INLINE static int8_t min(__m256i a, int8_t);
    WJR_INTRINSIC_INLINE static int16_t min(__m256i a, int16_t);
    WJR_INTRINSIC_INLINE static int32_t min(__m256i a, int32_t);
    WJR_INTRINSIC_INLINE static uint8_t min(__m256i a, uint8_t);
    WJR_INTRINSIC_INLINE static uint16_t min(__m256i a, uint16_t);
    WJR_INTRINSIC_INLINE static uint32_t min(__m256i a, uint32_t);

    WJR_INTRINSIC_INLINE static mask_type movemask_epi8(__m256i a);

    WJR_INTRINSIC_INLINE static __m256i mul_epi32(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i mul_epu32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i mulhi_epi16(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i mulhi_epu16(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i mullo_epi16(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i packs_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i packs_epi32(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i packus_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i packus_epi32(__m256i a, __m256i b);

    template <int imm>
    WJR_INTRINSIC_INLINE static __m256i shl(__m256i a);

    template <int imm>
    WJR_INTRINSIC_INLINE static __m256i shr(__m256i a);

    WJR_INTRINSIC_INLINE static __m256i shuffle_epi8(__m256i a, __m256i b);
    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i shuffle_epi32(__m256i a);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i shufflehi_epi16(__m256i a);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i shufflelo_epi16(__m256i a);

    WJR_INTRINSIC_INLINE static __m256i sll_epi16(__m256i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m256i sll_epi32(__m256i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m256i sll_epi64(__m256i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m256i sll(__m256i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i sll(__m256i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i sll(__m256i a, __m128i b, int64_t);
    WJR_INTRINSIC_INLINE static __m256i sll(__m256i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i sll(__m256i a, __m128i b, uint32_t);
    WJR_INTRINSIC_INLINE static __m256i sll(__m256i a, __m128i b, uint64_t);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i slli(__m256i a);
    WJR_INTRINSIC_INLINE static __m256i slli_epi16(__m256i a, int imm8);
    WJR_INTRINSIC_INLINE static __m256i slli_epi32(__m256i a, int imm8);
    WJR_INTRINSIC_INLINE static __m256i slli_epi64(__m256i a, int imm8);

    WJR_INTRINSIC_INLINE static __m256i slli(__m256i a, int imm8, int16_t);
    WJR_INTRINSIC_INLINE static __m256i slli(__m256i a, int imm8, int32_t);
    WJR_INTRINSIC_INLINE static __m256i slli(__m256i a, int imm8, int64_t);
    WJR_INTRINSIC_INLINE static __m256i slli(__m256i a, int imm8, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i slli(__m256i a, int imm8, uint32_t);
    WJR_INTRINSIC_INLINE static __m256i slli(__m256i a, int imm8, uint64_t);

    WJR_INTRINSIC_INLINE static __m256i sra_epi16(__m256i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m256i sra_epi32(__m256i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m256i sra(__m256i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i sra(__m256i a, __m128i b, int32_t);

    WJR_INTRINSIC_INLINE static __m256i srai_epi16(__m256i a, int imm8);
    WJR_INTRINSIC_INLINE static __m256i srai_epi32(__m256i a, int imm8);

    WJR_INTRINSIC_INLINE static __m256i srai(__m256i a, int imm8, int16_t);
    WJR_INTRINSIC_INLINE static __m256i srai(__m256i a, int imm8, int32_t);

    WJR_INTRINSIC_INLINE static __m256i stream_load(__m256i const *p);

    WJR_INTRINSIC_INLINE static __m256i srl_epi16(__m256i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m256i srl_epi32(__m256i a, __m128i b);
    WJR_INTRINSIC_INLINE static __m256i srl_epi64(__m256i a, __m128i b);

    WJR_INTRINSIC_INLINE static __m256i srl(__m256i a, __m128i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i srl(__m256i a, __m128i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i srl(__m256i a, __m128i b, int64_t);
    WJR_INTRINSIC_INLINE static __m256i srl(__m256i a, __m128i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i srl(__m256i a, __m128i b, uint32_t);
    WJR_INTRINSIC_INLINE static __m256i srl(__m256i a, __m128i b, uint64_t);

    template <int imm8>
    WJR_INTRINSIC_INLINE static __m256i srli(__m256i a);
    WJR_INTRINSIC_INLINE static __m256i srli_epi8(__m256i a, int imm8);
    WJR_INTRINSIC_INLINE static __m256i srli_epi16(__m256i a, int imm8);
    WJR_INTRINSIC_INLINE static __m256i srli_epi32(__m256i a, int imm8);
    WJR_INTRINSIC_INLINE static __m256i srli_epi64(__m256i a, int imm8);

    WJR_INTRINSIC_INLINE static __m256i srli(__m256i a, int imm8, int8_t);
    WJR_INTRINSIC_INLINE static __m256i srli(__m256i a, int imm8, int16_t);
    WJR_INTRINSIC_INLINE static __m256i srli(__m256i a, int imm8, int32_t);
    WJR_INTRINSIC_INLINE static __m256i srli(__m256i a, int imm8, int64_t);
    WJR_INTRINSIC_INLINE static __m256i srli(__m256i a, int imm8, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i srli(__m256i a, int imm8, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i srli(__m256i a, int imm8, uint32_t);
    WJR_INTRINSIC_INLINE static __m256i srli(__m256i a, int imm8, uint64_t);

    WJR_INTRINSIC_INLINE static __m256i sub_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i sub_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i sub_epi32(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i sub_epi64(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i sub(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i sub(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i sub(__m256i a, __m256i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i sub(__m256i a, __m256i b, int64_t);
    WJR_INTRINSIC_INLINE static __m256i sub(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i sub(__m256i a, __m256i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i sub(__m256i a, __m256i b, uint32_t);
    WJR_INTRINSIC_INLINE static __m256i sub(__m256i a, __m256i b, uint64_t);

    WJR_INTRINSIC_INLINE static __m256i subs_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i subs_epi16(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i subs_epu8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i subs_epu16(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i subs(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i subs(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i subs(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i subs(__m256i a, __m256i b, uint16_t);

    WJR_INTRINSIC_INLINE static int test_all_ones(__m256i a);

    WJR_INTRINSIC_INLINE static __m256i unpackhi_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i unpackhi_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i unpackhi_epi32(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i unpackhi_epi64(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i unpackhi(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i unpackhi(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i unpackhi(__m256i a, __m256i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i unpackhi(__m256i a, __m256i b, int64_t);
    WJR_INTRINSIC_INLINE static __m256i unpackhi(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i unpackhi(__m256i a, __m256i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i unpackhi(__m256i a, __m256i b, uint32_t);
    WJR_INTRINSIC_INLINE static __m256i unpackhi(__m256i a, __m256i b, uint64_t);

    WJR_INTRINSIC_INLINE static __m256i unpacklo_epi8(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i unpacklo_epi16(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i unpacklo_epi32(__m256i a, __m256i b);
    WJR_INTRINSIC_INLINE static __m256i unpacklo_epi64(__m256i a, __m256i b);

    WJR_INTRINSIC_INLINE static __m256i unpacklo(__m256i a, __m256i b, int8_t);
    WJR_INTRINSIC_INLINE static __m256i unpacklo(__m256i a, __m256i b, int16_t);
    WJR_INTRINSIC_INLINE static __m256i unpacklo(__m256i a, __m256i b, int32_t);
    WJR_INTRINSIC_INLINE static __m256i unpacklo(__m256i a, __m256i b, int64_t);
    WJR_INTRINSIC_INLINE static __m256i unpacklo(__m256i a, __m256i b, uint8_t);
    WJR_INTRINSIC_INLINE static __m256i unpacklo(__m256i a, __m256i b, uint16_t);
    WJR_INTRINSIC_INLINE static __m256i unpacklo(__m256i a, __m256i b, uint32_t);

#endif // AVX2
};

namespace sse_details {
#if WJR_HAS_SIMD(SSE2)

const static __m128i srli_epi8_mask[8] = {
    sse::set1_epi16(0xFFFF), sse::set1_epi16(0x7F7F), sse::set1_epi16(0x3F3F),
    sse::set1_epi16(0x1F1F), sse::set1_epi16(0xF0F),  sse::set1_epi16(0x707),
    sse::set1_epi16(0x303),  sse::set1_epi16(0x101),
};

#endif
} // namespace sse_details

namespace avx_details {
#if WJR_HAS_SIMD(AVX2)

const static __m256i srli_epi8_mask[8] = {
    avx::set1_epi16(0xFFFF), avx::set1_epi16(0x7F7F), avx::set1_epi16(0x3F3F),
    avx::set1_epi16(0x1F1F), avx::set1_epi16(0xF0F),  avx::set1_epi16(0x707),
    avx::set1_epi16(0x303),  avx::set1_epi16(0x101),
};

#endif
} // namespace avx_details

#if WJR_HAS_SIMD(SSE2)

template <>
struct broadcast_fn<uint8_t, __m128i_t> {
    WJR_INTRINSIC_INLINE __m128i operator()(uint8_t v) const { return _mm_set1_epi8(v); }
};

template <>
struct broadcast_fn<uint16_t, __m128i_t> {
    WJR_INTRINSIC_INLINE __m128i operator()(uint16_t v) const {
        return _mm_set1_epi16(v);
    }
};

template <>
struct broadcast_fn<uint32_t, __m128i_t> {
    WJR_INTRINSIC_INLINE __m128i operator()(uint32_t v) const {
        return _mm_set1_epi32(v);
    }
};

template <>
struct broadcast_fn<uint64_t, __m128i_t> {
    WJR_INTRINSIC_INLINE __m128i operator()(uint64_t v) const {
        return _mm_set1_epi64x(v);
    }
};

template <>
struct broadcast_fn<__m128i_t, __m128i_t> {
    WJR_INTRINSIC_INLINE __m128i operator()(__m128i v) const { return v; }
};

#endif // SSE2

#if WJR_HAS_SIMD(AVX)

template <>
struct broadcast_fn<uint8_t, __m256i_t> {
    WJR_INTRINSIC_INLINE __m256i operator()(uint8_t v) const {
        return _mm256_set1_epi8(v);
    }
};

template <>
struct broadcast_fn<uint16_t, __m256i_t> {
    WJR_INTRINSIC_INLINE __m256i operator()(uint16_t v) const {
        return _mm256_set1_epi16(v);
    }
};

template <>
struct broadcast_fn<uint32_t, __m256i_t> {
    WJR_INTRINSIC_INLINE __m256i operator()(uint32_t v) const {
        return _mm256_set1_epi32(v);
    }
};

template <>
struct broadcast_fn<uint64_t, __m256i_t> {
    WJR_INTRINSIC_INLINE __m256i operator()(uint64_t v) const {
        return _mm256_set1_epi64x(v);
    }
};

template <>
struct broadcast_fn<__m256i_t, __m256i_t> {
    WJR_INTRINSIC_INLINE __m256i operator()(__m256i v) const { return v; }
};

template <>
struct broadcast_fn<__m128i_t, __m256i_t> {
    WJR_INTRINSIC_INLINE __m256i operator()(__m128i v) const {
#if WJR_HAS_SIMD(AVX2)
        return _mm256_broadcastsi128_si256(v);
#else
        return _mm256_insertf128_si256(_mm256_castsi128_si256(v), v, 1);
#endif
    }
};

#endif // AVX

/*------------------------sse------------------------*/

constexpr size_t sse::width() { return 128; }

constexpr sse::mask_type sse::mask() { return 0xFFFF; }

#if WJR_HAS_SIMD(SSE)

sse::mask_type sse::movemask_ps(__m128 v) {
    return static_cast<sse::mask_type>(_mm_movemask_ps(v));
}

void sse::sfence() { return _mm_sfence(); }

template <int imm8>
__m128 sse::shuffle_ps(__m128 a, __m128 b) {
    static_assert(imm8 >= 0 && imm8 <= 255, "imm8 must be in range [0, 255]");
    return _mm_shuffle_ps(a, b, imm8);
}

#endif // SSE

#if WJR_HAS_SIMD(SSE2)

__m128i sse::add_epi8(__m128i a, __m128i b) { return _mm_add_epi8(a, b); }
__m128i sse::add_epi16(__m128i a, __m128i b) { return _mm_add_epi16(a, b); }
__m128i sse::add_epi32(__m128i a, __m128i b) { return _mm_add_epi32(a, b); }
__m128i sse::add_epi64(__m128i a, __m128i b) { return _mm_add_epi64(a, b); }

__m128i sse::add(__m128i a, __m128i b, int8_t) { return add_epi8(a, b); }
__m128i sse::add(__m128i a, __m128i b, int16_t) { return add_epi16(a, b); }
__m128i sse::add(__m128i a, __m128i b, int32_t) { return add_epi32(a, b); }
__m128i sse::add(__m128i a, __m128i b, int64_t) { return add_epi64(a, b); }
__m128i sse::add(__m128i a, __m128i b, uint8_t) { return add_epi8(a, b); }
__m128i sse::add(__m128i a, __m128i b, uint16_t) { return add_epi16(a, b); }
__m128i sse::add(__m128i a, __m128i b, uint32_t) { return add_epi32(a, b); }
__m128i sse::add(__m128i a, __m128i b, uint64_t) { return add_epi64(a, b); }

int8_t sse::add_epi8(__m128i a) { return static_cast<int8_t>(add_epu8(a)); }
int16_t sse::add_epi16(__m128i a) { return static_cast<int16_t>(add_epu16(a)); }
int32_t sse::add_epi32(__m128i a) { return static_cast<int32_t>(add_epu32(a)); }
int64_t sse::add_epi64(__m128i a) { return static_cast<int64_t>(add_epu64(a)); }

uint8_t sse::add_epu8(__m128i a) {
    auto b = shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a);
    a = add(a, b, uint8_t());
    b = zeros();
    a = sad_epu8(a, b);
    return simd_cast<__m128i_t, uint8_t>(a);
}

uint16_t sse::add_epu16(__m128i a) {
    a = add(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a), uint16_t());
    a = add(a, shuffle_epi32<_MM_SHUFFLE(1, 1, 1, 1)>(a), uint16_t());
    a = add(a, srli<2>(a), uint16_t());
    return simd_cast<__m128i_t, uint16_t>(a);
}

uint32_t sse::add_epu32(__m128i a) {
    a = add(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a), uint32_t());
    a = add(a, shuffle_epi32<_MM_SHUFFLE(1, 1, 1, 1)>(a), uint32_t());
    return simd_cast<__m128i_t, uint32_t>(a);
}

uint64_t sse::add_epu64(__m128i a) {
    a = add(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a), uint64_t());
    return simd_cast<__m128i_t, uint64_t>(a);
}

int8_t sse::add(__m128i a, int8_t) { return add_epi8(a); }
int16_t sse::add(__m128i a, int16_t) { return add_epi16(a); }
int32_t sse::add(__m128i a, int32_t) { return add_epi32(a); }
int64_t sse::add(__m128i a, int64_t) { return add_epi64(a); }
uint8_t sse::add(__m128i a, uint8_t) { return add_epu8(a); }
uint16_t sse::add(__m128i a, uint16_t) { return add_epu16(a); }
uint32_t sse::add(__m128i a, uint32_t) { return add_epu32(a); }
uint64_t sse::add(__m128i a, uint64_t) { return add_epu64(a); }

__m128i sse::adds_epi8(__m128i a, __m128i b) { return _mm_adds_epi8(a, b); }
__m128i sse::adds_epi16(__m128i a, __m128i b) { return _mm_adds_epi16(a, b); }

__m128i sse::adds_epu8(__m128i a, __m128i b) { return _mm_adds_epu8(a, b); }
__m128i sse::adds_epu16(__m128i a, __m128i b) { return _mm_adds_epu16(a, b); }

__m128i sse::adds(__m128i a, __m128i b, int8_t) { return adds_epi8(a, b); }
__m128i sse::adds(__m128i a, __m128i b, int16_t) { return adds_epi16(a, b); }
__m128i sse::adds(__m128i a, __m128i b, uint8_t) { return adds_epu8(a, b); }
__m128i sse::adds(__m128i a, __m128i b, uint16_t) { return adds_epu16(a, b); }

template <int imm8>
__m128i sse::alignr(__m128i a, __m128i b) {
    constexpr int s = imm8 & 0x1F;
#if WJR_HAS_SIMD(SSSE3)
    return _mm_alignr_epi8(a, b, s);
#else
    if constexpr (s == 0) {
        return b;
    }
    if constexpr (s == 16) {
        return a;
    }
    if constexpr (s < 16) {
        return Or(slli<16 - s>(a), srli<s>(b));
    }
    return srli<s - 16>(a);
#endif // SSSE3
}

__m128i sse::alignr_epi16(__m128i a, __m128i b, int c) {
    return Or(slli(a, 16 - c, uint16_t()), srli(b, c, uint16_t()));
}

__m128i sse::alignr_epi32(__m128i a, __m128i b, int c) {
    return Or(slli(a, 32 - c, uint32_t()), srli(b, c, uint32_t()));
}

__m128i sse::alignr_epi64(__m128i a, __m128i b, int c) {
    return Or(slli(a, 64 - c, uint64_t()), srli(b, c, uint64_t()));
}

__m128i sse::alignr(__m128i a, __m128i b, int c, int16_t) {
    return alignr_epi16(a, b, c);
}
__m128i sse::alignr(__m128i a, __m128i b, int c, int32_t) {
    return alignr_epi32(a, b, c);
}
__m128i sse::alignr(__m128i a, __m128i b, int c, int64_t) {
    return alignr_epi64(a, b, c);
}
__m128i sse::alignr(__m128i a, __m128i b, int c, uint16_t) {
    return alignr_epi16(a, b, c);
}
__m128i sse::alignr(__m128i a, __m128i b, int c, uint32_t) {
    return alignr_epi32(a, b, c);
}
__m128i sse::alignr(__m128i a, __m128i b, int c, uint64_t) {
    return alignr_epi64(a, b, c);
}

__m128i sse::And(__m128i a, __m128i b) { return _mm_and_si128(a, b); }

__m128i sse::AndNot(__m128i a, __m128i b) { return _mm_andnot_si128(a, b); }

__m128i sse::avg_epu8(__m128i a, __m128i b) { return _mm_avg_epu8(a, b); }
__m128i sse::avg_epu16(__m128i a, __m128i b) { return _mm_avg_epu16(a, b); }

__m128i sse::avg(__m128i a, __m128i b, int8_t) { return avg_epu8(a, b); }
__m128i sse::avg(__m128i a, __m128i b, int16_t) { return avg_epu16(a, b); }
__m128i sse::avg(__m128i a, __m128i b, uint8_t) { return avg_epu8(a, b); }
__m128i sse::avg(__m128i a, __m128i b, uint16_t) { return avg_epu16(a, b); }

// notice that mask must be 0 or 255(every byte)
__m128i sse::blendv_epi8(__m128i a, __m128i b, __m128i mask) {
#if WJR_HAS_SIMD(SSE4_1)
    return _mm_blendv_epi8(a, b, mask);
#elif defined(WJR_COMPILER_GCC)
    return ((~mask) & a) | (mask & b);
#else
    return Or(AndNot(mask, a), And(mask, b));
#endif
}

__m128i sse::blendv_epi16(__m128i a, __m128i b, __m128i mask) {
    return blendv_epi8(b, a, logical_not(mask, uint16_t()));
}

__m128i sse::blendv_epi32(__m128i a, __m128i b, __m128i mask) {
    return blendv_epi8(b, a, logical_not(mask, uint32_t()));
}

__m128i sse::blendv(__m128i a, __m128i b, __m128i mask, int8_t) {
    return blendv_epi8(a, b, mask);
}

__m128i sse::blendv(__m128i a, __m128i b, __m128i mask, int16_t) {
    return blendv_epi16(a, b, mask);
}

__m128i sse::blendv(__m128i a, __m128i b, __m128i mask, int32_t) {
    return blendv_epi32(a, b, mask);
}

__m128i sse::blendv(__m128i a, __m128i b, __m128i mask, uint8_t) {
    return blendv_epi8(a, b, mask);
}

__m128i sse::blendv(__m128i a, __m128i b, __m128i mask, uint16_t) {
    return blendv_epi16(a, b, mask);
}

__m128i sse::blendv(__m128i a, __m128i b, __m128i mask, uint32_t) {
    return blendv_epi32(a, b, mask);
}

template <int imm8>
__m128i sse::bslli(__m128i val) {
    return _mm_bslli_si128(val, imm8);
}

template <int imm8>
__m128i sse::bsrli(__m128i val) {
    return _mm_bsrli_si128(val, imm8);
}

__m128i sse::cmpeq_epi8(__m128i a, __m128i b) { return _mm_cmpeq_epi8(a, b); }
__m128i sse::cmpeq_epi16(__m128i a, __m128i b) { return _mm_cmpeq_epi16(a, b); }
__m128i sse::cmpeq_epi32(__m128i a, __m128i b) { return _mm_cmpeq_epi32(a, b); }

__m128i sse::cmpeq(__m128i a, __m128i b, int8_t) { return cmpeq_epi8(a, b); }
__m128i sse::cmpeq(__m128i a, __m128i b, int16_t) { return cmpeq_epi16(a, b); }
__m128i sse::cmpeq(__m128i a, __m128i b, int32_t) { return cmpeq_epi32(a, b); }
__m128i sse::cmpeq(__m128i a, __m128i b, uint8_t) { return cmpeq_epi8(a, b); }
__m128i sse::cmpeq(__m128i a, __m128i b, uint16_t) { return cmpeq_epi16(a, b); }
__m128i sse::cmpeq(__m128i a, __m128i b, uint32_t) { return cmpeq_epi32(a, b); }

__m128i sse::cmpge_epi8(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(XOP)
    return _mm_comge_epi8(a, b);
#elif WJR_HAS_SIMD(SSE4_1)
    return cmpeq(min(a, b, int8_t()), b, uint8_t());
#else
    return Not(cmpgt(b, a, int8_t()));
#endif
}

__m128i sse::cmpge_epi16(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(XOP)
    return _mm_comge_epi16(a, b);
#else
    return cmpeq(min(a, b, int16_t()), b, uint16_t());
#endif
}

__m128i sse::cmpge_epi32(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(XOP)
    return _mm_comge_epi32(a, b);
#elif WJR_HAS_SIMD(SSE4_1)
    return cmpeq(min(a, b, int32_t()), b, uint32_t());
#else
    return Not(cmpgt(b, a, int32_t()));
#endif
}

__m128i sse::cmpge_epu8(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(XOP)
    return _mm_comge_epu8(a, b);
#else
    return cmpeq(min(a, b, uint8_t()), b, uint8_t());
#endif
}

__m128i sse::cmpge_epu16(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(XOP)
    return _mm_comge_epu16(a, b);
#elif WJR_HAS_SIMD(SSE4_1)
    return cmpeq(min(a, b, uint16_t()), b, uint16_t());
#else
    return logical_not(subs(b, a, uint16_t()), uint16_t());
#endif
}

__m128i sse::cmpge_epu32(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(XOP)
    return _mm_comge_epu32(a, b);
#elif WJR_HAS_SIMD(SSE4_1)
    return cmpeq(min(a, b, uint32_t()), b, uint32_t());
#else
    return Not(cmpgt(b, a, uint32_t()));
#endif
}

__m128i sse::cmpge(__m128i a, __m128i b, int8_t) { return cmpge_epi8(a, b); }
__m128i sse::cmpge(__m128i a, __m128i b, int16_t) { return cmpge_epi16(a, b); }
__m128i sse::cmpge(__m128i a, __m128i b, int32_t) { return cmpge_epi32(a, b); }
__m128i sse::cmpge(__m128i a, __m128i b, uint8_t) { return cmpge_epu8(a, b); }
__m128i sse::cmpge(__m128i a, __m128i b, uint16_t) { return cmpge_epu16(a, b); }
__m128i sse::cmpge(__m128i a, __m128i b, uint32_t) { return cmpge_epu32(a, b); }

__m128i sse::cmpgt_epi8(__m128i a, __m128i b) { return _mm_cmpgt_epi8(a, b); }
__m128i sse::cmpgt_epi16(__m128i a, __m128i b) { return _mm_cmpgt_epi16(a, b); }
__m128i sse::cmpgt_epi32(__m128i a, __m128i b) { return _mm_cmpgt_epi32(a, b); }

__m128i sse::cmpgt_epu8(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(XOP)
    return _mm_comgt_epu8(a, b);
#else
    return cmpgt_epi8(Xor(a, setmin_epi8()), Xor(b, setmin_epi8()));
#endif
}

__m128i sse::cmpgt_epu16(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(XOP)
    return _mm_comgt_epu16(a, b);
#else
    return cmpgt_epi16(Xor(a, setmin_epi16()), Xor(b, setmin_epi16()));
#endif
}

__m128i sse::cmpgt_epu32(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(XOP)
    return _mm_comgt_epu32(a, b);
#else
    return cmpgt_epi32(Xor(a, setmin_epi32()), Xor(b, setmin_epi32()));
#endif
}

__m128i sse::cmpgt(__m128i a, __m128i b, int8_t) { return cmpgt_epi8(a, b); }
__m128i sse::cmpgt(__m128i a, __m128i b, int16_t) { return cmpgt_epi16(a, b); }
__m128i sse::cmpgt(__m128i a, __m128i b, int32_t) { return cmpgt_epi32(a, b); }
__m128i sse::cmpgt(__m128i a, __m128i b, uint8_t) { return cmpgt_epu8(a, b); }
__m128i sse::cmpgt(__m128i a, __m128i b, uint16_t) { return cmpgt_epu16(a, b); }
__m128i sse::cmpgt(__m128i a, __m128i b, uint32_t) { return cmpgt_epu32(a, b); }

__m128i sse::cmple_epi8(__m128i a, __m128i b) { return cmpge_epi8(b, a); }
__m128i sse::cmple_epi16(__m128i a, __m128i b) { return cmpge_epi16(b, a); }
__m128i sse::cmple_epi32(__m128i a, __m128i b) { return cmpge_epi32(b, a); }

__m128i sse::cmple_epu8(__m128i a, __m128i b) { return cmpge_epu8(b, a); }
__m128i sse::cmple_epu16(__m128i a, __m128i b) { return cmpge_epu16(b, a); }
__m128i sse::cmple_epu32(__m128i a, __m128i b) { return cmpge_epu32(b, a); }

__m128i sse::cmple(__m128i a, __m128i b, int8_t) { return cmple_epi8(a, b); }
__m128i sse::cmple(__m128i a, __m128i b, int16_t) { return cmple_epi16(a, b); }
__m128i sse::cmple(__m128i a, __m128i b, int32_t) { return cmple_epi32(a, b); }
__m128i sse::cmple(__m128i a, __m128i b, uint8_t) { return cmple_epu8(a, b); }
__m128i sse::cmple(__m128i a, __m128i b, uint16_t) { return cmple_epu16(a, b); }
__m128i sse::cmple(__m128i a, __m128i b, uint32_t) { return cmple_epu32(a, b); }

__m128i sse::cmplt_epi8(__m128i a, __m128i b) { return _mm_cmplt_epi8(a, b); }
__m128i sse::cmplt_epi16(__m128i a, __m128i b) { return _mm_cmplt_epi16(a, b); }
__m128i sse::cmplt_epi32(__m128i a, __m128i b) { return _mm_cmplt_epi32(a, b); }

__m128i sse::cmplt_epu8(__m128i a, __m128i b) { return cmpgt_epu8(b, a); }
__m128i sse::cmplt_epu16(__m128i a, __m128i b) { return cmpgt_epu16(b, a); }
__m128i sse::cmplt_epu32(__m128i a, __m128i b) { return cmpgt_epu32(b, a); }

__m128i sse::cmplt(__m128i a, __m128i b, int8_t) { return cmplt_epi8(a, b); }
__m128i sse::cmplt(__m128i a, __m128i b, int16_t) { return cmplt_epi16(a, b); }
__m128i sse::cmplt(__m128i a, __m128i b, int32_t) { return cmplt_epi32(a, b); }
__m128i sse::cmplt(__m128i a, __m128i b, uint8_t) { return cmplt_epu8(a, b); }
__m128i sse::cmplt(__m128i a, __m128i b, uint16_t) { return cmplt_epu16(a, b); }
__m128i sse::cmplt(__m128i a, __m128i b, uint32_t) { return cmplt_epu32(a, b); }

__m128i sse::cmpne_epi8(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(XOP)
    return _mm_comneq_epi8(a, b);
#else
    return Not(cmpeq_epi8(a, b));
#endif
}

__m128i sse::cmpne_epi16(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(XOP)
    return _mm_comneq_epi16(a, b);
#else
    return Not(cmpeq_epi16(a, b));
#endif
}

__m128i sse::cmpne_epi32(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(XOP)
    return _mm_comneq_epi32(a, b);
#else
    return Not(cmpeq_epi32(a, b));
#endif
}

__m128i sse::cmpne(__m128i a, __m128i b, int8_t) { return cmpne_epi8(a, b); }
__m128i sse::cmpne(__m128i a, __m128i b, int16_t) { return cmpne_epi16(a, b); }
__m128i sse::cmpne(__m128i a, __m128i b, int32_t) { return cmpne_epi32(a, b); }
__m128i sse::cmpne(__m128i a, __m128i b, uint8_t) { return cmpne_epi8(a, b); }
__m128i sse::cmpne(__m128i a, __m128i b, uint16_t) { return cmpne_epi16(a, b); }
__m128i sse::cmpne(__m128i a, __m128i b, uint32_t) { return cmpne_epi32(a, b); }

template <typename T>
__m128i sse::cmp(__m128i a, __m128i b, std::equal_to<>, T) {
    return cmpeq(a, b, T());
}

template <typename T>
__m128i sse::cmp(__m128i a, __m128i b, std::not_equal_to<>, T) {
    return cmpne(a, b, T());
}

template <typename T>
__m128i sse::cmp(__m128i a, __m128i b, std::greater<>, T) {
    return cmpgt(a, b, T());
}

template <typename T>
__m128i sse::cmp(__m128i a, __m128i b, std::greater_equal<>, T) {
    return cmpge(a, b, T());
}

template <typename T>
__m128i sse::cmp(__m128i a, __m128i b, std::less<>, T) {
    return cmplt(a, b, T());
}

template <typename T>
__m128i sse::cmp(__m128i a, __m128i b, std::less_equal<>, T) {
    return cmple(a, b, T());
}

__m128i sse::concat(uint64_t lo, uint64_t hi) { return set_epi64x(hi, lo); }

template <int imm8>
int sse::extract_epi8(__m128i a) {
    static_assert(imm8 >= 0 && imm8 < 16, "imm8 must be in range [0, 15]");
#if WJR_HAS_SIMD(SSE4_1)
    return _mm_extract_epi8(a, imm8);
#else
    if constexpr (imm8 & 1) {
        return extract_epi16<(imm8 >> 1)>(a) >> 8;
    } else {
        return extract_epi16<(imm8 >> 1)>(a) & 0xff;
    }
#endif
}

template <int imm8>
int sse::extract_epi16(__m128i a) {
    static_assert(imm8 >= 0 && imm8 < 8, "imm8 must be in range [0, 7]");
    return _mm_extract_epi16(a, imm8);
}

template <int imm8>
int sse::extract_epi32(__m128i a) {
    static_assert(imm8 >= 0 && imm8 < 4, "imm8 must be in range [0, 3]");
#if WJR_HAS_SIMD(SSE4_1)
    return _mm_extract_epi32(a, imm8);
#else
    if constexpr (imm8 == 0) {
        return simd_cast<__m128i_t, uint32_t>(a);
    } else if constexpr (imm8 == 1) {
        return static_cast<uint32_t>(simd_cast<__m128i_t, uint64_t>(a) >> 32);
    } else if constexpr (imm8 == 2) {
        return simd_cast<__m128i_t, uint32_t>(shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a));
    } else {
        return simd_cast<__m128i_t, uint32_t>(shuffle_epi32<_MM_SHUFFLE(3, 3, 3, 3)>(a));
    }
#endif
}

template <int imm8>
int64_t sse::extract_epi64(__m128i a) {
    static_assert(imm8 >= 0 && imm8 < 2, "imm8 must be in range [0, 1]");
#if WJR_HAS_SIMD(SSE4_1)
    return _mm_extract_epi64(a, imm8);
#else
    if constexpr (imm8 == 0) {
        return simd_cast<__m128i_t, uint64_t>(a);
    } else {
        return simd_cast<__m128i_t, uint64_t>(shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a));
    }
#endif
}

template <int imm8>
int sse::extract(__m128i a, int8_t) {
    return extract_epi8<imm8>(a);
}

template <int imm8>
int sse::extract(__m128i a, int16_t) {
    return extract_epi16<imm8>(a);
}

template <int imm8>
int sse::extract(__m128i a, int32_t) {
    return extract_epi32<imm8>(a);
}

template <int imm8>
int64_t sse::extract(__m128i a, int64_t) {
    return extract_epi64<imm8>(a);
}

template <int imm8>
int sse::extract(__m128i a, uint8_t) {
    return extract_epi8<imm8>(a);
}

template <int imm8>
int sse::extract(__m128i a, uint16_t) {
    return extract_epi16<imm8>(a);
}

template <int imm8>
int sse::extract(__m128i a, uint32_t) {
    return extract_epi32<imm8>(a);
}

template <int imm8>
int64_t sse::extract(__m128i a, uint64_t) {
    return extract_epi64<imm8>(a);
}

uint64_t sse::getlow(__m128i v) { return simd_cast<__m128i_t, uint64_t>(v); }
uint64_t sse::gethigh(__m128i v) { return extract_epi64<1>(v); }

template <int imm8>
__m128i sse::insert_epi16(__m128i a, int i) {
    return _mm_insert_epi16(a, i, imm8);
}

template <int imm8>
__m128i sse::insert(__m128i a, int i, int16_t) {
    return insert_epi16<imm8>(a, i);
}

template <int imm8>
__m128i sse::insert(__m128i a, int i, uint16_t) {
    return insert_epi16<imm8>(a, i);
}

void sse::lfence() { _mm_lfence(); }

__m128i sse::load(const __m128i *ptr) { return _mm_load_si128(ptr); }
__m128i sse::loadu(const __m128i *ptr) { return _mm_loadu_si128(ptr); }
__m128i sse::loadu_si16(const void *ptr) {
    return simd_cast<uint16_t, __m128i_t>(read_memory<uint16_t>(ptr));
}

__m128i sse::loadu_si32(const void *ptr) {
    return simd_cast<uint32_t, __m128i_t>(read_memory<uint32_t>(ptr));
}

__m128i sse::loadu_si64(const void *ptr) {
    return simd_cast<uint64_t, __m128i_t>(read_memory<uint64_t>(ptr));
}

template <typename T, WJR_REQUIRES_I(is_any_of_v<T, int8_t, int16_t, int32_t, int64_t,
                                                 uint8_t, uint16_t, uint32_t, uint64_t>)>
__m128i sse::logical_and(__m128i a, __m128i b, T) {
    return Not(Or(logical_not(a, T()), logical_not(b, T())));
}

template <typename T, WJR_REQUIRES_I(is_any_of_v<T, int8_t, int16_t, int32_t, int64_t,
                                                 uint8_t, uint16_t, uint32_t, uint64_t>)>
__m128i sse::logical_not(__m128i v, T) {
    auto Zero = zeros();
    return cmpeq(v, Zero, T());
}

template <typename T, WJR_REQUIRES_I(is_any_of_v<T, int8_t, int16_t, int32_t, int64_t,
                                                 uint8_t, uint16_t, uint32_t, uint64_t>)>
__m128i sse::logical_or(__m128i a, __m128i b, T) {
    return Not(logical_not(Or(a, b), T()));
}

__m128i sse::madd_epi16(__m128i a, __m128i b) { return _mm_madd_epi16(a, b); }

void sse::maskmoveu(__m128i a, __m128i mask, char *mem_addr) {
    return _mm_maskmoveu_si128(a, mask, mem_addr);
}

__m128i sse::max_epi8(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(SSE4_1)
    return _mm_max_epi8(a, b);
#else
    return blendv_epi8(b, a, cmpgt_epi8(a, b));
#endif
}

__m128i sse::max_epi16(__m128i a, __m128i b) { return _mm_max_epi16(a, b); }

__m128i sse::max_epi32(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(SSE4_1)
    return _mm_max_epi32(a, b);
#else
    return blendv_epi8(b, a, cmpgt_epi32(a, b));
#endif
}

__m128i sse::max_epu8(__m128i a, __m128i b) { return _mm_max_epu8(a, b); }

__m128i sse::max_epu16(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(SSE4_1)
    return _mm_max_epu16(a, b);
#else
    return add(subs_epu16(b, a), a, uint16_t());
#endif
}

__m128i sse::max_epu32(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(SSE4_1)
    return _mm_max_epu32(a, b);
#else
    return blendv_epi8(b, a, cmpgt_epu32(a, b));
#endif
}

__m128i sse::max(__m128i a, __m128i b, int8_t) { return max_epi8(a, b); }
__m128i sse::max(__m128i a, __m128i b, int16_t) { return max_epi16(a, b); }
__m128i sse::max(__m128i a, __m128i b, int32_t) { return max_epi32(a, b); }
__m128i sse::max(__m128i a, __m128i b, uint8_t) { return max_epu8(a, b); }
__m128i sse::max(__m128i a, __m128i b, uint16_t) { return max_epu16(a, b); }
__m128i sse::max(__m128i a, __m128i b, uint32_t) { return max_epu32(a, b); }

int8_t sse::max_epi8(__m128i a) { return 0x7fu ^ min_epu8(Xor(a, set1_epi8(0x7fu))); }

int16_t sse::max_epi16(__m128i a) {
#if WJR_HAS_SIMD(SSE4_1)
    return 0x7fffu ^ min_epu16(Xor(a, set1_epi16(0x7fffu)));
#else
    a = max_epi16(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a));
    a = max_epi16(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 3, 2)>(a));
    a = max_epi16(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 1, 0)>(a));
    return simd_cast<__m128i_t, int16_t>(a);
#endif
}

int32_t sse::max_epi32(__m128i a) {
    a = max_epi32(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a));
    a = max_epi32(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 3, 2)>(a));
    return simd_cast<__m128i_t, int32_t>(a);
}

uint8_t sse::max_epu8(__m128i a) {
#if WJR_HAS_SIMD(SSE4_1)
    return 0xffu ^ min_epu8(Xor(a, ones()));
#else
    a = max_epu8(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a));
    a = max_epu8(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 3, 2)>(a));
    a = max_epu8(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 1, 0)>(a));
    auto X = simd_cast<__m128i_t, uint32_t>(a);
    return std::max((uint8_t)X, (uint8_t)(X >> 8));
#endif
}

uint16_t sse::max_epu16(__m128i a) {
#if WJR_HAS_SIMD(SSE4_1)
    return 0xffffu ^ min_epu16(Xor(a, ones()));
#else
    a = max_epu16(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a));
    a = max_epu16(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 3, 2)>(a));
    a = max_epu16(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 1, 0)>(a));
    return simd_cast<__m128i_t, uint16_t>(a);
#endif
}

uint32_t sse::max_epu32(__m128i a) {
    a = max_epu32(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a));
    a = max_epu32(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 3, 2)>(a));
    return simd_cast<__m128i_t, uint32_t>(a);
}

int8_t sse::max(__m128i a, int8_t) { return max_epi8(a); }
int16_t sse::max(__m128i a, int16_t) { return max_epi16(a); }
int32_t sse::max(__m128i a, int32_t) { return max_epi32(a); }
uint8_t sse::max(__m128i a, uint8_t) { return max_epu8(a); }
uint16_t sse::max(__m128i a, uint16_t) { return max_epu16(a); }
uint32_t sse::max(__m128i a, uint32_t) { return max_epu32(a); }

void sse::mfence() { _mm_mfence(); }

__m128i sse::min_epi8(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(SSE4_1)
    return _mm_min_epi8(a, b);
#else
    return blendv_epi8(a, b, cmpgt_epi8(a, b));
#endif
}

__m128i sse::min_epi16(__m128i a, __m128i b) { return _mm_min_epi16(a, b); }

__m128i sse::min_epi32(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(SSE4_1)
    return _mm_min_epi32(a, b);
#else
    return blendv_epi8(a, b, cmpgt_epi32(a, b));
#endif
}

__m128i sse::min_epu8(__m128i a, __m128i b) { return _mm_min_epu8(a, b); }

__m128i sse::min_epu16(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(SSE4_1)
    return _mm_min_epu16(a, b);
#else
    return blendv_epi8(a, b, cmpgt_epu16(a, b));
#endif
}

__m128i sse::min_epu32(__m128i a, __m128i b) {
#if WJR_HAS_SIMD(SSE4_1)
    return _mm_min_epu32(a, b);
#else
    return blendv_epi8(a, b, cmpgt_epu32(a, b));
#endif
}

__m128i sse::min(__m128i a, __m128i b, int8_t) { return min_epi8(a, b); }
__m128i sse::min(__m128i a, __m128i b, int16_t) { return min_epi16(a, b); }
__m128i sse::min(__m128i a, __m128i b, int32_t) { return min_epi32(a, b); }
__m128i sse::min(__m128i a, __m128i b, uint8_t) { return min_epu8(a, b); }
__m128i sse::min(__m128i a, __m128i b, uint16_t) { return min_epu16(a, b); }
__m128i sse::min(__m128i a, __m128i b, uint32_t) { return min_epu32(a, b); }

int8_t sse::min_epi8(__m128i a) { return 0x80u ^ min_epu8(Xor(a, setmin_epi8())); }

int16_t sse::min_epi16(__m128i a) {
#if WJR_HAS_SIMD(SSE4_1)
    return 0x8000u ^ min_epu16(Xor(a, setmin_epi16()));
#else
    a = min_epi16(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a));
    a = min_epi16(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 3, 2)>(a));
    a = min_epi16(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 1, 0)>(a));
    return simd_cast<__m128i_t, int16_t>(a);
#endif
}

int32_t sse::min_epi32(__m128i a) {
    a = min_epi32(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a));
    a = min_epi32(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 3, 2)>(a));
    return simd_cast<__m128i_t, int32_t>(a);
}

uint8_t sse::min_epu8(__m128i a) {
#if WJR_HAS_SIMD(SSE4_1)
    a = min_epu8(a, srli_epi16(a, 8));
    a = _mm_minpos_epu16(a);
    return simd_cast<__m128i_t, uint8_t>(a);
#else
    a = min_epu8(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a));
    a = min_epu8(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 3, 2)>(a));
    a = min_epu8(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 1, 0)>(a));
    auto X = simd_cast<__m128i_t, uint32_t>(a);
    return std::min((uint8_t)X, (uint8_t)(X >> 8));
#endif
}

uint16_t sse::min_epu16(__m128i a) {
#if WJR_HAS_SIMD(SSE4_1)
    return simd_cast<__m128i_t, uint16_t>(_mm_minpos_epu16(a));
#else
    a = min_epu16(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a));
    a = min_epu16(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 3, 2)>(a));
    a = min_epu16(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 1, 0)>(a));
    return simd_cast<__m128i_t, uint16_t>(a);
#endif
}

uint32_t sse::min_epu32(__m128i a) {
    a = min_epu32(a, shuffle_epi32<_MM_SHUFFLE(3, 2, 3, 2)>(a));
    a = min_epu32(a, shufflelo_epi16<_MM_SHUFFLE(1, 0, 3, 2)>(a));
    return simd_cast<__m128i_t, uint32_t>(a);
}

int8_t sse::min(__m128i a, int8_t) { return min_epi8(a); }
int16_t sse::min(__m128i a, int16_t) { return min_epi16(a); }
int32_t sse::min(__m128i a, int32_t) { return min_epi32(a); }
uint8_t sse::min(__m128i a, uint8_t) { return min_epu8(a); }
uint16_t sse::min(__m128i a, uint16_t) { return min_epu16(a); }
uint32_t sse::min(__m128i a, uint32_t) { return min_epu32(a); }

__m128i sse::move_epi64(__m128i a) { return _mm_move_epi64(a); }

sse::mask_type sse::movemask_epi8(__m128i a) {
    return static_cast<mask_type>(_mm_movemask_epi8(a));
}
sse::mask_type sse::movemask_pd(__m128d v) {
    return static_cast<mask_type>(_mm_movemask_pd(v));
}

sse::mask_type sse::movemask(__m128i v, int8_t) { return movemask_epi8(v); }
sse::mask_type sse::movemask(__m128i v, int32_t) {
    return movemask_ps(simd_cast<__m128i_t, __m128_t>(v));
}
sse::mask_type sse::movemask(__m128i v, int64_t) {
    return movemask_pd(simd_cast<__m128i_t, __m128d_t>(v));
}
sse::mask_type sse::movemask(__m128i v, uint8_t) { return movemask(v, int8_t()); }
sse::mask_type sse::movemask(__m128i v, uint32_t) { return movemask(v, int32_t()); }
sse::mask_type sse::movemask(__m128i v, uint64_t) { return movemask(v, int64_t()); }

__m128i sse::mul_epu32(__m128i a, __m128i b) { return _mm_mul_epu32(a, b); }

__m128i sse::mulhi_epi16(__m128i a, __m128i b) { return _mm_mulhi_epi16(a, b); }

__m128i sse::mulhi_epu16(__m128i a, __m128i b) { return _mm_mulhi_epu16(a, b); }

__m128i sse::mullo_epi16(__m128i a, __m128i b) { return _mm_mullo_epi16(a, b); }

__m128i sse::negate_epi8(__m128i a) {
#if WJR_HAS_SIMD(SSSE3)
    return sign_epi8(a, ones());
#else
    return sub_epi8(zeros(), a);
#endif
}

__m128i sse::negate_epi16(__m128i a) {
#if WJR_HAS_SIMD(SSSE3)
    return sign_epi16(a, ones());
#else
    return sub_epi16(zeros(), a);
#endif
}

__m128i sse::negate_epi32(__m128i a) {
#if WJR_HAS_SIMD(SSSE3)
    return sign_epi32(a, ones());
#else
    return sub_epi32(zeros(), a);
#endif
}

__m128i sse::negate_epi64(__m128i a) { return sub_epi64(zeros(), a); }

__m128i sse::negate(__m128i a, int8_t) { return negate_epi8(a); }
__m128i sse::negate(__m128i a, int16_t) { return negate_epi16(a); }
__m128i sse::negate(__m128i a, int32_t) { return negate_epi32(a); }
__m128i sse::negate(__m128i a, int64_t) { return negate_epi64(a); }
__m128i sse::negate(__m128i a, uint8_t) { return negate_epi8(a); }
__m128i sse::negate(__m128i a, uint16_t) { return negate_epi16(a); }
__m128i sse::negate(__m128i a, uint32_t) { return negate_epi32(a); }
__m128i sse::negate(__m128i a, uint64_t) { return negate_epi64(a); }

__m128i sse::Not(__m128i v) { return Xor(v, ones()); }

__m128i sse::Or(__m128i a, __m128i b) { return _mm_or_si128(a, b); }

__m128i sse::packs_epi16(__m128i a, __m128i b) { return _mm_packs_epi16(a, b); }
__m128i sse::packs_epi32(__m128i a, __m128i b) { return _mm_packs_epi32(a, b); }

__m128i sse::packus_epi16(__m128i a, __m128i b) { return _mm_packus_epi16(a, b); }

__m128i sse::preloadu_si16(const void *ptr) { return loadu_si16(ptr); }
__m128i sse::preloadu_si32(const void *ptr) { return loadu_si32(ptr); }

__m128i sse::preloadu_si48(const void *ptr) {
    return insert_epi16<2>(preloadu_si32(ptr),
                           reinterpret_cast<const uint16_t *>(ptr)[2]);
}

__m128i sse::preloadu_si64(const void *ptr) { return loadu_si64(ptr); }

__m128i sse::preloadu_si80(const void *ptr) {
    return insert_epi16<4>(preloadu_si64(ptr),
                           reinterpret_cast<const uint16_t *>(ptr)[4]);
}

__m128i sse::preloadu_si96(const void *ptr) {
#if WJR_HAS_SIMD(SSE4_1)
    return insert_epi32<2>(preloadu_si64(ptr),
                           reinterpret_cast<const uint32_t *>(ptr)[2]);
#else
    return insert_epi16<5>(preloadu_si80(ptr),
                           reinterpret_cast<const uint16_t *>(ptr)[5]);
#endif
}

__m128i sse::preloadu_si112(const void *ptr) {
    return insert_epi16<6>(preloadu_si96(ptr),
                           reinterpret_cast<const uint16_t *>(ptr)[6]);
}

__m128i sse::preloadu_si128(const void *ptr) { return loadu((__m128i *)ptr); }

__m128i sse::preloadu_si16x(const void *ptr, int n) {
    // preloadu_si(n * 16)
    switch (n) {
    case 0:
        return zeros();
    case 1:
        return preloadu_si16(ptr);
    case 2:
        return preloadu_si32(ptr);
    case 3:
        return preloadu_si48(ptr);
    case 4:
        return preloadu_si64(ptr);
    case 5:
        return preloadu_si80(ptr);
    case 6:
        return preloadu_si96(ptr);
    case 7:
        return preloadu_si112(ptr);
    default:
        return preloadu_si128(ptr);
    }
}

__m128i sse::sad_epu8(__m128i a, __m128i b) { return _mm_sad_epu8(a, b); }

__m128i sse::zeros() { return _mm_setzero_si128(); }
__m128i sse::ones() { return _mm_set1_epi32(-1); }

__m128i sse::set_epi8(char e15, char e14, char e13, char e12, char e11, char e10, char e9,
                      char e8, char e7, char e6, char e5, char e4, char e3, char e2,
                      char e1, char e0) {
    return _mm_set_epi8(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1,
                        e0);
}

__m128i sse::set_epi16(short e7, short e6, short e5, short e4, short e3, short e2,
                       short e1, short e0) {
    return _mm_set_epi16(e7, e6, e5, e4, e3, e2, e1, e0);
}
__m128i sse::set_epi32(int e3, int e2, int e1, int e0) {
    return _mm_set_epi32(e3, e2, e1, e0);
}
__m128i sse::set_epi64x(long long e1, long long e0) { return _mm_set_epi64x(e1, e0); }

__m128i sse::setr_epi8(char e15, char e14, char e13, char e12, char e11, char e10,
                       char e9, char e8, char e7, char e6, char e5, char e4, char e3,
                       char e2, char e1, char e0) {
    return _mm_setr_epi8(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1,
                         e0);
}

__m128i sse::setr_epi16(short e7, short e6, short e5, short e4, short e3, short e2,
                        short e1, short e0) {
    return _mm_setr_epi16(e7, e6, e5, e4, e3, e2, e1, e0);
}
__m128i sse::setr_epi32(int e3, int e2, int e1, int e0) {
    return _mm_setr_epi32(e3, e2, e1, e0);
}

__m128i sse::set1_epi8(int8_t val) { return _mm_set1_epi8(val); }
__m128i sse::set1_epi16(int16_t val) { return _mm_set1_epi16(val); }
__m128i sse::set1_epi32(int32_t val) { return _mm_set1_epi32(val); }
__m128i sse::set1_epi64(int64_t val) { return _mm_set1_epi64x(val); }

__m128i sse::set1(int8_t val, int8_t) { return set1_epi8(val); }
__m128i sse::set1(int16_t val, int16_t) { return set1_epi16(val); }
__m128i sse::set1(int32_t val, int32_t) { return set1_epi32(val); }
__m128i sse::set1(int64_t val, int64_t) { return set1_epi64(val); }
__m128i sse::set1(uint8_t val, uint8_t) { return set1_epi8(val); }
__m128i sse::set1(uint16_t val, uint16_t) { return set1_epi16(val); }
__m128i sse::set1(uint32_t val, uint32_t) { return set1_epi32(val); }
__m128i sse::set1(uint64_t val, uint64_t) { return set1_epi64(val); }

__m128i sse::setmin_epi8() { return set1_epi8(0x80u); }
__m128i sse::setmin_epi16() { return set1_epi16(0x8000u); }
__m128i sse::setmin_epi32() { return set1_epi32(0x80000000u); }

__m128i sse::setmin(int8_t) { return setmin_epi8(); }
__m128i sse::setmin(int16_t) { return setmin_epi16(); }
__m128i sse::setmin(int32_t) { return setmin_epi32(); }
__m128i sse::setmin(uint8_t) { return set1_epi32(0); }
__m128i sse::setmin(uint16_t) { return set1_epi32(0); }
__m128i sse::setmin(uint32_t) { return set1_epi32(0); }

__m128i sse::setmax_epi8() { return set1_epi8(0x7F); }
__m128i sse::setmax_epi16() { return set1_epi16(0x7FFF); }
__m128i sse::setmax_epi32() { return set1_epi32(0x7FFFFFFF); }

__m128i sse::setmax(int8_t) { return setmax_epi8(); }
__m128i sse::setmax(int16_t) { return setmax_epi16(); }
__m128i sse::setmax(int32_t) { return setmax_epi32(); }
__m128i sse::setmax(uint8_t) { return set1_epi32(0xFFFFFFFF); }
__m128i sse::setmax(uint16_t) { return set1_epi32(0xFFFFFFFF); }
__m128i sse::setmax(uint32_t) { return set1_epi32(0xFFFFFFFF); }

template <int imm>
__m128i sse::shl(__m128i a) {
    if constexpr (imm >= 64) {
        a = slli<8>(a);
        a = slli_epi64(a, imm - 64);
        return a;
    } else {
        auto b = slli_epi64(a, imm);
        auto c = slli<8>(a);
        c = srli_epi64(c, 64 - imm);
        return Or(b, c);
    }
}

template <int imm>
__m128i sse::shr(__m128i a) {
    if constexpr (imm >= 64) {
        a = srli<8>(a);
        a = srli_epi64(a, imm - 64);
        return a;
    } else {
        auto b = srli_epi64(a, imm);
        auto c = srli<8>(a);
        c = slli_epi64(c, 64 - imm);
        return Or(b, c);
    }
}

template <int imm8>
__m128i sse::shuffle_epi32(__m128i v) {
    static_assert(imm8 >= 0 && imm8 <= 255, "imm8 must be in range [0, 255]");
    return _mm_shuffle_epi32(v, imm8);
}

template <int imm8>
__m128i sse::shufflehi_epi16(__m128i v) {
    return _mm_shufflehi_epi16(v, imm8);
}

template <int imm8>
__m128i sse::shufflelo_epi16(__m128i v) {
    return _mm_shufflelo_epi16(v, imm8);
}

__m128i sse::sll_epi16(__m128i a, __m128i b) { return _mm_sll_epi16(a, b); }
__m128i sse::sll_epi32(__m128i a, __m128i b) { return _mm_sll_epi32(a, b); }
__m128i sse::sll_epi64(__m128i a, __m128i b) { return _mm_sll_epi64(a, b); }

__m128i sse::sll(__m128i a, __m128i b, int16_t) { return sll_epi16(a, b); }
__m128i sse::sll(__m128i a, __m128i b, int32_t) { return sll_epi32(a, b); }
__m128i sse::sll(__m128i a, __m128i b, int64_t) { return sll_epi64(a, b); }
__m128i sse::sll(__m128i a, __m128i b, uint16_t) { return sll_epi16(a, b); }
__m128i sse::sll(__m128i a, __m128i b, uint32_t) { return sll_epi32(a, b); }
__m128i sse::sll(__m128i a, __m128i b, uint64_t) { return sll_epi64(a, b); }

template <int imm8>
__m128i sse::slli(__m128i v) {
    return _mm_slli_si128(v, imm8);
}
__m128i sse::slli_epi16(__m128i a, int imm8) {
    if (WJR_BUILTIN_CONSTANT_P(imm8 == 1) && imm8 == 1) {
        return sse::add_epi16(a, a);
    }

    return _mm_slli_epi16(a, imm8);
}
__m128i sse::slli_epi32(__m128i a, int imm8) {
    if (WJR_BUILTIN_CONSTANT_P(imm8 == 1) && imm8 == 1) {
        return sse::add_epi32(a, a);
    }

    return _mm_slli_epi32(a, imm8);
}
__m128i sse::slli_epi64(__m128i a, int imm8) {
    if (WJR_BUILTIN_CONSTANT_P(imm8 == 1) && imm8 == 1) {
        return sse::add_epi64(a, a);
    }

    return _mm_slli_epi64(a, imm8);
}

__m128i sse::slli(__m128i a, int imm8, int16_t) { return slli_epi16(a, imm8); }
__m128i sse::slli(__m128i a, int imm8, int32_t) { return slli_epi32(a, imm8); }
__m128i sse::slli(__m128i a, int imm8, int64_t) { return slli_epi64(a, imm8); }
__m128i sse::slli(__m128i a, int imm8, uint16_t) { return slli_epi16(a, imm8); }
__m128i sse::slli(__m128i a, int imm8, uint32_t) { return slli_epi32(a, imm8); }
__m128i sse::slli(__m128i a, int imm8, uint64_t) { return slli_epi64(a, imm8); }

__m128i sse::sra_epi16(__m128i a, __m128i b) { return _mm_sra_epi16(a, b); }
__m128i sse::sra_epi32(__m128i a, __m128i b) { return _mm_sra_epi32(a, b); }

__m128i sse::sra(__m128i a, __m128i b, int16_t) { return sra_epi16(a, b); }
__m128i sse::sra(__m128i a, __m128i b, int32_t) { return sra_epi32(a, b); }

__m128i sse::srai_epi16(__m128i a, int imm8) { return _mm_srai_epi16(a, imm8); }
__m128i sse::srai_epi32(__m128i a, int imm8) { return _mm_srai_epi32(a, imm8); }

__m128i sse::srai(__m128i a, int imm8, int16_t) { return srai_epi16(a, imm8); }
__m128i sse::srai(__m128i a, int imm8, int32_t) { return srai_epi32(a, imm8); }

__m128i sse::srl_epi16(__m128i a, __m128i b) { return _mm_srl_epi16(a, b); }
__m128i sse::srl_epi32(__m128i a, __m128i b) { return _mm_srl_epi32(a, b); }
__m128i sse::srl_epi64(__m128i a, __m128i b) { return _mm_srl_epi64(a, b); }

__m128i sse::srl(__m128i a, __m128i b, int16_t) { return srl_epi16(a, b); }
__m128i sse::srl(__m128i a, __m128i b, int32_t) { return srl_epi32(a, b); }
__m128i sse::srl(__m128i a, __m128i b, int64_t) { return srl_epi64(a, b); }
__m128i sse::srl(__m128i a, __m128i b, uint16_t) { return srl_epi16(a, b); }
__m128i sse::srl(__m128i a, __m128i b, uint32_t) { return srl_epi32(a, b); }
__m128i sse::srl(__m128i a, __m128i b, uint64_t) { return srl_epi64(a, b); }

template <int imm8>
__m128i sse::srli(__m128i v) {
    return _mm_srli_si128(v, imm8);
}
__m128i sse::srli_epi8(__m128i a, int imm8) {
    return And(srli_epi16(a, imm8), sse_details::srli_epi8_mask[imm8]);
}
__m128i sse::srli_epi16(__m128i a, int imm8) { return _mm_srli_epi16(a, imm8); }
__m128i sse::srli_epi32(__m128i a, int imm8) { return _mm_srli_epi32(a, imm8); }
__m128i sse::srli_epi64(__m128i a, int imm8) { return _mm_srli_epi64(a, imm8); }

__m128i sse::srli(__m128i a, int imm8, int8_t) { return srli_epi8(a, imm8); }
__m128i sse::srli(__m128i a, int imm8, int16_t) { return srli_epi16(a, imm8); }
__m128i sse::srli(__m128i a, int imm8, int32_t) { return srli_epi32(a, imm8); }
__m128i sse::srli(__m128i a, int imm8, int64_t) { return srli_epi64(a, imm8); }
__m128i sse::srli(__m128i a, int imm8, uint8_t) { return srli_epi8(a, imm8); }
__m128i sse::srli(__m128i a, int imm8, uint16_t) { return srli_epi16(a, imm8); }
__m128i sse::srli(__m128i a, int imm8, uint32_t) { return srli_epi32(a, imm8); }
__m128i sse::srli(__m128i a, int imm8, uint64_t) { return srli_epi64(a, imm8); }

void sse::stream(__m128i *ptr, __m128i v) { _mm_stream_si128(ptr, v); }

void sse::store(__m128i *ptr, __m128i val) { _mm_store_si128(ptr, val); }
void sse::storeu(__m128i *ptr, __m128i val) { _mm_storeu_si128(ptr, val); }

__m128i sse::sub_epi8(__m128i a, __m128i b) { return _mm_sub_epi8(a, b); }
__m128i sse::sub_epi16(__m128i a, __m128i b) { return _mm_sub_epi16(a, b); }
__m128i sse::sub_epi32(__m128i a, __m128i b) { return _mm_sub_epi32(a, b); }
__m128i sse::sub_epi64(__m128i a, __m128i b) { return _mm_sub_epi64(a, b); }

__m128i sse::sub(__m128i a, __m128i b, int8_t) { return sub_epi8(a, b); }
__m128i sse::sub(__m128i a, __m128i b, int16_t) { return sub_epi16(a, b); }
__m128i sse::sub(__m128i a, __m128i b, int32_t) { return sub_epi32(a, b); }
__m128i sse::sub(__m128i a, __m128i b, int64_t) { return sub_epi64(a, b); }
__m128i sse::sub(__m128i a, __m128i b, uint8_t) { return sub_epi8(a, b); }
__m128i sse::sub(__m128i a, __m128i b, uint16_t) { return sub_epi16(a, b); }
__m128i sse::sub(__m128i a, __m128i b, uint32_t) { return sub_epi32(a, b); }
__m128i sse::sub(__m128i a, __m128i b, uint64_t) { return sub_epi64(a, b); }

__m128i sse::subs_epi8(__m128i a, __m128i b) { return _mm_subs_epi8(a, b); }
__m128i sse::subs_epi16(__m128i a, __m128i b) { return _mm_subs_epi16(a, b); }

__m128i sse::subs_epu8(__m128i a, __m128i b) { return _mm_subs_epu8(a, b); }
__m128i sse::subs_epu16(__m128i a, __m128i b) { return _mm_subs_epu16(a, b); }

__m128i sse::subs(__m128i a, __m128i b, int8_t) { return subs_epi8(a, b); }
__m128i sse::subs(__m128i a, __m128i b, int16_t) { return subs_epi16(a, b); }
__m128i sse::subs(__m128i a, __m128i b, uint8_t) { return subs_epu8(a, b); }
__m128i sse::subs(__m128i a, __m128i b, uint16_t) { return subs_epu16(a, b); }

__m128i sse::unpackhi_epi8(__m128i a, __m128i b) { return _mm_unpackhi_epi8(a, b); }
__m128i sse::unpackhi_epi16(__m128i a, __m128i b) { return _mm_unpackhi_epi16(a, b); }
__m128i sse::unpackhi_epi32(__m128i a, __m128i b) { return _mm_unpackhi_epi32(a, b); }
__m128i sse::unpackhi_epi64(__m128i a, __m128i b) { return _mm_unpackhi_epi64(a, b); }

__m128i sse::unpackhi(__m128i a, __m128i b, int8_t) { return unpackhi_epi8(a, b); }
__m128i sse::unpackhi(__m128i a, __m128i b, int16_t) { return unpackhi_epi16(a, b); }
__m128i sse::unpackhi(__m128i a, __m128i b, int32_t) { return unpackhi_epi32(a, b); }
__m128i sse::unpackhi(__m128i a, __m128i b, int64_t) { return unpackhi_epi64(a, b); }
__m128i sse::unpackhi(__m128i a, __m128i b, uint8_t) { return unpackhi_epi8(a, b); }
__m128i sse::unpackhi(__m128i a, __m128i b, uint16_t) { return unpackhi_epi16(a, b); }
__m128i sse::unpackhi(__m128i a, __m128i b, uint32_t) { return unpackhi_epi32(a, b); }
__m128i sse::unpackhi(__m128i a, __m128i b, uint64_t) { return unpackhi_epi64(a, b); }

__m128i sse::unpacklo_epi8(__m128i a, __m128i b) { return _mm_unpacklo_epi8(a, b); }
__m128i sse::unpacklo_epi16(__m128i a, __m128i b) { return _mm_unpacklo_epi16(a, b); }
__m128i sse::unpacklo_epi32(__m128i a, __m128i b) { return _mm_unpacklo_epi32(a, b); }
__m128i sse::unpacklo_epi64(__m128i a, __m128i b) { return _mm_unpacklo_epi64(a, b); }

__m128i sse::unpacklo(__m128i a, __m128i b, int8_t) { return unpacklo_epi8(a, b); }
__m128i sse::unpacklo(__m128i a, __m128i b, int16_t) { return unpacklo_epi16(a, b); }
__m128i sse::unpacklo(__m128i a, __m128i b, int32_t) { return unpacklo_epi32(a, b); }
__m128i sse::unpacklo(__m128i a, __m128i b, int64_t) { return unpacklo_epi64(a, b); }
__m128i sse::unpacklo(__m128i a, __m128i b, uint8_t) { return unpacklo_epi8(a, b); }
__m128i sse::unpacklo(__m128i a, __m128i b, uint16_t) { return unpacklo_epi16(a, b); }
__m128i sse::unpacklo(__m128i a, __m128i b, uint32_t) { return unpacklo_epi32(a, b); }

__m128i sse::Xor(__m128i a, __m128i b) { return _mm_xor_si128(a, b); }

#endif

#if WJR_HAS_SIMD(SSE3)

__m128i sse::lddqu(const __m128i *ptr) { return _mm_lddqu_si128(ptr); }

#endif

#if WJR_HAS_SIMD(SSSE3)

__m128i sse::abs_epi8(__m128i val) { return _mm_abs_epi8(val); }
__m128i sse::abs_epi16(__m128i val) { return _mm_abs_epi16(val); }
__m128i sse::abs_epi32(__m128i val) { return _mm_abs_epi32(val); }

__m128i sse::abs(__m128i val, int8_t) { return abs_epi8(val); }
__m128i sse::abs(__m128i val, int16_t) { return abs_epi16(val); }
__m128i sse::abs(__m128i val, int32_t) { return abs_epi32(val); }
__m128i sse::abs(__m128i val, uint8_t) { return val; }
__m128i sse::abs(__m128i val, uint16_t) { return val; }
__m128i sse::abs(__m128i val, uint32_t) { return val; }

__m128i sse::shuffle_epi8(__m128i v, __m128i imm8) { return _mm_shuffle_epi8(v, imm8); }

__m128i sse::sign_epi8(__m128i a, __m128i b) { return _mm_sign_epi8(a, b); }
__m128i sse::sign_epi16(__m128i a, __m128i b) { return _mm_sign_epi16(a, b); }
__m128i sse::sign_epi32(__m128i a, __m128i b) { return _mm_sign_epi32(a, b); }

__m128i sse::sign(__m128i a, __m128i b, int8_t) { return sign_epi8(a, b); }
__m128i sse::sign(__m128i a, __m128i b, int16_t) { return sign_epi16(a, b); }
__m128i sse::sign(__m128i a, __m128i b, int32_t) { return sign_epi32(a, b); }
__m128i sse::sign(__m128i a, __m128i b, uint8_t) { return sign_epi8(a, b); }
__m128i sse::sign(__m128i a, __m128i b, uint16_t) { return sign_epi16(a, b); }
__m128i sse::sign(__m128i a, __m128i b, uint32_t) { return sign_epi32(a, b); }

#endif

#if WJR_HAS_SIMD(SSE4_1)

template <int imm8>
__m128i sse::blend_epi16(__m128i a, __m128i b) {
    return _mm_blend_epi16(a, b, imm8);
}

__m128i sse::cmpeq_epi64(__m128i a, __m128i b) { return _mm_cmpeq_epi64(a, b); }

__m128i sse::cmpeq(__m128i a, __m128i b, int64_t) { return cmpeq_epi64(a, b); }
__m128i sse::cmpeq(__m128i a, __m128i b, uint64_t) { return cmpeq_epi64(a, b); }

__m128i sse::cmpgt_epi64(__m128i a, __m128i b) { return _mm_cmpgt_epi64(a, b); }

__m128i sse::cmpgt(__m128i a, __m128i b, int64_t) { return cmpgt_epi64(a, b); }

template <int imm8>
__m128i sse::insert_epi8(__m128i a, int i) {
    return _mm_insert_epi8(a, i, imm8);
}

template <int imm8>
__m128i sse::insert_epi32(__m128i a, int i) {
    return _mm_insert_epi32(a, i, imm8);
}

template <int imm8>
__m128i sse::insert_epi64(__m128i a, int64_t i) {
    return _mm_insert_epi64(a, i, imm8);
}

template <int imm8>
__m128i sse::insert(__m128i a, int i, int8_t) {
    return insert_epi8<imm8>(a, i);
}

template <int imm8>
__m128i sse::insert(__m128i a, int i, int32_t) {
    return insert_epi32<imm8>(a, i);
}

template <int imm8>
__m128i sse::insert(__m128i a, int64_t i, int64_t) {
    return insert_epi64<imm8>(a, i);
}

template <int imm8>
__m128i sse::insert(__m128i a, int i, uint8_t) {
    return insert_epi8<imm8>(a, i);
}

template <int imm8>
__m128i sse::insert(__m128i a, int i, uint32_t) {
    return insert_epi32<imm8>(a, i);
}

template <int imm8>
__m128i sse::insert(__m128i a, int64_t i, uint64_t) {
    return insert_epi64<imm8>(a, i);
}

__m128i sse::minpos_epu16(__m128i a) { return _mm_minpos_epu16(a); }

__m128i sse::mul_epi32(__m128i a, __m128i b) { return _mm_mul_epi32(a, b); }

__m128i sse::mullo_epi32(__m128i a, __m128i b) { return _mm_mullo_epi32(a, b); }

__m128i sse::packus_epi32(__m128i a, __m128i b) { return _mm_packus_epi32(a, b); }

__m128i sse::stream_load(__m128i *p) { return _mm_stream_load_si128(p); }

int sse::test_all_ones(__m128i a) { return _mm_test_all_ones(a); }

int sse::test_all_zeros(__m128i a, __m128i b) { return _mm_test_all_zeros(a, b); }

int sse::test_all_zeros(__m128i a) { return _mm_test_all_zeros(a, a); }

int sse::test_mix_ones_zeros(__m128i a, __m128i b) {
    return _mm_test_mix_ones_zeros(a, b);
}

int sse::testc(__m128i a, __m128i b) { return _mm_testc_si128(a, b); }

int sse::testnzc(__m128i a, __m128i b) { return _mm_testnzc_si128(a, b); }

int sse::testz(__m128i a, __m128i b) { return _mm_testz_si128(a, b); }

#endif

/*------------------------avx------------------------*/

constexpr size_t avx::width() { return 256; }

constexpr avx::mask_type avx::mask() { return 0xffffffff; }

#if WJR_HAS_SIMD(AVX)

__m256i avx::concat(__m128i a, __m128i b) {
    return insert_si128<1>(simd_cast<__m128i_t, __m256i_t>(a), b);
}

template <int imm8>
int avx::extract_epi32(__m256i v) {
    return _mm256_extract_epi32(v, imm8);
}

template <int imm8>
int64_t avx::extract_epi64(__m256i v) {
    return _mm256_extract_epi64(v, imm8);
}

template <int imm8>
int avx::extract(__m256i v, int32_t) {
    return extract_epi32<imm8>(v);
}

template <int imm8>
int64_t avx::extract(__m256i v, int64_t) {
    return extract_epi64<imm8>(v);
}

template <int imm8>
__m128i avx::extract_si128(__m256i v) {
#if WJR_HAS_SIMD(AV2)
    return _mm256_extracti128_si256(v, imm8);
#else
    return _mm256_extractf128_si256(v, imm8);
#endif
}

__m128i avx::getlow(__m256i a) { return simd_cast<__m256i_t, __m128i_t>(a); }

__m128i avx::gethigh(__m256i a) { return extract_si128<1>(a); }

template <int imm8>
__m256i avx::insert_epi8(__m256i v, int8_t i) {
    return _mm256_insert_epi8(v, i, imm8);
}

template <int imm8>
__m256i avx::insert_epi16(__m256i v, int16_t i) {
    return _mm256_insert_epi16(v, i, imm8);
}

template <int imm8>
__m256i avx::insert_epi32(__m256i v, int32_t i) {
    return _mm256_insert_epi32(v, i, imm8);
}

template <int imm8>
__m256i avx::insert_epi64(__m256i v, int64_t i) {
    return _mm256_insert_epi64(v, i, imm8);
}

template <int imm8>
__m256i avx::insert_si128(__m256i a, __m128i b) {
#if WJR_HAS_SIMD(AVX2)
    return _mm256_inserti128_si256(a, b, imm8);
#else
    return _mm256_insertf128_si256(a, b, imm8);
#endif
}

__m256i avx::load(const __m256i *p) { return _mm256_load_si256(p); }
__m256i avx::loadu(const __m256i *p) { return _mm256_loadu_si256(p); }

__m256i avx::ones() { return _mm256_set1_epi32(-1); }

__m256i avx::preloadu_si16(const void *ptr) {
    return simd_cast<__m128i_t, __m256i_t>(sse::preloadu_si16(ptr));
}

__m256i avx::preloadu_si32(const void *ptr) {
    return simd_cast<__m128i_t, __m256i_t>(sse::preloadu_si32(ptr));
}

__m256i avx::preloadu_si48(const void *ptr) {
    return simd_cast<__m128i_t, __m256i_t>(sse::preloadu_si48(ptr));
}

__m256i avx::preloadu_si64(const void *ptr) {
    return simd_cast<__m128i_t, __m256i_t>(sse::preloadu_si64(ptr));
}

__m256i avx::preloadu_si80(const void *ptr) {
    return simd_cast<__m128i_t, __m256i_t>(sse::preloadu_si80(ptr));
}

__m256i avx::preloadu_si96(const void *ptr) {
    return simd_cast<__m128i_t, __m256i_t>(sse::preloadu_si96(ptr));
}

__m256i avx::preloadu_si112(const void *ptr) {
    return simd_cast<__m128i_t, __m256i_t>(sse::preloadu_si112(ptr));
}

__m256i avx::preloadu_si128(const void *ptr) {
    return simd_cast<__m128i_t, __m256i_t>(sse::preloadu_si128(ptr));
}

__m256i avx::preloadu_si144(const void *ptr) {
    return concat(sse::preloadu_si128(ptr), sse::preloadu_si16((const char *)ptr + 16));
}

__m256i avx::preloadu_si160(const void *ptr) {
    return concat(sse::preloadu_si128(ptr), sse::preloadu_si32((const char *)ptr + 16));
}

__m256i avx::preloadu_si176(const void *ptr) {
    return concat(sse::preloadu_si128(ptr), sse::preloadu_si48((const char *)ptr + 16));
}

__m256i avx::preloadu_si192(const void *ptr) {
    return concat(sse::preloadu_si128(ptr), sse::preloadu_si64((const char *)ptr + 16));
}

__m256i avx::preloadu_si208(const void *ptr) {
    return concat(sse::preloadu_si128(ptr), sse::preloadu_si80((const char *)ptr + 16));
}

__m256i avx::preloadu_si224(const void *ptr) {
    return concat(sse::preloadu_si128(ptr), sse::preloadu_si96((const char *)ptr + 16));
}
__m256i avx::preloadu_si240(const void *ptr) {
    return concat(sse::preloadu_si128(ptr), sse::preloadu_si112((const char *)ptr + 16));
}

__m256i avx::preloadu_si256(const void *ptr) { return loadu((const __m256i *)ptr); }

__m256i avx::preloadu_si16x(const void *ptr, int n) {
    switch (n) {
    case 0:
        return zeros();
    case 1:
        return preloadu_si16(ptr);
    case 2:
        return preloadu_si32(ptr);
    case 3:
        return preloadu_si48(ptr);
    case 4:
        return preloadu_si64(ptr);
    case 5:
        return preloadu_si80(ptr);
    case 6:
        return preloadu_si96(ptr);
    case 7:
        return preloadu_si112(ptr);
    case 8:
        return preloadu_si128(ptr);
    case 9:
        return preloadu_si144(ptr);
    case 10:
        return preloadu_si160(ptr);
    case 11:
        return preloadu_si176(ptr);
    case 12:
        return preloadu_si192(ptr);
    case 13:
        return preloadu_si208(ptr);
    case 14:
        return preloadu_si224(ptr);
    case 15:
        return preloadu_si240(ptr);
    default:
        return preloadu_si256(ptr);
    }
}

__m256i avx::set_epi8(char e31, char e30, char e29, char e28, char e27, char e26,
                      char e25, char e24, char e23, char e22, char e21, char e20,
                      char e19, char e18, char e17, char e16, char e15, char e14,
                      char e13, char e12, char e11, char e10, char e9, char e8, char e7,
                      char e6, char e5, char e4, char e3, char e2, char e1, char e0) {
    return _mm256_set_epi8(e31, e30, e29, e28, e27, e26, e25, e24, e23, e22, e21, e20,
                           e19, e18, e17, e16, e15, e14, e13, e12, e11, e10, e9, e8, e7,
                           e6, e5, e4, e3, e2, e1, e0);
}

__m256i avx::set_epi16(short e15, short e14, short e13, short e12, short e11, short e10,
                       short e9, short e8, short e7, short e6, short e5, short e4,
                       short e3, short e2, short e1, short e0) {
    return _mm256_set_epi16(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2,
                            e1, e0);
}

__m256i avx::set_epi32(int e7, int e6, int e5, int e4, int e3, int e2, int e1, int e0) {
    return _mm256_set_epi32(e7, e6, e5, e4, e3, e2, e1, e0);
}

__m256i avx::set_epi64x(long long e3, long long e2, long long e1, long long e0) {
    return _mm256_set_epi64x(e3, e2, e1, e0);
}

__m256i avx::setr_epi8(char e31, char e30, char e29, char e28, char e27, char e26,
                       char e25, char e24, char e23, char e22, char e21, char e20,
                       char e19, char e18, char e17, char e16, char e15, char e14,
                       char e13, char e12, char e11, char e10, char e9, char e8, char e7,
                       char e6, char e5, char e4, char e3, char e2, char e1, char e0) {
    return _mm256_setr_epi8(e31, e30, e29, e28, e27, e26, e25, e24, e23, e22, e21, e20,
                            e19, e18, e17, e16, e15, e14, e13, e12, e11, e10, e9, e8, e7,
                            e6, e5, e4, e3, e2, e1, e0);
}

__m256i avx::setr_epi16(short e15, short e14, short e13, short e12, short e11, short e10,
                        short e9, short e8, short e7, short e6, short e5, short e4,
                        short e3, short e2, short e1, short e0) {
    return _mm256_setr_epi16(e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2,
                             e1, e0);
}

__m256i avx::setr_epi32(int e7, int e6, int e5, int e4, int e3, int e2, int e1, int e0) {
    return _mm256_setr_epi32(e7, e6, e5, e4, e3, e2, e1, e0);
}

__m256i avx::setr_epi64x(long long e3, long long e2, long long e1, long long e0) {
    return _mm256_setr_epi64x(e3, e2, e1, e0);
}

__m256i avx::set1_epi8(int8_t a) { return _mm256_set1_epi8(a); }
__m256i avx::set1_epi16(int16_t a) { return _mm256_set1_epi16(a); }
__m256i avx::set1_epi32(int32_t a) { return _mm256_set1_epi32(a); }
__m256i avx::set1_epi64(int64_t a) { return _mm256_set1_epi64x(a); }

__m256i avx::set1(int8_t a, int8_t) { return set1_epi8(a); }
__m256i avx::set1(int16_t a, int16_t) { return set1_epi16(a); }
__m256i avx::set1(int32_t a, int32_t) { return set1_epi32(a); }
__m256i avx::set1(int64_t a, int64_t) { return set1_epi64(a); }
__m256i avx::set1(uint8_t a, uint8_t) { return set1_epi8(a); }
__m256i avx::set1(uint16_t a, uint16_t) { return set1_epi16(a); }
__m256i avx::set1(uint32_t a, uint32_t) { return set1_epi32(a); }
__m256i avx::set1(uint64_t a, uint64_t) { return set1_epi64(a); }

__m256i avx::setmin_epi8() { return set1_epi8(0x80u); }
__m256i avx::setmin_epi16() { return set1_epi16(0x8000u); }
__m256i avx::setmin_epi32() { return set1_epi32(0x80000000u); }
__m256i avx::setmin_epi64() { return set1_epi64(0x8000000000000000ull); }

__m256i avx::setmin(int8_t) { return setmin_epi8(); }
__m256i avx::setmin(int16_t) { return setmin_epi16(); }
__m256i avx::setmin(int32_t) { return setmin_epi32(); }
__m256i avx::setmin(int64_t) { return setmin_epi64(); }

__m256i avx::setmax_epi8() { return set1_epi8(0x7f); }
__m256i avx::setmax_epi16() { return set1_epi16(0x7fff); }
__m256i avx::setmax_epi32() { return set1_epi32(0x7fffffff); }
__m256i avx::setmax_epi64() { return set1_epi64(0x7fffffffffffffff); }

__m256i avx::setmax(int8_t) { return setmax_epi8(); }
__m256i avx::setmax(int16_t) { return setmax_epi16(); }
__m256i avx::setmax(int32_t) { return setmax_epi32(); }
__m256i avx::setmax(int64_t) { return setmax_epi64(); }

void avx::stream(__m256i *p, __m256i a) { _mm256_stream_si256(p, a); }

void avx::store(__m256i *p, __m256i a) { _mm256_store_si256(p, a); }
void avx::storeu(__m256i *p, __m256i a) { _mm256_storeu_si256(p, a); }

int avx::test_all_zeros(__m256i a) { return testz(a, a); }

int avx::testc(__m256i a, __m256i b) { return _mm256_testc_si256(a, b); }

int avx::testnzc(__m256i a, __m256i b) { return _mm256_testnzc_si256(a, b); }

int avx::testz(__m256i a, __m256i b) { return _mm256_testz_si256(a, b); }

__m256i avx::zeros() { return _mm256_setzero_si256(); }

#endif

#if WJR_HAS_SIMD(AVX2)

__m256i avx::And(__m256i a, __m256i b) { return _mm256_and_si256(a, b); }

__m256i avx::AndNot(__m256i a, __m256i b) { return _mm256_andnot_si256(a, b); }

__m256i avx::Or(__m256i a, __m256i b) { return _mm256_or_si256(a, b); }

__m256i avx::Xor(__m256i a, __m256i b) { return _mm256_xor_si256(a, b); }

__m256i avx::Not(__m256i v) { return _mm256_xor_si256(v, ones()); }

__m256i avx::abs_epi8(__m256i v) { return _mm256_abs_epi8(v); }
__m256i avx::abs_epi16(__m256i v) { return _mm256_abs_epi16(v); }
__m256i avx::abs_epi32(__m256i v) { return _mm256_abs_epi32(v); }

__m256i avx::abs(__m256i v, int8_t) { return abs_epi8(v); }
__m256i avx::abs(__m256i v, int16_t) { return abs_epi16(v); }
__m256i avx::abs(__m256i v, int32_t) { return abs_epi32(v); }
__m256i avx::abs(__m256i v, int64_t) { return abs_epi32(v); }

__m256i avx::add_epi8(__m256i a, __m256i b) { return _mm256_add_epi8(a, b); }
__m256i avx::add_epi16(__m256i a, __m256i b) { return _mm256_add_epi16(a, b); }
__m256i avx::add_epi32(__m256i a, __m256i b) { return _mm256_add_epi32(a, b); }
__m256i avx::add_epi64(__m256i a, __m256i b) { return _mm256_add_epi64(a, b); }

__m256i avx::add(__m256i a, __m256i b, int8_t) { return add_epi8(a, b); }
__m256i avx::add(__m256i a, __m256i b, int16_t) { return add_epi16(a, b); }
__m256i avx::add(__m256i a, __m256i b, int32_t) { return add_epi32(a, b); }
__m256i avx::add(__m256i a, __m256i b, int64_t) { return add_epi64(a, b); }
__m256i avx::add(__m256i a, __m256i b, uint8_t) { return add_epi8(a, b); }
__m256i avx::add(__m256i a, __m256i b, uint16_t) { return add_epi16(a, b); }
__m256i avx::add(__m256i a, __m256i b, uint32_t) { return add_epi32(a, b); }
__m256i avx::add(__m256i a, __m256i b, uint64_t) { return add_epi64(a, b); }

uint8_t avx::add_epu8(__m256i v) {
    return sse::add_epu8(sse::add_epi8(getlow(v), gethigh(v)));
}

uint16_t avx::add_epu16(__m256i v) {
    return sse::add_epu16(sse::add_epi16(getlow(v), gethigh(v)));
}

uint32_t avx::add_epu32(__m256i v) {
    return sse::add_epu32(sse::add_epi32(getlow(v), gethigh(v)));
}

uint64_t avx::add_epu64(__m256i v) {
    return sse::add_epu64(sse::add_epi64(getlow(v), gethigh(v)));
}

int8_t avx::add_epi8(__m256i v) { return add_epu8(v); }
int16_t avx::add_epi16(__m256i v) { return add_epu16(v); }
int32_t avx::add_epi32(__m256i v) { return add_epu32(v); }
int64_t avx::add_epi64(__m256i v) { return add_epu64(v); }

int8_t avx::add(__m256i v, int8_t) { return add_epi8(v); }
int16_t avx::add(__m256i v, int16_t) { return add_epi16(v); }
int32_t avx::add(__m256i v, int32_t) { return add_epi32(v); }
int64_t avx::add(__m256i v, int64_t) { return add_epi64(v); }
uint8_t avx::add(__m256i v, uint8_t) { return add_epu8(v); }
uint16_t avx::add(__m256i v, uint16_t) { return add_epu16(v); }
uint32_t avx::add(__m256i v, uint32_t) { return add_epu32(v); }
uint64_t avx::add(__m256i v, uint64_t) { return add_epu64(v); }

__m256i avx::adds_epi8(__m256i a, __m256i b) { return _mm256_adds_epi8(a, b); }
__m256i avx::adds_epi16(__m256i a, __m256i b) { return _mm256_adds_epi16(a, b); }

__m256i avx::adds_epu8(__m256i a, __m256i b) { return _mm256_adds_epu8(a, b); }
__m256i avx::adds_epu16(__m256i a, __m256i b) { return _mm256_adds_epu16(a, b); }

__m256i avx::adds(__m256i a, __m256i b, int8_t) { return adds_epi8(a, b); }
__m256i avx::adds(__m256i a, __m256i b, int16_t) { return adds_epi16(a, b); }
__m256i avx::adds(__m256i a, __m256i b, uint8_t) { return adds_epu8(a, b); }
__m256i avx::adds(__m256i a, __m256i b, uint16_t) { return adds_epu16(a, b); }

template <int imm8>
__m256i avx::alignr(__m256i a, __m256i b) {
    return _mm256_alignr_epi8(a, b, imm8);
}

__m256i avx::alignr_epi16(__m256i a, __m256i b, int c) {
    return Or(slli_epi16(a, 16 - c), srli_epi16(b, c));
}

__m256i avx::alignr_epi32(__m256i a, __m256i b, int c) {
    return Or(slli_epi32(a, 32 - c), srli_epi32(b, c));
}

__m256i avx::alignr_epi64(__m256i a, __m256i b, int c) {
    return Or(slli_epi64(a, 64 - c), srli_epi64(b, c));
}

__m256i avx::alignr(__m256i a, __m256i b, int c, int16_t) {
    return alignr_epi16(a, b, c);
}

__m256i avx::alignr(__m256i a, __m256i b, int c, int32_t) {
    return alignr_epi32(a, b, c);
}

__m256i avx::alignr(__m256i a, __m256i b, int c, int64_t) {
    return alignr_epi64(a, b, c);
}

__m256i avx::alignr(__m256i a, __m256i b, int c, uint16_t) {
    return alignr_epi16(a, b, c);
}

__m256i avx::alignr(__m256i a, __m256i b, int c, uint32_t) {
    return alignr_epi32(a, b, c);
}

__m256i avx::alignr(__m256i a, __m256i b, int c, uint64_t) {
    return alignr_epi64(a, b, c);
}

__m256i avx::avg_epu8(__m256i a, __m256i b) { return _mm256_avg_epu8(a, b); }
__m256i avx::avg_epu16(__m256i a, __m256i b) { return _mm256_avg_epu16(a, b); }

__m256i avx::avg(__m256i a, __m256i b, int8_t) { return avg_epu8(a, b); }
__m256i avx::avg(__m256i a, __m256i b, int16_t) { return avg_epu16(a, b); }
__m256i avx::avg(__m256i a, __m256i b, uint8_t) { return avg_epu8(a, b); }
__m256i avx::avg(__m256i a, __m256i b, uint16_t) { return avg_epu16(a, b); }

template <int imm8>
__m256i avx::blend_epi16(__m256i a, __m256i b) {
    return _mm256_blend_epi16(a, b, imm8);
}

template <int imm8>
__m256i avx::blend_epi32(__m256i a, __m256i b) {
    return _mm256_blend_epi32(a, b, imm8);
}

__m256i avx::blendv_epi8(__m256i a, __m256i b, __m256i mask) {
    return _mm256_blendv_epi8(a, b, mask);
}

template <int imm8>
__m256i avx::bslli_epi128(__m256i a) {
    return _mm256_bslli_epi128(a, imm8);
}

template <int imm8>
__m256i avx::bsrli_epi128(__m256i a) {
    return _mm256_bsrli_epi128(a, imm8);
}

__m256i avx::cmpeq_epi8(__m256i a, __m256i b) { return _mm256_cmpeq_epi8(a, b); }
__m256i avx::cmpeq_epi16(__m256i a, __m256i b) { return _mm256_cmpeq_epi16(a, b); }
__m256i avx::cmpeq_epi32(__m256i a, __m256i b) { return _mm256_cmpeq_epi32(a, b); }
__m256i avx::cmpeq_epi64(__m256i a, __m256i b) { return _mm256_cmpeq_epi64(a, b); }

__m256i avx::cmpeq(__m256i a, __m256i b, int8_t) { return cmpeq_epi8(a, b); }
__m256i avx::cmpeq(__m256i a, __m256i b, int16_t) { return cmpeq_epi16(a, b); }
__m256i avx::cmpeq(__m256i a, __m256i b, int32_t) { return cmpeq_epi32(a, b); }
__m256i avx::cmpeq(__m256i a, __m256i b, int64_t) { return cmpeq_epi64(a, b); }
__m256i avx::cmpeq(__m256i a, __m256i b, uint8_t) { return cmpeq_epi8(a, b); }
__m256i avx::cmpeq(__m256i a, __m256i b, uint16_t) { return cmpeq_epi16(a, b); }
__m256i avx::cmpeq(__m256i a, __m256i b, uint32_t) { return cmpeq_epi32(a, b); }
__m256i avx::cmpeq(__m256i a, __m256i b, uint64_t) { return cmpeq_epi64(a, b); }

__m256i avx::cmpge_epi8(__m256i a, __m256i b) {
    return cmpeq(min(a, b, int8_t()), b, uint8_t());
}

__m256i avx::cmpge_epi16(__m256i a, __m256i b) {
    return cmpeq(min(a, b, int16_t()), b, uint16_t());
}

__m256i avx::cmpge_epi32(__m256i a, __m256i b) {
    return cmpeq(min(a, b, int32_t()), b, uint8_t());
}

__m256i avx::cmpge_epu8(__m256i a, __m256i b) {
    return cmpeq(min(a, b, uint8_t()), b, uint8_t());
}

__m256i avx::cmpge_epu16(__m256i a, __m256i b) {
    return cmpeq(min(a, b, uint16_t()), b, uint16_t());
}

__m256i avx::cmpge_epu32(__m256i a, __m256i b) {
    return cmpeq(min(a, b, uint32_t()), b, uint32_t());
}

__m256i avx::cmpge(__m256i a, __m256i b, int8_t) { return cmpge_epi8(a, b); }
__m256i avx::cmpge(__m256i a, __m256i b, int16_t) { return cmpge_epi16(a, b); }
__m256i avx::cmpge(__m256i a, __m256i b, int32_t) { return cmpge_epi32(a, b); }
__m256i avx::cmpge(__m256i a, __m256i b, uint8_t) { return cmpge_epu8(a, b); }
__m256i avx::cmpge(__m256i a, __m256i b, uint16_t) { return cmpge_epu16(a, b); }
__m256i avx::cmpge(__m256i a, __m256i b, uint32_t) { return cmpge_epu32(a, b); }

__m256i avx::cmpgt_epi8(__m256i a, __m256i b) { return _mm256_cmpgt_epi8(a, b); }
__m256i avx::cmpgt_epi16(__m256i a, __m256i b) { return _mm256_cmpgt_epi16(a, b); }
__m256i avx::cmpgt_epi32(__m256i a, __m256i b) { return _mm256_cmpgt_epi32(a, b); }
__m256i avx::cmpgt_epi64(__m256i a, __m256i b) { return _mm256_cmpgt_epi64(a, b); }

__m256i avx::cmpgt_epu8(__m256i a, __m256i b) {
    return cmpgt_epi8(Xor(a, setmin_epi8()), Xor(b, setmin_epi8()));
}

__m256i avx::cmpgt_epu16(__m256i a, __m256i b) {
    return cmpgt_epi16(Xor(a, setmin_epi16()), Xor(b, setmin_epi16()));
}

__m256i avx::cmpgt_epu32(__m256i a, __m256i b) {
    return cmpgt_epi32(Xor(a, setmin_epi32()), Xor(b, setmin_epi32()));
}

__m256i avx::cmpgt_epu64(__m256i a, __m256i b) {
    return cmpgt_epi64(Xor(a, setmin_epi64()), Xor(b, setmin_epi64()));
}

__m256i avx::cmpgt(__m256i a, __m256i b, int8_t) { return cmpgt_epi8(a, b); }
__m256i avx::cmpgt(__m256i a, __m256i b, int16_t) { return cmpgt_epi16(a, b); }
__m256i avx::cmpgt(__m256i a, __m256i b, int32_t) { return cmpgt_epi32(a, b); }
__m256i avx::cmpgt(__m256i a, __m256i b, int64_t) { return cmpgt_epi64(a, b); }
__m256i avx::cmpgt(__m256i a, __m256i b, uint8_t) { return cmpgt_epu8(a, b); }
__m256i avx::cmpgt(__m256i a, __m256i b, uint16_t) { return cmpgt_epu16(a, b); }
__m256i avx::cmpgt(__m256i a, __m256i b, uint32_t) { return cmpgt_epu32(a, b); }
__m256i avx::cmpgt(__m256i a, __m256i b, uint64_t) { return cmpgt_epu64(a, b); }

__m256i avx::cmple_epi8(__m256i a, __m256i b) { return cmpge_epi8(b, a); }

__m256i avx::cmple_epi16(__m256i a, __m256i b) { return cmpge_epi16(b, a); }

__m256i avx::cmple_epi32(__m256i a, __m256i b) { return cmpge_epi32(b, a); }

__m256i avx::cmple_epu8(__m256i a, __m256i b) { return cmpge_epu8(b, a); }

__m256i avx::cmple_epu16(__m256i a, __m256i b) { return cmpge_epu16(b, a); }

__m256i avx::cmple_epu32(__m256i a, __m256i b) { return cmpge_epu32(b, a); }

__m256i avx::cmple(__m256i a, __m256i b, int8_t) { return cmple_epi8(a, b); }
__m256i avx::cmple(__m256i a, __m256i b, int16_t) { return cmple_epi16(a, b); }
__m256i avx::cmple(__m256i a, __m256i b, int32_t) { return cmple_epi32(a, b); }
__m256i avx::cmple(__m256i a, __m256i b, uint8_t) { return cmple_epu8(a, b); }
__m256i avx::cmple(__m256i a, __m256i b, uint16_t) { return cmple_epu16(a, b); }
__m256i avx::cmple(__m256i a, __m256i b, uint32_t) { return cmple_epu32(a, b); }

__m256i avx::cmplt_epi8(__m256i a, __m256i b) { return cmpgt_epi8(b, a); }
__m256i avx::cmplt_epi16(__m256i a, __m256i b) { return cmpgt_epi16(b, a); }
__m256i avx::cmplt_epi32(__m256i a, __m256i b) { return cmpgt_epi32(b, a); }

__m256i avx::cmplt_epu8(__m256i a, __m256i b) { return cmpgt_epu8(b, a); }
__m256i avx::cmplt_epu16(__m256i a, __m256i b) { return cmpgt_epu16(b, a); }
__m256i avx::cmplt_epu32(__m256i a, __m256i b) { return cmpgt_epu32(b, a); }

__m256i avx::cmplt(__m256i a, __m256i b, int8_t) { return cmplt_epi8(a, b); }
__m256i avx::cmplt(__m256i a, __m256i b, int16_t) { return cmplt_epi16(a, b); }
__m256i avx::cmplt(__m256i a, __m256i b, int32_t) { return cmplt_epi32(a, b); }
__m256i avx::cmplt(__m256i a, __m256i b, uint8_t) { return cmplt_epu8(a, b); }
__m256i avx::cmplt(__m256i a, __m256i b, uint16_t) { return cmplt_epu16(a, b); }
__m256i avx::cmplt(__m256i a, __m256i b, uint32_t) { return cmplt_epu32(a, b); }

__m256i avx::cmpne_epi8(__m256i a, __m256i b) { return Not(cmpeq_epi8(a, b)); }
__m256i avx::cmpne_epi16(__m256i a, __m256i b) { return Not(cmpeq_epi16(a, b)); }
__m256i avx::cmpne_epi32(__m256i a, __m256i b) { return Not(cmpeq_epi32(a, b)); }

__m256i avx::cmpne(__m256i a, __m256i b, int8_t) { return cmpne_epi8(a, b); }
__m256i avx::cmpne(__m256i a, __m256i b, int16_t) { return cmpne_epi16(a, b); }
__m256i avx::cmpne(__m256i a, __m256i b, int32_t) { return cmpne_epi32(a, b); }
__m256i avx::cmpne(__m256i a, __m256i b, uint8_t) { return cmpne_epi8(a, b); }
__m256i avx::cmpne(__m256i a, __m256i b, uint16_t) { return cmpne_epi16(a, b); }
__m256i avx::cmpne(__m256i a, __m256i b, uint32_t) { return cmpne_epi32(a, b); }

template <typename T>
__m256i avx::cmp(__m256i a, __m256i b, std::equal_to<>, T) {
    return cmpeq(a, b, T());
}

template <typename T>
__m256i avx::cmp(__m256i a, __m256i b, std::not_equal_to<>, T) {
    return cmpne(a, b, T());
}

template <typename T>
__m256i avx::cmp(__m256i a, __m256i b, std::greater<>, T) {
    return cmpgt(a, b, T());
}

template <typename T>
__m256i avx::cmp(__m256i a, __m256i b, std::greater_equal<>, T) {
    return cmpge(a, b, T());
}

template <typename T>
__m256i avx::cmp(__m256i a, __m256i b, std::less<>, T) {
    return cmplt(a, b, T());
}

template <typename T>
__m256i avx::cmp(__m256i a, __m256i b, std::less_equal<>, T) {
    return cmple(a, b, T());
}

template <int imm8>
int avx::extract_epi8(__m256i v) {
    return _mm256_extract_epi8(v, imm8);
}

template <int imm8>
int avx::extract_epi16(__m256i v) {
    return _mm256_extract_epi16(v, imm8);
}

template <int imm8>
int avx::extract(__m256i v, int8_t) {
    return extract_epi8<imm8>(v);
}

template <int imm8>
int avx::extract(__m256i v, int16_t) {
    return extract_epi16<imm8>(v);
}

__m256i avx::hadd_epi16(__m256i a, __m256i b) { return _mm256_hadd_epi16(a, b); }
__m256i avx::hadd_epi32(__m256i a, __m256i b) { return _mm256_hadd_epi32(a, b); }

__m256i avx::hadd(__m256i a, __m256i b, int16_t) { return hadd_epi16(a, b); }
__m256i avx::hadd(__m256i a, __m256i b, int32_t) { return hadd_epi32(a, b); }

__m256i avx::hadds_epi16(__m256i a, __m256i b) { return _mm256_hadds_epi16(a, b); }

__m256i avx::hsub_epi16(__m256i a, __m256i b) { return _mm256_hsub_epi16(a, b); }
__m256i avx::hsub_epi32(__m256i a, __m256i b) { return _mm256_hsub_epi32(a, b); }

__m256i avx::hsub(__m256i a, __m256i b, int16_t) { return hsub_epi16(a, b); }
__m256i avx::hsub(__m256i a, __m256i b, int32_t) { return hsub_epi32(a, b); }

__m256i avx::hsubs_epi16(__m256i a, __m256i b) { return _mm256_hsubs_epi16(a, b); }

template <typename T, WJR_REQUIRES_I(is_any_of_v<T, int8_t, int16_t, int32_t, int64_t,
                                                 uint8_t, uint16_t, uint32_t, uint64_t>)>
__m256i avx::logical_and(__m256i a, __m256i b, T) {
    return Not(Or(logical_not(a, T()), logical_not(b, T())));
}

template <typename T, WJR_REQUIRES_I(is_any_of_v<T, int8_t, int16_t, int32_t, int64_t,
                                                 uint8_t, uint16_t, uint32_t, uint64_t>)>
__m256i avx::logical_not(__m256i v, T) {
    auto Zero = zeros();
    return cmpeq(v, Zero, T());
}

template <typename T, WJR_REQUIRES_I(is_any_of_v<T, int8_t, int16_t, int32_t, int64_t,
                                                 uint8_t, uint16_t, uint32_t, uint64_t>)>
__m256i avx::logical_or(__m256i a, __m256i b, T) {
    return Not(logical_not(Or(a, b), T()));
}

__m256i avx::madd_epi16(__m256i a, __m256i b) { return _mm256_madd_epi16(a, b); }

__m256i avx::max_epi8(__m256i a, __m256i b) { return _mm256_max_epi8(a, b); }
__m256i avx::max_epi16(__m256i a, __m256i b) { return _mm256_max_epi16(a, b); }
__m256i avx::max_epi32(__m256i a, __m256i b) { return _mm256_max_epi32(a, b); }

__m256i avx::max_epu8(__m256i a, __m256i b) { return _mm256_max_epu8(a, b); }
__m256i avx::max_epu16(__m256i a, __m256i b) { return _mm256_max_epu16(a, b); }
__m256i avx::max_epu32(__m256i a, __m256i b) { return _mm256_max_epu32(a, b); }

__m256i avx::max(__m256i a, __m256i b, int8_t) { return max_epi8(a, b); }
__m256i avx::max(__m256i a, __m256i b, int16_t) { return max_epi16(a, b); }
__m256i avx::max(__m256i a, __m256i b, uint8_t) { return max_epu8(a, b); }
__m256i avx::max(__m256i a, __m256i b, uint16_t) { return max_epu16(a, b); }
__m256i avx::max(__m256i a, __m256i b, int32_t) { return max_epi32(a, b); }
__m256i avx::max(__m256i a, __m256i b, uint32_t) { return max_epu32(a, b); }

int8_t avx::max_epi8(__m256i a) {
    return sse::max_epi8(sse::max_epi8(getlow(a), gethigh(a)));
}

int16_t avx::max_epi16(__m256i a) {
    return sse::max_epi16(sse::max_epi16(getlow(a), gethigh(a)));
}

int32_t avx::max_epi32(__m256i a) {
    return sse::max_epi32(sse::max_epi32(getlow(a), gethigh(a)));
}

uint8_t avx::max_epu8(__m256i a) {
    return sse::max_epu8(sse::max_epu8(getlow(a), gethigh(a)));
}

uint16_t avx::max_epu16(__m256i a) {
    return sse::max_epu16(sse::max_epu16(getlow(a), gethigh(a)));
}

uint32_t avx::max_epu32(__m256i a) {
    return sse::max_epu32(sse::max_epu32(getlow(a), gethigh(a)));
}

int8_t avx::max(__m256i a, int8_t) { return max_epi8(a); }
int16_t avx::max(__m256i a, int16_t) { return max_epi16(a); }
int32_t avx::max(__m256i a, int32_t) { return max_epi32(a); }
uint8_t avx::max(__m256i a, uint8_t) { return max_epu8(a); }
uint16_t avx::max(__m256i a, uint16_t) { return max_epu16(a); }
uint32_t avx::max(__m256i a, uint32_t) { return max_epu32(a); }

__m256i avx::min_epi8(__m256i a, __m256i b) { return _mm256_min_epi8(a, b); }
__m256i avx::min_epi16(__m256i a, __m256i b) { return _mm256_min_epi16(a, b); }
__m256i avx::min_epi32(__m256i a, __m256i b) { return _mm256_min_epi32(a, b); }

__m256i avx::min_epu8(__m256i a, __m256i b) { return _mm256_min_epu8(a, b); }
__m256i avx::min_epu16(__m256i a, __m256i b) { return _mm256_min_epu16(a, b); }
__m256i avx::min_epu32(__m256i a, __m256i b) { return _mm256_min_epu32(a, b); }

__m256i avx::min(__m256i a, __m256i b, int8_t) { return min_epi8(a, b); }
__m256i avx::min(__m256i a, __m256i b, int16_t) { return min_epi16(a, b); }
__m256i avx::min(__m256i a, __m256i b, uint8_t) { return min_epu8(a, b); }
__m256i avx::min(__m256i a, __m256i b, uint16_t) { return min_epu16(a, b); }
__m256i avx::min(__m256i a, __m256i b, int32_t) { return min_epi32(a, b); }
__m256i avx::min(__m256i a, __m256i b, uint32_t) { return min_epu32(a, b); }

int8_t avx::min_epi8(__m256i a) {
    return sse::min_epi8(sse::min_epi8(getlow(a), gethigh(a)));
}

int16_t avx::min_epi16(__m256i a) {
    return sse::min_epi16(sse::min_epi16(getlow(a), gethigh(a)));
}

int32_t avx::min_epi32(__m256i a) {
    return sse::min_epi32(sse::min_epi32(getlow(a), gethigh(a)));
}

uint8_t avx::min_epu8(__m256i a) {
    return sse::min_epu8(sse::min_epu8(getlow(a), gethigh(a)));
}

uint16_t avx::min_epu16(__m256i a) {
    return sse::min_epu16(sse::min_epu16(getlow(a), gethigh(a)));
}

uint32_t avx::min_epu32(__m256i a) {
    return sse::min_epu32(sse::min_epu32(getlow(a), gethigh(a)));
}

int8_t avx::min(__m256i a, int8_t) { return min_epi8(a); }
int16_t avx::min(__m256i a, int16_t) { return min_epi16(a); }
int32_t avx::min(__m256i a, int32_t) { return min_epi32(a); }
uint8_t avx::min(__m256i a, uint8_t) { return min_epu8(a); }
uint16_t avx::min(__m256i a, uint16_t) { return min_epu16(a); }
uint32_t avx::min(__m256i a, uint32_t) { return min_epu32(a); }

avx::mask_type avx::movemask_epi8(__m256i a) { return _mm256_movemask_epi8(a); }

__m256i avx::mul_epi32(__m256i a, __m256i b) { return _mm256_mul_epi32(a, b); }
__m256i avx::mul_epu32(__m256i a, __m256i b) { return _mm256_mul_epu32(a, b); }

__m256i avx::mulhi_epi16(__m256i a, __m256i b) { return _mm256_mulhi_epi16(a, b); }

__m256i avx::mulhi_epu16(__m256i a, __m256i b) { return _mm256_mulhi_epu16(a, b); }

__m256i avx::mullo_epi16(__m256i a, __m256i b) { return _mm256_mullo_epi16(a, b); }

__m256i avx::packs_epi16(__m256i a, __m256i b) { return _mm256_packs_epi16(a, b); }
__m256i avx::packs_epi32(__m256i a, __m256i b) { return _mm256_packs_epi32(a, b); }

__m256i avx::packus_epi16(__m256i a, __m256i b) { return _mm256_packus_epi16(a, b); }
__m256i avx::packus_epi32(__m256i a, __m256i b) { return _mm256_packus_epi32(a, b); }

template <int imm>
__m256i avx::shl(__m256i a) {
    if constexpr (imm >= 64 * 3) {
        a = slli<8 * 3>(a);
        a = slli_epi64(a, imm - 64 * 3);
        return a;
    } else if constexpr (imm >= 64 * 2) {
        a = slli<8 * 2>(a);
        constexpr auto I = imm - 64 * 2;
        auto b = slli_epi64(a, I);
        auto c = slli<8>(a);
        c = srli_epi64(c, 64 - I);
        return Or(b, c);
    } else if constexpr (imm >= 64) {
        a = slli<8>(a);
        constexpr auto I = imm - 64;
        auto b = slli_epi64(a, I);
        auto c = slli<8>(a);
        c = srli_epi64(c, 64 - I);
        return Or(b, c);
    } else {
        auto b = slli_epi64(a, imm);
        auto c = slli<8>(a);
        c = srli_epi64(c, 64 - imm);
        return Or(b, c);
    }
}

template <int imm>
__m256i avx::shr(__m256i a) {
    if constexpr (imm >= 64 * 3) {
        a = srli<8 * 3>(a);
        a = srli_epi64(a, imm - 64 * 3);
        return a;
    } else if constexpr (imm >= 64 * 2) {
        a = srli<8 * 2>(a);
        constexpr auto I = imm - 64 * 2;
        auto b = srli_epi64(a, I);
        auto c = srli<8>(a);
        c = slli_epi64(c, 64 - I);
        return Or(b, c);
    } else if constexpr (imm >= 64) {
        a = srli<8>(a);
        constexpr auto I = imm - 64;
        auto b = srli_epi64(a, I);
        auto c = srli<8>(a);
        c = slli_epi64(c, 64 - I);
        return Or(b, c);
    } else {
        auto b = srli_epi64(a, imm);
        auto c = srli<8>(a);
        c = slli_epi64(c, 64 - imm);
        return Or(b, c);
    }
}

__m256i avx::shuffle_epi8(__m256i a, __m256i b) { return _mm256_shuffle_epi8(a, b); }

template <int imm8>
__m256i avx::shuffle_epi32(__m256i a) {
    return _mm256_shuffle_epi32(a, imm8);
}

template <int imm8>
__m256i avx::shufflehi_epi16(__m256i a) {
    return _mm256_shufflehi_epi16(a, imm8);
}

template <int imm8>
__m256i avx::shufflelo_epi16(__m256i a) {
    return _mm256_shufflelo_epi16(a, imm8);
}

__m256i avx::sll_epi16(__m256i a, __m128i b) { return _mm256_sll_epi16(a, b); }
__m256i avx::sll_epi32(__m256i a, __m128i b) { return _mm256_sll_epi32(a, b); }
__m256i avx::sll_epi64(__m256i a, __m128i b) { return _mm256_sll_epi64(a, b); }

__m256i avx::sll(__m256i a, __m128i b, int16_t) { return sll_epi16(a, b); }
__m256i avx::sll(__m256i a, __m128i b, int32_t) { return sll_epi32(a, b); }
__m256i avx::sll(__m256i a, __m128i b, int64_t) { return sll_epi64(a, b); }
__m256i avx::sll(__m256i a, __m128i b, uint16_t) { return sll_epi16(a, b); }
__m256i avx::sll(__m256i a, __m128i b, uint32_t) { return sll_epi32(a, b); }
__m256i avx::sll(__m256i a, __m128i b, uint64_t) { return sll_epi64(a, b); }

template <int imm8>
__m256i avx::slli(__m256i a) {
    return _mm256_slli_si256(a, imm8);
}
__m256i avx::slli_epi16(__m256i a, int imm8) { return _mm256_slli_epi16(a, imm8); }
__m256i avx::slli_epi32(__m256i a, int imm8) { return _mm256_slli_epi32(a, imm8); }
__m256i avx::slli_epi64(__m256i a, int imm8) { return _mm256_slli_epi64(a, imm8); }

__m256i avx::slli(__m256i a, int imm8, int16_t) { return slli_epi16(a, imm8); }
__m256i avx::slli(__m256i a, int imm8, int32_t) { return slli_epi32(a, imm8); }
__m256i avx::slli(__m256i a, int imm8, int64_t) { return slli_epi64(a, imm8); }
__m256i avx::slli(__m256i a, int imm8, uint16_t) { return slli_epi16(a, imm8); }
__m256i avx::slli(__m256i a, int imm8, uint32_t) { return slli_epi32(a, imm8); }
__m256i avx::slli(__m256i a, int imm8, uint64_t) { return slli_epi64(a, imm8); }

__m256i avx::sra_epi16(__m256i a, __m128i b) { return _mm256_sra_epi16(a, b); }
__m256i avx::sra_epi32(__m256i a, __m128i b) { return _mm256_sra_epi32(a, b); }

__m256i avx::sra(__m256i a, __m128i b, int16_t) { return sra_epi16(a, b); }
__m256i avx::sra(__m256i a, __m128i b, int32_t) { return sra_epi32(a, b); }

__m256i avx::srai_epi16(__m256i a, int imm8) { return _mm256_srai_epi16(a, imm8); }
__m256i avx::srai_epi32(__m256i a, int imm8) { return _mm256_srai_epi32(a, imm8); }

__m256i avx::srai(__m256i a, int imm8, int16_t) { return srai_epi16(a, imm8); }
__m256i avx::srai(__m256i a, int imm8, int32_t) { return srai_epi32(a, imm8); }

__m256i avx::stream_load(__m256i const *p) { return _mm256_stream_load_si256(p); }

__m256i avx::srl_epi16(__m256i a, __m128i b) { return _mm256_srl_epi16(a, b); }
__m256i avx::srl_epi32(__m256i a, __m128i b) { return _mm256_srl_epi32(a, b); }
__m256i avx::srl_epi64(__m256i a, __m128i b) { return _mm256_srl_epi64(a, b); }

__m256i avx::srl(__m256i a, __m128i b, int16_t) { return srl_epi16(a, b); }
__m256i avx::srl(__m256i a, __m128i b, int32_t) { return srl_epi32(a, b); }
__m256i avx::srl(__m256i a, __m128i b, int64_t) { return srl_epi64(a, b); }
__m256i avx::srl(__m256i a, __m128i b, uint16_t) { return srl_epi16(a, b); }
__m256i avx::srl(__m256i a, __m128i b, uint32_t) { return srl_epi32(a, b); }
__m256i avx::srl(__m256i a, __m128i b, uint64_t) { return srl_epi64(a, b); }

template <int imm8>
__m256i avx::srli(__m256i a) {
    return _mm256_srli_si256(a, imm8);
}

__m256i avx::srli_epi8(__m256i a, int imm8) {
    return And(srli_epi16(a, imm8), avx_details::srli_epi8_mask[imm8]);
}
__m256i avx::srli_epi16(__m256i a, int imm8) { return _mm256_srli_epi16(a, imm8); }
__m256i avx::srli_epi32(__m256i a, int imm8) { return _mm256_srli_epi32(a, imm8); }
__m256i avx::srli_epi64(__m256i a, int imm8) { return _mm256_srli_epi64(a, imm8); }

__m256i avx::srli(__m256i a, int imm8, int8_t) { return srli_epi8(a, imm8); }
__m256i avx::srli(__m256i a, int imm8, int16_t) { return srli_epi16(a, imm8); }
__m256i avx::srli(__m256i a, int imm8, int32_t) { return srli_epi32(a, imm8); }
__m256i avx::srli(__m256i a, int imm8, int64_t) { return srli_epi64(a, imm8); }
__m256i avx::srli(__m256i a, int imm8, uint8_t) { return srli_epi8(a, imm8); }
__m256i avx::srli(__m256i a, int imm8, uint16_t) { return srli_epi16(a, imm8); }
__m256i avx::srli(__m256i a, int imm8, uint32_t) { return srli_epi32(a, imm8); }
__m256i avx::srli(__m256i a, int imm8, uint64_t) { return srli_epi64(a, imm8); }

__m256i avx::sub_epi8(__m256i a, __m256i b) { return _mm256_sub_epi8(a, b); }
__m256i avx::sub_epi16(__m256i a, __m256i b) { return _mm256_sub_epi16(a, b); }
__m256i avx::sub_epi32(__m256i a, __m256i b) { return _mm256_sub_epi32(a, b); }
__m256i avx::sub_epi64(__m256i a, __m256i b) { return _mm256_sub_epi64(a, b); }

__m256i avx::sub(__m256i a, __m256i b, int8_t) { return sub_epi8(a, b); }
__m256i avx::sub(__m256i a, __m256i b, int16_t) { return sub_epi16(a, b); }
__m256i avx::sub(__m256i a, __m256i b, int32_t) { return sub_epi32(a, b); }
__m256i avx::sub(__m256i a, __m256i b, int64_t) { return sub_epi64(a, b); }
__m256i avx::sub(__m256i a, __m256i b, uint8_t) { return sub_epi8(a, b); }
__m256i avx::sub(__m256i a, __m256i b, uint16_t) { return sub_epi16(a, b); }
__m256i avx::sub(__m256i a, __m256i b, uint32_t) { return sub_epi32(a, b); }
__m256i avx::sub(__m256i a, __m256i b, uint64_t) { return sub_epi64(a, b); }

__m256i avx::subs_epi8(__m256i a, __m256i b) { return _mm256_subs_epi8(a, b); }
__m256i avx::subs_epi16(__m256i a, __m256i b) { return _mm256_subs_epi16(a, b); }

__m256i avx::subs_epu8(__m256i a, __m256i b) { return _mm256_subs_epu8(a, b); }
__m256i avx::subs_epu16(__m256i a, __m256i b) { return _mm256_subs_epu16(a, b); }

__m256i avx::subs(__m256i a, __m256i b, int8_t) { return subs_epi8(a, b); }
__m256i avx::subs(__m256i a, __m256i b, int16_t) { return subs_epi16(a, b); }
__m256i avx::subs(__m256i a, __m256i b, uint8_t) { return subs_epu8(a, b); }
__m256i avx::subs(__m256i a, __m256i b, uint16_t) { return subs_epu16(a, b); }

int avx::test_all_ones(__m256i a) { return testc(a, cmpeq_epi32(a, a)); }

__m256i avx::unpackhi_epi8(__m256i a, __m256i b) { return _mm256_unpackhi_epi8(a, b); }
__m256i avx::unpackhi_epi16(__m256i a, __m256i b) { return _mm256_unpackhi_epi16(a, b); }
__m256i avx::unpackhi_epi32(__m256i a, __m256i b) { return _mm256_unpackhi_epi32(a, b); }
__m256i avx::unpackhi_epi64(__m256i a, __m256i b) { return _mm256_unpackhi_epi64(a, b); }

__m256i avx::unpackhi(__m256i a, __m256i b, int8_t) { return unpackhi_epi8(a, b); }
__m256i avx::unpackhi(__m256i a, __m256i b, int16_t) { return unpackhi_epi16(a, b); }
__m256i avx::unpackhi(__m256i a, __m256i b, int32_t) { return unpackhi_epi32(a, b); }
__m256i avx::unpackhi(__m256i a, __m256i b, int64_t) { return unpackhi_epi64(a, b); }
__m256i avx::unpackhi(__m256i a, __m256i b, uint8_t) { return unpackhi_epi8(a, b); }
__m256i avx::unpackhi(__m256i a, __m256i b, uint16_t) { return unpackhi_epi16(a, b); }
__m256i avx::unpackhi(__m256i a, __m256i b, uint32_t) { return unpackhi_epi32(a, b); }
__m256i avx::unpackhi(__m256i a, __m256i b, uint64_t) { return unpackhi_epi64(a, b); }

__m256i avx::unpacklo_epi8(__m256i a, __m256i b) { return _mm256_unpacklo_epi8(a, b); }
__m256i avx::unpacklo_epi16(__m256i a, __m256i b) { return _mm256_unpacklo_epi16(a, b); }
__m256i avx::unpacklo_epi32(__m256i a, __m256i b) { return _mm256_unpacklo_epi32(a, b); }
__m256i avx::unpacklo_epi64(__m256i a, __m256i b) { return _mm256_unpacklo_epi64(a, b); }

__m256i avx::unpacklo(__m256i a, __m256i b, int8_t) { return unpacklo_epi8(a, b); }
__m256i avx::unpacklo(__m256i a, __m256i b, int16_t) { return unpacklo_epi16(a, b); }
__m256i avx::unpacklo(__m256i a, __m256i b, int32_t) { return unpacklo_epi32(a, b); }
__m256i avx::unpacklo(__m256i a, __m256i b, int64_t) { return unpacklo_epi64(a, b); }
__m256i avx::unpacklo(__m256i a, __m256i b, uint8_t) { return unpacklo_epi8(a, b); }
__m256i avx::unpacklo(__m256i a, __m256i b, uint16_t) { return unpacklo_epi16(a, b); }
__m256i avx::unpacklo(__m256i a, __m256i b, uint32_t) { return unpacklo_epi32(a, b); }

#endif

} // namespace wjr

#endif

#endif // WJR_SIMD_SIMD_HPP__

#ifndef WJR_X86
#error "x86 required"
#endif

namespace wjr {

#if WJR_HAS_SIMD(SSE4_1) && WJR_HAS_SIMD(X86_SIMD)
#define WJR_HAS_BUILTIN_FIND_N WJR_HAS_DEF
#define WJR_HAS_BUILTIN_REVERSE_FIND_N WJR_HAS_DEF
#define WJR_HAS_BUILTIN_FIND_NOT_N WJR_HAS_DEF
#define WJR_HAS_BUILTIN_REVERSE_FIND_NOT_N WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(FIND_N)

template <typename T>
WJR_PURE WJR_COLD size_t large_builtin_find_n(const T *src0, const T *src1, size_t n) {
#define WJR_REGISTER_FIND_N_AVX(index)                                                   \
    do {                                                                                 \
        auto x = avx::loadu((__m256i *)(src0 + (index)));                                \
        auto y = avx::loadu((__m256i *)(src1 + (index)));                                \
        auto r = avx::cmpeq_epi64(x, y);                                                 \
                                                                                         \
        avx::mask_type mask = avx::movemask_epi8(r);                                     \
        if (WJR_LIKELY(mask != 0)) {                                                     \
            return (index) + ctz(mask) / 8;                                              \
        }                                                                                \
    } while (0)

    size_t rem = n & 7;

    if (rem > 4) {
#if !WJR_HAS_SIMD(AVX2)
        auto x0 = sse::loadu((__m128i *)(src0 + (rem - 4)));
        auto x1 = sse::loadu((__m128i *)(src0 + (rem - 2)));
        auto y1 = sse::loadu((__m128i *)(src1 + (rem - 2)));
        auto y0 = sse::loadu((__m128i *)(src1 + (rem - 4)));

        auto r0 = sse::cmpeq_epi64(x0, y0);
        auto r1 = sse::cmpeq_epi64(x1, y1);

        if (WJR_LIKELY(!sse::test_all_zeros(sse::Or(r0, r1)))) {
            sse::mask_type mask = sse::movemask_epi8(r0);
            if (mask != 0) {
                return (rem - 4) + (mask == 0xFF00);
            }

            mask = sse::movemask_epi8(r1);
            return (rem - 2) + (mask == 0xFF00);
        }
#else
        WJR_REGISTER_FIND_N_AVX(rem - 4);
#endif
    }

    if (WJR_UNLIKELY(rem == n)) {
        return n;
    }

#if !WJR_HAS_SIMD(AVX2)
    do {
        auto x0 = sse::loadu((__m128i *)(src0 + rem));
        auto x1 = sse::loadu((__m128i *)(src0 + rem + 2));
        auto x2 = sse::loadu((__m128i *)(src0 + rem + 4));
        auto x3 = sse::loadu((__m128i *)(src0 + rem + 6));
        auto y0 = sse::loadu((__m128i *)(src1 + rem));
        auto y1 = sse::loadu((__m128i *)(src1 + rem + 2));
        auto y2 = sse::loadu((__m128i *)(src1 + rem + 4));
        auto y3 = sse::loadu((__m128i *)(src1 + rem + 6));

        auto r0 = sse::cmpeq_epi64(x0, y0);
        auto r1 = sse::cmpeq_epi64(x1, y1);
        auto r2 = sse::cmpeq_epi64(x2, y2);
        auto r3 = sse::cmpeq_epi64(x3, y3);

        auto z = sse::Or(sse::Or(r0, r1), sse::Or(r2, r3));

        if (WJR_UNLIKELY(!sse::test_all_zeros(z))) {
            sse::mask_type mask = sse::movemask_epi8(r0);
            if (mask != 0) {
                return rem + (mask == 0xFF00);
            }

            mask = sse::movemask_epi8(r1);
            if (mask != 0) {
                return rem + 2 + (mask == 0xFF00);
            }

            mask = sse::movemask_epi8(r2);
            if (mask != 0) {
                return rem + 4 + (mask == 0xFF00);
            }

            mask = sse::movemask_epi8(r3);
            return rem + 6 + (mask == 0xFF00);
        }

        rem += 8;
    } while (WJR_LIKELY(rem != n));
#else
    if ((n - rem) & 8) {
        WJR_REGISTER_FIND_N_AVX(rem);
        WJR_REGISTER_FIND_N_AVX(rem + 4);

        rem += 8;

        if (WJR_UNLIKELY(rem == n)) {
            return n;
        }
    }

    do {
        auto x0 = avx::loadu((__m256i *)(src0 + rem));
        auto x2 = avx::loadu((__m256i *)(src0 + rem + 8));
        auto x3 = avx::loadu((__m256i *)(src0 + rem + 12));
        auto x1 = avx::loadu((__m256i *)(src0 + rem + 4));
        auto y0 = avx::loadu((__m256i *)(src1 + rem));
        auto y1 = avx::loadu((__m256i *)(src1 + rem + 4));
        auto y2 = avx::loadu((__m256i *)(src1 + rem + 8));
        auto y3 = avx::loadu((__m256i *)(src1 + rem + 12));

        auto r0 = avx::cmpeq_epi64(x0, y0);
        auto r1 = avx::cmpeq_epi64(x1, y1);
        auto r2 = avx::cmpeq_epi64(x2, y2);
        auto r3 = avx::cmpeq_epi64(x3, y3);

        auto z = avx::Or(avx::Or(r0, r1), avx::Or(r2, r3));

        if (WJR_UNLIKELY(!avx::test_all_zeros(z))) {
            avx::mask_type mask = avx::movemask_epi8(r0);
            if (mask != 0) {
                return rem + ctz(mask) / 8;
            }

            mask = avx::movemask_epi8(r1);
            if (mask != 0) {
                return rem + 4 + ctz(mask) / 8;
            }

            mask = avx::movemask_epi8(r2);
            if (mask != 0) {
                return rem + 8 + ctz(mask) / 8;
            }

            mask = avx::movemask_epi8(r3);
            return rem + 12 + ctz(mask) / 8;
        }

        rem += 16;
    } while (WJR_LIKELY(rem != n));
#endif

    return n;

#undef WJR_REGISTER_FIND_N_AVX
}

template <typename T>
WJR_INTRINSIC_INLINE size_t builtin_find_n(const T *src0, const T *src1, size_t n) {
    if (WJR_UNLIKELY(n == 0 || src0[0] == src1[0])) {
        return 0;
    }

    if (n == 1 || WJR_UNLIKELY(src0[1] == src1[1])) {
        return 1;
    }

    if (n == 2 || WJR_UNLIKELY(src0[2] == src1[2])) {
        return 2;
    }

    if (n == 3 || WJR_UNLIKELY(src0[3] == src1[3])) {
        return 3;
    }

    if (n == 4) {
        return 4;
    }

    size_t ret = large_builtin_find_n(src0, src1, n);
    WJR_ASSUME(ret >= 4);
    return ret;
}

template <typename T>
WJR_PURE WJR_COLD size_t large_builtin_find_n(const T *src, T val, size_t n) {
#define WJR_REGISTER_FIND_N_AVX(index)                                                   \
    do {                                                                                 \
        auto x = avx::loadu((__m256i *)(src + (index)));                                 \
        auto r = avx::cmpeq_epi64(x, y);                                                 \
                                                                                         \
        auto mask = avx::movemask_epi8(r);                                               \
        if (WJR_LIKELY(mask != 0)) {                                                     \
            return (index) + ctz(mask) / 8;                                              \
        }                                                                                \
    } while (0)

#if !WJR_HAS_SIMD(AVX2)
    auto y = sse::set1(val, T());
#else
    auto y = avx::set1(val, T());
#endif

    size_t rem = n & 7;

    if (rem > 4) {
#if !WJR_HAS_SIMD(AVX2)
        auto x0 = sse::loadu((__m128i *)(src + (rem - 4)));
        auto x1 = sse::loadu((__m128i *)(src + (rem - 2)));

        auto r0 = sse::cmpeq_epi64(x0, y);
        auto r1 = sse::cmpeq_epi64(x1, y);

        if (WJR_LIKELY(!sse::test_all_zeros(sse::Or(r0, r1)))) {
            sse::mask_type mask = sse::movemask_epi8(r0);
            if (mask != 0) {
                return rem - 4 + (mask == 0xFF00);
            }

            mask = sse::movemask_epi8(r1);
            return rem - 2 + (mask == 0xFF00);
        }
#else
        WJR_REGISTER_FIND_N_AVX(rem - 4);
#endif
    }

    if (WJR_UNLIKELY(rem == n)) {
        return n;
    }

#if !WJR_HAS_SIMD(AVX2)
    do {
        auto x0 = sse::loadu((__m128i *)(src + rem));
        auto x1 = sse::loadu((__m128i *)(src + rem + 2));
        auto x2 = sse::loadu((__m128i *)(src + rem + 4));
        auto x3 = sse::loadu((__m128i *)(src + rem + 6));

        auto r0 = sse::cmpeq_epi64(x0, y);
        auto r1 = sse::cmpeq_epi64(x1, y);
        auto r2 = sse::cmpeq_epi64(x2, y);
        auto r3 = sse::cmpeq_epi64(x3, y);

        auto z = sse::Or(sse::Or(r0, r1), sse::Or(r2, r3));

        if (WJR_UNLIKELY(!sse::test_all_zeros(z))) {
            sse::mask_type mask = sse::movemask_epi8(r0);
            if (mask != 0) {
                return rem + (mask == 0xFF00);
            }

            mask = sse::movemask_epi8(r1);
            if (mask != 0) {
                return rem + 2 + (mask == 0xFF00);
            }

            mask = sse::movemask_epi8(r2);
            if (mask != 0) {
                return rem + 4 + (mask == 0xFF00);
            }

            mask = sse::movemask_epi8(r3);
            return rem + 6 + (mask == 0xFF00);
        }

        rem += 8;
    } while (WJR_LIKELY(rem != n));
#else
    if ((n - rem) & 8) {
        WJR_REGISTER_FIND_N_AVX(rem);
        WJR_REGISTER_FIND_N_AVX(rem + 4);

        rem += 8;

        if (WJR_UNLIKELY(rem == n)) {
            return n;
        }
    }

    do {
        auto x0 = avx::loadu((__m256i *)(src + rem));
        auto x1 = avx::loadu((__m256i *)(src + rem + 4));
        auto x2 = avx::loadu((__m256i *)(src + rem + 8));
        auto x3 = avx::loadu((__m256i *)(src + rem + 12));

        auto r0 = avx::cmpeq_epi64(x0, y);
        auto r1 = avx::cmpeq_epi64(x1, y);
        auto r2 = avx::cmpeq_epi64(x2, y);
        auto r3 = avx::cmpeq_epi64(x3, y);

        auto z = avx::Or(avx::Or(r0, r1), avx::Or(r2, r3));

        if (WJR_UNLIKELY(!avx::test_all_zeros(z))) {
            avx::mask_type mask = avx::movemask_epi8(r0);
            if (mask != 0) {
                return rem + ctz(mask) / 8;
            }

            mask = avx::movemask_epi8(r1);
            if (mask != 0) {
                return rem + 4 + ctz(mask) / 8;
            }

            mask = avx::movemask_epi8(r2);
            if (mask != 0) {
                return rem + 8 + ctz(mask) / 8;
            }

            mask = avx::movemask_epi8(r3);
            return rem + 12 + ctz(mask) / 8;
        }

        rem += 16;
    } while (WJR_LIKELY(rem != n));
#endif

    return n;

#undef WJR_REGISTER_FIND_N_AVX
}

template <typename T>
WJR_INTRINSIC_INLINE size_t builtin_find_n(const T *src, T val, size_t n) {
    if (WJR_UNLIKELY(n == 0 || src[0] == val)) {
        return 0;
    }

    if (n == 1 || WJR_UNLIKELY(src[1] == val)) {
        return 1;
    }

    if (n == 2 || WJR_UNLIKELY(src[2] == val)) {
        return 2;
    }

    if (n == 3 || WJR_UNLIKELY(src[3] == val)) {
        return 3;
    }

    if (n == 4) {
        return 4;
    }

    size_t ret = large_builtin_find_n(src, val, n);
    WJR_ASSUME(ret >= 4);
    return ret;
}

#endif // WJR_HAS_BUILTIN(FIND_N)

#if WJR_HAS_BUILTIN(FIND_NOT_N)

template <typename T>
WJR_PURE WJR_COLD size_t large_builtin_find_not_n(const T *src0, const T *src1,
                                                  size_t n) {
#define WJR_REGISTER_FIND_NOT_N_AVX(index)                                               \
    do {                                                                                 \
        auto x = avx::loadu((__m256i *)(src0 + (index)));                                \
        auto y = avx::loadu((__m256i *)(src1 + (index)));                                \
        auto r = avx::cmpeq_epi64(x, y);                                                 \
                                                                                         \
        avx::mask_type mask = ~avx::movemask_epi8(r);                                    \
        if (WJR_LIKELY(mask != 0)) {                                                     \
            return (index) + ctz(mask) / 8;                                              \
        }                                                                                \
    } while (0)

    size_t rem = n & 7;

    if (rem > 4) {
#if !WJR_HAS_SIMD(AVX2)
        auto x0 = sse::loadu((__m128i *)(src0 + (rem - 4)));
        auto x1 = sse::loadu((__m128i *)(src0 + (rem - 2)));
        auto y0 = sse::loadu((__m128i *)(src1 + (rem - 4)));
        auto y1 = sse::loadu((__m128i *)(src1 + (rem - 2)));

        auto r0 = sse::cmpeq_epi64(x0, y0);
        auto r1 = sse::cmpeq_epi64(x1, y1);

        if (WJR_LIKELY(!sse::test_all_ones(sse::And(r0, r1)))) {
            sse::mask_type mask = ~sse::movemask_epi8(r0);
            if (mask != 0) {
                return rem - 4 + ctz(mask) / 8;
            }

            mask = ~sse::movemask_epi8(r1);
            return rem - 2 + ctz(mask) / 8;
        }
#else
        WJR_REGISTER_FIND_NOT_N_AVX(rem - 4);
#endif
    }

    if (WJR_UNLIKELY(rem == n)) {
        return n;
    }

#if !WJR_HAS_SIMD(AVX2)
    do {
        auto x0 = sse::loadu((__m128i *)(src0 + rem));
        auto x1 = sse::loadu((__m128i *)(src0 + rem + 2));
        auto x2 = sse::loadu((__m128i *)(src0 + rem + 4));
        auto x3 = sse::loadu((__m128i *)(src0 + rem + 6));
        auto y0 = sse::loadu((__m128i *)(src1 + rem));
        auto y1 = sse::loadu((__m128i *)(src1 + rem + 2));
        auto y2 = sse::loadu((__m128i *)(src1 + rem + 4));
        auto y3 = sse::loadu((__m128i *)(src1 + rem + 6));

        auto r0 = sse::cmpeq_epi64(x0, y0);
        auto r1 = sse::cmpeq_epi64(x1, y1);
        auto r2 = sse::cmpeq_epi64(x2, y2);
        auto r3 = sse::cmpeq_epi64(x3, y3);

        auto z = sse::And(sse::And(r0, r1), sse::And(r2, r3));

        if (WJR_UNLIKELY(!sse::test_all_ones(z))) {
            sse::mask_type mask = ~sse::movemask_epi8(r0);
            if (mask != 0) {
                return rem + ctz(mask) / 8;
            }

            mask = ~sse::movemask_epi8(r1);
            if (mask != 0) {
                return rem + 2 + ctz(mask) / 8;
            }

            mask = ~sse::movemask_epi8(r2);
            if (mask != 0) {
                return rem + 4 + ctz(mask) / 8;
            }

            mask = ~sse::movemask_epi8(r3);
            return rem + 6 + ctz(mask) / 8;
        }

        rem += 8;
    } while (WJR_LIKELY(rem != n));
#else
    if ((n - rem) & 8) {
        WJR_REGISTER_FIND_NOT_N_AVX(rem);
        WJR_REGISTER_FIND_NOT_N_AVX(rem + 4);

        rem += 8;

        if (WJR_UNLIKELY(rem == n)) {
            return n;
        }
    }

    do {
        auto x0 = avx::loadu((__m256i *)(src0 + rem));
        auto x1 = avx::loadu((__m256i *)(src0 + rem + 4));
        auto x2 = avx::loadu((__m256i *)(src0 + rem + 8));
        auto x3 = avx::loadu((__m256i *)(src0 + rem + 12));
        auto y0 = avx::loadu((__m256i *)(src1 + rem));
        auto y1 = avx::loadu((__m256i *)(src1 + rem + 4));
        auto y2 = avx::loadu((__m256i *)(src1 + rem + 8));
        auto y3 = avx::loadu((__m256i *)(src1 + rem + 12));

        auto r0 = avx::cmpeq_epi64(x0, y0);
        auto r1 = avx::cmpeq_epi64(x1, y1);
        auto r2 = avx::cmpeq_epi64(x2, y2);
        auto r3 = avx::cmpeq_epi64(x3, y3);

        auto z = avx::And(avx::And(r0, r1), avx::And(r2, r3));

        if (WJR_UNLIKELY(!avx::test_all_ones(z))) {
            avx::mask_type mask = ~avx::movemask_epi8(r0);
            if (mask != 0) {
                return rem + ctz(mask) / 8;
            }

            mask = ~avx::movemask_epi8(r1);
            if (mask != 0) {
                return rem + 4 + ctz(mask) / 8;
            }

            mask = ~avx::movemask_epi8(r2);
            if (mask != 0) {
                return rem + 8 + ctz(mask) / 8;
            }

            mask = ~avx::movemask_epi8(r3);
            return rem + 12 + ctz(mask) / 8;
        }

        rem += 16;
    } while (WJR_LIKELY(rem != n));
#endif

    return n;

#undef WJR_REGISTER_FIND_NOT_N_AVX
}

extern template WJR_PURE WJR_COLD size_t
large_builtin_find_not_n<uint64_t>(const uint64_t *src0, const uint64_t *src1, size_t n);

template <typename T>
WJR_INTRINSIC_INLINE size_t builtin_find_not_n(const T *src0, const T *src1, size_t n) {
    if (WJR_UNLIKELY(n == 0) || WJR_LIKELY(src0[0] != src1[0])) {
        return 0;
    }

    if (n == 1 || WJR_LIKELY(src0[1] != src1[1])) {
        return 1;
    }

    if (n == 2 || WJR_LIKELY(src0[2] != src1[2])) {
        return 2;
    }

    if (n == 3 || WJR_LIKELY(src0[3] != src1[3])) {
        return 3;
    }

    if (n == 4) {
        return 4;
    }

    size_t ret = large_builtin_find_not_n(src0, src1, n);
    WJR_ASSUME(ret >= 4);
    return ret;
}

template <typename T>
WJR_PURE WJR_COLD size_t large_builtin_find_not_n(const T *src, T val, size_t n) {
#define WJR_REGISTER_FIND_NOT_N_AVX(index)                                               \
    do {                                                                                 \
        auto x = avx::loadu((__m256i *)(src + (index)));                                 \
        auto r = avx::cmpeq_epi64(x, y);                                                 \
                                                                                         \
        auto mask = ~avx::movemask_epi8(r);                                              \
        if (WJR_LIKELY(mask != 0)) {                                                     \
            return (index) + ctz(mask) / 8;                                              \
        }                                                                                \
    } while (0)

#if !WJR_HAS_SIMD(AVX2)
    auto y = sse::set1(val, T());
#else
    auto y = avx::set1(val, T());
#endif

    size_t rem = n & 7;

    if (rem > 4) {
#if !WJR_HAS_SIMD(AVX2)
        auto x0 = sse::loadu((__m128i *)(src + (rem - 4)));
        auto x1 = sse::loadu((__m128i *)(src + (rem - 2)));

        auto r0 = sse::cmpeq_epi64(x0, y);
        auto r1 = sse::cmpeq_epi64(x1, y);

        if (WJR_LIKELY(!sse::test_all_ones(sse::And(r0, r1)))) {
            sse::mask_type mask = ~sse::movemask_epi8(r0);
            if (mask != 0) {
                return rem - 4 + ctz(mask) / 8;
            }

            mask = ~sse::movemask_epi8(r1);
            return rem - 2 + ctz(mask) / 8;
        }
#else
        WJR_REGISTER_FIND_NOT_N_AVX(rem - 4);
#endif
    }

    if (WJR_UNLIKELY(rem == n)) {
        return n;
    }

#if !WJR_HAS_SIMD(AVX2)
    do {
        auto x0 = sse::loadu((__m128i *)(src + rem));
        auto x1 = sse::loadu((__m128i *)(src + rem + 2));
        auto x2 = sse::loadu((__m128i *)(src + rem + 4));
        auto x3 = sse::loadu((__m128i *)(src + rem + 6));

        auto r0 = sse::cmpeq_epi64(x0, y);
        auto r1 = sse::cmpeq_epi64(x1, y);
        auto r2 = sse::cmpeq_epi64(x2, y);
        auto r3 = sse::cmpeq_epi64(x3, y);

        auto z = sse::And(sse::And(r0, r1), sse::And(r2, r3));

        if (WJR_UNLIKELY(!sse::test_all_ones(z))) {
            sse::mask_type mask = ~sse::movemask_epi8(r0);
            if (mask != 0) {
                return rem + ctz(mask) / 8;
            }

            mask = ~sse::movemask_epi8(r1);
            if (mask != 0) {
                return rem + 2 + ctz(mask) / 8;
            }

            mask = ~sse::movemask_epi8(r2);
            if (mask != 0) {
                return rem + 4 + ctz(mask) / 8;
            }

            mask = ~sse::movemask_epi8(r3);
            return rem + 6 + ctz(mask) / 8;
        }

        rem += 8;
    } while (WJR_LIKELY(rem != n));
#else
    if ((n - rem) & 8) {
        WJR_REGISTER_FIND_NOT_N_AVX(rem);
        WJR_REGISTER_FIND_NOT_N_AVX(rem + 4);

        rem += 8;

        if (WJR_UNLIKELY(rem == n)) {
            return n;
        }
    }

    do {
        auto x0 = avx::loadu((__m256i *)(src + rem));
        auto x1 = avx::loadu((__m256i *)(src + rem + 4));
        auto x2 = avx::loadu((__m256i *)(src + rem + 8));
        auto x3 = avx::loadu((__m256i *)(src + rem + 12));

        auto r0 = avx::cmpeq_epi64(x0, y);
        auto r1 = avx::cmpeq_epi64(x1, y);
        auto r2 = avx::cmpeq_epi64(x2, y);
        auto r3 = avx::cmpeq_epi64(x3, y);

        auto z = avx::And(avx::And(r0, r1), avx::And(r2, r3));

        if (WJR_UNLIKELY(!avx::test_all_ones(z))) {
            avx::mask_type mask = ~avx::movemask_epi8(r0);
            if (mask != 0) {
                return rem + ctz(mask) / 8;
            }

            mask = ~avx::movemask_epi8(r1);
            if (mask != 0) {
                return rem + 4 + ctz(mask) / 8;
            }

            mask = ~avx::movemask_epi8(r2);
            if (mask != 0) {
                return rem + 8 + ctz(mask) / 8;
            }

            mask = ~avx::movemask_epi8(r3);
            return rem + 12 + ctz(mask) / 8;
        }

        rem += 16;
    } while (WJR_LIKELY(rem != n));
#endif

    return n;

#undef WJR_REGISTER_FIND_NOT_N_AVX
}

extern template WJR_PURE WJR_COLD size_t
large_builtin_find_not_n<uint64_t>(const uint64_t *src, uint64_t val, size_t n);

template <typename T>
WJR_INTRINSIC_INLINE size_t builtin_find_not_n(const T *src, T val, size_t n) {
    if (WJR_UNLIKELY(n == 0) || WJR_LIKELY(src[0] != val)) {
        return 0;
    }

    if (n == 1 || WJR_LIKELY(src[1] != val)) {
        return 1;
    }

    if (n == 2 || WJR_LIKELY(src[2] != val)) {
        return 2;
    }

    if (n == 3 || WJR_LIKELY(src[3] != val)) {
        return 3;
    }

    if (n == 4) {
        return 4;
    }

    size_t ret = large_builtin_find_not_n(src, val, n);
    WJR_ASSUME(ret >= 4);
    return ret;
}

#endif // WJR_HAS_BUILTIN(FIND_NOT_N)

#if WJR_HAS_BUILTIN(REVERSE_FIND_N)

template <typename T>
WJR_PURE WJR_COLD size_t large_builtin_reverse_find_n(const T *src0, const T *src1,
                                                      size_t n) {
#define WJR_REGISTER_REVERSE_FIND_N_AVX(index)                                           \
    do {                                                                                 \
        auto x = avx::loadu((__m256i *)(src0 - 4 + (index)));                            \
        auto y = avx::loadu((__m256i *)(src1 - 4 + (index)));                            \
        auto r = avx::cmpeq_epi64(x, y);                                                 \
                                                                                         \
        avx::mask_type mask = avx::movemask_epi8(r);                                     \
        if (WJR_LIKELY(mask != 0)) {                                                     \
            return (index)-clz(mask) / 8;                                                \
        }                                                                                \
    } while (0)

    const size_t rem = n & 7;
    n -= rem;

    if (rem > 4) {
#if !WJR_HAS_SIMD(AVX2)
        auto x0 = sse::loadu((__m128i *)(src0 + n + 2));
        auto x1 = sse::loadu((__m128i *)(src0 + n));
        auto y0 = sse::loadu((__m128i *)(src1 + n + 2));
        auto y1 = sse::loadu((__m128i *)(src1 + n));

        auto r0 = sse::cmpeq_epi64(x0, y0);
        auto r1 = sse::cmpeq_epi64(x1, y1);

        if (WJR_LIKELY(!sse::test_all_zeros(sse::Or(r0, r1)))) {
            sse::mask_type mask = sse::movemask_epi8(r0);
            if (mask != 0) {
                return n + 4 - (mask == 0x00FF);
            }

            mask = sse::movemask_epi8(r1);
            return n + 2 - (mask == 0x00FF);
        }
#else
        WJR_REGISTER_REVERSE_FIND_N_AVX(n + 4);
#endif
    }

    if (WJR_UNLIKELY(n == 0)) {
        return 0;
    }

#if !WJR_HAS_SIMD(AVX2)
    do {
        auto x0 = sse::loadu((__m128i *)(src0 + n - 8));
        auto x1 = sse::loadu((__m128i *)(src0 + n - 6));
        auto x2 = sse::loadu((__m128i *)(src0 + n - 4));
        auto x3 = sse::loadu((__m128i *)(src0 + n - 2));
        auto y0 = sse::loadu((__m128i *)(src1 + n - 8));
        auto y1 = sse::loadu((__m128i *)(src1 + n - 6));
        auto y2 = sse::loadu((__m128i *)(src1 + n - 4));
        auto y3 = sse::loadu((__m128i *)(src1 + n - 2));

        auto r0 = sse::cmpeq_epi64(x0, y0);
        auto r1 = sse::cmpeq_epi64(x1, y1);
        auto r2 = sse::cmpeq_epi64(x2, y2);
        auto r3 = sse::cmpeq_epi64(x3, y3);

        auto z = sse::Or(sse::Or(r0, r1), sse::Or(r2, r3));

        if (WJR_UNLIKELY(!sse::test_all_zeros(z))) {
            sse::mask_type mask = sse::movemask_epi8(r3);
            if (mask != 0) {
                return n - (mask == 0x00FF);
            }

            mask = sse::movemask_epi8(r2);
            if (mask != 0) {
                return n - 2 - (mask == 0x00FF);
            }

            mask = sse::movemask_epi8(r1);
            if (mask != 0) {
                return n - 4 - (mask == 0x00FF);
            }

            mask = sse::movemask_epi8(r0);
            return n - 6 - (mask == 0x00FF);
        }

        n -= 8;
    } while (WJR_LIKELY(n != 0));
#else
    if ((n & 8) != 0) {
        WJR_REGISTER_REVERSE_FIND_N_AVX(n);
        WJR_REGISTER_REVERSE_FIND_N_AVX(n - 4);

        n -= 8;

        if (WJR_UNLIKELY(n == 0)) {
            return 0;
        }
    }

    do {
        auto x0 = avx::loadu((__m256i *)(src0 + n - 16));
        auto x1 = avx::loadu((__m256i *)(src0 + n - 12));
        auto x2 = avx::loadu((__m256i *)(src0 + n - 8));
        auto x3 = avx::loadu((__m256i *)(src0 + n - 4));
        auto y0 = avx::loadu((__m256i *)(src1 + n - 16));
        auto y1 = avx::loadu((__m256i *)(src1 + n - 12));
        auto y2 = avx::loadu((__m256i *)(src1 + n - 8));
        auto y3 = avx::loadu((__m256i *)(src1 + n - 4));

        auto r0 = avx::cmpeq_epi64(x0, y0);
        auto r1 = avx::cmpeq_epi64(x1, y1);
        auto r2 = avx::cmpeq_epi64(x2, y2);
        auto r3 = avx::cmpeq_epi64(x3, y3);

        auto z = avx::Or(avx::Or(r0, r1), avx::Or(r2, r3));

        if (WJR_UNLIKELY(!avx::test_all_zeros(z))) {
            avx::mask_type mask = avx::movemask_epi8(r3);
            if (mask != 0) {
                return n - clz(mask) / 8;
            }

            mask = avx::movemask_epi8(r2);
            if (mask != 0) {
                return n - 4 - clz(mask) / 8;
            }

            mask = avx::movemask_epi8(r1);
            if (mask != 0) {
                return n - 8 - clz(mask) / 8;
            }

            mask = avx::movemask_epi8(r0);
            return n - 12 - clz(mask) / 8;
        }

        n -= 16;
    } while (WJR_LIKELY(n != 0));
#endif

    return 0;

#undef WJR_REGISTER_REVERSE_FIND_N_AVX
}

template <typename T>
WJR_INTRINSIC_INLINE size_t builtin_reverse_find_n(const T *src0, const T *src1,
                                                   size_t n) {
    if (WJR_UNLIKELY(n == 0 || src0[n - 1] == src1[n - 1])) {
        return n;
    }

    if (n == 1 || WJR_UNLIKELY(src0[n - 2] == src1[n - 2])) {
        return n - 1;
    }

    if (n == 2 || WJR_UNLIKELY(src0[n - 3] == src1[n - 3])) {
        return n - 2;
    }

    if (n == 3 || WJR_UNLIKELY(src0[n - 4] == src1[n - 4])) {
        return n - 3;
    }

    if (n == 4) {
        return n - 4;
    }

    size_t ret = large_builtin_reverse_find_n(src0, src1, n);
    WJR_ASSUME(n > 4);
    WJR_ASSUME(ret <= n - 4);
    return ret;
}

template <typename T>
WJR_PURE WJR_COLD size_t large_builtin_reverse_find_n(const T *src, T val, size_t n) {
#define WJR_REGISTER_REVERSE_FIND_N_AVX(index)                                           \
    do {                                                                                 \
        auto x = avx::loadu((__m256i *)(src - 4 + (index)));                             \
        auto r = avx::cmpeq_epi64(x, y);                                                 \
                                                                                         \
        avx::mask_type mask = avx::movemask_epi8(r);                                     \
        if (WJR_LIKELY(mask != 0)) {                                                     \
            return (index)-clz(mask) / 8;                                                \
        }                                                                                \
    } while (0)

#if !WJR_HAS_SIMD(AVX2)
    auto y = sse::set1(val, T());
#else
    auto y = avx::set1(val, T());
#endif

    const size_t rem = n & 7;
    n -= rem;

    if (rem > 4) {
#if !WJR_HAS_SIMD(AVX2)
        auto x0 = sse::loadu((__m128i *)(src + n + 2));
        auto x1 = sse::loadu((__m128i *)(src + n));

        auto r0 = sse::cmpeq_epi64(x0, y);
        auto r1 = sse::cmpeq_epi64(x1, y);

        if (WJR_LIKELY(!sse::test_all_zeros(sse::Or(r0, r1)))) {
            sse::mask_type mask = sse::movemask_epi8(r0);
            if (mask != 0) {
                return n + 4 - (mask == 0x00FF);
            }

            mask = sse::movemask_epi8(r1);
            return n + 2 - (mask == 0x00FF);
        }
#else
        WJR_REGISTER_REVERSE_FIND_N_AVX(n + 4);
#endif
    }

    if (WJR_UNLIKELY(n == 0)) {
        return 0;
    }

#if !WJR_HAS_SIMD(AVX2)
    do {
        auto x0 = sse::loadu((__m128i *)(src + n - 8));
        auto x1 = sse::loadu((__m128i *)(src + n - 6));
        auto x2 = sse::loadu((__m128i *)(src + n - 4));
        auto x3 = sse::loadu((__m128i *)(src + n - 2));

        auto r0 = sse::cmpeq_epi64(x0, y);
        auto r1 = sse::cmpeq_epi64(x1, y);
        auto r2 = sse::cmpeq_epi64(x2, y);
        auto r3 = sse::cmpeq_epi64(x3, y);

        auto z = sse::Or(sse::Or(r0, r1), sse::Or(r2, r3));

        if (WJR_UNLIKELY(!sse::test_all_zeros(z))) {
            sse::mask_type mask = sse::movemask_epi8(r3);
            if (mask != 0) {
                return n - (mask == 0x00FF);
            }

            mask = sse::movemask_epi8(r2);
            if (mask != 0) {
                return n - 2 - (mask == 0x00FF);
            }

            mask = sse::movemask_epi8(r1);
            if (mask != 0) {
                return n - 4 - (mask == 0x00FF);
            }

            mask = sse::movemask_epi8(r0);
            return n - 6 - (mask == 0x00FF);
        }

        n -= 8;
    } while (WJR_LIKELY(n != 0));
#else
    if ((n & 8) != 0) {
        WJR_REGISTER_REVERSE_FIND_N_AVX(n);
        WJR_REGISTER_REVERSE_FIND_N_AVX(n - 4);

        n -= 8;

        if (WJR_UNLIKELY(n == 0)) {
            return 0;
        }
    }

    do {
        auto x0 = avx::loadu((__m256i *)(src + n - 16));
        auto x1 = avx::loadu((__m256i *)(src + n - 12));
        auto x2 = avx::loadu((__m256i *)(src + n - 8));
        auto x3 = avx::loadu((__m256i *)(src + n - 4));

        auto r0 = avx::cmpeq_epi64(x0, y);
        auto r1 = avx::cmpeq_epi64(x1, y);
        auto r2 = avx::cmpeq_epi64(x2, y);
        auto r3 = avx::cmpeq_epi64(x3, y);

        auto z = avx::Or(avx::Or(r0, r1), avx::Or(r2, r3));

        if (WJR_UNLIKELY(!avx::test_all_zeros(z))) {
            avx::mask_type mask = avx::movemask_epi8(r3);
            if (mask != 0) {
                return n - clz(mask) / 8;
            }

            mask = avx::movemask_epi8(r2);
            if (mask != 0) {
                return n - 4 - clz(mask) / 8;
            }

            mask = avx::movemask_epi8(r1);
            if (mask != 0) {
                return n - 8 - clz(mask) / 8;
            }

            mask = avx::movemask_epi8(r0);
            return n - 12 - clz(mask) / 8;
        }

        n -= 16;
    } while (WJR_LIKELY(n != 0));
#endif

    return 0;

#undef WJR_REGISTER_REVERSE_FIND_N_AVX
}

template <typename T>
WJR_INTRINSIC_INLINE size_t builtin_reverse_find_n(const T *src, T val, size_t n) {
    if (WJR_UNLIKELY(n == 0 || src[n - 1] == val)) {
        return n;
    }

    if (n == 1 || WJR_UNLIKELY(src[n - 2] == val)) {
        return n - 1;
    }

    if (n == 2 || WJR_UNLIKELY(src[n - 3] == val)) {
        return n - 2;
    }

    if (n == 3 || WJR_UNLIKELY(src[n - 4] == val)) {
        return n - 3;
    }

    if (n == 4) {
        return n - 4;
    }

    size_t ret = large_builtin_reverse_find_n(src, val, n);
    WJR_ASSUME(n > 4);
    WJR_ASSUME(ret <= n - 4);
    return ret;
}

#endif // WJR_HAS_BUILTIN(REVERSE_FIND_N)

#if WJR_HAS_BUILTIN(REVERSE_FIND_NOT_N)

template <typename T>
WJR_PURE WJR_COLD size_t large_builtin_reverse_find_not_n(const T *src0, const T *src1,
                                                          size_t n) {
#define WJR_REGISTER_REVERSE_FIND_N_AVX(index)                                           \
    do {                                                                                 \
        auto x = avx::loadu((__m256i *)(src0 - 4 + (index)));                            \
        auto y = avx::loadu((__m256i *)(src1 - 4 + (index)));                            \
        auto r = avx::cmpeq_epi64(x, y);                                                 \
                                                                                         \
        avx::mask_type mask = ~avx::movemask_epi8(r);                                    \
        if (WJR_LIKELY(mask != 0)) {                                                     \
            return (index)-clz(mask) / 8;                                                \
        }                                                                                \
    } while (0)

    const size_t rem = n & 7;
    n -= rem;

    if (rem > 4) {
#if !WJR_HAS_SIMD(AVX2)
        auto x0 = sse::loadu((__m128i *)(src0 + n + 2));
        auto x1 = sse::loadu((__m128i *)(src0 + n));
        auto y0 = sse::loadu((__m128i *)(src1 + n + 2));
        auto y1 = sse::loadu((__m128i *)(src1 + n));

        auto r0 = sse::cmpeq_epi64(x0, y0);
        auto r1 = sse::cmpeq_epi64(x1, y1);

        if (WJR_LIKELY(!sse::test_all_ones(sse::And(r0, r1)))) {
            sse::mask_type mask = ~sse::movemask_epi8(r0);
            if (mask != 0) {
                return n + 4 - (mask == 0x00FF);
            }

            mask = ~sse::movemask_epi8(r1);
            return n + 2 - (mask == 0x00FF);
        }
#else
        WJR_REGISTER_REVERSE_FIND_N_AVX(n + 4);
#endif
    }

    if (WJR_UNLIKELY(n == 0)) {
        return 0;
    }

#if !WJR_HAS_SIMD(AVX2)
    do {
        auto x0 = sse::loadu((__m128i *)(src0 + n - 8));
        auto x1 = sse::loadu((__m128i *)(src0 + n - 6));
        auto x2 = sse::loadu((__m128i *)(src0 + n - 4));
        auto x3 = sse::loadu((__m128i *)(src0 + n - 2));
        auto y0 = sse::loadu((__m128i *)(src1 + n - 8));
        auto y1 = sse::loadu((__m128i *)(src1 + n - 6));
        auto y2 = sse::loadu((__m128i *)(src1 + n - 4));
        auto y3 = sse::loadu((__m128i *)(src1 + n - 2));

        auto r0 = sse::cmpeq_epi64(x0, y0);
        auto r1 = sse::cmpeq_epi64(x1, y1);
        auto r2 = sse::cmpeq_epi64(x2, y2);
        auto r3 = sse::cmpeq_epi64(x3, y3);

        auto z = sse::And(sse::And(r0, r1), sse::And(r2, r3));

        if (WJR_UNLIKELY(!sse::test_all_ones(z))) {
            sse::mask_type mask = ~sse::movemask_epi8(r3);
            if (mask != 0) {
                return n - (mask == 0x00FF);
            }

            mask = ~sse::movemask_epi8(r2);
            if (mask != 0) {
                return n - 2 - (mask == 0x00FF);
            }

            mask = ~sse::movemask_epi8(r1);
            if (mask != 0) {
                return n - 4 - (mask == 0x00FF);
            }

            mask = ~sse::movemask_epi8(r0);
            return n - 6 - (mask == 0x00FF);
        }

        n -= 8;
    } while (WJR_LIKELY(n != 0));
#else
    if ((n & 8) != 0) {
        WJR_REGISTER_REVERSE_FIND_N_AVX(n);
        WJR_REGISTER_REVERSE_FIND_N_AVX(n - 4);

        n -= 8;

        if (WJR_UNLIKELY(n == 0)) {
            return 0;
        }
    }

    do {
        auto x0 = avx::loadu((__m256i *)(src0 + n - 16));
        auto x1 = avx::loadu((__m256i *)(src0 + n - 12));
        auto x2 = avx::loadu((__m256i *)(src0 + n - 8));
        auto x3 = avx::loadu((__m256i *)(src0 + n - 4));
        auto y0 = avx::loadu((__m256i *)(src1 + n - 16));
        auto y1 = avx::loadu((__m256i *)(src1 + n - 12));
        auto y2 = avx::loadu((__m256i *)(src1 + n - 8));
        auto y3 = avx::loadu((__m256i *)(src1 + n - 4));

        auto r0 = avx::cmpeq_epi64(x0, y0);
        auto r1 = avx::cmpeq_epi64(x1, y1);
        auto r2 = avx::cmpeq_epi64(x2, y2);
        auto r3 = avx::cmpeq_epi64(x3, y3);

        auto z = avx::And(avx::And(r0, r1), avx::And(r2, r3));

        if (WJR_UNLIKELY(!avx::test_all_ones(z))) {
            avx::mask_type mask = ~avx::movemask_epi8(r3);
            if (mask != 0) {
                return n - clz(mask) / 8;
            }

            mask = ~avx::movemask_epi8(r2);
            if (mask != 0) {
                return n - 4 - clz(mask) / 8;
            }

            mask = ~avx::movemask_epi8(r1);
            if (mask != 0) {
                return n - 8 - clz(mask) / 8;
            }

            mask = ~avx::movemask_epi8(r0);
            return n - 12 - clz(mask) / 8;
        }

        n -= 16;
    } while (WJR_LIKELY(n != 0));
#endif

    return 0;

#undef WJR_REGISTER_REVERSE_FIND_N_AVX
}

extern template WJR_PURE WJR_COLD size_t large_builtin_reverse_find_not_n<uint64_t>(
    const uint64_t *src0, const uint64_t *src1, size_t n);

template <typename T>
WJR_INTRINSIC_INLINE size_t builtin_reverse_find_not_n(const T *src0, const T *src1,
                                                       size_t n) {
    if (WJR_UNLIKELY(n == 0) || WJR_LIKELY(src0[n - 1] != src1[n - 1])) {
        return n;
    }

    if (n == 1 || WJR_LIKELY(src0[n - 2] != src1[n - 2])) {
        return n - 1;
    }

    if (n == 2 || WJR_LIKELY(src0[n - 3] != src1[n - 3])) {
        return n - 2;
    }

    if (n == 3 || WJR_LIKELY(src0[n - 4] != src1[n - 4])) {
        return n - 3;
    }

    if (n == 4) {
        return n - 4;
    }

    size_t ret = large_builtin_reverse_find_not_n(src0, src1, n);
    WJR_ASSUME(n > 4);
    WJR_ASSUME(ret <= n - 4);
    return ret;
}

template <typename T>
WJR_PURE WJR_COLD size_t large_builtin_reverse_find_not_n(const T *src, T val, size_t n) {
#define WJR_REGISTER_REVERSE_FIND_N_AVX(index)                                           \
    do {                                                                                 \
        auto x = avx::loadu((__m256i *)(src - 4 + (index)));                             \
        auto r = avx::cmpeq_epi64(x, y);                                                 \
                                                                                         \
        avx::mask_type mask = ~avx::movemask_epi8(r);                                    \
        if (WJR_LIKELY(mask != 0)) {                                                     \
            return (index)-clz(mask) / 8;                                                \
        }                                                                                \
    } while (0)

#if !WJR_HAS_SIMD(AVX2)
    auto y = sse::set1(val, T());
#else
    auto y = avx::set1(val, T());
#endif

    const size_t rem = n & 7;
    n -= rem;

    if (rem > 4) {
#if !WJR_HAS_SIMD(AVX2)
        auto x0 = sse::loadu((__m128i *)(src + n + 2));
        auto x1 = sse::loadu((__m128i *)(src + n));

        auto r0 = sse::cmpeq_epi64(x0, y);
        auto r1 = sse::cmpeq_epi64(x1, y);

        if (WJR_LIKELY(!sse::test_all_ones(sse::And(r0, r1)))) {
            sse::mask_type mask = ~sse::movemask_epi8(r0);
            if (mask != 0) {
                return n + 4 - (mask == 0x00FF);
            }

            mask = ~sse::movemask_epi8(r1);
            return n + 2 - (mask == 0x00FF);
        }
#else
        WJR_REGISTER_REVERSE_FIND_N_AVX(n + 4);
#endif
    }

    if (WJR_UNLIKELY(n == 0)) {
        return 0;
    }

#if !WJR_HAS_SIMD(AVX2)
    do {
        auto x0 = sse::loadu((__m128i *)(src + n - 8));
        auto x1 = sse::loadu((__m128i *)(src + n - 6));
        auto x2 = sse::loadu((__m128i *)(src + n - 4));
        auto x3 = sse::loadu((__m128i *)(src + n - 2));

        auto r0 = sse::cmpeq_epi64(x0, y);
        auto r1 = sse::cmpeq_epi64(x1, y);
        auto r2 = sse::cmpeq_epi64(x2, y);
        auto r3 = sse::cmpeq_epi64(x3, y);

        auto z = sse::And(sse::And(r0, r1), sse::And(r2, r3));

        if (WJR_UNLIKELY(!sse::test_all_ones(z))) {
            sse::mask_type mask = ~sse::movemask_epi8(r3);
            if (mask != 0) {
                return n - (mask == 0x00FF);
            }

            mask = ~sse::movemask_epi8(r2);
            if (mask != 0) {
                return n - 2 - (mask == 0x00FF);
            }

            mask = ~sse::movemask_epi8(r1);
            if (mask != 0) {
                return n - 4 - (mask == 0x00FF);
            }

            mask = ~sse::movemask_epi8(r0);
            return n - 6 - (mask == 0x00FF);
        }

        n -= 8;
    } while (WJR_LIKELY(n != 0));
#else
    if ((n & 8) != 0) {
        WJR_REGISTER_REVERSE_FIND_N_AVX(n);
        WJR_REGISTER_REVERSE_FIND_N_AVX(n - 4);

        n -= 8;

        if (WJR_UNLIKELY(n == 0)) {
            return 0;
        }
    }

    do {
        auto x0 = avx::loadu((__m256i *)(src + n - 16));
        auto x1 = avx::loadu((__m256i *)(src + n - 12));
        auto x2 = avx::loadu((__m256i *)(src + n - 8));
        auto x3 = avx::loadu((__m256i *)(src + n - 4));

        auto r0 = avx::cmpeq_epi64(x0, y);
        auto r1 = avx::cmpeq_epi64(x1, y);
        auto r2 = avx::cmpeq_epi64(x2, y);
        auto r3 = avx::cmpeq_epi64(x3, y);

        auto z = avx::And(avx::And(r0, r1), avx::And(r2, r3));

        if (WJR_UNLIKELY(!avx::test_all_ones(z))) {
            avx::mask_type mask = ~avx::movemask_epi8(r3);
            if (mask != 0) {
                return n - clz(mask) / 8;
            }

            mask = ~avx::movemask_epi8(r2);
            if (mask != 0) {
                return n - 4 - clz(mask) / 8;
            }

            mask = ~avx::movemask_epi8(r1);
            if (mask != 0) {
                return n - 8 - clz(mask) / 8;
            }

            mask = ~avx::movemask_epi8(r0);
            return n - 12 - clz(mask) / 8;
        }

        n -= 16;
    } while (WJR_LIKELY(n != 0));
#endif

    return 0;

#undef WJR_REGISTER_REVERSE_FIND_N_AVX
}

extern template WJR_PURE WJR_COLD size_t
large_builtin_reverse_find_not_n<uint64_t>(const uint64_t *src, uint64_t val, size_t n);

template <typename T>
WJR_INTRINSIC_INLINE size_t builtin_reverse_find_not_n(const T *src, T val, size_t n) {
    if (WJR_UNLIKELY(n == 0) || WJR_LIKELY(src[n - 1] != val)) {
        return n;
    }

    if (n == 1 || WJR_LIKELY(src[n - 2] != val)) {
        return n - 1;
    }

    if (n == 2 || WJR_LIKELY(src[n - 3] != val)) {
        return n - 2;
    }

    if (n == 3 || WJR_LIKELY(src[n - 4] != val)) {
        return n - 3;
    }

    if (n == 4) {
        return n - 4;
    }

    size_t ret = large_builtin_reverse_find_not_n(src, val, n);
    WJR_ASSUME(n > 4);
    WJR_ASSUME(ret <= n - 4);
    return ret;
}

#endif // WJR_HAS_BUILTIN(REVERSE_FIND_NOT_N)

} // namespace wjr

#endif // WJR_X86_MATH_FIND_HPP__
#endif

namespace wjr {

template <typename T>
WJR_INTRINSIC_CONSTEXPR size_t fallback_find_n(const T *src0, const T *src1, size_t n) {
    size_t idx = 0;

    WJR_UNROLL(4)
    for (; idx < n; ++idx) {
        if (src0[idx] == src1[idx]) {
            break;
        }
    }

    return idx;
}

template <typename T>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t find_n_impl(const T *src0, const T *src1,
                                                      size_t n) {
    if (WJR_BUILTIN_CONSTANT_P(src0 == src1) && src0 == src1) {
        return 0;
    }

#if WJR_HAS_BUILTIN(FIND_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_find_n(src0, src1, n);
        }

        return builtin_find_n(src0, src1, n);
    } else {
        return fallback_find_n(src0, src1, n);
    }
#else
    return fallback_find_n(src0, src1, n);
#endif
}

template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t find_n(const T *src0, const T *src1, size_t n) {
    using uT = std::make_unsigned_t<T>;
    return find_n_impl<uT>(reinterpret_cast<const uT *>(src0),
                           reinterpret_cast<const uT *>(src1), n);
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR size_t fallback_find_n(const T *src, T val, size_t n) {
    size_t idx = 0;

    WJR_UNROLL(4)
    for (; idx < n; ++idx) {
        if (src[idx] == val) {
            break;
        }
    }

    return idx;
}

template <typename T>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t find_n_impl(const T *src,
                                                      type_identity_t<T> val, size_t n) {
#if WJR_HAS_BUILTIN(FIND_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_find_n(src, val, n);
        }

        return builtin_find_n(src, val, n);
    } else {
        return fallback_find_n(src, val, n);
    }
#else
    return fallback_find_n(src, val, n);
#endif
}

template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t find_n(const T *src, type_identity_t<T> val,
                                                 size_t n) {
    using uT = std::make_unsigned_t<T>;
    return find_n_impl<uT>(reinterpret_cast<const uT *>(src), val, n);
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR size_t fallback_find_not_n(const T *src0, const T *src1,
                                                   size_t n) {
    size_t idx = 0;

    WJR_UNROLL(4)
    for (; idx < n; ++idx) {
        if (src0[idx] != src1[idx]) {
            break;
        }
    }

    return idx;
}

template <typename T>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t find_not_n_impl(const T *src0, const T *src1,
                                                          size_t n) {
    if (WJR_BUILTIN_CONSTANT_P(src0 == src1) && src0 == src1) {
        return n;
    }

#if WJR_HAS_BUILTIN(FIND_NOT_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_find_not_n(src0, src1, n);
        }

        return builtin_find_not_n(src0, src1, n);
    } else {
        return fallback_find_not_n(src0, src1, n);
    }
#else
    return fallback_find_not_n(src0, src1, n);
#endif
}

template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t find_not_n(const T *src0, const T *src1,
                                                     size_t n) {
    using uT = std::make_unsigned_t<T>;
    return find_not_n_impl<uT>(reinterpret_cast<const uT *>(src0),
                               reinterpret_cast<const uT *>(src1), n);
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR size_t fallback_find_not_n(const T *src, T val, size_t n) {
    size_t idx = 0;

    WJR_UNROLL(4)
    for (; idx < n; ++idx) {
        if (src[idx] != val) {
            break;
        }
    }

    return idx;
}

template <typename T>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t find_not_n_impl(const T *src,
                                                          type_identity_t<T> val,
                                                          size_t n) {
#if WJR_HAS_BUILTIN(FIND_NOT_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_find_not_n(src, val, n);
        }

        return builtin_find_not_n(src, val, n);
    } else {
        return fallback_find_not_n(src, val, n);
    }
#else
    return fallback_find_not_n(src, val, n);
#endif
}

template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t find_not_n(const T *src, type_identity_t<T> val,
                                                     size_t n) {
    using uT = std::make_unsigned_t<T>;
    return find_not_n_impl<uT>(reinterpret_cast<const uT *>(src), val, n);
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR size_t fallback_reverse_find_n(const T *src0, const T *src1,
                                                       size_t n) {
    size_t idx = 0;
    src0 += n;
    src1 += n;

    WJR_UNROLL(4)
    for (; idx < n; ++idx) {
        if (src0[-1 - idx] == src1[-1 - idx]) {
            break;
        }
    }

    return n - idx;
}

template <typename T>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t reverse_find_n_impl(const T *src0,
                                                              const T *src1, size_t n) {
    if (WJR_BUILTIN_CONSTANT_P(src0 == src1) && src0 == src1) {
        return n;
    }

#if WJR_HAS_BUILTIN(REVERSE_FIND_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_reverse_find_n(src0, src1, n);
        }

        return builtin_reverse_find_n(src0, src1, n);
    } else {
        return fallback_reverse_find_n(src0, src1, n);
    }
#else
    return fallback_reverse_find_n(src0, src1, n);
#endif
}

template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t reverse_find_n(const T *src0, const T *src1,
                                                         size_t n) {
    using uT = std::make_unsigned_t<T>;
    return reverse_find_n_impl<uT>(reinterpret_cast<const uT *>(src0),
                                   reinterpret_cast<const uT *>(src1), n);
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR size_t fallback_reverse_find_n(const T *src, T val, size_t n) {
    size_t idx = 0;
    src += n;

    WJR_UNROLL(4)
    for (; idx < n; ++idx) {
        if (src[-1 - idx] == val) {
            break;
        }
    }

    return n - idx;
}

template <typename T>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t reverse_find_n_impl(const T *src,
                                                              type_identity_t<T> val,
                                                              size_t n) {
#if WJR_HAS_BUILTIN(REVERSE_FIND_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_reverse_find_n(src, val, n);
        }

        return builtin_reverse_find_n(src, val, n);
    } else {
        return fallback_reverse_find_n(src, val, n);
    }
#else
    return fallback_reverse_find_n(src, val, n);
#endif
}

template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t reverse_find_n(const T *src,
                                                         type_identity_t<T> val,
                                                         size_t n) {
    using uT = std::make_unsigned_t<T>;
    return reverse_find_n_impl<uT>(reinterpret_cast<const uT *>(src), val, n);
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR size_t fallback_reverse_find_not_n(const T *src0, const T *src1,
                                                           size_t n) {
    size_t idx = 0;
    src0 += n;
    src1 += n;

    WJR_UNROLL(4)
    for (; idx < n; ++idx) {
        if (src0[-1 - idx] != src1[-1 - idx]) {
            break;
        }
    }

    return n - idx;
}

template <typename T>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t reverse_find_not_n_impl(const T *src0,
                                                                  const T *src1,
                                                                  size_t n) {
    if (WJR_BUILTIN_CONSTANT_P(src0 == src1) && src0 == src1) {
        return 0;
    }

#if WJR_HAS_BUILTIN(REVERSE_FIND_NOT_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_reverse_find_not_n(src0, src1, n);
        }

        return builtin_reverse_find_not_n(src0, src1, n);
    } else {
        return fallback_reverse_find_not_n(src0, src1, n);
    }
#else
    return fallback_reverse_find_not_n(src0, src1, n);
#endif
}

template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t reverse_find_not_n(const T *src0, const T *src1,
                                                             size_t n) {
    using uT = std::make_unsigned_t<T>;
    return reverse_find_not_n_impl<uT>(reinterpret_cast<const uT *>(src0),
                                       reinterpret_cast<const uT *>(src1), n);
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR size_t fallback_reverse_find_not_n(const T *src, T val,
                                                           size_t n) {
    size_t idx = 0;
    src += n;

    WJR_UNROLL(4)
    for (; idx < n; ++idx) {
        if (src[-1 - idx] != val) {
            break;
        }
    }

    return n - idx;
}

template <typename T>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t reverse_find_not_n_impl(const T *src,
                                                                  type_identity_t<T> val,
                                                                  size_t n) {
#if WJR_HAS_BUILTIN(REVERSE_FIND_NOT_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_reverse_find_not_n(src, val, n);
        }

        return builtin_reverse_find_not_n(src, val, n);
    } else {
        return fallback_reverse_find_not_n(src, val, n);
    }
#else
    return fallback_reverse_find_not_n(src, val, n);
#endif
}

template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E size_t reverse_find_not_n(const T *src,
                                                             type_identity_t<T> val,
                                                             size_t n) {
    using uT = std::make_unsigned_t<T>;
    return reverse_find_not_n_impl<uT>(reinterpret_cast<const uT *>(src), val, n);
}

} // namespace wjr

#endif // WJR_MATH_FIND_HPP__
#ifndef WJR_MATH_SET_HPP__
#define WJR_MATH_SET_HPP__

#include <cstring>

// Already included
// Already included

#if defined(WJR_X86)
#ifndef WJR_X86_MATH_SET_HPP__
#define WJR_X86_MATH_SET_HPP__

#include <cstring>

// Already included

#ifndef WJR_X86
#error "x86 required"
#endif

namespace wjr {

#if WJR_HAS_SIMD(SSE2) && WJR_HAS_SIMD(X86_SIMD)
#define WJR_HAS_BUILTIN_SET_N WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(SET_N)

template <typename T>
WJR_COLD void large_builtin_set_n(T *dst, T val, size_t n) {
    constexpr auto nd = std::numeric_limits<T>::digits;
    constexpr auto is_avx = WJR_HAS_SIMD(AVX2);

    using simd = std::conditional_t<is_avx, avx, sse>;
    using simd_int = typename simd::int_type;
    constexpr auto simd_width = simd::width();
    constexpr auto type_width = simd_width / nd;
    constexpr auto u8_width = simd_width / 8;
    constexpr auto mask = u8_width * 4;

    WJR_ASSUME(n > type_width * 4);

    auto y = simd::set1(val, T());

    simd::storeu((simd_int *)(dst), y);
    simd::storeu((simd_int *)(dst + n - type_width), y);
    simd::storeu((simd_int *)(dst + type_width), y);
    simd::storeu((simd_int *)(dst + n - type_width * 2), y);
    simd::storeu((simd_int *)(dst + type_width * 2), y);
    simd::storeu((simd_int *)(dst + n - type_width * 3), y);
    simd::storeu((simd_int *)(dst + type_width * 3), y);
    simd::storeu((simd_int *)(dst + n - type_width * 4), y);

    uintptr_t ps = reinterpret_cast<uintptr_t>(dst);
    uintptr_t pe = reinterpret_cast<uintptr_t>(dst + n);

    ps += mask;
    ps &= -mask;
    pe &= -mask;

    if (WJR_UNLIKELY(ps == pe)) {
        return;
    }

    const uintptr_t mo = reinterpret_cast<uintptr_t>(dst) % sizeof(T);

    if (WJR_UNLIKELY(mo != 0)) {
        T stk[2] = {val, val};
        std::memcpy(&val, (char *)(stk) + mo, sizeof(T));
        y = simd::set1(val, T());
    }

    do {
        simd::store((simd_int *)(ps), y);
        simd::store((simd_int *)(ps + u8_width * 1), y);
        simd::store((simd_int *)(ps + u8_width * 2), y);
        simd::store((simd_int *)(ps + u8_width * 3), y);

        ps += u8_width * 4;
    } while (WJR_LIKELY(ps != pe));
    return;
}

template <typename T>
WJR_INTRINSIC_INLINE void builtin_set_n(T *dst, T val, size_t n) {
    constexpr auto nd = std::numeric_limits<T>::digits;
    constexpr auto is_avx = WJR_HAS_SIMD(AVX2);

    using simd = std::conditional_t<is_avx, avx, sse>;
    using simd_int = typename simd::int_type;
    constexpr auto simd_width = simd::width();
    constexpr auto type_width = simd_width / nd;

    using sse_int = typename sse::int_type;
    constexpr auto sse_width = sse::width();
    constexpr auto sse_loop = sse_width / nd;

    if (WJR_UNLIKELY(n == 0)) {
        return;
    }

    if (WJR_BUILTIN_CONSTANT_P(n) && n <= 4) {
        for (size_t i = 0; i < n; ++i) {
            dst[i] = val;
        }

        return;
    }

    if (WJR_UNLIKELY(n > type_width * 2)) {

        if (WJR_UNLIKELY(n > type_width * 4)) {
            return large_builtin_set_n(dst, val, n);
        }

        auto y = simd::set1(val, T());

        simd::storeu((simd_int *)(dst), y);
        simd::storeu((simd_int *)(dst + type_width), y);
        simd::storeu((simd_int *)(dst + n - type_width), y);
        simd::storeu((simd_int *)(dst + n - type_width * 2), y);
        return;
    }

    const auto x = broadcast<T, uint64_t>(val);
    const auto y = broadcast<uint64_t, __m128i_t>(x);

    if (WJR_UNLIKELY(n <= sse_loop * 2)) {
        if (WJR_UNLIKELY(n >= sse_loop)) {
            sse::storeu((sse_int *)(dst), y);
            sse::storeu((sse_int *)(dst + n - sse_loop), y);
            return;
        }

        if (WJR_UNLIKELY(n >= sse_loop / 2)) {
            std::memcpy(dst, &x, 8);
            if constexpr (sse_loop != 2) {
                std::memcpy(dst + n - sse_loop / 2, &x, 8);
            }
            return;
        }

        if constexpr (sse_loop >= 4) {
            if (WJR_UNLIKELY(n >= sse_loop / 4)) {
                std::memcpy(dst, &x, 4);
                if constexpr (sse_loop != 4) {
                    std::memcpy(dst + n - sse_loop / 4, &x, 4);
                }
                return;
            }
        }

        if constexpr (sse_loop >= 8) {
            if (WJR_UNLIKELY(n >= sse_loop / 8)) {
                std::memcpy(dst, &x, 2);
                if constexpr (sse_loop != 8) {
                    std::memcpy(dst + n - sse_loop / 8, &x, 2);
                }
                return;
            }
        }

        if constexpr (sse_loop >= 16) {
            if (WJR_UNLIKELY(n >= sse_loop / 16)) {
                std::memcpy(dst, &x, 1);
                if constexpr (sse_loop != 16) {
                    std::memcpy(dst + n - sse_loop / 16, &x, 1);
                }
                return;
            }
        }

        return;
    }

#if WJR_HAS_SIMD(AVX2)
    if constexpr (is_avx) {
        auto z = broadcast<__m128i_t, __m256i_t>(y);
        avx::storeu((simd_int *)(dst), z);
        avx::storeu((simd_int *)(dst + n - type_width), z);
        return;
    }
#endif

    WJR_UNREACHABLE();
}

#endif

} // namespace wjr

#endif // WJR_X86_MATH_SET_HPP__
#endif

namespace wjr {

template <typename T>
WJR_INTRINSIC_CONSTEXPR void fallback_set_n(T *dst, T val, size_t n) {
    for (size_t i = 0; i < n; ++i) {
        dst[i] = val;
    }
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR_E void set_n(T *dst, type_identity_t<T> val, size_t n) {
#if WJR_HAS_BUILTIN(SET_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_set_n(dst, val, n);
        }

        if (WJR_BUILTIN_CONSTANT_P(val) && broadcast<uint8_t, T>(val) == val) {
            if (WJR_UNLIKELY(n >= 2048 / sizeof(T))) {
                std::memset(dst, static_cast<uint8_t>(val), n * sizeof(T));
                return;
            }
        }

        return builtin_set_n(dst, val, n);
    } else {
        return fallback_set_n(dst, val, n);
    }
#else
    return fallback_set_n(dst, val, n);
#endif
}

} // namespace wjr

#endif // WJR_MATH_SET_HPP__

namespace wjr {

// find the first position(ret) that is not equal to number "from"
// and replace [0, ret) to number "to"
// For example, inc replaces a continuous segment of -1 with 0. And dec replaces a
// continuous segment of 0 with -1
template <typename T>
WJR_INTRINSIC_CONSTEXPR_E size_t replace_find_not(T *dst, const T *src, size_t n,
                                                  type_identity_t<T> from,
                                                  type_identity_t<T> to) {

    size_t ret = find_not_n(src, from, n);
    if (WJR_UNLIKELY(ret != 0) && WJR_LIKELY(dst != src || from != to)) {
        set_n(dst, to, ret);
    }

    return ret;
}

// find the last position(ret-1) that is not equal to number "from"
// and replace [ret, n) to number "to"
template <typename T>
WJR_INTRINSIC_CONSTEXPR_E size_t reverse_replace_find_not(T *dst, const T *src, size_t n,
                                                          type_identity_t<T> from,
                                                          type_identity_t<T> to) {
    size_t ret = reverse_find_not_n(src, from, n);
    if (WJR_UNLIKELY(ret != n) && WJR_LIKELY(dst != src || from != to)) {
        set_n(dst + ret, to, n - ret);
    }

    return ret;
}

} // namespace wjr

#endif // WJR_MATH_REPLACE_HPP__
#ifndef WJR_MATH_SUB_IMPL_HPP__
#define WJR_MATH_SUB_IMPL_HPP__

// Already included

namespace wjr {

template <typename T, typename U,
          WJR_REQUIRES(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E T subc(T a, T b, type_identity_t<U> c_in,
                                               U &c_out);

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E T subc_cc(T a, T b, uint8_t c_in, uint8_t &c_out);

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E bool sub_overflow(type_identity_t<T> a,
                                                          type_identity_t<T> b, T &ret);

template <typename T, typename U = T,
          WJR_REQUIRES(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E U subc_1(T *dst, const T *src0, size_t n,
                                                 type_identity_t<T> src1, U c_in = 0);

template <typename T, typename U = T,
          WJR_REQUIRES(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E U subc_n(T *dst, const T *src0, const T *src1,
                                                 size_t n, U c_in = 0);

template <typename T, typename U = T,
          WJR_REQUIRES(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E U subc_s(T *dst, const T *src0, size_t n,
                                                 const T *src1, size_t m, U c_in = 0);

template <typename T, typename U = T,
          WJR_REQUIRES(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E U subc_sz(T *dst, const T *src0, size_t n,
                                                  const T *src1, size_t m, U c_in = 0);

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E ssize_t abs_subc_n(T *dst, const T *src0,
                                                           const T *src1, size_t n);

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E ssize_t abs_subc_n_pos(T *dst, const T *src0,
                                                               const T *src1, size_t n);

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E ssize_t abs_subc_s(T *dst, const T *src0,
                                                           size_t n, const T *src1,
                                                           size_t m);

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E ssize_t abs_subc_s_pos(T *dst, const T *src0,
                                                               size_t n, const T *src1,
                                                               size_t m);

template <typename T, typename U,
          WJR_REQUIRES(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E ssize_t abs_subc_n(T *dst, const T *src0,
                                                           const T *src1, size_t n,
                                                           U &c_out,
                                                           type_identity_t<U> cf0,
                                                           type_identity_t<U> cf1);

// preview :

WJR_INTRINSIC_CONSTEXPR_E void __sub_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                         uint64_t hi0, uint64_t lo1, uint64_t hi1);

WJR_INTRINSIC_CONSTEXPR_E uint64_t __subc_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                              uint64_t hi0, uint64_t lo1, uint64_t hi1,
                                              uint64_t c_in);

WJR_INTRINSIC_CONSTEXPR_E uint8_t __subc_cc_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                                uint64_t hi0, uint64_t lo1, uint64_t hi1,
                                                uint8_t c_in);

} // namespace wjr

#endif // WJR_MATH_SUB_IMPL_HPP__

#if defined(WJR_X86)
#ifndef WJR_X86_SUB_HPP__
#define WJR_X86_SUB_HPP__

// Already included

#ifndef WJR_X86
#error "x86 required"
#endif

namespace wjr {

#if WJR_HAS_FEATURE(GCC_STYLE_INLINE_ASM)
#define WJR_HAS_BUILTIN_ASM_SUBC WJR_HAS_DEF
#define WJR_HAS_BUILTIN_ASM_SUBC_N WJR_HAS_DEF
#define WJR_HAS_BUILTIN___ASM_SUB_128 WJR_HAS_DEF
#define WJR_HAS_BUILTIN___ASM_SUBC_128 WJR_HAS_DEF

#if WJR_HAS_FEATURE(INLINE_ASM_CCCOND)
#define WJR_HAS_BUILTIN_ASM_SUBC_CC WJR_HAS_DEF
#define WJR_HAS_BUILTIN___ASM_SUBC_CC_128 WJR_HAS_DEF
#endif

#endif

#if WJR_HAS_BUILTIN(ASM_SUBC)

template <typename U>
WJR_INTRINSIC_INLINE uint64_t asm_subc(uint64_t a, uint64_t b, U c_in, U &c_out) {
    if (WJR_BUILTIN_CONSTANT_P(c_in == 1) && c_in == 1) {
        if (WJR_BUILTIN_CONSTANT_P(b) && __is_in_i32_range(b)) {
            asm("stc\n\t"
                "sbb{q %2, %0| %0, %2}\n\t"
                "setb %b1"
                : "=r"(a), "+r"(c_in)
                : "ri"(b), "0"(a)
                : "cc");
        } else {
            asm("stc\n\t"
                "sbb{q %2, %0| %0, %2}\n\t"
                "setb %b1"
                : "=r"(a), "+r"(c_in)
                : "r"(b), "0"(a)
                : "cc");
        }
        c_out = c_in;
        return a;
    }

    if (WJR_BUILTIN_CONSTANT_P(b) && __is_in_i32_range(b)) {
        asm("add{b $255, %b1| %b1, 255}\n\t"
            "sbb{q %2, %0| %0, %2}\n\t"
            "setb %b1"
            : "=r"(a), "+&r"(c_in)
            : "ri"(b), "0"(a)
            : "cc");
    } else {
        asm("add{b $255, %b1| %b1, 255}\n\t"
            "sbb{q %2, %0| %0, %2}\n\t"
            "setb %b1"
            : "=r"(a), "+&r"(c_in)
            : "r"(b), "0"(a)
            : "cc");
    }
    c_out = c_in;
    return a;
}

#endif

#if WJR_HAS_BUILTIN(ASM_SUBC_CC)

WJR_INTRINSIC_INLINE uint64_t asm_subc_cc(uint64_t a, uint64_t b, uint8_t c_in,
                                          uint8_t &c_out) {
    if (WJR_BUILTIN_CONSTANT_P(c_in == 1) && c_in == 1) {
        if (WJR_BUILTIN_CONSTANT_P(b) && __is_in_i32_range(b)) {
            asm("stc\n\t"
                "sbb{q %2, %0| %0, %2}\n\t" WJR_ASM_CCSET(c)
                : "=r"(a), WJR_ASM_CCOUT(c)(c_out)
                : "ri"(b), "0"(a)
                : "cc");
        } else {
            asm("stc\n\t"
                "sbb{q %2, %0| %0, %2}\n\t" WJR_ASM_CCSET(c)
                : "=r"(a), WJR_ASM_CCOUT(c)(c_out)
                : "r"(b), "0"(a)
                : "cc");
        }
        return a;
    }

    if (WJR_BUILTIN_CONSTANT_P(b) && __is_in_i32_range(b)) {
        asm("add{b $255, %b1| %b1, 255}\n\t"
            "sbb{q %3, %0| %0, %3}\n\t" WJR_ASM_CCSET(c)
            : "=r"(a), "+&r"(c_in), WJR_ASM_CCOUT(c)(c_out)
            : "ri"(b), "0"(a)
            : "cc");
    } else {
        asm("add{b $255, %b1| %b1, 255}\n\t"
            "sbb{q %3, %0| %0, %3}\n\t" WJR_ASM_CCSET(c)
            : "=r"(a), "+&r"(c_in), WJR_ASM_CCOUT(c)(c_out)
            : "r"(b), "0"(a)
            : "cc");
    }
    return a;
}

#endif

#if WJR_HAS_BUILTIN(ASM_SUBC_N)
#define WJR_ADDSUB_I 0
// WJR_ADDSUB_I :
// 0 : SUB
// 1 : ADD

// Already included

#ifndef WJR_ADDSUB_I
#error "abort"
#endif

#define WJR_addcsubc WJR_PP_BOOL_IF(WJR_ADDSUB_I, addc, subc)
#define WJR_adcsbb WJR_PP_BOOL_IF(WJR_ADDSUB_I, adc, sbb)

inline uint64_t WJR_PP_CONCAT(asm_, WJR_PP_CONCAT(WJR_addcsubc, _n_impl))(
    uint64_t *dst, const uint64_t *src0, const uint64_t *src1, size_t n, uint64_t c_in) {
    size_t rcx = n / 8;
    uint64_t r8 = c_in, r9, r10 = n & 7, r11;

    asm volatile(
        "add{b $255, %b[r8]| %b[r8], 255}\n\t"
        "lea{q| %[r9], [rip +} .Llookup%={(%%rip), %[r9]|]}\n\t"
        "movs{lq (%[r9], %[r10], 4), %[r10]|xd %[r10], DWORD PTR [%[r9] + %[r10] * 4]}\n\t"
        "lea{q (%[r9], %[r10], 1), %[r10]| %[r10], [%[r9] + %[r10]]}\n\t"
        "jmp{q *%[r10]| %[r10]}\n\t"
        
        ".align 8\n\t"
        ".Llookup%=:\n\t"
        ".long .Ll0%=-.Llookup%=\n\t"
        ".long .Ll1%=-.Llookup%=\n\t"
        ".long .Ll2%=-.Llookup%=\n\t"
        ".long .Ll3%=-.Llookup%=\n\t"
        ".long .Ll4%=-.Llookup%=\n\t"
        ".long .Ll5%=-.Llookup%=\n\t"
        ".long .Ll6%=-.Llookup%=\n\t"
        ".long .Ll7%=-.Llookup%=\n\t"
        ".align 16\n\t"
        
        ".Ll0%=:\n\t"
        "mov{q (%[src0]), %[r9]| %[r9], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r11]| %[r11], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r9]| %[r9], [%[src1]]}\n\t"
        "jmp .Lb0%=\n\t"

        ".Ld1%=:\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r10]| %[r10], [%[src1]]}\n\t"
        "mov{q %[r10], (%[dst])| [%[dst]], %[r10]}\n\t"
        "jmp .Ldone%=\n\t"

        ".Ll1%=:\n\t"
        "mov{q (%[src0]), %[r10]| %[r10], [%[src0]]}\n\t"
        "jrcxz .Ld1%=\n\t"
        "mov{q 8(%[src0]), %[r9]| %[r9], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r10]| %[r10], [%[src1]]}\n\t"
        "lea{q 8(%[src0]), %[src0]| %[src0], [%[src0] + 8]}\n\t"
        "lea{q 8(%[src1]), %[src1]| %[src1], [%[src1] + 8]}\n\t"
        "lea{q 8(%[dst]), %[dst]| %[dst], [%[dst] + 8]}\n\t"
        "jmp .Lb1%=\n\t"

        ".Ll3%=:\n\t"
        "mov{q (%[src0]), %[r11]| %[r11], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r8]| %[r8], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r11]| %[r11], [%[src1]]}\n\t"
        "lea{q -40(%[src0]), %[src0]| %[src0], [%[src0] - 40]}\n\t"
        "lea{q -40(%[src1]), %[src1]| %[src1], [%[src1] - 40]}\n\t"
        "lea{q -40(%[dst]), %[dst]| %[dst], [%[dst] - 40]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb3%=\n\t"

        ".Ll4%=:\n\t"
        "mov{q (%[src0]), %[r9]| %[r9], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r11]| %[r11], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r9]| %[r9], [%[src1]]}\n\t"
        "lea{q -32(%[src0]), %[src0]| %[src0], [%[src0] - 32]}\n\t"
        "lea{q -32(%[src1]), %[src1]| %[src1], [%[src1] - 32]}\n\t"
        "lea{q -32(%[dst]), %[dst]| %[dst], [%[dst] - 32]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb4%=\n\t"

        ".Ll5%=:\n\t"
        "mov{q (%[src0]), %[r10]| %[r10], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r9]| %[r9], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r10]| %[r10], [%[src1]]}\n\t"
        "lea{q -24(%[src0]), %[src0]| %[src0], [%[src0] - 24]}\n\t"
        "lea{q -24(%[src1]), %[src1]| %[src1], [%[src1] - 24]}\n\t"
        "lea{q -24(%[dst]), %[dst]| %[dst], [%[dst] - 24]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb5%=\n\t"

        ".Ll6%=:\n\t"
        "mov{q (%[src0]), %[r8]| %[r8], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r10]| %[r10], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r8]| %[r8], [%[src1]]}\n\t"
        "lea{q -16(%[src0]), %[src0]| %[src0], [%[src0] - 16]}\n\t"
        "lea{q -16(%[src1]), %[src1]| %[src1], [%[src1] - 16]}\n\t"
        "lea{q -16(%[dst]), %[dst]| %[dst], [%[dst] - 16]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb6%=\n\t"

        ".Ll7%=:\n\t"
        "mov{q (%[src0]), %[r11]| %[r11], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r8]| %[r8], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r11]| %[r11], [%[src1]]}\n\t"
        "lea{q -8(%[src0]), %[src0]| %[src0], [%[src0] - 8]}\n\t"
        "lea{q -8(%[src1]), %[src1]| %[src1], [%[src1] - 8]}\n\t"
        "lea{q -8(%[dst]), %[dst]| %[dst], [%[dst] - 8]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb7%=\n\t"

        ".Ld2%=:\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 8(%[src1]), %[r10]| %[r10], [%[src1] + 8]}\n\t"
        "mov{q %[r8], (%[dst])| [%[dst]], %[r8]}\n\t"
        "mov{q %[r10], 8(%[dst])| [%[dst] + 8], %[r10]}\n\t"
        "jmp .Ldone%=\n\t"

        ".Ll2%=:\n\t"
        "mov{q (%[src0]), %[r8]| %[r8], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r10]| %[r10], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r8]| %[r8], [%[src1]]}\n\t"
        "jrcxz .Ld2%=\n\t"
        "lea{q 16(%[src0]), %[src0]| %[src0], [%[src0] + 16]}\n\t"
        "lea{q 16(%[src1]), %[src1]| %[src1], [%[src1] + 16]}\n\t"
        "lea{q 16(%[dst]), %[dst]| %[dst], [%[dst] + 16]}\n\t"

        ".align 32\n\t"
        ".Lloop%=:\n\t"

        ".Lb2%=:\n\t"
        "mov{q (%[src0]), %[r9]| %[r9], [%[src0]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q -8(%[src1]), %[r10]| %[r10], [%[src1] - 8]}\n\t"
        "mov{q %[r8], -16(%[dst])| [%[dst] - 16], %[r8]}\n\t"

        ".Lb1%=:\n\t"
        "mov{q 8(%[src0]), %[r11]| %[r11], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r9]| %[r9], [%[src1]]}\n\t"
        "mov{q %[r10], -8(%[dst])| [%[dst] - 8], %[r10]}\n\t"

        ".Lb0%=:\n\t"
        "mov{q 16(%[src0]), %[r8]| %[r8], [%[src0] + 16]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 8(%[src1]), %[r11]| %[r11], [%[src1] + 8]}\n\t"
        "mov{q %[r9], (%[dst])| [%[dst]], %[r9]}\n\t"

        ".Lb7%=:\n\t"
        "mov{q 24(%[src0]), %[r10]| %[r10], [%[src0] + 24]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 16(%[src1]), %[r8]| %[r8], [%[src1] + 16]}\n\t"
        "mov{q %[r11], 8(%[dst])| [%[dst] + 8], %[r11]}\n\t"

        ".Lb6%=:\n\t"
        "mov{q 32(%[src0]), %[r9]| %[r9], [%[src0] + 32]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 24(%[src1]), %[r10]| %[r10], [%[src1] + 24]}\n\t"
        "mov{q %[r8], 16(%[dst])| [%[dst] + 16], %[r8]}\n\t"

        ".Lb5%=:\n\t"
        "mov{q 40(%[src0]), %[r11]| %[r11], [%[src0] + 40]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 32(%[src1]), %[r9]| %[r9], [%[src1] + 32]}\n\t"
        "mov{q %[r10], 24(%[dst])| [%[dst] + 24], %[r10]}\n\t"

        ".Lb4%=:\n\t"
        "mov{q 48(%[src0]), %[r8]| %[r8], [%[src0] + 48]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 40(%[src1]), %[r11]| %[r11], [%[src1] + 40]}\n\t"
        "mov{q %[r9], 32(%[dst])| [%[dst] + 32], %[r9]}\n\t"

        ".Lb3%=:\n\t"
        "mov{q 56(%[src0]), %[r10]| %[r10], [%[src0] + 56]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 48(%[src1]), %[r8]| %[r8], [%[src1] + 48]}\n\t"
        "mov{q %[r11], 40(%[dst])| [%[dst] + 40], %[r11]}\n\t"

        // TODO : optimize pipeline
        "lea{q 64(%[src0]), %[src0]| %[src0], [%[src0] + 64]}\n\t"
        "lea{q 64(%[src1]), %[src1]| %[src1], [%[src1] + 64]}\n\t"
        "lea{q 64(%[dst]), %[dst]| %[dst], [%[dst] + 64]}\n\t"
        "dec %[rcx]\n\t"
        
        "jne .Lloop%=\n\t"

        WJR_PP_STR(WJR_adcsbb) "{q -8(%[src1]), %[r10]| %[r10], [%[src1] - 8]}\n\t"
        "mov{q %[r8], -16(%[dst])| [%[dst] - 16], %[r8]}\n\t"
        "mov{q %[r10], -8(%[dst])| [%[dst] - 8], %[r10]}\n\t"

        ".Ldone%=:\n\t"
        "mov %k[rcx], %k[r9]\n\t"
        "adc{l %k[rcx], %k[r9]| %k[r9], %k[rcx]}"

        : [dst] "+r"(dst), [src0] "+r"(src0), [src1] "+r"(src1), [rcx] "+c"(rcx), 
          [r8] "+r"(r8), [r9] "=&r"(r9), [r10] "+r"(r10), [r11] "=&r"(r11)
        :
        : "cc", "memory");

    WJR_ASSERT_ASSUME(rcx == 0);
    WJR_ASSERT_ASSUME(r9 <= 1);

    return r9;
}

WJR_INTRINSIC_INLINE uint64_t WJR_PP_CONCAT(asm_, WJR_PP_CONCAT(WJR_addcsubc, _n))(
    uint64_t *dst, const uint64_t *src0, const uint64_t *src1, size_t n, uint64_t c_in) {
    if (WJR_BUILTIN_CONSTANT_P(n)) {
        if (n == 1) {
            dst[0] = WJR_PP_CONCAT(asm_, WJR_addcsubc)(src0[0], src1[0], c_in, c_in);
            return c_in;
        }
    }

    return WJR_PP_CONCAT(asm_, WJR_PP_CONCAT(WJR_addcsubc, _n_impl))(dst, src0, src1, n,
                                                                     c_in);
}

#undef WJR_adcsbb
#undef WJR_addcsubc

#undef WJR_ADDSUB_I
#endif

#if WJR_HAS_BUILTIN(__ASM_SUB_128)

WJR_INTRINSIC_INLINE void __asm_sub_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                        uint64_t hi0, uint64_t lo1, uint64_t hi1) {
    if (WJR_BUILTIN_CONSTANT_P(hi1) && hi1 <= UINT32_MAX) {
        asm("sub{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
            "sbb{q %[hi1], %[hi0]| %[hi0], %[hi1]}"
            : [lo0] "+&r"(lo0), [hi0] "+r"(hi0)
            : [lo1] "r"(lo1), [hi1] "i"(hi1)
            : "cc");
        al = lo0;
        ah = hi0;
        return;
    }
    asm("sub{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
        "sbb{q %[hi1], %[hi0]| %[hi0], %[hi1]}"
        : [lo0] "+&r"(lo0), [hi0] "+r"(hi0)
        : [lo1] "r"(lo1), [hi1] "r"(hi1)
        : "cc");
    al = lo0;
    ah = hi0;
}

#endif

#if WJR_HAS_BUILTIN(__ASM_SUBC_128) || WJR_HAS_BUILTIN(__ASM_SUBC_CC_128)

WJR_INTRINSIC_INLINE uint8_t __asm_subc_cc_zero_128(uint64_t &al, uint64_t &ah,
                                                    uint64_t lo0, uint64_t hi0,
                                                    uint64_t lo1, uint64_t hi1) {
    uint8_t c_out;
    if (WJR_BUILTIN_CONSTANT_P(hi1) && hi1 <= UINT32_MAX) {
        asm("sub{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
            "sbb{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t" WJR_ASM_CCSET(c)
            : [lo0] "+&r"(lo0), [hi0] "+r"(hi0), WJR_ASM_CCOUT(c)(c_out)
            : [lo1] "r"(lo1), [hi1] "i"(hi1)
            : "cc");
        al = lo0;
        ah = hi0;
        return c_out;
    }

    asm("sub{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
        "sbb{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t" WJR_ASM_CCSET(c)
        : [lo0] "+&r"(lo0), [hi0] "+r"(hi0), WJR_ASM_CCOUT(c)(c_out)
        : [lo1] "r"(lo1), [hi1] "r"(hi1)
        : "cc");
    al = lo0;
    ah = hi0;
    return c_out;
}

#endif

#if WJR_HAS_BUILTIN(__ASM_SUBC_128)

WJR_INTRINSIC_INLINE uint64_t __asm_subc_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                             uint64_t hi0, uint64_t lo1, uint64_t hi1,
                                             uint64_t c_in) {
    if (WJR_BUILTIN_CONSTANT_P(c_in == 0) && c_in == 0) {
        return __asm_subc_cc_zero_128(al, ah, lo0, hi0, lo1, hi1);
    }

    if (WJR_BUILTIN_CONSTANT_P(hi1) && hi1 <= UINT32_MAX) {
        asm("add{b $0xff, %b[c_in]| %b[c_in], 0xff}\n\t"
            "sbb{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
            "sbb{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t"
            "setb %b[c_in]"
            : [lo0] "+&r"(lo0), [hi0] "+r"(hi0), [c_in] "+&r"(c_in)
            : [lo1] "r"(lo1), [hi1] "i"(hi1)
            : "cc");
        al = lo0;
        ah = hi0;
        return c_in;
    }

    asm("add{b $0xff, %b[c_in]| %b[c_in], 0xff}\n\t"
        "sbb{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
        "sbb{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t"
        "setb %b[c_in]"
        : [lo0] "+&r"(lo0), [hi0] "+r"(hi0), [c_in] "+&r"(c_in)
        : [lo1] "r"(lo1), [hi1] "r"(hi1)
        : "cc");
    al = lo0;
    ah = hi0;
    return c_in;
}

#endif

#if WJR_HAS_BUILTIN(__ASM_SUBC_CC_128)

WJR_INTRINSIC_INLINE uint8_t __asm_subc_cc_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                               uint64_t hi0, uint64_t lo1, uint64_t hi1,
                                               uint8_t c_in) {
    if (WJR_BUILTIN_CONSTANT_P(c_in == 0) && c_in == 0) {
        return __asm_subc_cc_zero_128(al, ah, lo0, hi0, lo1, hi1);
    }

    uint8_t c_out;
    if (WJR_BUILTIN_CONSTANT_P(hi1) && hi1 <= UINT32_MAX) {
        asm("add{b $0xff, %b[c_in]| %b[c_in], 0xff}\n\t"
            "sbb{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
            "sbb{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t" WJR_ASM_CCSET(c)
            : [lo0] "+&r"(lo0), [hi0] "+r"(hi0), [c_in] "+&r"(c_in),
              WJR_ASM_CCOUT(c)(c_out)
            : [lo1] "r"(lo1), [hi1] "i"(hi1)
            : "cc");
        al = lo0;
        ah = hi0;
        return c_out;
    }

    asm("add{b $0xff, %b[c_in]| %b[c_in], 0xff}\n\t"
        "sbb{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
        "sbb{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t" WJR_ASM_CCSET(c)
        : [lo0] "+&r"(lo0), [hi0] "+r"(hi0), [c_in] "+&r"(c_in), WJR_ASM_CCOUT(c)(c_out)
        : [lo1] "r"(lo1), [hi1] "r"(hi1)
        : "cc");
    al = lo0;
    ah = hi0;
    return c_out;
}

#endif

} // namespace wjr

#endif // WJR_X86_SUB_HPP__
#endif

namespace wjr {

template <typename T, typename U>
WJR_INTRINSIC_CONSTEXPR T fallback_subc(T a, T b, U c_in, U &c_out) {
    T ret = a - b;
    U c = ret > a;
    a = ret;
    ret -= c_in;
    c |= ret > a;
    c_out = c;
    return ret;
}

#if WJR_HAS_BUILTIN(__builtin_subc)
#define WJR_HAS_BUILTIN_SUBC WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(SUBC)

template <typename T, typename U>
WJR_INTRINSIC_INLINE T builtin_subc(T a, T b, U c_in, U &c_out) {
    constexpr auto nd = std::numeric_limits<T>::digits;

#define WJR_REGISTER_BUILTIN_SUBC(suffix, type)                                          \
    if constexpr (nd <= std::numeric_limits<type>::digits) {                             \
        type __c_out;                                                                    \
        T ret = __builtin_subc##suffix(a, b, static_cast<type>(c_in), &__c_out);         \
        c_out = static_cast<U>(__c_out);                                                 \
        return ret;                                                                      \
    } else

    WJR_REGISTER_BUILTIN_SUBC(b, unsigned char)
    WJR_REGISTER_BUILTIN_SUBC(s, unsigned short)
    WJR_REGISTER_BUILTIN_SUBC(, unsigned int)
    WJR_REGISTER_BUILTIN_SUBC(l, unsigned long)
    WJR_REGISTER_BUILTIN_SUBC(ll, unsigned long long) {
        static_assert(nd <= 64, "not supported yet");
    }

#undef WJR_REGISTER_BUILTIN_SUBC
}

#endif // WJR_HAS_BUILTIN(SUBC)

template <typename T, typename U,
          WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_INTRINSIC_CONSTEXPR_E T subc(T a, T b, type_identity_t<U> c_in, U &c_out) {
    WJR_ASSERT_ASSUME_L1(c_in <= 1);

#if !WJR_HAS_BUILTIN(SUBC) && !WJR_HAS_BUILTIN(ASM_SUBC)
    return fallback_subc(a, b, c_in, c_out);
#else
    constexpr auto is_constant_or_zero = [](auto x) -> int {
        return WJR_BUILTIN_CONSTANT_P(x == 0) && x == 0 ? 2
               : WJR_BUILTIN_CONSTANT_P(x)              ? 1
                                                        : 0;
    };

    // The compiler should be able to optimize the judgment condition of if when enabling
    // optimization. If it doesn't work, then there should be a issue
    if (is_constant_evaluated() ||
        // constant value is zero or constant value number greater or equal than 2
        (is_constant_or_zero(a) + is_constant_or_zero(b) + is_constant_or_zero(c_in) >=
         2)) {
        return fallback_subc(a, b, c_in, c_out);
    }

    if constexpr (sizeof(T) == 8) {
        return WJR_PP_BOOL_IF(WJR_HAS_BUILTIN(ASM_SUBC), asm_subc,
                              WJR_PP_BOOL_IF(WJR_HAS_BUILTIN(SUBC), builtin_subc,
                                             fallback_subc))(a, b, c_in, c_out);
    } else {
        return WJR_PP_BOOL_IF(WJR_HAS_BUILTIN(SUBC), builtin_subc,
                              fallback_subc)(a, b, c_in, c_out);
    }
#endif
}

/*
 Used for subc and then jump according to cc flag. Therefore, the types of c_in and
 c_out are limited to uint8_t, while the default c_in and c_out types of normal subc are
 the same as T, so that the high register is not cleared. Currently, GCC/Clang @=cccond
 cannot know that the high register is not cleared, so the performance is worse than the
 normal version when cc flag is not needed immediately.
*/
template <typename T, WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E T subc_cc(T a, T b, uint8_t c_in, uint8_t &c_out) {
    WJR_ASSERT_ASSUME_L1(c_in <= 1);

#if WJR_HAS_BUILTIN(ASM_SUBC_CC)
    constexpr auto is_constant_or_zero = [](auto x) -> int {
        return WJR_BUILTIN_CONSTANT_P(x == 0) && x == 0 ? 2
               : WJR_BUILTIN_CONSTANT_P(x)              ? 1
                                                        : 0;
    };

    // The compiler should be able to optimize the judgment condition of if when enabling
    // optimization. If it doesn't work, then there should be a issue
    if (is_constant_evaluated() ||
        // constant value is zero or constant value number greater or equal than 2
        (is_constant_or_zero(a) + is_constant_or_zero(b) + is_constant_or_zero(c_in) >=
         2)) {
        return fallback_subc(a, b, c_in, c_out);
    }

    if constexpr (sizeof(T) == 8) {
        return asm_subc_cc(a, b, c_in, c_out);
    } else {
        return subc(a, b, c_in, c_out);
    }
#else
    return subc(a, b, c_in, c_out);
#endif
}

#if WJR_HAS_BUILTIN(__builtin_sub_overflow)
#define WJR_HAS_BUILTIN_SUB_OVERFLOW WJR_HAS_DEF
#endif

template <typename T>
WJR_INTRINSIC_CONSTEXPR_E bool fallback_sub_overflow(T a, T b, T &ret) {
    ret = a - b;
    return ret > a;
}

template <typename T, WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E bool sub_overflow(type_identity_t<T> a, type_identity_t<T> b,
                                            T &ret) {
#if WJR_HAS_BUILTIN(SUB_OVERFLOW)
    if (is_constant_evaluated() ||
        (WJR_BUILTIN_CONSTANT_P(a) && WJR_BUILTIN_CONSTANT_P(b))) {
        return fallback_sub_overflow(a, b, ret);
    }

    return __builtin_sub_overflow(a, b, &ret);
#else
    return fallback_sub_overflow(a, b, ret);
#endif
}

/*
require :
1. n >= 1
2. WJR_IS_SAME_OR_INCR_P(dst, n, src0, n)
*/
template <typename T, typename U,
          WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_INTRINSIC_CONSTEXPR_E U subc_1(T *dst, const T *src0, size_t n,
                                   type_identity_t<T> src1, U c_in) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src0, n));

    uint8_t overflow = 0;
    dst[0] = subc_cc(src0[0], src1, c_in, overflow);

    if (overflow) {
        size_t idx = 1 + replace_find_not(dst + 1, src0 + 1, n - 1, 0, -1);

        if (WJR_UNLIKELY(idx == n)) {
            return static_cast<U>(1);
        }

        dst[idx] = src0[idx] - 1;

        dst += idx;
        src0 += idx;
        n -= idx;
    }

    if (src0 != dst) {
        std::copy(src0 + 1, src0 + n, dst + 1);
    }

    return static_cast<U>(0);
}

template <typename T, typename U>
WJR_INTRINSIC_CONSTEXPR U fallback_subc_n(T *dst, const T *src0, const T *src1, size_t n,
                                          U c_in) {
    size_t m = n / 4;

    for (size_t i = 0; i < m; ++i) {
        dst[0] = subc(src0[0], src1[0], c_in, c_in);
        dst[1] = subc(src0[1], src1[1], c_in, c_in);
        dst[2] = subc(src0[2], src1[2], c_in, c_in);
        dst[3] = subc(src0[3], src1[3], c_in, c_in);

        dst += 4;
        src0 += 4;
        src1 += 4;
    }

    n &= 3;
    if (WJR_UNLIKELY(n == 0)) {
        return c_in;
    }

    dst += n;
    src0 += n;
    src1 += n;

    switch (n) {
    case 3: {
        dst[-3] = subc(src0[-3], src1[-3], c_in, c_in);
        WJR_FALLTHROUGH;
    }
    case 2: {
        dst[-2] = subc(src0[-2], src1[-2], c_in, c_in);
        WJR_FALLTHROUGH;
    }
    case 1: {
        dst[-1] = subc(src0[-1], src1[-1], c_in, c_in);
        WJR_FALLTHROUGH;
    }
    default: {
        break;
    }
    }

    return c_in;
}

/*
require :
1. n >= 1
2. WJR_IS_SAME_OR_INCR_P(dst, n, src0, n)
3. WJR_IS_SAME_OR_INCR_P(dst, n, src1, n)
*/
template <typename T, typename U,
          WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_INTRINSIC_CONSTEXPR_E U subc_n(T *dst, const T *src0, const T *src1, size_t n,
                                   U c_in) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src0, n));
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src1, n));

#if WJR_HAS_BUILTIN(ASM_SUBC_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_subc_n(dst, src0, src1, n, c_in);
        }

        return asm_subc_n(dst, src0, src1, n, c_in);
    } else {
        return fallback_subc_n(dst, src0, src1, n, c_in);
    }
#else
    return fallback_subc_n(dst, src0, src1, n, c_in);
#endif
}

/*
require :
1. m >= 1
2. n >= m
3. WJR_IS_SAME_OR_INCR_P(dst, n, src0, n)
4. WJR_IS_SAME_OR_INCR_P(dst, m, src1, m)
*/
template <typename T, typename U,
          WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_INTRINSIC_CONSTEXPR_E U subc_s(T *dst, const T *src0, size_t n, const T *src1,
                                   size_t m, U c_in) {
    WJR_ASSERT_ASSUME(m >= 1);
    WJR_ASSERT_ASSUME(n >= m);

    c_in = subc_n(dst, src0, src1, m, c_in);

    if (n != m) {
        c_in = subc_1(dst + m, src0 + m, n - m, 0, c_in);
    }

    return c_in;
}

/*
require :
1. n >= 0
2. n >= m
3. WJR_IS_SAME_OR_INCR_P(dst, n, src0, n)
4. WJR_IS_SAME_OR_INCR_P(dst, m, src1, m)
*/
template <typename T, typename U,
          WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_INTRINSIC_CONSTEXPR_E U subc_sz(T *dst, const T *src0, size_t n, const T *src1,
                                    size_t m, U c_in) {
    WJR_ASSERT_ASSUME(n >= m);

    if (WJR_LIKELY(m != 0)) {
        c_in = subc_n(dst, src0, src1, m, c_in);
    }

    if (n != m) {
        c_in = subc_1(dst + m, src0 + m, n - m, 0, c_in);
    }

    return c_in;
}

template <typename T, WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E ssize_t abs_subc_n(T *dst, const T *src0, const T *src1,
                                             size_t n) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_SEPARATE_P(dst, n, src0, n));
    WJR_ASSERT_L1(WJR_IS_SAME_OR_SEPARATE_P(dst, n, src1, n));

    size_t idx = reverse_find_not_n(src0, src1, n);

    if (WJR_UNLIKELY(idx != n)) {
        set_n(dst + idx, 0, n - idx);

        if (WJR_UNLIKELY(idx == 0)) {
            return 0;
        }
    }

    WJR_ASSUME(idx >= 1);

    uint64_t hi = 0;
    WJR_ASSUME(src0[idx - 1] != src1[idx - 1]);
    const bool overflow = sub_overflow(src0[idx - 1], src1[idx - 1], hi);

    if (overflow) {
        std::swap(src0, src1);
        hi = -hi;
    }

    do {
        if (WJR_UNLIKELY(idx == 1)) {
            dst[0] = hi;
            break;
        }

        hi -= subc_n(dst, src0, src1, idx - 1);
        dst[idx - 1] = hi;
    } while (0);

    return overflow ? -1 : 1;
}

template <typename T, WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E ssize_t abs_subc_n_pos(T *dst, const T *src0, const T *src1,
                                                 size_t n) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_SEPARATE_P(dst, n, src0, n));
    WJR_ASSERT_L1(WJR_IS_SAME_OR_SEPARATE_P(dst, n, src1, n));

    size_t idx = reverse_find_not_n(src0, src1, n);

    if (WJR_UNLIKELY(idx != n)) {
        set_n(dst + idx, 0, n - idx);

        if (WJR_UNLIKELY(idx == 0)) {
            return 0;
        }
    }

    WJR_ASSUME(idx >= 1);

    uint64_t hi = 0;
    WJR_ASSUME(src0[idx - 1] != src1[idx - 1]);
    const bool overflow = sub_overflow(src0[idx - 1], src1[idx - 1], hi);

    if (overflow) {
        std::swap(src0, src1);
        hi = -hi;
    }

    ssize_t ret = __fasts_from_unsigned(idx);
    WJR_ASSUME(ret >= 1);

    do {
        if (WJR_UNLIKELY(idx == 1)) {
            dst[0] = hi;
            break;
        }

        WJR_ASSUME(ret >= 2);

        hi -= subc_n(dst, src0, src1, idx - 1);

        if (WJR_LIKELY(hi != 0)) {
            dst[idx - 1] = hi;
        } else {
            --ret;
        }
    } while (0);

    WJR_ASSUME(ret > 0);
    return overflow ? -ret : ret;
}

template <typename T, WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E ssize_t abs_subc_s(T *dst, const T *src0, size_t n,
                                             const T *src1, size_t m) {
    WJR_ASSERT_ASSUME(m >= 1);
    WJR_ASSERT_ASSUME(n >= m);

    if (WJR_BUILTIN_CONSTANT_P(n == m) && n == m) {
        return abs_subc_n(dst, src0, src1, m);
    }

    if (WJR_BUILTIN_CONSTANT_P(n - m <= 1) && n - m <= 1) {
        do {
            if (n == m) {
                break;
            }

            if (WJR_UNLIKELY(src0[m] == 0)) {
                dst[m] = 0;
                break;
            }

            uint64_t hi = src0[m];
            hi -= subc_n(dst, src0, src1, m);
            dst[m] = hi;
            return 1;
        } while (0);

        return abs_subc_n(dst, src0, src1, m);
    }

    size_t idx = reverse_replace_find_not(dst + m, src0 + m, n - m, 0, 0);

    if (WJR_UNLIKELY(idx == 0)) {
        return abs_subc_n(dst, src0, src1, m);
    }

    (void)subc_s(dst, src0, m + idx, src1, m);
    return 1;
}

template <typename T, WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E ssize_t abs_subc_s_pos(T *dst, const T *src0, size_t n,
                                                 const T *src1, size_t m) {
    WJR_ASSERT_ASSUME(m >= 1);
    WJR_ASSERT_ASSUME(n >= m);

    if (WJR_BUILTIN_CONSTANT_P(n == m) && n == m) {
        return abs_subc_n_pos(dst, src0, src1, m);
    }

    if (WJR_BUILTIN_CONSTANT_P(n - m <= 1) && n - m <= 1) {
        do {
            if (n == m) {
                break;
            }

            if (WJR_UNLIKELY(src0[m] == 0)) {
                dst[m] = 0;
                break;
            }

            uint64_t hi = src0[m];
            hi -= subc_n(dst, src0, src1, m);
            ssize_t ret = __fasts_from_unsigned(m + 1);

            if (WJR_LIKELY(hi != 0)) {
                dst[m] = hi;
            } else {
                --ret;
            }

            WJR_ASSUME(ret > 0);
            return ret;
        } while (0);

        return abs_subc_n_pos(dst, src0, src1, m);
    }

    size_t idx = reverse_replace_find_not(dst + m, src0 + m, n - m, 0, 0);

    if (WJR_UNLIKELY(idx == 0)) {
        return abs_subc_n_pos(dst, src0, src1, m);
    }

    (void)subc_s(dst, src0, m + idx, src1, m);
    uint64_t ret = __fasts_from_unsigned(m + idx);
    WJR_ASSUME(ret >= 2);
    ret -= dst[m + idx - 1] == 0;
    WJR_ASSUME(ret >= 1);
    return ret;
}

// just like abs_subc_n.
template <typename T, typename U,
          WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_INTRINSIC_CONSTEXPR_E ssize_t abs_subc_n(T *dst, const T *src0, const T *src1,
                                             size_t n, U &c_out, type_identity_t<U> cf0,
                                             type_identity_t<U> cf1) {
    WJR_ASSERT_ASSUME(n >= 1);
    if (cf0 != cf1) {
        ssize_t ret = __fasts_from_unsigned(n);
        U cf = 0;
        if (cf0 < cf1) {
            std::swap(src0, src1);
            ret = __fasts_negate(ret);
            cf = cf1 - cf0;
        } else {
            cf = cf0 - cf1;
        }

        c_out = cf - subc_n(dst, src0, src1, n);
        return ret;
    } else {
        c_out = 0;
        return abs_subc_n(dst, src0, src1, n);
    }
}

WJR_INTRINSIC_CONSTEXPR void __fallback_sub_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                                uint64_t hi0, uint64_t lo1,
                                                uint64_t hi1) {
    uint64_t __al = lo0 - lo1;
    ah = hi0 - hi1 - (__al > lo0);
    al = __al;
}

#if WJR_HAS_FEATURE(FAST_INT128_ADDSUB)
#define WJR_HAS_BUILTIN___BUILTIN_SUB_128 WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(__BUILTIN_SUBC_128)

WJR_INTRINSIC_INLINE void __builtin_sub_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                            uint64_t hi0, uint64_t lo1, uint64_t hi1) {
    const auto x0 = static_cast<__uint128_t>(hi0) << 64 | lo0;
    const auto x1 = static_cast<__uint128_t>(hi1) << 64 | lo1;
    x0 -= x1;

    al = x0;
    ah = x0 >> 64;
}

#endif

// <ah, al> = <hi0, lo0> - <hi1, lo1>
WJR_INTRINSIC_CONSTEXPR_E void __sub_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                         uint64_t hi0, uint64_t lo1, uint64_t hi1) {
#if WJR_HAS_BUILTIN(__BUILTIN_SUB_128) || WJR_HAS_BUILTIN(__ASM_SUB_128)
    if (is_constant_evaluated() || (WJR_BUILTIN_CONSTANT_P(lo0 == 0) && lo0 == 0) ||
        (WJR_BUILTIN_CONSTANT_P(lo1 == 0) && lo1 == 0)) {
        return __fallback_sub_128(al, ah, lo0, hi0, lo1, hi1);
    }

    return WJR_PP_BOOL_IF(WJR_HAS_BUILTIN(__BUILTIN_SUB_128), __builtin_sub_128,
                          __asm_sub_128)(al, ah, lo0, hi0, lo1, hi1);
#else
    return __fallback_sub_128(al, ah, lo0, hi0, lo1, hi1);
#endif
}

WJR_INTRINSIC_CONSTEXPR_E uint64_t __fallback_subc_128(uint64_t &al, uint64_t &ah,
                                                       uint64_t lo0, uint64_t hi0,
                                                       uint64_t lo1, uint64_t hi1,
                                                       uint64_t c_in) {
    al = subc(lo0, lo1, c_in, c_in);
    ah = subc(hi0, hi1, c_in, c_in);
    return c_in;
}

// return c_out
WJR_INTRINSIC_CONSTEXPR_E uint64_t __subc_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                              uint64_t hi0, uint64_t lo1, uint64_t hi1,
                                              uint64_t c_in) {
#if WJR_HAS_BUILTIN(__ASM_ADDC_128)
    if (is_constant_evaluated()) {
        return __fallback_subc_128(al, ah, lo0, hi0, lo1, hi1, c_in);
    }

    return __asm_subc_128(al, ah, lo0, hi0, lo1, hi1, c_in);
#else
    return __fallback_subc_128(al, ah, lo0, hi0, lo1, hi1, c_in);
#endif
}

WJR_INTRINSIC_CONSTEXPR_E uint8_t __subc_cc_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                                uint64_t hi0, uint64_t lo1, uint64_t hi1,
                                                uint8_t c_in) {
#if WJR_HAS_BUILTIN(__ASM_ADDC_CC_128)
    if (is_constant_evaluated()) {
        return __fallback_subc_128(al, ah, lo0, hi0, lo1, hi1, c_in);
    }

    return __asm_subc_cc_128(al, ah, lo0, hi0, lo1, hi1, c_in);
#else
    return __subc_128(al, ah, lo0, hi0, lo1, hi1, c_in);
#endif
}

} // namespace wjr

#endif // WJR_MATH_SUB_HPP__
// Already included

#if defined(WJR_X86)
#ifndef WJR_X86_MATH_COMPARE_HPP__
#define WJR_X86_MATH_COMPARE_HPP__

// Already included
// Already included

#ifndef WJR_X86
#error "x86 required"
#endif

namespace wjr {

#if WJR_HAS_SIMD(SSE4_1) && WJR_HAS_SIMD(X86_SIMD)
#define WJR_HAS_BUILTIN_COMPARE_N WJR_HAS_DEF
#define WJR_HAS_BUILTIN_REVERSE_COMPARE_N WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(COMPARE_N)

/**
 * @brief Use SIMD to compare two arrays of uint64_t.
 *
 */
template <typename T>
WJR_PURE WJR_COLD int large_builtin_compare_n(const T *src0, const T *src1, size_t n) {
#define WJR_REGISTER_COMPARE_NOT_N_AVX(index)                                            \
    do {                                                                                 \
        auto x = avx::loadu((__m256i *)(src0 + (index)));                                \
        auto y = avx::loadu((__m256i *)(src1 + (index)));                                \
        auto r = avx::cmpeq_epi64(x, y);                                                 \
                                                                                         \
        avx::mask_type mask = ~avx::movemask_epi8(r);                                    \
        if (WJR_LIKELY(mask != 0)) {                                                     \
            auto offset = ctz(mask) / 8;                                                 \
            return src0[(index) + offset] < src1[(index) + offset] ? -1 : 1;             \
        }                                                                                \
    } while (0)

    size_t rem = n & 7;

    if (rem > 4) {
#if !WJR_HAS_SIMD(AVX2)
        auto x0 = sse::loadu((__m128i *)(src0 + (rem - 4)));
        auto x1 = sse::loadu((__m128i *)(src0 + (rem - 2)));
        auto y0 = sse::loadu((__m128i *)(src1 + (rem - 4)));
        auto y1 = sse::loadu((__m128i *)(src1 + (rem - 2)));

        auto r0 = sse::cmpeq_epi64(x0, y0);
        auto r1 = sse::cmpeq_epi64(x1, y1);

        if (WJR_LIKELY(!sse::test_all_ones(sse::And(r0, r1)))) {
            sse::mask_type mask = ~sse::movemask_epi8(r0);
            if (mask != 0) {
                if (mask == 0xFF00) {
                    return src0[(rem - 4) + 1] < src1[(rem - 4) + 1] ? -1 : 1;
                }
                return src0[(rem - 4)] < src1[(rem - 4)] ? -1 : 1;
            }

            mask = ~sse::movemask_epi8(r1);
            if (mask == 0xFF00) {
                return src0[(rem - 2) + 1] < src1[(rem - 2) + 1] ? -1 : 1;
            }
            return src0[(rem - 2)] < src1[(rem - 2)] ? -1 : 1;
        }
#else
        WJR_REGISTER_COMPARE_NOT_N_AVX(rem - 4);
#endif
    }

    if (WJR_UNLIKELY(rem == n)) {
        return 0;
    }

#if !WJR_HAS_SIMD(AVX2)
    do {
        auto x0 = sse::loadu((__m128i *)(src0 + rem));
        auto x1 = sse::loadu((__m128i *)(src0 + rem + 2));
        auto x2 = sse::loadu((__m128i *)(src0 + rem + 4));
        auto x3 = sse::loadu((__m128i *)(src0 + rem + 6));
        auto y0 = sse::loadu((__m128i *)(src1 + rem));
        auto y1 = sse::loadu((__m128i *)(src1 + rem + 2));
        auto y2 = sse::loadu((__m128i *)(src1 + rem + 4));
        auto y3 = sse::loadu((__m128i *)(src1 + rem + 6));

        auto r0 = sse::cmpeq_epi64(x0, y0);
        auto r1 = sse::cmpeq_epi64(x1, y1);
        auto r2 = sse::cmpeq_epi64(x2, y2);
        auto r3 = sse::cmpeq_epi64(x3, y3);

        auto z = sse::And(sse::And(r0, r1), sse::And(r2, r3));

        if (WJR_UNLIKELY(!sse::test_all_ones(z))) {
            sse::mask_type mask = ~sse::movemask_epi8(r0);
            if (mask != 0) {
                if (mask == 0xFF00) {
                    return src0[rem + 1] < src1[rem + 1] ? -1 : 1;
                }
                return src0[rem] < src1[rem] ? -1 : 1;
            }

            mask = ~sse::movemask_epi8(r1);
            if (mask != 0) {
                if (mask == 0xFF00) {
                    return src0[rem + 3] < src1[rem + 3] ? -1 : 1;
                }
                return src0[rem + 2] < src1[rem + 2] ? -1 : 1;
            }

            mask = ~sse::movemask_epi8(r2);
            if (mask != 0) {
                if (mask == 0xFF00) {
                    return src0[rem + 5] < src1[rem + 5] ? -1 : 1;
                }
                return src0[rem + 4] < src1[rem + 4] ? -1 : 1;
            }

            mask = ~sse::movemask_epi8(r3);
            if (mask == 0xFF00) {
                return src0[rem + 7] < src1[rem + 7] ? -1 : 1;
            }
            return src0[rem + 6] < src1[rem + 6] ? -1 : 1;
        }

        rem += 8;
    } while (WJR_LIKELY(rem != n));
#else
    if ((n - rem) & 8) {
        WJR_REGISTER_COMPARE_NOT_N_AVX(rem);
        WJR_REGISTER_COMPARE_NOT_N_AVX(rem + 4);

        rem += 8;

        if (WJR_UNLIKELY(rem == n)) {
            return 0;
        }
    }

    do {
        auto x0 = avx::loadu((__m256i *)(src0 + rem));
        auto x1 = avx::loadu((__m256i *)(src0 + rem + 4));
        auto x2 = avx::loadu((__m256i *)(src0 + rem + 8));
        auto x3 = avx::loadu((__m256i *)(src0 + rem + 12));
        auto y0 = avx::loadu((__m256i *)(src1 + rem));
        auto y1 = avx::loadu((__m256i *)(src1 + rem + 4));
        auto y2 = avx::loadu((__m256i *)(src1 + rem + 8));
        auto y3 = avx::loadu((__m256i *)(src1 + rem + 12));

        auto r0 = avx::cmpeq_epi64(x0, y0);
        auto r1 = avx::cmpeq_epi64(x1, y1);
        auto r2 = avx::cmpeq_epi64(x2, y2);
        auto r3 = avx::cmpeq_epi64(x3, y3);

        auto z = avx::And(avx::And(r0, r1), avx::And(r2, r3));

        if (WJR_UNLIKELY(!avx::test_all_ones(z))) {
            avx::mask_type mask = ~avx::movemask_epi8(r0);
            if (WJR_UNLIKELY(mask != 0)) {
                auto offset = ctz(mask) / 8;
                return src0[rem + offset] < src1[rem + offset] ? -1 : 1;
            }

            mask = ~avx::movemask_epi8(r1);
            if (WJR_UNLIKELY(mask != 0)) {
                auto offset = ctz(mask) / 8;
                return src0[rem + offset + 4] < src1[rem + offset + 4] ? -1 : 1;
            }

            mask = ~avx::movemask_epi8(r2);
            if (WJR_UNLIKELY(mask != 0)) {
                auto offset = ctz(mask) / 8;
                return src0[rem + offset + 8] < src1[rem + offset + 8] ? -1 : 1;
            }

            mask = ~avx::movemask_epi8(r3);
            auto offset = ctz(mask) / 8;
            return src0[rem + offset + 12] < src1[rem + offset + 12] ? -1 : 1;
        }

        rem += 16;
    } while (WJR_LIKELY(rem != n));
#endif

    return 0;

#undef WJR_REGISTER_COMPARE_NOT_N_AVX
}

extern template WJR_PURE WJR_COLD int
large_builtin_compare_n<uint64_t>(const uint64_t *src0, const uint64_t *src1, size_t n);

/**
 * @brief Compare two arrays of uint64_t.
 *
 * @details Expand first 4 elements to compare, then use @ref large_builtin_compare_n to
 * compare the rest.
 *
 * @tparam T Requires uint64_t currently.
 * @param src0 Pointer to the first array.
 * @param src1 Pointer to the second array.
 * @param n Number of elements to compare.
 * @return
 * \code
 * negative : src0 < src1
 * 0        : src0 == src1
 * positive : src0 > src1
 * \endcode
 */
template <typename T>
WJR_INTRINSIC_INLINE int builtin_compare_n(const T *src0, const T *src1, size_t n) {
    if (WJR_UNLIKELY(n == 0)) {
        return 0;
    }

    if (WJR_LIKELY(src0[0] != src1[0])) {
        return src0[0] < src1[0] ? -1 : 1;
    }

    if (n == 1) {
        return 0;
    }

    if (WJR_LIKELY(src0[1] != src1[1])) {
        return src0[1] < src1[1] ? -1 : 1;
    }

    if (n == 2) {
        return 0;
    }

    if (WJR_LIKELY(src0[2] != src1[2])) {
        return src0[2] < src1[2] ? -1 : 1;
    }

    if (n == 3) {
        return 0;
    }

    if (WJR_LIKELY(src0[3] != src1[3])) {
        return src0[3] < src1[3] ? -1 : 1;
    }

    if (n == 4) {
        return 0;
    }

    return large_builtin_compare_n(src0, src1, n);
}

#endif

#if WJR_HAS_BUILTIN(REVERSE_COMPARE_N)

/**
 * @brief Use SIMD to compare two arrays of uint64_t in reverse order.
 *
 * @details @ref large_builtin_compare_n in reverse order.
 *
 */
template <typename T>
WJR_PURE WJR_COLD int large_builtin_reverse_compare_n(const T *src0, const T *src1,
                                                      size_t n) {
#define WJR_REGISTER_REVERSE_COMPARE_NOT_N_AVX(index)                                    \
    do {                                                                                 \
        auto x = avx::loadu((__m256i *)(src0 - 4 + (index)));                            \
        auto y = avx::loadu((__m256i *)(src1 - 4 + (index)));                            \
        auto r = avx::cmpeq_epi64(x, y);                                                 \
                                                                                         \
        avx::mask_type mask = ~avx::movemask_epi8(r);                                    \
        if (WJR_LIKELY(mask != 0)) {                                                     \
            auto offset = clz(mask) / 8;                                                 \
            return src0[(index)-1 - offset] < src1[(index)-1 - offset] ? -1 : 1;         \
        }                                                                                \
    } while (0)

    const size_t rem = n & 7;
    n -= rem;

    if (rem > 4) {
#if !WJR_HAS_SIMD(AVX2)
        auto x0 = sse::loadu((__m128i *)(src0 + n + 2));
        auto x1 = sse::loadu((__m128i *)(src0 + n));
        auto y0 = sse::loadu((__m128i *)(src1 + n + 2));
        auto y1 = sse::loadu((__m128i *)(src1 + n));

        auto r0 = sse::cmpeq_epi64(x0, y0);
        auto r1 = sse::cmpeq_epi64(x1, y1);

        if (WJR_LIKELY(!sse::test_all_ones(sse::And(r0, r1)))) {
            sse::mask_type mask = ~sse::movemask_epi8(r0);
            if (mask != 0) {
                if (mask == 0x00FF) {
                    return src0[n + 2] < src1[n + 2] ? -1 : 1;
                }
                return src0[n + 3] < src1[n + 3] ? -1 : 1;
            }
            mask = ~sse::movemask_epi8(r1);
            if (mask == 0x00FF) {
                return src0[n] < src1[n] ? -1 : 1;
            }
            return src0[n + 1] < src1[n + 1] ? -1 : 1;
        }
#else
        WJR_REGISTER_REVERSE_COMPARE_NOT_N_AVX(n + 4);
#endif
    }

    if (WJR_UNLIKELY(n == 0)) {
        return 0;
    }

#if !WJR_HAS_SIMD(AVX2)
    do {
        auto x0 = sse::loadu((__m128i *)(src0 + n - 8));
        auto x1 = sse::loadu((__m128i *)(src0 + n - 6));
        auto x2 = sse::loadu((__m128i *)(src0 + n - 4));
        auto x3 = sse::loadu((__m128i *)(src0 + n - 2));
        auto y0 = sse::loadu((__m128i *)(src1 + n - 8));
        auto y1 = sse::loadu((__m128i *)(src1 + n - 6));
        auto y2 = sse::loadu((__m128i *)(src1 + n - 4));
        auto y3 = sse::loadu((__m128i *)(src1 + n - 2));

        auto r0 = sse::cmpeq_epi64(x0, y0);
        auto r1 = sse::cmpeq_epi64(x1, y1);
        auto r2 = sse::cmpeq_epi64(x2, y2);
        auto r3 = sse::cmpeq_epi64(x3, y3);

        auto z = sse::And(sse::And(r0, r1), sse::And(r2, r3));

        if (WJR_UNLIKELY(!sse::test_all_ones(z))) {
            sse::mask_type mask = ~sse::movemask_epi8(r3);
            if (mask != 0) {
                if (mask == 0x00FF) {
                    return src0[n - 2] < src1[n - 2] ? -1 : 1;
                }
                return src0[n - 1] < src1[n - 1] ? -1 : 1;
            }

            mask = ~sse::movemask_epi8(r2);
            if (mask != 0) {
                if (mask == 0x00FF) {
                    return src0[n - 4] < src1[n - 4] ? -1 : 1;
                }
                return src0[n - 3] < src1[n - 3] ? -1 : 1;
            }

            mask = ~sse::movemask_epi8(r1);
            if (mask != 0) {
                if (mask == 0x00FF) {
                    return src0[n - 6] < src1[n - 6] ? -1 : 1;
                }
                return src0[n - 5] < src1[n - 5] ? -1 : 1;
            }

            mask = ~sse::movemask_epi8(r0);
            if (mask == 0x00FF) {
                return src0[n - 8] < src1[n - 8] ? -1 : 1;
            }
            return src0[n - 7] < src1[n - 7] ? -1 : 1;
        }

        n -= 8;
    } while (WJR_LIKELY(n != 0));
#else
    if (n & 8) {
        WJR_REGISTER_REVERSE_COMPARE_NOT_N_AVX(n);
        WJR_REGISTER_REVERSE_COMPARE_NOT_N_AVX(n - 4);

        n -= 8;

        if (WJR_UNLIKELY(n == 0)) {
            return 0;
        }
    }

    do {
        auto x0 = avx::loadu((__m256i *)(src0 + n - 16));
        auto x1 = avx::loadu((__m256i *)(src0 + n - 12));
        auto x2 = avx::loadu((__m256i *)(src0 + n - 8));
        auto x3 = avx::loadu((__m256i *)(src0 + n - 4));
        auto y0 = avx::loadu((__m256i *)(src1 + n - 16));
        auto y1 = avx::loadu((__m256i *)(src1 + n - 12));
        auto y2 = avx::loadu((__m256i *)(src1 + n - 8));
        auto y3 = avx::loadu((__m256i *)(src1 + n - 4));

        auto r0 = avx::cmpeq_epi64(x0, y0);
        auto r1 = avx::cmpeq_epi64(x1, y1);
        auto r2 = avx::cmpeq_epi64(x2, y2);
        auto r3 = avx::cmpeq_epi64(x3, y3);

        auto z = avx::And(avx::And(r0, r1), avx::And(r2, r3));

        if (WJR_UNLIKELY(!avx::test_all_ones(z))) {
            avx::mask_type mask = ~avx::movemask_epi8(r3);
            if (mask != 0) {
                auto offset = clz(mask) / 8;
                return src0[n - 1 - offset] < src1[n - 1 - offset] ? -1 : 1;
            }

            mask = ~avx::movemask_epi8(r2);
            if (mask != 0) {
                auto offset = clz(mask) / 8;
                return src0[n - 5 - offset] < src1[n - 5 - offset] ? -1 : 1;
            }

            mask = ~avx::movemask_epi8(r1);
            if (mask != 0) {
                auto offset = clz(mask) / 8;
                return src0[n - 9 - offset] < src1[n - 9 - offset] ? -1 : 1;
            }

            mask = ~avx::movemask_epi8(r0);
            auto offset = clz(mask) / 8;
            return src0[n - 13 - offset] < src1[n - 13 - offset] ? -1 : 1;
        }

        n -= 16;
    } while (WJR_LIKELY(n != 0));
#endif

    return 0;

#undef WJR_REGISTER_REVERSE_COMPARE_NOT_N_AVX
}

extern template WJR_PURE WJR_COLD int
large_builtin_reverse_compare_n<uint64_t>(const uint64_t *src0, const uint64_t *src1,
                                          size_t n);

/**
 * @brief Compare two arrays of uint64_t in reverse order.
 *
 * @details @ref builtin_compare_n in reverse order.
 */
template <typename T>
WJR_INTRINSIC_INLINE int builtin_reverse_compare_n(const T *src0, const T *src1,
                                                   size_t n) {
    if (WJR_UNLIKELY(n == 0)) {
        return 0;
    }

    if (WJR_LIKELY(src0[n - 1] != src1[n - 1])) {
        return src0[n - 1] < src1[n - 1] ? -1 : 1;
    }

    if (n == 1) {
        return 0;
    }

    if (WJR_LIKELY(src0[n - 2] != src1[n - 2])) {
        return src0[n - 2] < src1[n - 2] ? -1 : 1;
    }

    if (n == 2) {
        return 0;
    }

    if (WJR_LIKELY(src0[n - 3] != src1[n - 3])) {
        return src0[n - 3] < src1[n - 3] ? -1 : 1;
    }

    if (n == 3) {
        return 0;
    }

    if (WJR_LIKELY(src0[n - 4] != src1[n - 4])) {
        return src0[n - 4] < src1[n - 4] ? -1 : 1;
    }

    if (n == 4) {
        return 0;
    }

    return large_builtin_reverse_compare_n(src0, src1, n);
}

#endif

// __uint128_t has certain bugs in GCC 13.2, resulting in low performance
#if WJR_HAS_FEATURE(GCC_STYLE_INLINE_ASM)
#define WJR_HAS_BUILTIN___ASM_LESS_128 WJR_HAS_DEF
#define WJR_HAS_BUILTIN___ASM_LESS_EQUAL_128 WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(__ASM_LESS_128)

WJR_CONST WJR_INTRINSIC_INLINE bool __asm_less_128(uint64_t lo0, uint64_t hi0,
                                                   uint64_t lo1, uint64_t hi1) {
    bool ret;
    asm("cmp{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
        "sbb{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t" WJR_ASM_CCSET(b)
        : WJR_ASM_CCOUT(b)(ret), [lo0] "+&r"(lo0), [hi0] "+r"(hi0)
        : [lo1] "r"(lo1), [hi1] "r"(hi1)
        : "cc");
    return ret;
}

#endif

#if WJR_HAS_BUILTIN(__ASM_LESS_EQUAL_128)

WJR_CONST WJR_INTRINSIC_INLINE bool __asm_less_equal_128(uint64_t lo0, uint64_t hi0,
                                                         uint64_t lo1, uint64_t hi1) {
    bool ret;
    asm("cmp{q %[lo0], %[lo1]| %[lo1], %[lo0]}\n\t"
        "sbb{q %[hi0], %[hi1]| %[hi1], %[hi0]}\n\t" WJR_ASM_CCSET(ae)
        : WJR_ASM_CCOUT(ae)(ret), [lo1] "+&r"(lo1), [hi1] "+r"(hi1)
        : [lo0] "r"(lo0), [hi0] "r"(hi0)
        : "cc");
    return ret;
}

#endif

} // namespace wjr

#endif // WJR_X86_MATH_COMPARE_HPP__
#endif

namespace wjr {

template <typename T>
WJR_INTRINSIC_CONSTEXPR int fallback_compare_n(const T *src0, const T *src1, size_t n) {
    for (size_t idx = 0; idx < n; ++idx) {
        if (src0[idx] != src1[idx]) {
            return src0[idx] < src1[idx] ? -1 : 1;
        }
    }

    return 0;
}

template <typename T>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E int compare_n(const T *src0, const T *src1, size_t n) {
    if (WJR_BUILTIN_CONSTANT_P(src0 == src1) && src0 == src1) {
        return 0;
    }

#if WJR_HAS_BUILTIN(COMPARE_N)
    if constexpr (sizeof(T) == 8) {
        static_assert(sizeof(T) != 8 || std::is_unsigned_v<T>, "T must be unsigned if sizeof(T) == 8");

        if (is_constant_evaluated()) {
            return fallback_compare_n(src0, src1, n);
        }

        return builtin_compare_n(src0, src1, n);
    } else {
        return fallback_compare_n(src0, src1, n);
    }
#else
    return fallback_compare_n(src0, src1, n);
#endif
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR int fallback_reverse_compare_n(const T *src0, const T *src1,
                                                       size_t n) {
    src0 += n;
    src1 += n;

    for (size_t idx = 0; idx < n; ++idx) {
        if (src0[-1 - idx] != src1[-1 - idx]) {
            return src0[-1 - idx] < src1[-1 - idx] ? -1 : 1;
        }
    }

    return 0;
}

template <typename T>
WJR_PURE WJR_INTRINSIC_CONSTEXPR_E int reverse_compare_n(const T *src0, const T *src1,
                                                         size_t n) {
    if (WJR_BUILTIN_CONSTANT_P(src0 == src1) && src0 == src1) {
        return 0;
    }

#if WJR_HAS_BUILTIN(COMPARE_N)
    if constexpr (sizeof(T) == 8) {
        static_assert(sizeof(T) != 8 || std::is_unsigned_v<T>, "T must be unsigned if sizeof(T) == 8");
        
        if (is_constant_evaluated()) {
            return fallback_reverse_compare_n(src0, src1, n);
        }

        return builtin_reverse_compare_n(src0, src1, n);
    } else {
        return fallback_reverse_compare_n(src0, src1, n);
    }
#else
    return fallback_reverse_compare_n(src0, src1, n);
#endif
}

#if WJR_HAS_FEATURE(FAST_INT128_COMPARE)
#define WJR_HAS_BUILTIN___BUILTIN_LESS_128 WJR_HAS_DEF
#define WJR_HAS_BUILTIN___BUILTIN_LESS_EQUAL_128 WJR_HAS_DEF
#endif

WJR_INTRINSIC_CONSTEXPR_E bool __fallback_less_128(uint64_t lo0, uint64_t hi0,
                                                   uint64_t lo1, uint64_t hi1) {
    uint8_t f = lo0 < lo1;
    (void)subc_cc(hi0, hi1, f, f);
    return f;
}

#if WJR_HAS_BUILTIN(__BUILTIN_LESS_128)

WJR_INTRINSIC_INLINE bool __builtin_less_128(uint64_t lo0, uint64_t hi0, uint64_t lo1,
                                             uint64_t hi1) {
    const auto x0 = static_cast<__uint128_t>(hi0) << 64 | lo0;
    const auto x1 = static_cast<__uint128_t>(hi1) << 64 | lo1;

    return x0 < x1;
}

#endif

// return <hi0, lo0> < <hi1, lo1>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E bool __less_128(uint64_t lo0, uint64_t hi0,
                                                    uint64_t lo1, uint64_t hi1) {
#if WJR_HAS_BUILTIN(__BUILTIN_LESS_128) || WJR_HAS_BUILTIN(__ASM_LESS_128)
    if (is_constant_evaluated()) {
        return __fallback_less_128(lo0, hi0, lo1, hi1);
    }

    return WJR_PP_BOOL_IF(WJR_HAS_BUILTIN(__BUILTIN_LESS_128), __builtin_less_128,
                          __asm_less_128)(lo0, hi0, lo1, hi1);
#else
    return __fallback_less_128(lo0, hi0, lo1, hi1);
#endif
}

WJR_INTRINSIC_CONSTEXPR_E bool __fallback_less_equal_128(uint64_t lo0, uint64_t hi0,
                                                         uint64_t lo1, uint64_t hi1) {
    return !__less_128(lo1, hi1, lo0, hi0);
}

#if WJR_HAS_BUILTIN(__BUILTIN_LESS_EQUAL_128)

WJR_INTRINSIC_INLINE bool __builtin_less_equal_128(uint64_t lo0, uint64_t hi0,
                                                   uint64_t lo1, uint64_t hi1) {
    const auto x0 = static_cast<__uint128_t>(hi0) << 64 | lo0;
    const auto x1 = static_cast<__uint128_t>(hi1) << 64 | lo1;

    return x0 <= x1;
}

#endif

// return <hi0, lo0> < <hi1, lo1>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E bool __less_equal_128(uint64_t lo0, uint64_t hi0,
                                                          uint64_t lo1, uint64_t hi1) {
#if WJR_HAS_BUILTIN(__BUILTIN_LESS_EQUAL_128) || WJR_HAS_BUILTIN(__ASM_LESS_EQUAL_128)
    if (is_constant_evaluated()) {
        return __fallback_less_equal_128(lo0, hi0, lo1, hi1);
    }

    return WJR_PP_BOOL_IF(WJR_HAS_BUILTIN(__BUILTIN_LESS_EQUAL_128),
                          __builtin_less_equal_128,
                          __asm_less_equal_128)(lo0, hi0, lo1, hi1);
#else
    return __fallback_less_equal_128(lo0, hi0, lo1, hi1);
#endif
}

WJR_CONST WJR_INTRINSIC_CONSTEXPR_E bool __greater_128(uint64_t lo0, uint64_t hi0,
                                                       uint64_t lo1, uint64_t hi1) {
    return __less_128(lo1, hi1, lo0, hi0);
}

WJR_CONST WJR_INTRINSIC_CONSTEXPR_E bool __greater_equal_128(uint64_t lo0, uint64_t hi0,
                                                             uint64_t lo1, uint64_t hi1) {
    return __less_equal_128(lo1, hi1, lo0, hi0);
}

} // namespace wjr

#endif // WJR_MATH_CMP_HPP__
#ifndef WJR_MATH_DIV_IMPL_HPP__
#define WJR_MATH_DIV_IMPL_HPP__

#include <utility>

#ifndef WJR_TUPLE_HPP__
#define WJR_TUPLE_HPP__

#include <tuple>

// Already included

namespace wjr {

template <typename... Args>
class tuple;

} // namespace wjr

namespace std {

template <typename... Args>
struct tuple_size<wjr::tuple<Args...>> : std::integral_constant<size_t, sizeof...(Args)> {
};

template <size_t I, typename... Args>
struct tuple_element<I, wjr::tuple<Args...>> {
    using type = wjr::tp_at_t<wjr::tuple<Args...>, I>;
};

template <typename... Args, WJR_REQUIRES(std::conjunction_v<wjr::is_swappable<Args>...>)>
constexpr void swap(wjr::tuple<Args...> &lhs,
                    wjr::tuple<Args...> &rhs) noexcept(noexcept(lhs.swap(rhs)));

template <size_t I, typename... Args>
constexpr tuple_element_t<I, wjr::tuple<Args...>> &get(wjr::tuple<Args...> &t) noexcept;

template <size_t I, typename... Args>
constexpr tuple_element_t<I, wjr::tuple<Args...>> &
get(const wjr::tuple<Args...> &t) noexcept;

template <size_t I, typename... Args>
constexpr tuple_element_t<I, wjr::tuple<Args...>> &&get(wjr::tuple<Args...> &&t) noexcept;

template <size_t I, typename... Args>
constexpr tuple_element_t<I, wjr::tuple<Args...>> &&
get(const wjr::tuple<Args...> &&t) noexcept;

template <typename T, typename... Args>
constexpr T &get(wjr::tuple<Args...> &t) noexcept;

template <typename T, typename... Args>
constexpr T &get(const wjr::tuple<Args...> &t) noexcept;

template <typename T, typename... Args>
constexpr T &&get(wjr::tuple<Args...> &&t) noexcept;

template <typename T, typename... Args>
constexpr T &&get(const wjr::tuple<Args...> &&t) noexcept;

} // namespace std

namespace wjr {

template <typename Indexs, typename... Args>
class tuple_impl;

template <size_t... Indexs, typename... Args>
class WJR_EMPTY_BASES tuple_impl<std::index_sequence<Indexs...>, Args...>
    : capture_leaf<std::tuple_element_t<Indexs, tuple<Args...>>,
                   enable_base_identity_t<
                       Indexs, tuple_impl<std::index_sequence<Indexs...>, Args...>>>...,
      enable_special_members_of_args_base<
          tuple_impl<std::index_sequence<Indexs...>, Args...>,
          capture_leaf<
              std::tuple_element_t<Indexs, tuple<Args...>>,
              enable_base_identity_t<
                  Indexs, tuple_impl<std::index_sequence<Indexs...>, Args...>>>...> {
    using Sequence = std::index_sequence<Indexs...>;

    template <size_t Idx>
    using Mybase = capture_leaf<std::tuple_element_t<Idx, tuple<Args...>>,
                                enable_base_identity_t<Idx, tuple_impl>>;

    using Mybase2 = enable_special_members_of_args_base<
        tuple_impl<std::index_sequence<Indexs...>, Args...>,
        capture_leaf<std::tuple_element_t<Indexs, tuple<Args...>>,
                     enable_base_identity_t<Indexs, tuple_impl>>...>;

    constexpr static size_t Size = sizeof...(Args);

public:
    template <typename S = Sequence,
              WJR_REQUIRES(
                  std::conjunction_v<std::is_same<S, Sequence>,
                                     std::is_default_constructible<Mybase<Indexs>>...>)>
    constexpr tuple_impl() noexcept(
        std::conjunction_v<std::is_nothrow_constructible<Args>...>)
        : Mybase2(enable_default_constructor) {}

    template <size_t... _Indexs, typename... _Args,
              WJR_REQUIRES(
                  std::conjunction_v<std::is_constructible<Mybase<_Indexs>, _Args>...>)>
    constexpr tuple_impl(std::index_sequence<_Indexs...>, _Args &&...args) noexcept(
        std::conjunction_v<std::is_nothrow_constructible<Args, _Args &&>...>)
        : Mybase<_Indexs>(std::forward<_Args>(args))...,
          Mybase2(enable_default_constructor) {}

    template <size_t I>
    constexpr auto &get() & noexcept {
        return Mybase<I>::get();
    }

    template <size_t I>
    constexpr const auto &get() const & noexcept {
        return Mybase<I>::get();
    }

    template <size_t I>
    constexpr auto &&get() && noexcept {
        return std::move(Mybase<I>::get());
    }

    template <size_t I>
    constexpr const auto &&get() const && noexcept {
        return std::move(Mybase<I>::get());
    }
};

template <typename Tuple>
struct __tuple_like;

template <template <typename...> typename Tuple, typename... Args>
struct __tuple_like<Tuple<Args...>>
    : std::disjunction<std::is_same<Tuple<Args...>, std::tuple<Args...>>,
                       std::is_same<Tuple<Args...>, std::pair<Args...>>> {};

template <>
class tuple<> {
public:
    constexpr tuple() noexcept = default;
    constexpr tuple(const tuple &) noexcept = default;
    constexpr tuple(tuple &&) noexcept = default;
    constexpr tuple &operator=(const tuple &) noexcept = default;
    constexpr tuple &operator=(tuple &&) noexcept = default;
    ~tuple() noexcept = default;

    constexpr void swap(tuple &) noexcept {}
};

template <typename This, typename... Args>
class tuple<This, Args...>
    : enable_special_members_of_args_base<
          tuple<This, Args...>,
          tuple_impl<std::index_sequence_for<This, Args...>, This, Args...>> {
    using Sequence = std::index_sequence_for<This, Args...>;
    using Impl = tuple_impl<Sequence, This, Args...>;
    using Mybase = enable_special_members_of_args_base<tuple<This, Args...>, Impl>;

    constexpr static size_t Size = sizeof...(Args) + 1;

public:
    template <typename T = This,
              WJR_REQUIRES(std::conjunction_v<std::is_default_constructible<T>,
                                              std::is_default_constructible<Args>...>
                               &&std::conjunction_v<is_default_convertible<T>,
                                                    is_default_convertible<Args>...>)>
    constexpr tuple() noexcept(std::is_nothrow_constructible_v<Impl>)
        : Mybase(enable_default_constructor), m_impl() {}

    template <typename T = This,
              WJR_REQUIRES(std::conjunction_v<std::is_default_constructible<T>,
                                              std::is_default_constructible<Args>...> &&
                           !std::conjunction_v<is_default_convertible<T>,
                                               is_default_convertible<Args>...>)>
    constexpr explicit tuple() noexcept(std::is_nothrow_constructible_v<Impl>)
        : Mybase(enable_default_constructor), m_impl() {}

    template <typename Other = This,
              WJR_REQUIRES(std::is_constructible_v<Impl, Sequence, const Other &,
                                                   const Args &...>)>
    constexpr tuple(const Other &first, const Args &...rest) noexcept(
        std::is_nothrow_constructible_v<Impl, Sequence, const Other &, const Args &...>)
        : Mybase(enable_default_constructor), m_impl(Sequence(), first, rest...) {}

    template <
        typename Other, typename... _Args,
        WJR_REQUIRES(sizeof...(_Args) + 1 == Size &&
                     std::conjunction_v<
                         std::negation<std::conjunction<
                             std::is_same<This, std::remove_reference_t<Other>>,
                             std::is_same<Args, std::remove_reference_t<_Args>>...>>,
                         std::is_constructible<Impl, Sequence, Other &&, _Args &&...>>)>
    constexpr tuple(Other &&other, _Args &&...args) noexcept(
        std::is_nothrow_constructible_v<Impl, Sequence, Other &&, _Args &&...>)
        : Mybase(enable_default_constructor),
          m_impl(Sequence(), std::forward<Other>(other), std::forward<_Args>(args)...) {}

private:
    template <size_t... _Indexs, typename TupleLike>
    constexpr tuple(
        std::index_sequence<_Indexs...>, TupleLike &&other,
        in_place_empty_t) noexcept(std::
                                       is_nothrow_constructible_v<
                                           Impl, Sequence,
                                           decltype(std::get<_Indexs>(
                                               std::forward<TupleLike>(other)))...>)
        : Mybase(enable_default_constructor),
          m_impl(Sequence(), std::get<_Indexs>(std::forward<TupleLike>(other))...) {}

public:
    template <typename TupleLike,
              WJR_REQUIRES(__is_tuple_test_v<std::is_constructible, tuple, TupleLike &&>)>
    constexpr tuple(TupleLike &&other) noexcept(
        noexcept(tuple(Sequence(), std::forward<TupleLike>(other), in_place_empty)))
        : tuple(Sequence(), std::forward<TupleLike>(other), in_place_empty) {}

private:
    template <size_t... _Indexs, typename Container>
    constexpr void __assign(std::index_sequence<_Indexs...>, Container &&other) noexcept(
        noexcept(((this->template get<_Indexs>() =
                       std::get<_Indexs>(std::forward<Container>(other))),
                  ...))) {
        ((this->template get<_Indexs>() =
              std::get<_Indexs>(std::forward<Container>(other))),
         ...);
    }

public:
    template <typename TupleLike,
              WJR_REQUIRES(__is_tuple_test_v<__is_tuple_assignable, tuple, TupleLike &&>)>
    constexpr tuple &operator=(TupleLike &&other) noexcept(
        noexcept(__assign(Sequence(), std::forward<TupleLike>(other)))) {
        __assign(Sequence(), std::forward<TupleLike>(other));
        return *this;
    }

private:
    template <size_t... _Indexs>
    constexpr void
    __swap(std::index_sequence<_Indexs...>, tuple &other) noexcept(noexcept(((
        std::swap(this->template get<_Indexs>(), other.template get<_Indexs>()), ...)))) {
        ((std::swap(this->template get<_Indexs>(), other.template get<_Indexs>()), ...));
    }

public:
    constexpr void swap(tuple &other) noexcept(noexcept(__swap(Sequence(), other))) {
        __swap(Sequence(), other);
    }

    template <size_t I>
    constexpr std::tuple_element_t<I, tuple> &get() & noexcept {
        return m_impl.template get<I>();
    }

    template <size_t I>
    constexpr const std::tuple_element_t<I, tuple> &get() const & noexcept {
        return m_impl.template get<I>();
    }

    template <size_t I>
    constexpr std::tuple_element_t<I, tuple> &&get() && noexcept {
        return std::move(m_impl.template get<I>());
    }

    template <size_t I>
    constexpr const std::tuple_element_t<I, tuple> &&get() const && noexcept {
        return std::move(m_impl.template get<I>());
    }

private:
    Impl m_impl;
};

template <typename... Args>
tuple(Args...) -> tuple<Args...>;

template <typename T1, typename T2>
tuple(std::pair<T1, T2>) -> tuple<T1, T2>;

template <typename... Args>
tuple(std::tuple<Args...>) -> tuple<Args...>;

template <typename... Args>
constexpr tuple<unref_wrapper_t<Args>...> make_tuple(Args &&...args) noexcept(
    std::conjunction_v<
        std::is_nothrow_constructible<unref_wrapper_t<Args>, Args &&>...>) {
    return tuple<unref_wrapper_t<Args>...>(std::forward<Args>(args)...);
}

template <typename... Args>
constexpr tuple<Args &...> tie(Args &...args) noexcept {
    return tuple<Args &...>(args...);
}

template <typename... Args>
constexpr tuple<Args &&...> forward_as_tuple(Args &&...args) noexcept(
    std::conjunction_v<std::is_nothrow_constructible<Args &&, Args &&>...>) {
    return tuple<Args &&...>(std::forward<Args>(args)...);
}

/// @private
template <typename Func, typename Tuple, size_t... Indexs>
constexpr decltype(auto)
apply_impl(Func &&fn, Tuple &&tp, std::index_sequence<Indexs...>) noexcept(noexcept(
    std::invoke(std::forward<Func>(fn), std::get<Indexs>(std::forward<Tuple>(tp))...))) {
    return std::invoke(std::forward<Func>(fn),
                       std::get<Indexs>(std::forward<Tuple>(tp))...);
}

template <typename Func, typename Tuple>
constexpr decltype(auto) apply(Func &&fn, Tuple &&tp) noexcept(noexcept(
    apply_impl(std::forward<Func>(fn), std::forward<Tuple>(tp),
               std::make_index_sequence<std::tuple_size_v<remove_cvref_t<Tuple>>>{}))) {
    return apply_impl(
        std::forward<Func>(fn), std::forward<Tuple>(tp),
        std::make_index_sequence<std::tuple_size_v<remove_cvref_t<Tuple>>>{});
}

/// @private
template <size_t I, typename Tuple>
struct __tuple_cat_single_helper {
    static constexpr size_t Size = std::tuple_size_v<Tuple>;
    using type0 = tp_repeat_t<tp_list<std::integral_constant<size_t, I>>, Size>;
    using type1 = tp_make_index_sequence<Size>;
};

/// @private
template <typename S, typename... Tuples>
struct __tuple_cat_helper_impl;

/// @private
template <size_t... Indexs, typename... Tuples>
struct __tuple_cat_helper_impl<std::index_sequence<Indexs...>, Tuples...> {
    using type0 =
        tp_concat_t<typename __tuple_cat_single_helper<Indexs, Tuples>::type0...>;
    using type1 =
        tp_concat_t<typename __tuple_cat_single_helper<Indexs, Tuples>::type1...>;
};

/// @private
template <typename... Tuples>
struct __tuple_cat_helper {
    using Sequence = std::index_sequence_for<Tuples...>;
    using Impl = __tuple_cat_helper_impl<Sequence, Tuples...>;
    using type0 = tp_make_std_index_sequence<typename Impl::type0>;
    using type1 = tp_make_std_index_sequence<typename Impl::type1>;
};

/// @private
template <size_t... I0, size_t... I1, typename... Tuples>
constexpr decltype(auto) __tuple_cat_impl(std::index_sequence<I0...>,
                                          std::index_sequence<I1...>,
                                          tuple<Tuples...> &&tuples) {
    return tuple(std::get<I1>(std::get<I0>(std::move(tuples)))...);
}

template <typename... Tuples>
constexpr decltype(auto) tuple_cat(Tuples &&...tuples) {
    using Helper = __tuple_cat_helper<remove_cvref_t<Tuples>...>;
    return __tuple_cat_impl(typename Helper::type0{}, typename Helper::type1{},
                            forward_as_tuple(std::forward<Tuples>(tuples)...));
}

template <typename... TArgs, typename... UArgs>
constexpr bool
operator==(const tuple<TArgs...> &lhs, const tuple<UArgs...> &rhs) noexcept(
    std::conjunction_v<has_noexcept_equal_to<const TArgs &, const UArgs &>...>) {
    return apply(
        [&rhs](const auto &...lhs_args) {
            return apply(
                [&lhs_args...](const auto &...rhs_args) {
                    return ((lhs_args == rhs_args) && ...);
                },
                rhs);
        },
        lhs);
}

template <typename... TArgs, typename... UArgs>
constexpr bool operator!=(const tuple<TArgs...> &lhs,
                          const tuple<UArgs...> &rhs) noexcept(noexcept(lhs == rhs)) {
    return !(lhs == rhs);
}

template <typename... TArgs, typename... UArgs>
constexpr bool operator<(const tuple<TArgs...> &lhs, const tuple<UArgs...> &rhs) noexcept(
    std::conjunction_v<
        std::conjunction<has_noexcept_less<const TArgs &, const UArgs &>,
                         has_noexcept_less<const UArgs &, const TArgs &>>...>) {
    bool ret = false;
    apply(
        [&rhs, &ret](const auto &...lhs_args) {
            return apply(
                [&lhs_args..., &ret](const auto &...rhs_args) {
                    (void)((lhs_args < rhs_args ? (ret = true, false)
                                                : (rhs_args < lhs_args ? false : true)) &&
                           ...);
                },
                rhs);
        },
        lhs);
    return ret;
}

template <typename... TArgs, typename... UArgs>
constexpr bool operator<=(const tuple<TArgs...> &lhs,
                          const tuple<UArgs...> &rhs) noexcept(noexcept(rhs < lhs)) {
    return !(rhs < lhs);
}

template <typename... TArgs, typename... UArgs>
constexpr bool operator>(const tuple<TArgs...> &lhs,
                         const tuple<UArgs...> &rhs) noexcept(noexcept(rhs < lhs)) {
    return rhs < lhs;
}

template <typename... TArgs, typename... UArgs>
constexpr bool operator>=(const tuple<TArgs...> &lhs,
                          const tuple<UArgs...> &rhs) noexcept(noexcept(lhs < rhs)) {
    return !(lhs < rhs);
}

template <size_t I, typename... Args>
struct __in_place_index_tuple_t_tag {};

template <size_t I, typename... Args>
using in_place_index_tuple_t =
    capture_leaf<tuple<Args...>, __in_place_index_tuple_t_tag<I, Args...>>;

template <size_t I, typename... Args>
constexpr in_place_index_tuple_t<I, Args &&...>
in_place_index_tuple(Args &&...args) noexcept(
    std::conjunction_v<std::is_nothrow_constructible<Args &&, Args &&>...>) {
    return in_place_index_tuple_t<I, Args &&...>(std::forward<Args>(args)...);
}

template <typename T, typename... Args>
struct __in_place_type_tuple_t_tag {};

template <typename T, typename... Args>
using in_place_type_tuple_t =
    capture_leaf<tuple<Args...>, __in_place_type_tuple_t_tag<T, Args...>>;

template <typename T, typename... Args>
constexpr in_place_type_tuple_t<T, Args &&...>
in_place_type_tuple(Args &&...args) noexcept(
    std::conjunction_v<std::is_nothrow_constructible<Args &&, Args &&>...>) {
    return in_place_type_tuple_t<T, Args &&...>(std::forward<Args>(args)...);
}

} // namespace wjr

namespace std {

template <typename... Args,
          WJR_REQUIRES_I(std::conjunction_v<wjr::is_swappable<Args>...>)>
constexpr void swap(wjr::tuple<Args...> &lhs,
                    wjr::tuple<Args...> &rhs) noexcept(noexcept(lhs.swap(rhs))) {
    lhs.swap(rhs);
}

template <size_t I, typename... Args>
constexpr tuple_element_t<I, wjr::tuple<Args...>> &get(wjr::tuple<Args...> &t) noexcept {
    return t.template get<I>();
}

template <size_t I, typename... Args>
constexpr tuple_element_t<I, wjr::tuple<Args...>> &
get(const wjr::tuple<Args...> &t) noexcept {
    return t.template get<I>();
}

template <size_t I, typename... Args>
constexpr tuple_element_t<I, wjr::tuple<Args...>> &&
get(wjr::tuple<Args...> &&t) noexcept {
    return std::move(t).template get<I>();
}

template <size_t I, typename... Args>
constexpr tuple_element_t<I, wjr::tuple<Args...>> &&
get(const wjr::tuple<Args...> &&t) noexcept {
    return std::move(t).template get<I>();
}

template <typename T, typename... Args>
constexpr T &get(wjr::tuple<Args...> &t) noexcept {
    return get<wjr::tp_find_v<wjr::tuple<Args...>, T>>(t);
}

template <typename T, typename... Args>
constexpr T &get(const wjr::tuple<Args...> &t) noexcept {
    return get<wjr::tp_find_v<wjr::tuple<Args...>, T>>(t);
}

template <typename T, typename... Args>
constexpr T &&get(wjr::tuple<Args...> &&t) noexcept {
    return get<wjr::tp_find_v<wjr::tuple<Args...>, T>>(std::move(t));
}

template <typename T, typename... Args>
constexpr T &&get(const wjr::tuple<Args...> &&t) noexcept {
    return get<wjr::tp_find_v<wjr::tuple<Args...>, T>>(std::move(t));
}

} // namespace std

#endif // WJR_TUPLE_HPP__
// Already included

namespace wjr {

template <typename T>
class div2by1_divider;

template <typename T>
class div3by2_divider;

template <typename T>
class divexact1_divider;

inline uint64_t div128by64to64(uint64_t &rem, uint64_t lo, uint64_t hi,
                               const div2by1_divider<uint64_t> &divider);

inline uint64_t div128by64to64(uint64_t &rem, uint64_t lo, uint64_t hi, uint64_t div);

inline tuple<uint64_t, uint64_t>
div128by64to128(uint64_t &rem, uint64_t lo, uint64_t hi,
                const div2by1_divider<uint64_t> &divider);

inline tuple<uint64_t, uint64_t> div128by64to128(uint64_t &rem, uint64_t lo, uint64_t hi,
                                                 uint64_t div);

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_CONSTEXPR20 void div_qr_1(T *dst, T &rem, const T *src, size_t n,
                                        const div2by1_divider<T> &div);

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_CONSTEXPR20 void div_qr_1(T *dst, T &rem, const T *src, size_t n,
                                        type_identity_t<T> div);

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_CONSTEXPR20 void div_qr_2(T *dst, T *rem, const T *src, size_t n,
                                        const div3by2_divider<T> &div);

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_CONSTEXPR20 void div_qr_2(T *dst, T *rem, const T *src, size_t n,
                                        const T *div);

template <typename T>
void div_qr_s(T *dst, T *rem, const T *src, size_t n, const T *div, size_t m);

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_CONSTEXPR_E T divexact_dbm1c(T *dst, const T *src, size_t n, T bd, T h);

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_CONSTEXPR_E void divexact_by3(T *dst, const T *src, size_t n);

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_CONSTEXPR_E void divexact_by5(T *dst, const T *src, size_t n);

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_CONSTEXPR_E void divexact_by15(T *dst, const T *src, size_t n);

template <typename T, T c, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_CONSTEXPR_E void divexact_byc(T *dst, const T *src, size_t n,
                                  std::integral_constant<T, c>);

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_CONSTEXPR_E void divexact_1(T *dst, const T *src, size_t n,
                                          const divexact1_divider<T> &div);

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_CONSTEXPR_E void divexact_1(T *dst, const T *src, size_t n,
                                          type_identity_t<T> div);

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_PURE WJR_CONSTEXPR20 T mod_1(const T *src, size_t n, div2by1_divider<T> div);

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_PURE WJR_CONSTEXPR20 T mod_1(const T *src, size_t n, type_identity_t<T> div);

} // namespace wjr

#endif // WJR_MATH_DIV_IMPL_HPP__
#ifndef WJR_MATH_DIVIDER_HPP__
#define WJR_MATH_DIVIDER_HPP__

#ifndef WJR_MATH_MUL_HPP__
#define WJR_MATH_MUL_HPP__

/**
 * @todo optimize temporary memory usage of mul_s, mul_n, sqr
 *
 */

#ifndef WJR_MATH_BIGNUM_CONFIG_HPP__
#define WJR_MATH_BIGNUM_CONFIG_HPP__

#ifndef WJR_TOOM22_MUL_THRESHOLD
#define WJR_TOOM22_MUL_THRESHOLD 26
#endif

#ifndef WJR_TOOM33_MUL_THRESHOLD
#define WJR_TOOM33_MUL_THRESHOLD 72
#endif

#ifndef WJR_TOOM44_MUL_THRESHOLD
#define WJR_TOOM44_MUL_THRESHOLD 208
#endif

#ifndef WJR_TOOM32_TO_TOOM43_MUL_THRESHOLD
#define WJR_TOOM32_TO_TOOM43_MUL_THRESHOLD 73
#endif

#ifndef WJR_TOOM32_TO_TOOM53_MUL_THRESHOLD
#define WJR_TOOM32_TO_TOOM53_MUL_THRESHOLD 153
#endif

#ifndef WJR_TOOM42_TO_TOOM53_MUL_THRESHOLD
#define WJR_TOOM42_TO_TOOM53_MUL_THRESHOLD 137
#endif

#ifndef WJR_TOOM2_SQR_THRESHOLD
#define WJR_TOOM2_SQR_THRESHOLD 32
#endif

#ifndef WJR_TOOM3_SQR_THRESHOLD
#define WJR_TOOM3_SQR_THRESHOLD 117
#endif

#ifndef WJR_TOOM4_SQR_THRESHOLD
#define WJR_TOOM4_SQR_THRESHOLD 336
#endif

#ifndef WJR_DC_DIV_QR_THRESHOLD
#define WJR_DC_DIV_QR_THRESHOLD (WJR_TOOM22_MUL_THRESHOLD * 2)
#endif // WJR_DC_DIV_QR_THRESHOLD

#ifndef WJR_DC_BIGNUM_TO_CHARS_THRESHOLD
#define WJR_DC_BIGNUM_TO_CHARS_THRESHOLD 20
#endif

#ifndef WJR_DC_BIGNUM_TO_CHARS_PRECOMPUTE_THRESHOLD
#define WJR_DC_BIGNUM_TO_CHARS_PRECOMPUTE_THRESHOLD 20
#endif

#ifndef WJR_DC_BIGNUM_FROM_CHARS_THRESHOLD
#define WJR_DC_BIGNUM_FROM_CHARS_THRESHOLD 1670
#endif

#ifndef WJR_DC_BIGNUM_FROM_CHARS_PRECOMPUTE_THRESHOLD
#define WJR_DC_BIGNUM_FROM_CHARS_PRECOMPUTE_THRESHOLD 3105
#endif

#endif // WJR_MATH_BIGNUM_CONFIG_HPP__
// Already included

#ifndef WJR_MATH_ADD_HPP__
#define WJR_MATH_ADD_HPP__

// Already included
#ifndef WJR_MATH_ADD_IMPL_HPP__
#define WJR_MATH_ADD_IMPL_HPP__

// Already included

namespace wjr {

template <typename T, typename U,
          WJR_REQUIRES(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E T addc(T a, T b, type_identity_t<U> c_in,
                                               U &c_out);

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E T addc_cc(T a, T b, uint8_t c_in, uint8_t &c_out);

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E bool add_overflow(type_identity_t<T> a,
                                                          type_identity_t<T> b, T &ret);

template <typename T, typename U = T,
          WJR_REQUIRES(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E U addc_1(T *dst, const T *src0, size_t n,
                                                 type_identity_t<T> src1, U c_in = 0);

template <typename T, typename U = T,
          WJR_REQUIRES(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E U addc_n(T *dst, const T *src0, const T *src1,
                                                 size_t n, U c_in = 0);

template <typename T, typename U = T,
          WJR_REQUIRES(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E U addc_s(T *dst, const T *src0, size_t n,
                                                 const T *src1, size_t m, U c_in = 0);

// m can be zero
template <typename T, typename U = T,
          WJR_REQUIRES(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E U addc_sz(T *dst, const T *src0, size_t n,
                                                  const T *src1, size_t m, U c_in = 0);

WJR_INTRINSIC_CONSTEXPR_E void __add_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                         uint64_t hi0, uint64_t lo1, uint64_t hi1);

WJR_INTRINSIC_CONSTEXPR_E uint64_t __addc_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                              uint64_t hi0, uint64_t lo1, uint64_t hi1,
                                              uint64_t c_in);

WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E uint8_t __addc_cc_128(uint64_t &al, uint64_t &ah,
                                                              uint64_t lo0, uint64_t hi0,
                                                              uint64_t lo1, uint64_t hi1,
                                                              uint8_t c_in);

} // namespace wjr

#endif // WJR_MATH_ADD_IMPL_HPP__
// Already included

#if defined(WJR_X86)
#ifndef WJR_X86_MATH_ADD_HPP__
#define WJR_X86_MATH_ADD_HPP__

// Already included

#ifndef WJR_X86
#error "x86 required"
#endif

namespace wjr {

#if WJR_HAS_FEATURE(GCC_STYLE_INLINE_ASM)
#define WJR_HAS_BUILTIN_ASM_ADDC WJR_HAS_DEF
#define WJR_HAS_BUILTIN_ASM_ADDC_N WJR_HAS_DEF
#define WJR_HAS_BUILTIN___ASM_ADD_128 WJR_HAS_DEF
#define WJR_HAS_BUILTIN___ASM_ADDC_128 WJR_HAS_DEF

#if WJR_HAS_FEATURE(INLINE_ASM_CCCOND)
#define WJR_HAS_BUILTIN_ASM_ADDC_CC WJR_HAS_DEF
#define WJR_HAS_BUILTIN___ASM_ADDC_CC_128 WJR_HAS_DEF
#endif

#endif

#if WJR_HAS_BUILTIN(ASM_ADDC)

/**
 * @brief Use inline assembly to add two 64-bit integers with carry-in and return the
 * carry-out.
 *
 * @details The carry-in and carry-out flags are both 0 or 1. \n
 * The carry-out flag is set to 1 if the result overflows. \n
 * Optimization: \n
 * 1. Use constraint "i" if a or b is a constant and is in i32 range. \n
 * 2. If c_in is a constant and c_in == 1, use "stc" to set the carry flag.
 *
 * @tparam U The type of the carry.
 * @param[in] c_in The carry-in flag.
 * @param[out] c_out The carry-out flag.
 * @return a + b + c_in
 */
template <typename U>
WJR_INTRINSIC_INLINE uint64_t asm_addc(uint64_t a, uint64_t b, U c_in, U &c_out) {
    if (WJR_BUILTIN_CONSTANT_P(c_in == 1) && c_in == 1) {
        if (WJR_BUILTIN_CONSTANT_P(b) && __is_in_i32_range(b)) {
            asm("stc\n\t"
                "adc{q %2, %0| %0, %2}\n\t"
                "setb %b1"
                : "=r"(a), "+r"(c_in)
                : "ri"(b), "0"(a)
                : "cc");
        } else {
            asm("stc\n\t"
                "adc{q %2, %0| %0, %2}\n\t"
                "setb %b1"
                : "=r"(a), "+r"(c_in)
                : "r"(b), "0"(a)
                : "cc");
        }
        c_out = c_in;
        return a;
    }

    if (WJR_BUILTIN_CONSTANT_P(a)) {
        if (__is_in_i32_range(a)) {
            asm("add{b $255, %b1| %b1, 255}\n\t"
                "adc{q %2, %0| %0, %2}\n\t"
                "setb %b1"
                : "=r"(b), "+&r"(c_in)
                : "ri"(a), "0"(b)
                : "cc");
        } else {
            asm("add{b $255, %b1| %b1, 255}\n\t"
                "adc{q %2, %0| %0, %2}\n\t"
                "setb %b1"
                : "=r"(b), "+&r"(c_in)
                : "r"(a), "0"(b)
                : "cc");
        }
        c_out = c_in;
        return b;
    }

    if (WJR_BUILTIN_CONSTANT_P(b) && __is_in_i32_range(b)) {
        asm("add{b $255, %b1| %b1, 255}\n\t"
            "adc{q %2, %0| %0, %2}\n\t"
            "setb %b1"
            : "=r"(a), "+&r"(c_in)
            : "ri"(b), "0"(a)
            : "cc");
    } else {
        asm("add{b $255, %b1| %b1, 255}\n\t"
            "adc{q %2, %0| %0, %2}\n\t"
            "setb %b1"
            : "=r"(a), "+&r"(c_in)
            : "r"(b), "0"(a)
            : "cc");
    }
    c_out = c_in;
    return a;
}

#endif

#if WJR_HAS_BUILTIN(ASM_ADDC_CC)

/**
 * @brief Use inline assembly to add two 64-bit integers with carry-in and return the
 * carry-out.
 *
 * @details Similar to asm_addc, but the carry-out flag is set by using constraint
 * "=@cccond" instead of "setb". \n
 *
 * @param[in] c_in
 * @param[out] c_out
 */
WJR_INTRINSIC_INLINE uint64_t asm_addc_cc(uint64_t a, uint64_t b, uint8_t c_in,
                                          uint8_t &c_out) {
    if (WJR_BUILTIN_CONSTANT_P(c_in == 1) && c_in == 1) {
        if (WJR_BUILTIN_CONSTANT_P(b) && __is_in_i32_range(b)) {
            asm("stc\n\t"
                "adc{q %2, %0| %0, %2}\n\t" WJR_ASM_CCSET(c)
                : "=r"(a), WJR_ASM_CCOUT(c)(c_out)
                : "ri"(b), "0"(a)
                : "cc");
        } else {
            asm("stc\n\t"
                "adc{q %2, %0| %0, %2}\n\t" WJR_ASM_CCSET(c)
                : "=r"(a), WJR_ASM_CCOUT(c)(c_out)
                : "r"(b), "0"(a)
                : "cc");
        }
        return a;
    }

    if (WJR_BUILTIN_CONSTANT_P(a)) {
        if (__is_in_i32_range(a)) {
            asm("add{b $255, %b1| %b1, 255}\n\t"
                "adc{q %3, %0| %0, %3}\n\t" WJR_ASM_CCSET(c)
                : "=r"(b), "+&r"(c_in), WJR_ASM_CCOUT(c)(c_out)
                : "ri"(a), "0"(b)
                : "cc");
        } else {
            asm("add{b $255, %b1| %b1, 255}\n\t"
                "adc{q %3, %0| %0, %3}\n\t" WJR_ASM_CCSET(c)
                : "=r"(b), "+&r"(c_in), WJR_ASM_CCOUT(c)(c_out)
                : "r"(a), "0"(b)
                : "cc");
        }
        return b;
    }

    if (WJR_BUILTIN_CONSTANT_P(b) && __is_in_i32_range(b)) {
        asm("add{b $255, %b1| %b1, 255}\n\t"
            "adc{q %3, %0| %0, %3}\n\t" WJR_ASM_CCSET(c)
            : "=r"(a), "+&r"(c_in), WJR_ASM_CCOUT(c)(c_out)
            : "ri"(b), "0"(a)
            : "cc");
    } else {
        asm("add{b $255, %b1| %b1, 255}\n\t"
            "adc{q %3, %0| %0, %3}\n\t" WJR_ASM_CCSET(c)
            : "=r"(a), "+&r"(c_in), WJR_ASM_CCOUT(c)(c_out)
            : "r"(b), "0"(a)
            : "cc");
    }
    return a;
}

#endif

#if WJR_HAS_BUILTIN(ASM_ADDC_N)
#define WJR_ADDSUB_I 1
// WJR_ADDSUB_I :
// 0 : SUB
// 1 : ADD

// Already included

#ifndef WJR_ADDSUB_I
#error "abort"
#endif

#define WJR_addcsubc WJR_PP_BOOL_IF(WJR_ADDSUB_I, addc, subc)
#define WJR_adcsbb WJR_PP_BOOL_IF(WJR_ADDSUB_I, adc, sbb)

inline uint64_t WJR_PP_CONCAT(asm_, WJR_PP_CONCAT(WJR_addcsubc, _n_impl))(
    uint64_t *dst, const uint64_t *src0, const uint64_t *src1, size_t n, uint64_t c_in) {
    size_t rcx = n / 8;
    uint64_t r8 = c_in, r9, r10 = n & 7, r11;

    asm volatile(
        "add{b $255, %b[r8]| %b[r8], 255}\n\t"
        "lea{q| %[r9], [rip +} .Llookup%={(%%rip), %[r9]|]}\n\t"
        "movs{lq (%[r9], %[r10], 4), %[r10]|xd %[r10], DWORD PTR [%[r9] + %[r10] * 4]}\n\t"
        "lea{q (%[r9], %[r10], 1), %[r10]| %[r10], [%[r9] + %[r10]]}\n\t"
        "jmp{q *%[r10]| %[r10]}\n\t"
        
        ".align 8\n\t"
        ".Llookup%=:\n\t"
        ".long .Ll0%=-.Llookup%=\n\t"
        ".long .Ll1%=-.Llookup%=\n\t"
        ".long .Ll2%=-.Llookup%=\n\t"
        ".long .Ll3%=-.Llookup%=\n\t"
        ".long .Ll4%=-.Llookup%=\n\t"
        ".long .Ll5%=-.Llookup%=\n\t"
        ".long .Ll6%=-.Llookup%=\n\t"
        ".long .Ll7%=-.Llookup%=\n\t"
        ".align 16\n\t"
        
        ".Ll0%=:\n\t"
        "mov{q (%[src0]), %[r9]| %[r9], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r11]| %[r11], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r9]| %[r9], [%[src1]]}\n\t"
        "jmp .Lb0%=\n\t"

        ".Ld1%=:\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r10]| %[r10], [%[src1]]}\n\t"
        "mov{q %[r10], (%[dst])| [%[dst]], %[r10]}\n\t"
        "jmp .Ldone%=\n\t"

        ".Ll1%=:\n\t"
        "mov{q (%[src0]), %[r10]| %[r10], [%[src0]]}\n\t"
        "jrcxz .Ld1%=\n\t"
        "mov{q 8(%[src0]), %[r9]| %[r9], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r10]| %[r10], [%[src1]]}\n\t"
        "lea{q 8(%[src0]), %[src0]| %[src0], [%[src0] + 8]}\n\t"
        "lea{q 8(%[src1]), %[src1]| %[src1], [%[src1] + 8]}\n\t"
        "lea{q 8(%[dst]), %[dst]| %[dst], [%[dst] + 8]}\n\t"
        "jmp .Lb1%=\n\t"

        ".Ll3%=:\n\t"
        "mov{q (%[src0]), %[r11]| %[r11], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r8]| %[r8], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r11]| %[r11], [%[src1]]}\n\t"
        "lea{q -40(%[src0]), %[src0]| %[src0], [%[src0] - 40]}\n\t"
        "lea{q -40(%[src1]), %[src1]| %[src1], [%[src1] - 40]}\n\t"
        "lea{q -40(%[dst]), %[dst]| %[dst], [%[dst] - 40]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb3%=\n\t"

        ".Ll4%=:\n\t"
        "mov{q (%[src0]), %[r9]| %[r9], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r11]| %[r11], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r9]| %[r9], [%[src1]]}\n\t"
        "lea{q -32(%[src0]), %[src0]| %[src0], [%[src0] - 32]}\n\t"
        "lea{q -32(%[src1]), %[src1]| %[src1], [%[src1] - 32]}\n\t"
        "lea{q -32(%[dst]), %[dst]| %[dst], [%[dst] - 32]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb4%=\n\t"

        ".Ll5%=:\n\t"
        "mov{q (%[src0]), %[r10]| %[r10], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r9]| %[r9], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r10]| %[r10], [%[src1]]}\n\t"
        "lea{q -24(%[src0]), %[src0]| %[src0], [%[src0] - 24]}\n\t"
        "lea{q -24(%[src1]), %[src1]| %[src1], [%[src1] - 24]}\n\t"
        "lea{q -24(%[dst]), %[dst]| %[dst], [%[dst] - 24]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb5%=\n\t"

        ".Ll6%=:\n\t"
        "mov{q (%[src0]), %[r8]| %[r8], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r10]| %[r10], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r8]| %[r8], [%[src1]]}\n\t"
        "lea{q -16(%[src0]), %[src0]| %[src0], [%[src0] - 16]}\n\t"
        "lea{q -16(%[src1]), %[src1]| %[src1], [%[src1] - 16]}\n\t"
        "lea{q -16(%[dst]), %[dst]| %[dst], [%[dst] - 16]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb6%=\n\t"

        ".Ll7%=:\n\t"
        "mov{q (%[src0]), %[r11]| %[r11], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r8]| %[r8], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r11]| %[r11], [%[src1]]}\n\t"
        "lea{q -8(%[src0]), %[src0]| %[src0], [%[src0] - 8]}\n\t"
        "lea{q -8(%[src1]), %[src1]| %[src1], [%[src1] - 8]}\n\t"
        "lea{q -8(%[dst]), %[dst]| %[dst], [%[dst] - 8]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb7%=\n\t"

        ".Ld2%=:\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 8(%[src1]), %[r10]| %[r10], [%[src1] + 8]}\n\t"
        "mov{q %[r8], (%[dst])| [%[dst]], %[r8]}\n\t"
        "mov{q %[r10], 8(%[dst])| [%[dst] + 8], %[r10]}\n\t"
        "jmp .Ldone%=\n\t"

        ".Ll2%=:\n\t"
        "mov{q (%[src0]), %[r8]| %[r8], [%[src0]]}\n\t"
        "mov{q 8(%[src0]), %[r10]| %[r10], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r8]| %[r8], [%[src1]]}\n\t"
        "jrcxz .Ld2%=\n\t"
        "lea{q 16(%[src0]), %[src0]| %[src0], [%[src0] + 16]}\n\t"
        "lea{q 16(%[src1]), %[src1]| %[src1], [%[src1] + 16]}\n\t"
        "lea{q 16(%[dst]), %[dst]| %[dst], [%[dst] + 16]}\n\t"

        ".align 32\n\t"
        ".Lloop%=:\n\t"

        ".Lb2%=:\n\t"
        "mov{q (%[src0]), %[r9]| %[r9], [%[src0]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q -8(%[src1]), %[r10]| %[r10], [%[src1] - 8]}\n\t"
        "mov{q %[r8], -16(%[dst])| [%[dst] - 16], %[r8]}\n\t"

        ".Lb1%=:\n\t"
        "mov{q 8(%[src0]), %[r11]| %[r11], [%[src0] + 8]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src1]), %[r9]| %[r9], [%[src1]]}\n\t"
        "mov{q %[r10], -8(%[dst])| [%[dst] - 8], %[r10]}\n\t"

        ".Lb0%=:\n\t"
        "mov{q 16(%[src0]), %[r8]| %[r8], [%[src0] + 16]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 8(%[src1]), %[r11]| %[r11], [%[src1] + 8]}\n\t"
        "mov{q %[r9], (%[dst])| [%[dst]], %[r9]}\n\t"

        ".Lb7%=:\n\t"
        "mov{q 24(%[src0]), %[r10]| %[r10], [%[src0] + 24]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 16(%[src1]), %[r8]| %[r8], [%[src1] + 16]}\n\t"
        "mov{q %[r11], 8(%[dst])| [%[dst] + 8], %[r11]}\n\t"

        ".Lb6%=:\n\t"
        "mov{q 32(%[src0]), %[r9]| %[r9], [%[src0] + 32]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 24(%[src1]), %[r10]| %[r10], [%[src1] + 24]}\n\t"
        "mov{q %[r8], 16(%[dst])| [%[dst] + 16], %[r8]}\n\t"

        ".Lb5%=:\n\t"
        "mov{q 40(%[src0]), %[r11]| %[r11], [%[src0] + 40]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 32(%[src1]), %[r9]| %[r9], [%[src1] + 32]}\n\t"
        "mov{q %[r10], 24(%[dst])| [%[dst] + 24], %[r10]}\n\t"

        ".Lb4%=:\n\t"
        "mov{q 48(%[src0]), %[r8]| %[r8], [%[src0] + 48]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 40(%[src1]), %[r11]| %[r11], [%[src1] + 40]}\n\t"
        "mov{q %[r9], 32(%[dst])| [%[dst] + 32], %[r9]}\n\t"

        ".Lb3%=:\n\t"
        "mov{q 56(%[src0]), %[r10]| %[r10], [%[src0] + 56]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 48(%[src1]), %[r8]| %[r8], [%[src1] + 48]}\n\t"
        "mov{q %[r11], 40(%[dst])| [%[dst] + 40], %[r11]}\n\t"

        // TODO : optimize pipeline
        "lea{q 64(%[src0]), %[src0]| %[src0], [%[src0] + 64]}\n\t"
        "lea{q 64(%[src1]), %[src1]| %[src1], [%[src1] + 64]}\n\t"
        "lea{q 64(%[dst]), %[dst]| %[dst], [%[dst] + 64]}\n\t"
        "dec %[rcx]\n\t"
        
        "jne .Lloop%=\n\t"

        WJR_PP_STR(WJR_adcsbb) "{q -8(%[src1]), %[r10]| %[r10], [%[src1] - 8]}\n\t"
        "mov{q %[r8], -16(%[dst])| [%[dst] - 16], %[r8]}\n\t"
        "mov{q %[r10], -8(%[dst])| [%[dst] - 8], %[r10]}\n\t"

        ".Ldone%=:\n\t"
        "mov %k[rcx], %k[r9]\n\t"
        "adc{l %k[rcx], %k[r9]| %k[r9], %k[rcx]}"

        : [dst] "+r"(dst), [src0] "+r"(src0), [src1] "+r"(src1), [rcx] "+c"(rcx), 
          [r8] "+r"(r8), [r9] "=&r"(r9), [r10] "+r"(r10), [r11] "=&r"(r11)
        :
        : "cc", "memory");

    WJR_ASSERT_ASSUME(rcx == 0);
    WJR_ASSERT_ASSUME(r9 <= 1);

    return r9;
}

WJR_INTRINSIC_INLINE uint64_t WJR_PP_CONCAT(asm_, WJR_PP_CONCAT(WJR_addcsubc, _n))(
    uint64_t *dst, const uint64_t *src0, const uint64_t *src1, size_t n, uint64_t c_in) {
    if (WJR_BUILTIN_CONSTANT_P(n)) {
        if (n == 1) {
            dst[0] = WJR_PP_CONCAT(asm_, WJR_addcsubc)(src0[0], src1[0], c_in, c_in);
            return c_in;
        }
    }

    return WJR_PP_CONCAT(asm_, WJR_PP_CONCAT(WJR_addcsubc, _n_impl))(dst, src0, src1, n,
                                                                     c_in);
}

#undef WJR_adcsbb
#undef WJR_addcsubc

#undef WJR_ADDSUB_I
#endif

#if WJR_HAS_BUILTIN(__ASM_ADD_128)

/**
 * @brief Use inline assembly to add two 64-bit integers and return the result.
 *
 */
WJR_INTRINSIC_INLINE void __asm_add_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                        uint64_t hi0, uint64_t lo1, uint64_t hi1) {
    if (WJR_BUILTIN_CONSTANT_P(hi0) && hi0 <= UINT32_MAX) {
        asm("add{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
            "adc{q %[hi0], %[hi1]| %[hi1], %[hi0]}"
            : [lo0] "+&r"(lo0), [hi1] "+r"(hi1)
            : [lo1] "r"(lo1), [hi0] "i"(hi0)
            : "cc");
        al = lo0;
        ah = hi1;
        return;
    } else if (WJR_BUILTIN_CONSTANT_P(hi1) && hi1 <= UINT32_MAX) {
        asm("add{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
            "adc{q %[hi1], %[hi0]| %[hi0], %[hi1]}"
            : [lo0] "+&r"(lo0), [hi0] "+r"(hi0)
            : [lo1] "r"(lo1), [hi1] "i"(hi1)
            : "cc");
        al = lo0;
        ah = hi0;
        return;
    }

    asm("add{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
        "adc{q %[hi1], %[hi0]| %[hi0], %[hi1]}"
        : [lo0] "+&r"(lo0), [hi0] "+r"(hi0)
        : [lo1] "r"(lo1), [hi1] "r"(hi1)
        : "cc");
    al = lo0;
    ah = hi0;
    return;
}

#endif

#if WJR_HAS_BUILTIN(__ASM_ADDC_128) || WJR_HAS_BUILTIN(__ASM_ADDC_CC_128)

/**
 * @brief Use inline assembly to add two 64-bit integers and return the
 * carry-out.
 *
 * @details Optimzation for __asm_addc_cc_128 and __asm_addc_128 when the carry-in is 0.
 *
 */
WJR_INTRINSIC_INLINE uint8_t __asm_addc_cc_zero_128(uint64_t &al, uint64_t &ah,
                                                    uint64_t lo0, uint64_t hi0,
                                                    uint64_t lo1, uint64_t hi1) {
    uint8_t c_out;
    if (WJR_BUILTIN_CONSTANT_P(hi0) && hi0 <= UINT32_MAX) {
        asm("add{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
            "adc{q %[hi0], %[hi1]| %[hi1], %[hi0]}\n\t" WJR_ASM_CCSET(c)
            : [lo0] "+&r"(lo0), [hi1] "+r"(hi1), WJR_ASM_CCOUT(c)(c_out)
            : [lo1] "r"(lo1), [hi0] "i"(hi0)
            : "cc");
        al = lo0;
        ah = hi1;
        return c_out;
    } else if (WJR_BUILTIN_CONSTANT_P(hi1) && hi1 <= UINT32_MAX) {
        asm("add{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
            "adc{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t" WJR_ASM_CCSET(c)
            : [lo0] "+&r"(lo0), [hi0] "+r"(hi0), WJR_ASM_CCOUT(c)(c_out)
            : [lo1] "r"(lo1), [hi1] "i"(hi1)
            : "cc");
        al = lo0;
        ah = hi0;
        return c_out;
    }

    asm("add{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
        "adc{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t" WJR_ASM_CCSET(c)
        : [lo0] "+&r"(lo0), [hi0] "+r"(hi0), WJR_ASM_CCOUT(c)(c_out)
        : [lo1] "r"(lo1), [hi1] "r"(hi1)
        : "cc");
    al = lo0;
    ah = hi0;
    return c_out;
}

#endif

#if WJR_HAS_BUILTIN(__ASM_ADDC_128)

/**
 * @brief Use inline assembly to add two 64-bit integers with carry-in and return the
 * carry-out.
 *
 */
WJR_INTRINSIC_INLINE uint64_t __asm_addc_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                             uint64_t hi0, uint64_t lo1, uint64_t hi1,
                                             uint64_t c_in) {
    if (WJR_BUILTIN_CONSTANT_P(c_in == 0) && c_in == 0) {
        return __asm_addc_cc_zero_128(al, ah, lo0, hi0, lo1, hi1);
    }

    if (WJR_BUILTIN_CONSTANT_P(hi0) && hi0 <= UINT32_MAX) {
        asm("add{b $0xff, %b[c_in]| %b[c_in], 0xff}\n\t"
            "adc{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
            "adc{q %[hi0], %[hi1]| %[hi1], %[hi0]}\n\t"
            "setb %b[c_in]"
            : [lo0] "+&r"(lo0), [hi1] "+r"(hi1), [c_in] "+&r"(c_in)
            : [lo1] "r"(lo1), [hi0] "i"(hi0)
            : "cc");
        al = lo0;
        ah = hi1;
        return c_in;
    } else if (WJR_BUILTIN_CONSTANT_P(hi1) && hi1 <= UINT32_MAX) {
        asm("add{b $0xff, %b[c_in]| %b[c_in], 0xff}\n\t"
            "adc{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
            "adc{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t"
            "setb %b[c_in]"
            : [lo0] "+&r"(lo0), [hi0] "+r"(hi0), [c_in] "+&r"(c_in)
            : [lo1] "r"(lo1), [hi1] "i"(hi1)
            : "cc");
        al = lo0;
        ah = hi0;
        return c_in;
    }

    asm("add{b $0xff, %b[c_in]| %b[c_in], 0xff}\n\t"
        "adc{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
        "adc{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t"
        "setb %b[c_in]"
        : [lo0] "+&r"(lo0), [hi0] "+r"(hi0), [c_in] "+&r"(c_in)
        : [lo1] "r"(lo1), [hi1] "r"(hi1)
        : "cc");
    al = lo0;
    ah = hi0;
    return c_in;
}

#endif

#if WJR_HAS_BUILTIN(__ASM_ADDC_CC_128)

/**
 * @brief Use inline assembly to add two 64-bit integers with carry-in and return the
 * carry-out.
 *
 * @details Similar to __asm_addc_128, but the carry-out flag is set by using constraint
 * "=@cccond" instead of "setb".
 *
 */
WJR_INTRINSIC_INLINE uint8_t __asm_addc_cc_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                               uint64_t hi0, uint64_t lo1, uint64_t hi1,
                                               uint8_t c_in) {
    if (WJR_BUILTIN_CONSTANT_P(c_in == 0) && c_in == 0) {
        return __asm_addc_cc_zero_128(al, ah, lo0, hi0, lo1, hi1);
    }

    uint8_t c_out;
    if (WJR_BUILTIN_CONSTANT_P(hi0) && hi0 <= UINT32_MAX) {
        asm("add{b $0xff, %b[c_in]| %b[c_in], 0xff}\n\t"
            "adc{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
            "adc{q %[hi0], %[hi1]| %[hi1], %[hi0]}\n\t" WJR_ASM_CCSET(c)
            : [lo0] "+&r"(lo0), [hi1] "+r"(hi1), [c_in] "+&r"(c_in),
              WJR_ASM_CCOUT(c)(c_out)
            : [lo1] "r"(lo1), [hi0] "i"(hi0)
            : "cc");
        al = lo0;
        ah = hi1;
        return c_out;
    } else if (WJR_BUILTIN_CONSTANT_P(hi1) && hi1 <= UINT32_MAX) {
        asm("add{b $0xff, %b[c_in]| %b[c_in], 0xff}\n\t"
            "adc{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
            "adc{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t" WJR_ASM_CCSET(c)
            : [lo0] "+&r"(lo0), [hi0] "+r"(hi0), [c_in] "+&r"(c_in),
              WJR_ASM_CCOUT(c)(c_out)
            : [lo1] "r"(lo1), [hi1] "i"(hi1)
            : "cc");
        al = lo0;
        ah = hi0;
        return c_out;
    }

    asm("add{b $0xff, %b[c_in]| %b[c_in], 0xff}\n\t"
        "adc{q %[lo1], %[lo0]| %[lo0], %[lo1]}\n\t"
        "adc{q %[hi1], %[hi0]| %[hi0], %[hi1]}\n\t" WJR_ASM_CCSET(c)
        : [lo0] "+&r"(lo0), [hi0] "+r"(hi0), [c_in] "+&r"(c_in), WJR_ASM_CCOUT(c)(c_out)
        : [lo1] "r"(lo1), [hi1] "r"(hi1)
        : "cc");
    al = lo0;
    ah = hi0;
    return c_out;
}

#endif

} // namespace wjr

#endif // WJR_X86_MATH_ADD_HPP__
#endif

namespace wjr {

template <typename T, typename U>
WJR_INTRINSIC_CONSTEXPR T fallback_addc(T a, T b, U c_in, U &c_out) {
    T ret = a;
    ret += b;
    U c = ret < b;
    ret += c_in;
    c |= ret < c_in;
    c_out = c;
    return ret;
}

#if WJR_HAS_BUILTIN(__builtin_addc)
#define WJR_HAS_BUILTIN_ADDC WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(ADDC)

template <typename T, typename U>
WJR_INTRINSIC_INLINE T builtin_addc(T a, T b, U c_in, U &c_out) {
    constexpr auto nd = std::numeric_limits<T>::digits;

#define WJR_REGISTER_BUILTIN_ADDC(suffix, type)                                          \
    if constexpr (nd <= std::numeric_limits<type>::digits) {                             \
        type __c_out;                                                                    \
        const T ret = __builtin_addc##suffix(a, b, static_cast<type>(c_in), &__c_out);   \
        c_out = static_cast<U>(__c_out);                                                 \
        return ret;                                                                      \
    } else

    WJR_REGISTER_BUILTIN_ADDC(b, unsigned char)
    WJR_REGISTER_BUILTIN_ADDC(s, unsigned short)
    WJR_REGISTER_BUILTIN_ADDC(, unsigned int)
    WJR_REGISTER_BUILTIN_ADDC(l, unsigned long)
    WJR_REGISTER_BUILTIN_ADDC(ll, unsigned long long) {
        static_assert(nd <= 64, "not supported yet");
    }

#undef WJR_REGISTER_BUILTIN_ADDC
}

#endif // WJR_HAS_BUILTIN(ADDC)

/**
 * @brief Add two numbers with carry-in, and return the result and carry-out
 *
 * @note The carry-in and carry-out are limited to 0 and 1
 * @tparam U Type of the carry-in and carry-out. It must be an unsigned integral type.
 * the default type is the same as `T`
 * @param[in] c_in The carry-in flag.
 * @param[out] c_out The carry-out flag.
 */
template <typename T, typename U,
          WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_INTRINSIC_CONSTEXPR_E T addc(T a, T b, type_identity_t<U> c_in, U &c_out) {
    WJR_ASSERT_ASSUME_L1(c_in <= 1);

#if !WJR_HAS_BUILTIN(ADDC) && !WJR_HAS_BUILTIN(ASM_ADDC)
    return fallback_addc(a, b, c_in, c_out);
#else
    constexpr auto is_constant_or_zero = [](auto x) -> int {
        return WJR_BUILTIN_CONSTANT_P(x == 0) && x == 0 ? 2
               : WJR_BUILTIN_CONSTANT_P(x)              ? 1
                                                        : 0;
    };

    // The compiler should be able to optimize the judgment condition of if when enabling
    // optimization. If it doesn't work, then there should be a issue
    if (is_constant_evaluated() ||
        // constant value is zero or constant value number greater or equal than 2
        (is_constant_or_zero(a) + is_constant_or_zero(b) + is_constant_or_zero(c_in) >=
         2)) {
        return fallback_addc(a, b, c_in, c_out);
    }

    if constexpr (sizeof(T) == 8) {
        return WJR_PP_BOOL_IF(WJR_HAS_BUILTIN(ASM_ADDC), asm_addc,
                              WJR_PP_BOOL_IF(WJR_HAS_BUILTIN(ADDC), builtin_addc,
                                             fallback_addc))(a, b, c_in, c_out);
    } else {
        return WJR_PP_BOOL_IF(WJR_HAS_BUILTIN(ADDC), builtin_addc,
                              fallback_addc)(a, b, c_in, c_out);
    }
#endif
}

/**
 * @brief Performs addition with carry-in and carry-out, optimized for subsequent
 * branching based on carry-out.
 *
 * @details This function, `addc_cc`, adds two numbers with a carry-in, and returns the
 * result and a carry-out. The carry-out (`c_out`) is optimized for subsequent code that
 * branches based on its value. For example, it can be used with jump instructions like
 * `je` or `jne`. This is in contrast to the `addc` function, which may use instructions
 * like `setc` or `test` for branching.
 *
 * @note The carry-in and carry-out are limited to 0 and 1
 * @tparam U Type of the carry-in and carry-out. It must be an unsigned integral type.
 * @param[in] c_in The carry-in flag.
 * @param[out] c_out The carry-out flag.
 */
template <typename T, WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E T addc_cc(T a, T b, uint8_t c_in, uint8_t &c_out) {
    WJR_ASSERT_ASSUME_L1(c_in <= 1);

#if WJR_HAS_BUILTIN(ASM_ADDC_CC)
    constexpr auto is_constant_or_zero = [](auto x) -> int {
        return WJR_BUILTIN_CONSTANT_P(x == 0) && x == 0 ? 2
               : WJR_BUILTIN_CONSTANT_P(x)              ? 1
                                                        : 0;
    };

    // The compiler should be able to optimize the judgment condition of if when enabling
    // optimization. If it doesn't work, then there should be a issue
    if (is_constant_evaluated() ||
        // constant value is zero or constant value number greater or equal than 2
        (is_constant_or_zero(a) + is_constant_or_zero(b) + is_constant_or_zero(c_in) >=
         2)) {
        return fallback_addc(a, b, c_in, c_out);
    }

    if constexpr (sizeof(T) == 8) {
        return asm_addc_cc(a, b, c_in, c_out);
    } else {
        return addc(a, b, c_in, c_out);
    }
#else
    return addc(a, b, c_in, c_out);
#endif
}

#if WJR_HAS_BUILTIN(__builtin_add_overflow)
#define WJR_HAS_BUILTIN_ADD_OVERFLOW WJR_HAS_DEF
#endif

template <typename T>
WJR_INTRINSIC_CONSTEXPR_E bool fallback_add_overflow(T a, T b, T &ret) {
    ret = a + b;
    return ret < a;
}

template <typename T, WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E bool add_overflow(type_identity_t<T> a, type_identity_t<T> b,
                                            T &ret) {
#if WJR_HAS_BUILTIN(ADD_OVERFLOW)
    if (is_constant_evaluated() ||
        (WJR_BUILTIN_CONSTANT_P(a) && WJR_BUILTIN_CONSTANT_P(b))) {
        return fallback_add_overflow(a, b, ret);
    }

    return __builtin_add_overflow(a, b, &ret);
#else
    return fallback_add_overflow(a, b, ret);
#endif
}

/**
 * @brief Add biginteger(src0) and number with carry-in, and return the result(dst) and
 * carry-out.
 *
 * @tparam U Type of the carry-in and carry-out. It must be an unsigned integral type.
 * @param[out] dst The result of the addition.
 * @param[in] src0 The biginteger to be added.
 * @param[in] n The number of elements in the biginteger.
 * @param[in] src1 The number to be added.
 * @param[in] c_in The carry-in flag.
 * @return The carry-out flag.
 */
template <typename T, typename U,
          WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_INTRINSIC_CONSTEXPR_E U addc_1(T *dst, const T *src0, size_t n,
                                   type_identity_t<T> src1, U c_in) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src0, n));
    WJR_ASSERT_ASSUME(c_in <= 1);

    uint8_t overflow = 0;
    dst[0] = addc_cc(src0[0], src1, c_in, overflow);

    if (overflow) {
        size_t idx = 1 + replace_find_not(dst + 1, src0 + 1, n - 1, -1, 0);

        if (WJR_UNLIKELY(idx == n)) {
            return static_cast<U>(1);
        }

        dst[idx] = src0[idx] + 1;

        dst += idx;
        src0 += idx;
        n -= idx;
    }

    if (src0 != dst) {
        std::copy(src0 + 1, src0 + n, dst + 1);
    }

    return static_cast<U>(0);
}

template <typename T, typename U>
WJR_INTRINSIC_CONSTEXPR U fallback_addc_n(T *dst, const T *src0, const T *src1, size_t n,
                                          U c_in) {
    size_t m = n / 4;

    for (size_t i = 0; i < m; ++i) {
        dst[0] = addc(src0[0], src1[0], c_in, c_in);
        dst[1] = addc(src0[1], src1[1], c_in, c_in);
        dst[2] = addc(src0[2], src1[2], c_in, c_in);
        dst[3] = addc(src0[3], src1[3], c_in, c_in);

        dst += 4;
        src0 += 4;
        src1 += 4;
    }

    n &= 3;
    if (WJR_UNLIKELY(n == 0)) {
        return c_in;
    }

    dst += n;
    src0 += n;
    src1 += n;

    switch (n) {
    case 3: {
        dst[-3] = addc(src0[-3], src1[-3], c_in, c_in);
        WJR_FALLTHROUGH;
    }
    case 2: {
        dst[-2] = addc(src0[-2], src1[-2], c_in, c_in);
        WJR_FALLTHROUGH;
    }
    case 1: {
        dst[-1] = addc(src0[-1], src1[-1], c_in, c_in);
        WJR_FALLTHROUGH;
    }
    default: {
        break;
    }
    }

    return c_in;
}

/**
 * @brief Add biginteger(src0) and biginteger(src1) with carry-in, and return the result
 * (dst) and carry-out.
 *
 * @tparam U Type of the carry-in and carry-out. It must be an unsigned integral type.
 * @param[out] dst The result of the addition.
 * @param[in] src0 The biginteger to be added.
 * @param[in] src1 The biginteger to be added.
 * @param[in] n The number of elements in the biginteger.
 * @param[in] c_in The carry-in flag.
 * @return The carry-out flag.
 */
template <typename T, typename U,
          WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_INTRINSIC_CONSTEXPR_E U addc_n(T *dst, const T *src0, const T *src1, size_t n,
                                   U c_in) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src0, n));
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src1, n));

#if WJR_HAS_BUILTIN(ASM_ADDC_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_addc_n(dst, src0, src1, n, c_in);
        }

        return asm_addc_n(dst, src0, src1, n, c_in);
    } else {
        return fallback_addc_n(dst, src0, src1, n, c_in);
    }
#else
    return fallback_addc_n(dst, src0, src1, n, c_in);
#endif
}

/*
require :
1. m >= 1
2. n >= m
3. WJR_IS_SAME_OR_INCR_P(dst, n, src0, n)
4. WJR_IS_SAME_OR_INCR_P(dst, m, src1, m)
*/
template <typename T, typename U,
          WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_INTRINSIC_CONSTEXPR_E U addc_s(T *dst, const T *src0, size_t n, const T *src1,
                                   size_t m, U c_in) {
    WJR_ASSERT_ASSUME(m >= 1);
    WJR_ASSERT_ASSUME(n >= m);

    c_in = addc_n(dst, src0, src1, m, c_in);

    if (n != m) {
        c_in = addc_1(dst + m, src0 + m, n - m, 0, c_in);
    }

    return c_in;
}

/*
require :
1. n >= 0
2. n >= m
3. WJR_IS_SAME_OR_INCR_P(dst, n, src0, n)
4. WJR_IS_SAME_OR_INCR_P(dst, m, src1, m)
*/
template <typename T, typename U,
          WJR_REQUIRES_I(is_nonbool_unsigned_integral_v<T> &&is_unsigned_integral_v<U>)>
WJR_INTRINSIC_CONSTEXPR_E U addc_sz(T *dst, const T *src0, size_t n, const T *src1,
                                    size_t m, U c_in) {
    WJR_ASSERT_ASSUME(n >= m);

    if (WJR_LIKELY(m != 0)) {
        c_in = addc_n(dst, src0, src1, m, c_in);
    }

    if (n != m) {
        c_in = addc_1(dst + m, src0 + m, n - m, 0, c_in);
    }

    return c_in;
}

WJR_INTRINSIC_CONSTEXPR void __fallback_add_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                                uint64_t hi0, uint64_t lo1,
                                                uint64_t hi1) {
    uint64_t __al = lo0 + lo1;
    ah = hi0 + hi1 + (__al < lo0);
    al = __al;
}

#if WJR_HAS_FEATURE(FAST_INT128_ADDSUB)
#define WJR_HAS_BUILTIN___BUILTIN_ADD_128 WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(__BUILTIN_ADD_128)

WJR_INTRINSIC_INLINE void __builtin_add_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                            uint64_t hi0, uint64_t lo1, uint64_t hi1) {
    const auto x0 = static_cast<__uint128_t>(hi0) << 64 | lo0;
    const auto x1 = static_cast<__uint128_t>(hi1) << 64 | lo1;
    x0 += x1;

    al = x0;
    ah = x0 >> 64;
}

#endif

// <ah, al> = <hi0, lo0> + <hi1, lo1>
WJR_INTRINSIC_CONSTEXPR_E void __add_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                         uint64_t hi0, uint64_t lo1, uint64_t hi1) {
#if WJR_HAS_BUILTIN(__BUILTIN_ADD_128) || WJR_HAS_BUILTIN(__ASM_ADD_128)
    if (is_constant_evaluated() || (WJR_BUILTIN_CONSTANT_P(lo0 == 0) && lo0 == 0) ||
        (WJR_BUILTIN_CONSTANT_P(lo1 == 0) && lo1 == 0)) {
        return __fallback_add_128(al, ah, lo0, hi0, lo1, hi1);
    }

    return WJR_PP_BOOL_IF(WJR_HAS_BUILTIN(__BUILTIN_ADD_128), __builtin_add_128,
                          __asm_add_128)(al, ah, lo0, hi0, lo1, hi1);
#else
    return __fallback_add_128(al, ah, lo0, hi0, lo1, hi1);
#endif
}

WJR_INTRINSIC_CONSTEXPR_E uint64_t __fallback_addc_128(uint64_t &al, uint64_t &ah,
                                                       uint64_t lo0, uint64_t hi0,
                                                       uint64_t lo1, uint64_t hi1,
                                                       uint64_t c_in) {
    al = addc(lo0, lo1, c_in, c_in);
    ah = addc(hi0, hi1, c_in, c_in);
    return c_in;
}

// return c_out
WJR_INTRINSIC_CONSTEXPR_E uint64_t __addc_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                              uint64_t hi0, uint64_t lo1, uint64_t hi1,
                                              uint64_t c_in) {
#if WJR_HAS_BUILTIN(__ASM_ADDC_128)
    if (is_constant_evaluated()) {
        return __fallback_addc_128(al, ah, lo0, hi0, lo1, hi1, c_in);
    }

    return __asm_addc_128(al, ah, lo0, hi0, lo1, hi1, c_in);
#else
    return __fallback_addc_128(al, ah, lo0, hi0, lo1, hi1, c_in);
#endif
}

WJR_INTRINSIC_CONSTEXPR_E uint8_t __addc_cc_128(uint64_t &al, uint64_t &ah, uint64_t lo0,
                                                uint64_t hi0, uint64_t lo1, uint64_t hi1,
                                                uint8_t c_in) {
#if WJR_HAS_BUILTIN(__ASM_ADDC_CC_128)
    if (is_constant_evaluated()) {
        return __fallback_addc_128(al, ah, lo0, hi0, lo1, hi1, c_in);
    }

    return __asm_addc_cc_128(al, ah, lo0, hi0, lo1, hi1, c_in);
#else
    return __addc_128(al, ah, lo0, hi0, lo1, hi1, c_in);
#endif
}

} // namespace wjr

#endif // WJR_MATH_ADD_HPP__
// Already included
#ifndef WJR_MATH_SHIFT_HPP__
#define WJR_MATH_SHIFT_HPP__

// Already included
// Already included

#if defined(WJR_X86)
#ifndef WJR_X86_MATH_SHIFT_HPP__
#define WJR_X86_MATH_SHIFT_HPP__

// Already included

#ifndef WJR_X86
#error "x86 required"
#endif

namespace wjr {

template <typename T>
WJR_INTRINSIC_CONSTEXPR_E T shld(T hi, T lo, unsigned int c);

template <typename T>
WJR_INTRINSIC_CONSTEXPR_E T shrd(T lo, T hi, unsigned int c);

#if WJR_HAS_SIMD(SSE2) && WJR_HAS_SIMD(X86_SIMD)
#define WJR_HAS_BUILTIN_LSHIFT_N WJR_HAS_DEF
#define WJR_HAS_BUILTIN_RSHIFT_N WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(LSHIFT_N) || WJR_HAS_BUILTIN(RSHIFT_N)

/// @private
template <bool is_constant>
WJR_INTRINSIC_INLINE auto __mm_get_shift(unsigned int c) {
    if constexpr (is_constant) {
        return c;
    } else {
        return simd_cast<unsigned int, __m128i_t>(c);
    }
}

/// @private
WJR_INTRINSIC_INLINE __m128i __mm_sll_epi64(__m128i x, unsigned int c) {
    return sse::slli_epi64(x, c);
}

/// @private
WJR_INTRINSIC_INLINE __m128i __mm_sll_epi64(__m128i x, __m128i c) {
    return sse::sll_epi64(x, c);
}

/// @private
WJR_INTRINSIC_INLINE __m128i __mm_srl_epi64(__m128i x, unsigned int c) {
    return sse::srli_epi64(x, c);
}

/// @private
WJR_INTRINSIC_INLINE __m128i __mm_srl_epi64(__m128i x, __m128i c) {
    return sse::srl_epi64(x, c);
}

#endif

#if WJR_HAS_BUILTIN(LSHIFT_N)

#define WJR_REGISTER_LSHIFT_N_IMPL_UNALIGNED(index)                                      \
    do {                                                                                 \
        __m128i x1 = sse::loadu((__m128i *)(src - 3 - (index)));                         \
        x0 = simd_cast<__m128_t, __m128i_t>(sse::template shuffle_ps<78>(                \
            simd_cast<__m128i_t, __m128_t>(x1), simd_cast<__m128i_t, __m128_t>(x0)));    \
                                                                                         \
        __m128i r0 = __mm_sll_epi64(x0, y);                                              \
        __m128i r1 = __mm_srl_epi64(x1, z);                                              \
                                                                                         \
        __m128i r = sse::Or(r0, r1);                                                     \
                                                                                         \
        sse::storeu((__m128i *)(dst - 2 - (index)), r);                                  \
                                                                                         \
        x0 = x1;                                                                         \
    } while (0)

template <bool is_constant, typename T>
void large_builtin_lshift_n_impl(T *dst, const T *src, size_t n, unsigned int c) {
    const auto y = __mm_get_shift<is_constant>(c);
    const auto z = __mm_get_shift<is_constant>(64 - c);

    if (n & 1) {
        dst[-1] = shld(src[-1], src[-2], c);
        --src;
        --dst;
    }

    __m128i x0 = sse::set1_epi64(src[-1]);

    if (n & 2) {
        WJR_REGISTER_LSHIFT_N_IMPL_UNALIGNED(0);

        src -= 2;
        dst -= 2;
    }

    if (n & 4) {
        WJR_REGISTER_LSHIFT_N_IMPL_UNALIGNED(0);
        WJR_REGISTER_LSHIFT_N_IMPL_UNALIGNED(2);

        src -= 4;
        dst -= 4;
    }

    size_t idx = n / 8;

    if (WJR_UNLIKELY(idx == 0)) {
        return;
    }

    do {
        WJR_REGISTER_LSHIFT_N_IMPL_UNALIGNED(0);
        WJR_REGISTER_LSHIFT_N_IMPL_UNALIGNED(2);
        WJR_REGISTER_LSHIFT_N_IMPL_UNALIGNED(4);
        WJR_REGISTER_LSHIFT_N_IMPL_UNALIGNED(6);

        src -= 8;
        dst -= 8;
        --idx;
    } while (WJR_LIKELY(idx != 0));
}

template <typename T>
WJR_INTRINSIC_INLINE void builtin_lshift_n_impl(T *dst, const T *src, size_t n,
                                                unsigned int c) {
    if (WJR_UNLIKELY(n < 4)) {
        switch (n) {
        case 3: {
            dst[2] = shld(src[2], src[1], c);
            WJR_FALLTHROUGH;
        }
        case 2: {
            dst[1] = shld(src[1], src[0], c);
            WJR_FALLTHROUGH;
        }
        case 1: {
            dst[0] = shld(src[0], src[-1], c);
            WJR_FALLTHROUGH;
        }
        case 0: {
            break;
        }
        default: {
            WJR_UNREACHABLE();
            break;
        }
        }

        return;
    }

    src += n;
    dst += n;

    if (WJR_BUILTIN_CONSTANT_P(c)) {
        return large_builtin_lshift_n_impl<true>(dst, src, n, c);
    }

    return large_builtin_lshift_n_impl<false>(dst, src, n, c);
}

#undef WJR_REGISTER_LSHIFT_N_IMPL_UNALIGNED

template <typename T>
WJR_INTRINSIC_INLINE T builtin_lshift_n(T *dst, const T *src, size_t n, unsigned int c,
                                        T lo) {
    const T ret = src[n - 1] >> (64 - c);
    builtin_lshift_n_impl(dst + 1, src + 1, n - 1, c);
    dst[0] = shld(src[0], lo, c);
    return ret;
}

#endif

#if WJR_HAS_BUILTIN(RSHIFT_N)

#define WJR_REGISTER_RSHIFT_N_IMPL_UNALIGNED(index)                                      \
    do {                                                                                 \
        __m128i x1 = sse::loadu((__m128i *)(src + 1 + (index)));                         \
        x0 = simd_cast<__m128_t, __m128i_t>(sse::template shuffle_ps<78>(                \
            simd_cast<__m128i_t, __m128_t>(x0), simd_cast<__m128i_t, __m128_t>(x1)));    \
                                                                                         \
        __m128i r0 = __mm_srl_epi64(x0, y);                                              \
        __m128i r1 = __mm_sll_epi64(x1, z);                                              \
                                                                                         \
        __m128i r = sse::Or(r0, r1);                                                     \
                                                                                         \
        sse::storeu((__m128i *)(dst + (index)), r);                                      \
                                                                                         \
        x0 = x1;                                                                         \
    } while (0)

template <bool is_constant, typename T>
void large_builtin_rshift_n_impl(T *dst, const T *src, size_t n, unsigned int c) {
    const auto y = __mm_get_shift<is_constant>(c);
    const auto z = __mm_get_shift<is_constant>(64 - c);

    if (n & 1) {
        dst[0] = shrd(src[0], src[1], c);
        ++src;
        ++dst;
    }

    __m128i x0 = sse::set1_epi64(src[0]);

    if (n & 2) {
        WJR_REGISTER_RSHIFT_N_IMPL_UNALIGNED(0);

        src += 2;
        dst += 2;
    }

    if (n & 4) {
        WJR_REGISTER_RSHIFT_N_IMPL_UNALIGNED(0);
        WJR_REGISTER_RSHIFT_N_IMPL_UNALIGNED(2);

        src += 4;
        dst += 4;
    }

    size_t idx = n / 8;

    if (WJR_UNLIKELY(idx == 0)) {
        return;
    }

    do {
        WJR_REGISTER_RSHIFT_N_IMPL_UNALIGNED(0);
        WJR_REGISTER_RSHIFT_N_IMPL_UNALIGNED(2);
        WJR_REGISTER_RSHIFT_N_IMPL_UNALIGNED(4);
        WJR_REGISTER_RSHIFT_N_IMPL_UNALIGNED(6);

        dst += 8;
        src += 8;
        --idx;
    } while (WJR_LIKELY(idx != 0));
}

template <typename T>
WJR_INTRINSIC_INLINE void builtin_rshift_n_impl(T *dst, const T *src, size_t n,
                                                unsigned int c) {
    if (WJR_UNLIKELY(n < 4)) {
        dst += n - 3;
        src += n - 3;

        switch (n) {
        case 3: {
            dst[0] = shrd(src[0], src[1], c);
            WJR_FALLTHROUGH;
        }
        case 2: {
            dst[1] = shrd(src[1], src[2], c);
            WJR_FALLTHROUGH;
        }
        case 1: {
            dst[2] = shrd(src[2], src[3], c);
            WJR_FALLTHROUGH;
        }
        case 0: {
            break;
        }
        default: {
            WJR_UNREACHABLE();
            break;
        }
        }

        return;
    }

    if (WJR_BUILTIN_CONSTANT_P(c)) {
        return large_builtin_rshift_n_impl<true>(dst, src, n, c);
    }

    return large_builtin_rshift_n_impl<false>(dst, src, n, c);
}

#undef WJR_REGISTER_RSHIFT_N_IMPL_UNALIGNED

template <typename T>
WJR_INTRINSIC_INLINE T builtin_rshift_n(T *dst, const T *src, size_t n, unsigned int c,
                                        T hi) {
    const T ret = src[0] << (64 - c);
    builtin_rshift_n_impl(dst, src, n - 1, c);
    dst[n - 1] = shrd(src[n - 1], hi, c);
    return ret;
}

#endif

} // namespace wjr

#endif // WJR_X86_MATH_SHIFT_HPP__
#endif

namespace wjr {

template <typename T>
WJR_INTRINSIC_CONSTEXPR T fallback_shld(T hi, T lo, unsigned int c) {
    constexpr auto digits = std::numeric_limits<T>::digits;
    return hi << c | lo >> (digits - c);
}

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E T shld(T hi, T lo, unsigned int c) {
    return fallback_shld(hi, lo, c);
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR T fallback_shrd(T lo, T hi, unsigned int c) {
    constexpr auto digits = std::numeric_limits<T>::digits;
    return lo >> c | hi << (digits - c);
}

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E T shrd(T lo, T hi, unsigned int c) {
    return fallback_shrd(lo, hi, c);
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR T fallback_lshift_n(T *dst, const T *src, size_t n,
                                            unsigned int c, T lo) {
    constexpr auto nd = std::numeric_limits<T>::digits;
    T ret = src[n - 1] >> (nd - c);
    for (size_t i = 0; i < n - 1; ++i) {
        dst[n - i - 1] = fallback_shld(src[n - i - 1], src[n - i - 2], c);
    }
    dst[0] = fallback_shld(src[0], lo, c);
    return ret;
}

/*
require :
1. n >= 1
2. WJR_IS_SAME_OR_DECR_P(dst, n, src, n)
*/
template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR_E T lshift_n(T *dst, const T *src, size_t n,
                                                   unsigned int c, T lo = 0) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_DECR_P(dst, n, src, n));
    WJR_ASSERT_L1(c < std::numeric_limits<T>::digits);

    if (WJR_UNLIKELY(c == 0)) {
        if (WJR_LIKELY(dst != src)) {
            std::copy_backward(src, src + n, dst + n);
        }

        return 0;
    }

#if WJR_HAS_BUILTIN(LSHIFT_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_lshift_n(dst, src, n, c, lo);
        }

        return builtin_lshift_n(dst, src, n, c, lo);
    } else {
        return fallback_lshift_n(dst, src, n, c, lo);
    }
#else
    return fallback_lshift_n(dst, src, n, c, lo);
#endif
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR T fallback_rshift_n(T *dst, const T *src, size_t n,
                                            unsigned int c, T hi) {
    constexpr auto nd = std::numeric_limits<T>::digits;
    T ret = src[0] << (nd - c);
    for (size_t i = 0; i < n - 1; ++i) {
        dst[i] = fallback_shrd(src[i], src[i + 1], c);
    }
    dst[n - 1] = fallback_shrd(src[n - 1], hi, c);
    return ret;
}

/*
require :
1. n >= 1
2. WJR_IS_SAME_OR_INCR_P(dst, n, src, n)
*/
template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E T rshift_n(T *dst, const T *src, size_t n, unsigned int c,
                                     T hi = 0) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src, n));
    WJR_ASSERT_L1(c < std::numeric_limits<T>::digits);

    if (WJR_UNLIKELY(c == 0)) {
        if (WJR_LIKELY(dst != src)) {
            std::copy(src, src + n, dst);
        }

        return 0;
    }

#if WJR_HAS_BUILTIN(RSHIFT_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_rshift_n(dst, src, n, c, hi);
        }

        return builtin_rshift_n(dst, src, n, c, hi);
    } else {
        return fallback_rshift_n(dst, src, n, c, hi);
    }
#else
    return fallback_rshift_n(dst, src, n, c, hi);
#endif
}

} // namespace wjr

#endif // WJR_MATH_SHIFT_HPP__
#ifndef WJR_MATH_STACK_ALLOCATOR_HPP__
#define WJR_MATH_STACK_ALLOCATOR_HPP__

#ifndef WJR_STACK_ALLOCATOR_HPP__
#define WJR_STACK_ALLOCATOR_HPP__

#include <algorithm>

#ifndef WJR_CRTP_NONSENDABLE_HPP__
#define WJR_CRTP_NONSENDABLE_HPP__

#include <memory>

// Already included

#if WJR_DEBUG_LEVEL > 2
#define WJR_HAS_DEBUG_NONSENDABLE_CHECKER WJR_HAS_DEF
#endif

#if WJR_HAS_DEBUG(NONSENDABLE_CHECKER)
#include <thread>
#endif

namespace wjr {

#if WJR_HAS_DEBUG(NONSENDABLE_CHECKER)

/**
 * @brief Disable sending the object to another thread and check the thread id.
 *
 * @note Only check if WJR_DEBUG_LEVEL > 2.
 */
template <typename Tag = void>
class __nonsendable_checker {
public:
    static constexpr bool is_nonsendable = true;

protected:
    __nonsendable_checker() : m_thread_id(std::this_thread::get_id()) {}
    __nonsendable_checker(const __nonsendable_checker &) = default;
    __nonsendable_checker(__nonsendable_checker &&) = default;
    __nonsendable_checker &operator=(const __nonsendable_checker &) = default;
    __nonsendable_checker &operator=(__nonsendable_checker &&) = default;
    ~__nonsendable_checker() { check(); }

    void check() const {
        WJR_ASSERT_LX(m_thread_id == std::this_thread::get_id(),
                      "Cross-thread access detected when using a nonsendable object.");
    }

    friend bool operator==(const __nonsendable_checker &lhs,
                           const __nonsendable_checker &rhs) {
        return lhs.m_thread_id == rhs.m_thread_id;
    }

    friend bool operator!=(const __nonsendable_checker &lhs,
                           const __nonsendable_checker &rhs) {
        return lhs.m_thread_id != rhs.m_thread_id;
    }

private:
    std::thread::id m_thread_id;
};

#else

/**
 * @brief Disable sending the object to another thread without checking.
 *
 */
template <typename Tag = void>
class __nonsendable_checker {
public:
    static constexpr bool is_nonsendable = true;

protected:
    constexpr static void check(){};

    friend bool operator==(const __nonsendable_checker &, const __nonsendable_checker &) {
        return true;
    }

    friend bool operator!=(const __nonsendable_checker &, const __nonsendable_checker &) {
        return false;
    }
};

#endif

/**
 * @brief A type to disable sending the object to another thread.
 *
 * @note By default, only check if object is destroyed and WJR_DEBUG_LEVEL > 2.
 * Use nonsendable::check() to manually check.
 *
 */
template <typename Tag = void>
using nonsendable = __nonsendable_checker<Tag>;

} // namespace wjr

#endif // WJR_CRTP_NONSENDABLE_HPP__
#ifndef WJR_MEMORY_MEMORY_POOL_HPP__
#define WJR_MEMORY_MEMORY_POOL_HPP__

// Already included
// Already included

namespace wjr {

template <int __inst>
class __malloc_alloc_template__ {
private:
    WJR_NOINLINE static void *_S_oom_malloc(size_t);
    static void *_S_oom_realloc(void *, size_t);

#ifndef __STL_STATIC_TEMPLATE_MEMBER_BUG
    static void (*__malloc_alloc_oom_handler)();
#endif

public:
    WJR_INTRINSIC_INLINE static void *allocate(size_t __n) {
        void *__result = malloc(__n);
        if (WJR_LIKELY(0 != __result))
            return __result;
        return _S_oom_malloc(__n);
    }

    static void deallocate(void *__p, size_t /* __n */) { free(__p); }

    static void *reallocate(void *__p, size_t /* old_sz */, size_t __new_sz) {
        void *__result = realloc(__p, __new_sz);
        if (WJR_LIKELY(0 != __result))
            return __result;
        return _S_oom_realloc(__p, __new_sz);
    }

    static void (*__set_malloc_handler(void (*__f)()))() {
        void (*__old)() = __malloc_alloc_oom_handler;
        __malloc_alloc_oom_handler = __f;
        return (__old);
    }
};

// malloc_alloc out-of-memory handling

#ifndef __STL_STATIC_TEMPLATE_MEMBER_BUG
template <int __inst>
void (*__malloc_alloc_template__<__inst>::__malloc_alloc_oom_handler)() = 0;
#endif

template <int __inst>
void *__malloc_alloc_template__<__inst>::_S_oom_malloc(size_t __n) {
    void (*__my_malloc_handler)();
    void *__result;

    for (;;) {
        __my_malloc_handler = __malloc_alloc_oom_handler;
        (*__my_malloc_handler)();
        __result = malloc(__n);
        if (__result)
            return (__result);
    }
}

template <int __inst>
void *__malloc_alloc_template__<__inst>::_S_oom_realloc(void *__p, size_t __n) {
    void (*__my_malloc_handler)();
    void *__result;

    for (;;) {
        __my_malloc_handler = __malloc_alloc_oom_handler;

        (*__my_malloc_handler)();
        __result = realloc(__p, __n);
        if (__result)
            return (__result);
    }
}

template <int inst>
class __default_alloc_template__ {
private:
    using allocator_type = __malloc_alloc_template__<0>;

    enum { ALLOC_ALIGN = 8 };
    enum { ALLOC_MAX_BYTES = 256 };
    enum { ALLOC_NFRELISTS = ALLOC_MAX_BYTES / ALLOC_ALIGN };

    enum { __ALIGN = ALLOC_ALIGN };
    enum { __MAX_BYTES = ALLOC_MAX_BYTES };
    enum { __NFREELISTS = ALLOC_NFRELISTS };

    union obj {
        union obj *free_list_link;
        char client_data[1];
    };

    struct object {
        obj *volatile free_list[__NFREELISTS] = {nullptr};
        char *start_free = nullptr;
        char *end_free = nullptr;
        size_t heap_size = 0;
    };

    static object &get_instance() noexcept {
        static thread_local object instance;
        return instance;
    }

    static obj *volatile *get_free_list() noexcept { return get_instance().free_list; }
    static char *&get_start_free() noexcept { return get_instance().start_free; }
    static char *&get_end_free() noexcept { return get_instance().end_free; }
    static size_t &get_heap_size() noexcept { return get_instance().heap_size; }

    static inline size_t ROUND_UP(size_t bytes) {
        return (((bytes) + __ALIGN - 1) & ~(__ALIGN - 1));
    }

    static inline size_t FREELIST_INDEX(size_t bytes) {
        return (((bytes) + __ALIGN - 1) / __ALIGN - 1);
    }

    // Returns an object of size n, and optionally adds to size n free list.
    WJR_NOINLINE static void *refill(size_t n) noexcept;

    // Allocates a chunk for nobjs of size "size".  nobjs may be reduced
    // if it is inconvenient to allocate the requested number.
    static char *chunk_alloc(size_t size, int &nobjs) noexcept;

public:
    inline static void *allocate(size_t n) noexcept // n must be > 0
    {
        if (n > (size_t)__MAX_BYTES) {
            return allocator_type::allocate(n);
        }
        obj *volatile *my_free_list = get_free_list() + FREELIST_INDEX(n);
        obj *result = *my_free_list;
        if (WJR_LIKELY(result != nullptr)) {
            *my_free_list = result->free_list_link;
            return result;
        }
        return refill(ROUND_UP(n));
    }

    inline static void deallocate(void *p, size_t n) noexcept // p may not be 0
    {

        if (n > (size_t)__MAX_BYTES) {
            allocator_type::deallocate(p, n);
            return;
        }

        obj *q = (obj *)p;
        obj *volatile *my_free_list = get_free_list() + FREELIST_INDEX(n);
        q->free_list_link = *my_free_list;
        *my_free_list = q;
    }
};

//----------------------------------------------
// We allocate memory in large chunks in order to
// avoid fragmentingthe malloc heap too much.
// We assume that size is properly aligned.
// We hold the allocation lock.
//----------------------------------------------
template <int inst>
char *__default_alloc_template__<inst>::chunk_alloc(size_t size, int &nobjs) noexcept {
    char *result;
    size_t total_bytes = size * nobjs;
    const auto bytes_left = static_cast<size_t>(get_end_free() - get_start_free());

    if (bytes_left >= total_bytes) {
        result = get_start_free();
        get_start_free() += total_bytes;
        return (result);
    }

    if (bytes_left >= size) {
        nobjs = static_cast<int>(bytes_left / size);
        total_bytes = size * nobjs;
        result = get_start_free();
        get_start_free() += total_bytes;
        return (result);
    }

    const size_t bytes_to_get = 2 * total_bytes + ROUND_UP(get_heap_size() >> 4);
    // Try to make use of the left-over piece.
    if (bytes_left > 0) {
        obj *volatile *my_free_list = get_free_list() + FREELIST_INDEX(bytes_left);

        ((obj *)get_start_free())->free_list_link = *my_free_list;
        *my_free_list = (obj *)get_start_free();
    }
    get_start_free() = (char *)malloc(bytes_to_get);
    if (WJR_UNLIKELY(0 == get_start_free())) {
        obj *volatile *my_free_list, *p;

        // Try to make do with what we have. That can't
        // hurt. We do not try smaller requests, since that tends
        // to result in disaster on multi-process machines.
        for (int i = static_cast<int>(size); i <= __MAX_BYTES; i += __ALIGN) {
            my_free_list = get_free_list() + FREELIST_INDEX(i);
            p = *my_free_list;
            if (0 != p) {
                *my_free_list = p->free_list_link;
                get_start_free() = (char *)p;
                get_end_free() = get_start_free() + i;
                return (chunk_alloc(size, nobjs));
                // Any leftover piece will eventually make it to the
                // right free list.
            }
        }
        get_end_free() = 0; // In case of exception.
        get_start_free() = (char *)malloc(bytes_to_get);
        // This should either throw an exception or
        // remedy the situation. Thus we assume it
        // succeeded.
    }
    get_heap_size() += bytes_to_get;
    get_end_free() = get_start_free() + bytes_to_get;
    return (chunk_alloc(size, nobjs));
}

//----------------------------------------------
// Returns an object of size n, and optionally adds
// to size n free list.We assume that n is properly aligned.
// We hold the allocation lock.
//----------------------------------------------
template <int inst>
void *__default_alloc_template__<inst>::refill(size_t n) noexcept {
    int nobjs = 20;
    char *chunk = chunk_alloc(n, nobjs);
    obj *current_obj;
    obj *next_obj;

    if (1 == nobjs)
        return (chunk);
    obj *volatile *my_free_list = get_free_list() + FREELIST_INDEX(n);

    // Build free list in chunk
    obj *result = (obj *)chunk;

    *my_free_list = current_obj = (obj *)(chunk + n);
    nobjs -= 2;
    while (nobjs) {
        --nobjs;
        next_obj = (obj *)((char *)current_obj + n);
        current_obj->free_list_link = next_obj;
        current_obj = next_obj;
    }
    current_obj->free_list_link = 0;
    return (result);
}

template <typename Ty>
class memory_pool {
private:
    using allocator_type = __default_alloc_template__<0>;

public:
    using value_type = Ty;
    using size_type = size_t;
    using difference_type = ptrdiff_t;
    using propagate_on_container_move_assignment = std::true_type;
    using is_always_equal = std::true_type;
    using is_trivially_allocator = std::true_type;

    template <typename Other>
    struct rebind {
        using other = memory_pool<Other>;
    };

    constexpr memory_pool() noexcept = default;
    constexpr memory_pool(const memory_pool &) noexcept = default;
    template <typename Other>
    constexpr memory_pool(const memory_pool<Other> &) noexcept {}
    ~memory_pool() = default;
    memory_pool &operator=(const memory_pool &) noexcept = default;

    WJR_NODISCARD WJR_CONSTEXPR20 WJR_MALLOC Ty *allocate(size_t n) const noexcept {
        if (WJR_UNLIKELY(0 == n)) {
            return nullptr;
        }

        return static_cast<Ty *>(allocator_type::allocate(sizeof(Ty) * n));
    }

    WJR_CONSTEXPR20 void deallocate(Ty *ptr, size_t n) const noexcept {
        if (WJR_UNLIKELY(0 == n)) {
            return;
        }

        return allocator_type::deallocate(static_cast<void *>(ptr), sizeof(Ty) * n);
    }

    constexpr size_t max_size() const { return static_cast<size_t>(-1) / sizeof(Ty); }
};

template <typename T, typename U>
constexpr bool operator==(const memory_pool<T> &, const memory_pool<U> &) {
    return true;
}

template <typename T, typename U>
constexpr bool operator!=(const memory_pool<T> &, const memory_pool<U> &) {
    return false;
}

} // namespace wjr

#endif // WJR_MEMORY_MEMORY_POOL_HPP__
// Already included

namespace wjr {

template <typename StackAllocator>
class unique_stack_allocator;

template <size_t Cache>
class stack_allocator_object {
    template <typename StackAllocator>
    friend class unique_stack_allocator;

    constexpr static uint16_t bufsize = 5;

    struct alloc_node {
        char *ptr;
        char *end;
    };

    struct large_stack_top {
        large_stack_top *prev;
        char buffer[];
    };

public:
    struct stack_top {
        char *ptr;
        uint16_t idx;
        large_stack_top *large;
    };

private:
    WJR_CONSTEXPR20 void *__large_allocate(size_t n, stack_top &top) {
        const auto buffer = (large_stack_top *)malloc(sizeof(large_stack_top) + n);
        buffer->prev = top.large;
        top.large = buffer;
        return buffer->buffer;
    }

    WJR_NOINLINE WJR_CONSTEXPR20 void __small_reallocate(stack_top &top) {
        if (WJR_UNLIKELY(top.idx == (uint16_t)in_place_max)) {
            top.idx = m_idx;
        }

        ++m_idx;
        if (WJR_UNLIKELY(m_idx == m_size)) {

            if (WJR_UNLIKELY(m_size == m_capacity)) {
                uint16_t new_capacity = m_idx + 2 * (bufsize - 1);
                memory_pool<alloc_node> pool;
                auto new_ptr = pool.allocate(new_capacity);
                if (WJR_LIKELY(m_idx != 0)) {
                    std::copy_n(m_ptr, m_idx, new_ptr);
                    pool.deallocate(m_ptr, m_capacity);
                }
                m_ptr = new_ptr;
                m_capacity = new_capacity;
            }

            ++m_size;

            const size_t capacity = Cache << ((3 * m_idx + 2) / 5);
            const auto buffer = static_cast<char *>(malloc(capacity));
            alloc_node node = {buffer, buffer + capacity};
            m_ptr[m_idx] = node;

            if (WJR_UNLIKELY(m_idx == 0)) {
                top.ptr = node.ptr;
                top.idx = 0;
            }

            m_cache = node;
        } else {
            m_cache = m_ptr[m_idx];
        }

        WJR_ASSERT(top.ptr != nullptr);
    }

    WJR_COLD WJR_CONSTEXPR20 void __small_redeallocate() {
        const uint16_t new_size = m_idx + bufsize - 1;

        for (uint16_t i = new_size; i < m_size; ++i) {
            free(m_ptr[i].ptr);
        }

        m_size = new_size;
    }

    WJR_CONSTEXPR20 void __small_deallocate(const stack_top &top) {
        if (WJR_UNLIKELY(top.ptr == nullptr)) {
            return;
        }

        m_cache.ptr = top.ptr;

        if (WJR_UNLIKELY(top.idx != (uint16_t)in_place_max)) {
            const uint16_t idx = top.idx;
            m_cache.end = m_ptr[idx].end;
            m_idx = idx;
            if (WJR_UNLIKELY(m_size - idx >= bufsize)) {
                __small_redeallocate();
            }
        }
    }

    WJR_MALLOC WJR_CONSTEXPR20 void *__small_allocate(size_t n, stack_top &top) {
        auto ptr = m_cache.ptr;

        if (WJR_UNLIKELY(static_cast<size_t>(m_cache.end - ptr) < n)) {
            __small_reallocate(top);
            ptr = m_cache.ptr;
        }

        WJR_ASSERT_ASSUME_L1(m_cache.ptr != nullptr);
        WJR_ASSERT_ASSUME_L1(top.ptr != nullptr);

        m_cache.ptr += n;
        return ptr;
    }

public:
    using value_type = void;
    using size_type = size_t;
    using difference_type = ptrdiff_t;
    using propagate_on_container_move_assignment = std::true_type;

    stack_allocator_object() noexcept = default;
    stack_allocator_object(stack_allocator_object &) = delete;
    stack_allocator_object(stack_allocator_object &&) = delete;
    stack_allocator_object &operator=(stack_allocator_object &) = delete;
    stack_allocator_object &operator=(stack_allocator_object &&) = delete;
    WJR_NOINLINE ~stack_allocator_object() {
        for (uint16_t i = 0; i < m_size; ++i) {
            free(m_ptr[i].ptr);
        }

        memory_pool<alloc_node> pool;
        pool.deallocate(m_ptr, m_capacity);
    }

    WJR_NODISCARD WJR_MALLOC WJR_CONSTEXPR20 void *allocate(size_t n, stack_top &top,
                                                            size_t threshold) {
        if (WJR_UNLIKELY(n >= threshold)) {
            return __large_allocate(n, top);
        }

        return __small_allocate(n, top);
    }

    WJR_CONSTEXPR20 void deallocate(const stack_top &top) {
        __small_deallocate(top);

        auto buffer = top.large;
        while (WJR_UNLIKELY(buffer != nullptr)) {
            auto prev = buffer->prev;
            free(buffer);
            buffer = prev;
        }
    }

    WJR_CONSTEXPR20 void set(stack_top &top) const noexcept {
        top.ptr = m_cache.ptr;
        top.idx = in_place_max;
        top.large = nullptr;
    }

private:
    alloc_node m_cache = {nullptr, nullptr};
    uint16_t m_idx = in_place_max;
    alignas(32) uint16_t m_size = 0;
    uint16_t m_capacity = 0;
    alloc_node *m_ptr = nullptr;
};

/**
 * @brief A stack allocator for fast simulation of stack memory on the heap, singleton
 * mode.
 *
 * @tparam Cache The size of the first heap memory allocation
 */
template <size_t Cache, size_t DefaultThreshold>
struct singleton_stack_allocator_object {
    using Instance = stack_allocator_object<Cache>;
    constexpr static size_t __default_threshold = DefaultThreshold;

    static_assert(DefaultThreshold < Cache, "DefaultThreshold must be less than Cache.");

    static Instance &get_instance() noexcept {
        static thread_local Instance instance;
        return instance;
    }
};

/**
 * @details Used for container. This allocator won't deallocate memory allocated by
 * __small_allocate until container is destroyed.
 *
 */
template <typename T, typename StackAllocator>
class weak_stack_allocator;

/**
 * @brief A unique stack allocator for fast simulation of stack memory on the heap.
 *
 * @details When a unique_stack_allocator object is destroyed, all the memory it allocates
 * is released.\n And a new unique_stack_allocator constructed in the lifetime of a
 * unique_stack_allocator object must be destroyed in the current lifetime.
 *
 */
template <typename StackAllocator>
class unique_stack_allocator
    : public nonsendable<unique_stack_allocator<StackAllocator>> {
    using Mybase = nonsendable<unique_stack_allocator<StackAllocator>>;
    using Instance = typename StackAllocator::Instance;
    using stack_top = typename Instance::stack_top;

    constexpr static size_t __default_threshold = StackAllocator::__default_threshold;

    template <typename T, typename S>
    friend class weak_stack_allocator;

public:
    unique_stack_allocator(const StackAllocator &al) noexcept
        : m_instance(&(al.get_instance())) {
        m_instance->set(m_top);
    }

    unique_stack_allocator(const unique_stack_allocator &) = delete;
    unique_stack_allocator(unique_stack_allocator &&) = delete;
    unique_stack_allocator &operator=(const unique_stack_allocator &) = delete;
    unique_stack_allocator &operator=(unique_stack_allocator &&) = delete;

    ~unique_stack_allocator() { m_instance->deallocate(m_top); }

    WJR_NODISCARD WJR_MALLOC WJR_CONSTEXPR20 void *
    allocate(size_t n, size_t threshold = __default_threshold) {
        Mybase::check();
        return m_instance->allocate(n, m_top, threshold);
    }

private:
    WJR_NODISCARD WJR_MALLOC WJR_CONSTEXPR20 void *__small_allocate(size_t n) {
        Mybase::check();
        return m_instance->__small_allocate(n, m_top);
    }

    Instance *m_instance;
    stack_top m_top;
};

template <typename StackAllocator>
unique_stack_allocator(const StackAllocator &) -> unique_stack_allocator<StackAllocator>;

/**
 * @brief Point to unique_stack_allocator.
 *
 * @details Use a pointer to unique_stack_allocator to allocate memory. This class must be
 * used carefully. If recursively using it as a reference and allocating memory,
 * unique_stack_allocator should be avoided from being reused in the current function.
 *
 */
template <typename T, size_t Cache, size_t DefaultThreshold>
class weak_stack_allocator<T, singleton_stack_allocator_object<Cache, DefaultThreshold>> {
    using StackAllocator = singleton_stack_allocator_object<Cache, DefaultThreshold>;
    using UniqueStackAllocator = unique_stack_allocator<StackAllocator>;

    constexpr static size_t __default_threshold = StackAllocator::__default_threshold;

    template <typename U, typename A>
    friend class weak_stack_allocator;

public:
    using value_type = T;
    using size_type = size_t;
    using difference_type = ptrdiff_t;
    using propagate_on_container_move_assignment = std::true_type;
    using is_trivially_allocator = std::true_type;

    weak_stack_allocator() noexcept = default;
    weak_stack_allocator(UniqueStackAllocator &alloc) noexcept : m_alloc(&alloc) {}
    weak_stack_allocator(const weak_stack_allocator &) noexcept = default;
    weak_stack_allocator &operator=(const weak_stack_allocator &) noexcept = default;
    weak_stack_allocator(weak_stack_allocator &&) noexcept = default;
    weak_stack_allocator &operator=(weak_stack_allocator &&) noexcept = default;
    ~weak_stack_allocator() noexcept = default;

    template <typename U>
    weak_stack_allocator(const weak_stack_allocator<U, StackAllocator> &other) noexcept
        : m_alloc(other.m_alloc) {}

    WJR_NODISCARD WJR_MALLOC WJR_CONSTEXPR20 T *allocate(size_type n) {
        const size_t size = n * sizeof(T);
        if (WJR_UNLIKELY(size >= __default_threshold)) {
            return static_cast<T *>(malloc(size));
        }

        return static_cast<T *>(m_alloc->__small_allocate(size));
    }

    WJR_CONSTEXPR20 void deallocate(WJR_MAYBE_UNUSED T *ptr,
                                    WJR_MAYBE_UNUSED size_type n) {
        const size_t size = n * sizeof(T);
        if (WJR_UNLIKELY(size >= __default_threshold)) {
            free(ptr);
        }
    }

private:
    UniqueStackAllocator *m_alloc = nullptr;
};

} // namespace wjr

#endif // WJR_STACK_ALLOCATOR_HPP__

namespace wjr::math_details {

using stack_alloc_object = singleton_stack_allocator_object<36 * 1024, 16 * 1024>;
inline constexpr stack_alloc_object stack_alloc = {};

using unique_stack_alloc = unique_stack_allocator<stack_alloc_object>;

template <typename T>
using weak_stack_alloc = weak_stack_allocator<T, stack_alloc_object>;

} // namespace wjr::math_details

#endif // WJR_MATH_STACK_ALLOCATOR_HPP__
// Already included

#if defined(WJR_X86)
#ifndef WJR_X86_MATH_MUL_HPP__
#define WJR_X86_MATH_MUL_HPP__

// Already included
#ifndef WJR_X86_MATH_MUL_IMPL_HPP__
#define WJR_X86_MATH_MUL_IMPL_HPP__

// Already included

#ifndef WJR_X86
#error "x86 required"
#endif

namespace wjr {

#define WJR_HAS_BUILTIN_MUL64 WJR_HAS_DEF

#if WJR_HAS_FEATURE(INT128)
#define WJR_HAS_BUILTIN_INT128_MUL64 WJR_HAS_DEF
#elif WJR_HAS_FEATURE(GCC_STYLE_INLINE_ASM)
#define WJR_HAS_BUILTIN_ASM_MUL64 WJR_HAS_DEF
#else
#undef WJR_HAS_BUILTIN_MUL64
#endif

#if defined(__BMI2__)
#define WJR_HAS_BUILTIN_MULX_U64 WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(MULX_U64) && WJR_HAS_FEATURE(GCC_STYLE_INLINE_ASM)
#define WJR_HAS_BUILTIN_ASM_MUL_1 WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(ASM_MUL_1) && defined(__ADX__)
#define WJR_HAS_BUILTIN_ASM_ADDMUL_1 WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(ASM_MUL_1) && WJR_HAS_BUILTIN(ASM_ADDMUL_1)
#define WJR_HAS_BUILTIN_ASM_BASECASE_MUL_S WJR_HAS_DEF
#define WJR_HAS_BUILTIN_ASM_BASECASE_SQR WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(ASM_ADDMUL_1)
#define WJR_HAS_BUILTIN_ASM_SUBMUL_1 WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(ASM_MUL_1)
#define WJR_HAS_BUILTIN_ASM_ADDLSH_N WJR_HAS_DEF
#define WJR_HAS_BUILTIN_ASM_RSBLSH_N WJR_HAS_DEF
#endif

} // namespace wjr

#endif // WJR_X86_MATH_MUL_IMPL_HPP__

namespace wjr {

#if WJR_HAS_BUILTIN(MUL64)

WJR_INTRINSIC_INLINE uint64_t builtin_mul64(uint64_t a, uint64_t b, uint64_t &hi) {
#if WJR_HAS_BUILTIN(INT128_MUL64)
    const __uint128_t x = static_cast<__uint128_t>(a) * b;
    hi = x >> 64;
    return static_cast<uint64_t>(x);
#elif WJR_HAS_BUILTIN(ASM_MUL64)
    uint64_t lo;
    asm("mul{q %3| %3}\n\t" : "=a,a"(lo), "=d,d"(hi) : "%a,r"(a), "r,a"(b) : "cc");
    return lo;
#endif
}

#endif

#if WJR_HAS_BUILTIN(MULX_U64)

template <typename T>
WJR_INTRINSIC_INLINE T mulx(T a, T b, T &hi) {
    static_assert(sizeof(T) == 8, "");

#if defined(WJR_COMPILER_GCC)
    T lo;
    asm("mulx{q %3, %0, %1| %1, %0, %3}" : "=r"(lo), "=r"(hi) : "%d"(a), "r"(b));
    return lo;
#else
    unsigned long long hi_;
    unsigned long long lo = _mulx_u64(a, b, &hi_);
    hi = hi_;
    return lo;
#endif
}

#endif

#if WJR_HAS_BUILTIN(ASM_MUL_1)

inline uint64_t asm_mul_1(uint64_t *dst, const uint64_t *src, size_t n, uint64_t rdx) {
    size_t rcx = n / 8;
    uint64_t r8, r9, r10 = n, r11;

    const auto pdst = dst;
    const auto psrc = src;

    (void)(pdst);
    (void)(psrc);

    asm volatile(
        "and{l $7, %k[r10]| %k[r10], 7}\n\t"
        "lea{q| %[r9], [rip +} .Llookup%={(%%rip), %[r9]|]}\n\t"
        "movs{lq (%[r9], %[r10], 4), %[r10]|xd %[r10], DWORD PTR [%[r9] + "
        "%[r10] * 4]}\n\t"
        "lea{q (%[r9], %[r10], 1), %[r10]| %[r10], [%[r10] + %[r9]]}\n\t"
        "jmp{q *%[r10]| %[r10]}\n\t"

        ".align 8\n\t"
        ".Llookup%=:\n\t"
        ".long .Ll0%=-.Llookup%=\n\t"
        ".long .Ll1%=-.Llookup%=\n\t"
        ".long .Ll2%=-.Llookup%=\n\t"
        ".long .Ll3%=-.Llookup%=\n\t"
        ".long .Ll4%=-.Llookup%=\n\t"
        ".long .Ll5%=-.Llookup%=\n\t"
        ".long .Ll6%=-.Llookup%=\n\t"
        ".long .Ll7%=-.Llookup%=\n\t"
        ".align 16\n\t"

        ".Ll0%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "jmp .Lb0%=\n\t"

        ".Ll2%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "lea{q -48(%[src]), %[src]| %[src], [%[src] - 48]}\n\t"
        "lea{q -48(%[dst]), %[dst]| %[dst], [%[dst] - 48]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb2%=\n\t"

        ".Ll3%=:\n\t"
        "mulx{q (%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src]]}\n\t"
        "lea{q -40(%[src]), %[src]| %[src], [%[src] - 40]}\n\t"
        "lea{q -40(%[dst]), %[dst]| %[dst], [%[dst] - 40]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb3%=\n\t"

        ".Ll4%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "lea{q -32(%[src]), %[src]| %[src], [%[src] - 32]}\n\t"
        "lea{q -32(%[dst]), %[dst]| %[dst], [%[dst] - 32]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb4%=\n\t"

        ".Ll5%=:\n\t"
        "mulx{q (%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src]]}\n\t"
        "lea{q -24(%[src]), %[src]| %[src], [%[src] - 24]}\n\t"
        "lea{q -24(%[dst]), %[dst]| %[dst], [%[dst] - 24]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb5%=\n\t"

        ".Ll6%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "lea{q -16(%[src]), %[src]| %[src], [%[src] - 16]}\n\t"
        "lea{q -16(%[dst]), %[dst]| %[dst], [%[dst] - 16]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb6%=\n\t"

        ".Ll7%=:\n\t"
        "mulx{q (%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src]]}\n\t"
        "lea{q -8(%[src]), %[src]| %[src], [%[src] - 8]}\n\t"
        "lea{q -8(%[dst]), %[dst]| %[dst], [%[dst] - 8]}\n\t"
        "inc %[rcx]\n\t"
        "jmp .Lb7%=\n\t"

        ".Ld1%=:\n\t"
        "jmp .Ldone%=\n\t"

        ".Ll1%=:\n\t"
        "mulx{q (%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src]]}\n\t"
        "lea{q 8(%[src]), %[src]| %[src], [%[src] + 8]}\n\t"
        "lea{q 8(%[dst]), %[dst]| %[dst], [%[dst] + 8]}\n\t"
        "jrcxz .Ld1%=\n\t"

        ".align 32\n\t"
        ".Lloop%=:\n\t"

        ".Lb1%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "mov{q %[r8], -8(%[dst])| [%[dst] - 8], %[r8]}\n\t"
        "adc{q %[r9], %[r10]| %[r10], %[r9]}\n\t"

        ".Lb0%=:\n\t"
        "mulx{q 8(%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src] + 8]}\n\t"
        "mov{q %[r10], (%[dst])| [%[dst]], %[r10]}\n\t"
        "adc{q %[r11], %[r8]| %[r8], %[r11]}\n\t"

        ".Lb7%=:\n\t"
        "mulx{q 16(%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src] + 16]}\n\t"
        "mov{q %[r8], 8(%[dst])| [%[dst] + 8], %[r8]}\n\t"
        "adc{q %[r9], %[r10]| %[r10], %[r9]}\n\t"

        ".Lb6%=:\n\t"
        "mulx{q 24(%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src] + 24]}\n\t"
        "mov{q %[r10], 16(%[dst])| [%[dst] + 16], %[r10]}\n\t"
        "adc{q %[r11], %[r8]| %[r8], %[r11]}\n\t"

        ".Lb5%=:\n\t"
        "mulx{q 32(%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src] + 32]}\n\t"
        "mov{q %[r8], 24(%[dst])| [%[dst] + 24], %[r8]}\n\t"
        "adc{q %[r9], %[r10]| %[r10], %[r9]}\n\t"

        ".Lb4%=:\n\t"
        "mulx{q 40(%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src] + 40]}\n\t"
        "mov{q %[r10], 32(%[dst])| [%[dst] + 32], %[r10]}\n\t"
        "adc{q %[r11], %[r8]| %[r8], %[r11]}\n\t"

        ".Lb3%=:\n\t"
        "mulx{q 48(%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src] + 48]}\n\t"
        "mov{q %[r8], 40(%[dst])| [%[dst] + 40], %[r8]}\n\t"
        "adc{q %[r9], %[r10]| %[r10], %[r9]}\n\t"

        ".Lb2%=:\n\t"
        "mulx{q 56(%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src] + 56]}\n\t"
        "mov{q %[r10], 48(%[dst])| [%[dst] + 48], %[r10]}\n\t"
        "adc{q %[r11], %[r8]| %[r8], %[r11]}\n\t"

        "lea{q 64(%[src]), %[src]| %[src], [%[src] + 64]}\n\t"
        "lea{q 64(%[dst]), %[dst]| %[dst], [%[dst] + 64]}\n\t"

        "dec %[rcx]\n\t"
        "jne .Lloop%=\n\t"

        "adc{q %[rcx], %[r9]| %[r9], %[rcx]}\n\t"
        ".Ldone%=:\n\t"
        "mov{q %[r8], -8(%[dst])| [%[dst] - 8], %[r8]}\n\t"

        : [dst] "+&r"(dst), [src] "+&r"(src), [rcx] "+&c"(rcx), [r8] "=&r"(r8),
          [r9] "=&r"(r9), [r10] "+&r"(r10), [r11] "=&r"(r11)
        : "d"(rdx)
        : "cc", "memory");

    WJR_ASSERT_ASSUME(rcx == 0);
    WJR_ASSERT_ASSUME(dst == pdst + n);
    WJR_ASSERT_ASSUME(src == psrc + n);

    return r9;
}

#endif

#if WJR_HAS_BUILTIN(ASM_ADDMUL_1)

inline uint64_t asm_addmul_1(uint64_t *dst, const uint64_t *src, size_t n, uint64_t rdx) {
    size_t rcx = n / 8;
    uint64_t r8, r9, r10 = n, r11;

    const auto pdst = dst;
    const auto psrc = src;

    (void)(pdst);
    (void)(psrc);

    asm volatile(
        "and{l $7, %k[r10]| %k[r10], 7}\n\t"
        "lea{q| %[r9], [rip +} .Llookup%={(%%rip), %[r9]|]}\n\t"
        "movs{lq (%[r9], %[r10], 4), %[r10]|xd %[r10], DWORD PTR [%[r9] + "
        "%[r10] * 4]}\n\t"
        "lea{q (%[r9], %[r10], 1), %[r10]| %[r10], [%[r10] + %[r9]]}\n\t"
        "jmp{q *%[r10]| %[r10]}\n\t"

        ".align 8\n\t"
        ".Llookup%=:\n\t"
        ".long .Ll0%=-.Llookup%=\n\t"
        ".long .Ll1%=-.Llookup%=\n\t"
        ".long .Ll2%=-.Llookup%=\n\t"
        ".long .Ll3%=-.Llookup%=\n\t"
        ".long .Ll4%=-.Llookup%=\n\t"
        ".long .Ll5%=-.Llookup%=\n\t"
        ".long .Ll6%=-.Llookup%=\n\t"
        ".long .Ll7%=-.Llookup%=\n\t"
        ".align 16\n\t"

        ".Ll0%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "jmp .Lb0%=\n\t"

        ".Ll2%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "lea{q 16(%[src]), %[src]| %[src], [%[src] + 16]}\n\t"
        "lea{q -48(%[dst]), %[dst]| %[dst], [%[dst] - 48]}\n\t"
        "jmp .Lb2%=\n\t"

        ".Ll3%=:\n\t"
        "mulx{q (%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src]]}\n\t"
        "lea{q 24(%[src]), %[src]| %[src], [%[src] + 24]}\n\t"
        "lea{q -40(%[dst]), %[dst]| %[dst], [%[dst] - 40]}\n\t"
        "jmp .Lb3%=\n\t"

        ".Ll4%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "lea{q 32(%[src]), %[src]| %[src], [%[src] + 32]}\n\t"
        "lea{q -32(%[dst]), %[dst]| %[dst], [%[dst] - 32]}\n\t"
        "jmp .Lb4%=\n\t"

        ".Ll5%=:\n\t"
        "mulx{q (%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src]]}\n\t"
        "lea{q 40(%[src]), %[src]| %[src], [%[src] + 40]}\n\t"
        "lea{q -24(%[dst]), %[dst]| %[dst], [%[dst] - 24]}\n\t"
        "jmp .Lb5%=\n\t"

        ".Ll6%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "lea{q -16(%[src]), %[src]| %[src], [%[src] - 16]}\n\t"
        "lea{q -16(%[dst]), %[dst]| %[dst], [%[dst] - 16]}\n\t"
        "jmp .Lb6%=\n\t"

        ".Ll7%=:\n\t"
        "mulx{q (%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src]]}\n\t"
        "lea{q -8(%[src]), %[src]| %[src], [%[src] - 8]}\n\t"
        "lea{q -8(%[dst]), %[dst]| %[dst], [%[dst] - 8]}\n\t"
        "jmp .Lb7%=\n\t"

        ".Ld1%=:\n\t"
        "add{q -8(%[dst]), %[r8]| [%[dst] - 8], %[r8]}\n\t"
        "mov{q %[r8], -8(%[dst])| [%[dst] - 8], %[r8]}\n\t"
        "adc{q %[rcx], %[r9]| %[r9], %[rcx]}\n\t"
        "jmp .Ldone%=\n\t"

        ".Ll1%=:\n\t"
        "mulx{q (%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src]]}\n\t"
        "lea{q 8(%[src]), %[src]| %[src], [%[src] + 8]}\n\t"
        "lea{q 8(%[dst]), %[dst]| %[dst], [%[dst] + 8]}\n\t"
        "jrcxz .Ld1%=\n\t"

        ".align 32\n\t"
        ".Lloop%=:\n\t"

        ".Lb1%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "adcx{q -8(%[dst]), %[r8]| %[r8], [%[dst] - 8]}\n\t"
        "adox{q %[r9], %[r10]| %[r10], %[r9]}\n\t"
        "mov{q %[r8], -8(%[dst])| [%[dst] - 8], %[r8]}\n\t"

        ".Lb0%=:\n\t"
        "mulx{q 8(%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src] + 8]}\n\t"
        "lea{q -1(%[rcx]), %[rcx]| %[rcx], [%[rcx] - 1]}\n\t"
        "adox{q %[r11], %[r8]| %[r8], %[r11]}\n\t"
        "adcx{q (%[dst]), %[r10]| %[r10], [%[dst]]}\n\t"
        "mov{q %[r10], (%[dst])| [%[dst]], %[r10]}\n\t"

        ".Lb7%=:\n\t"
        "mulx{q 16(%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src] + 16]}\n\t"
        "adcx{q 8(%[dst]), %[r8]| %[r8], [%[dst] + 8]}\n\t"
        "adox{q %[r9], %[r10]| %[r10], %[r9]}\n\t"
        "mov{q %[r8], 8(%[dst])| [%[dst] + 8], %[r8]}\n\t"

        ".Lb6%=:\n\t"
        "mulx{q 24(%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src] + 24]}\n\t"
        "lea{q 64(%[src]), %[src]| %[src], [%[src] + 64]}\n\t"
        "adox{q %[r11], %[r8]| %[r8], %[r11]}\n\t"
        "adcx{q 16(%[dst]), %[r10]| %[r10], [%[dst] + 16]}\n\t"
        "mov{q %[r10], 16(%[dst])| [%[dst] + 16], %[r10]}\n\t"

        ".Lb5%=:\n\t"
        "mulx{q -32(%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src] - 32]}\n\t"
        "adcx{q 24(%[dst]), %[r8]| %[r8], [%[dst] + 24]}\n\t"
        "adox{q %[r9], %[r10]| %[r10], %[r9]}\n\t"
        "mov{q %[r8], 24(%[dst])| [%[dst] + 24], %[r8]}\n\t"

        ".Lb4%=:\n\t"
        "mulx{q -24(%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src] - 24]}\n\t"
        "adox{q %[r11], %[r8]| %[r8], %[r11]}\n\t"
        "adcx{q 32(%[dst]), %[r10]| %[r10], [%[dst] + 32]}\n\t"
        "mov{q %[r10], 32(%[dst])| [%[dst] + 32], %[r10]}\n\t"

        ".Lb3%=:\n\t"
        "mulx{q -16(%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src] - 16]}\n\t"
        "adcx{q 40(%[dst]), %[r8]| %[r8], [%[dst] + 40]}\n\t"
        "adox{q %[r9], %[r10]| %[r10], %[r9]}\n\t"
        "mov{q %[r8], 40(%[dst])| [%[dst] + 40], %[r8]}\n\t"

        ".Lb2%=:\n\t"
        "mulx{q -8(%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src] - 8]}\n\t"
        "adox{q %[r11], %[r8]| %[r8], %[r11]}\n\t"
        "adcx{q 48(%[dst]), %[r10]| %[r10], [%[dst] + 48]}\n\t"
        "mov{q %[r10], 48(%[dst])| [%[dst] + 48], %[r10]}\n\t"

        "lea{q 64(%[dst]), %[dst]| %[dst], [%[dst] + 64]}\n\t"

        "jrcxz .Lloop_out%=\n\t"
        "jmp .Lloop%=\n\t"
        ".Lloop_out%=:\n\t"

        "adcx{q -8(%[dst]), %[r8]| %[r8], [%[dst] - 8]}\n\t"
        "mov{q %[r8], -8(%[dst])| [%[dst] - 8], %[r8]}\n\t"
        "adox{q %[rcx], %[r9]| %[r9], %[rcx]}\n\t"
        "adc{q %[rcx], %[r9]| %[r9], %[rcx]}\n\t"

        ".Ldone%=:"

        : [dst] "+&r"(dst), [src] "+&r"(src), [rcx] "+&c"(rcx), [r8] "=&r"(r8),
          [r9] "=&r"(r9), [r10] "+&r"(r10), [r11] "=&r"(r11)
        : "d"(rdx)
        : "cc", "memory");

    WJR_ASSERT_ASSUME(rcx == 0);
    WJR_ASSERT_ASSUME(dst == pdst + n);
    WJR_ASSERT_ASSUME(src == psrc + n);

    return r9;
}

#endif

#if WJR_HAS_BUILTIN(ASM_BASECASE_MUL_S)

extern void __asm_basecase_mul_s_impl(uint64_t *dst, const uint64_t *src0, size_t rdx,
                                      const uint64_t *src1, size_t m);

inline void asm_basecase_mul_s(uint64_t *dst, const uint64_t *src0, size_t n,
                               const uint64_t *src1, size_t m) {
    WJR_ASSERT(n >= m);
    WJR_ASSERT(m >= 1);

    return __asm_basecase_mul_s_impl(dst, src0, n, src1, m);
}

#endif

#if WJR_HAS_BUILTIN(ASM_BASECASE_SQR)

extern void __asm_basecase_sqr_impl(uint64_t *dst, const uint64_t *src, size_t rdx);

inline void asm_basecase_sqr(uint64_t *dst, const uint64_t *src, size_t n) {
    WJR_ASSERT(n >= 1);

    return __asm_basecase_sqr_impl(dst, src, n);
}

#endif

#if WJR_HAS_BUILTIN(ASM_SUBMUL_1)

// slower than asm_addmul_1
inline uint64_t asm_submul_1(uint64_t *dst, const uint64_t *src, size_t n, uint64_t rdx) {
    WJR_ASSERT(n != 0);

    size_t rcx = n / 8;
    uint64_t r8, r9, r10 = static_cast<uint32_t>(n), r11;

    asm volatile(
        // set CF = 1, OF = 0
        "and{l $7, %k[r10]| %k[r10], 7}\n\t"
        "stc\n\t"

        "lea{q| %[r9], [rip +} .Llookup%={(%%rip), %[r9]|]}\n\t"
        "movs{lq (%[r9], %[r10], 4), %[r10]|xd %[r10], DWORD PTR [%[r9] + %[r10] * "
        "4]}\n\t"
        "lea{q (%[r9], %[r10], 1), %[r10]| %[r10], [%[r10] + %[r9]]}\n\t"
        "jmp{q *%[r10]| %[r10]}\n\t"

        ".align 8\n\t"
        ".Llookup%=:\n\t"
        ".long .Ll0%=-.Llookup%=\n\t"
        ".long .Ll1%=-.Llookup%=\n\t"
        ".long .Ll2%=-.Llookup%=\n\t"
        ".long .Ll3%=-.Llookup%=\n\t"
        ".long .Ll4%=-.Llookup%=\n\t"
        ".long .Ll5%=-.Llookup%=\n\t"
        ".long .Ll6%=-.Llookup%=\n\t"
        ".long .Ll7%=-.Llookup%=\n\t"
        ".align 16\n\t"

        ".Ll0%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "jmp .Lb0%=\n\t"

        ".Ll2%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "lea{q 16(%[src]), %[src]| %[src], [%[src] + 16]}\n\t"
        "lea{q -48(%[dst]), %[dst]| %[dst], [%[dst] - 48]}\n\t"
        "jmp .Lb2%=\n\t"

        ".Ll3%=:\n\t"
        "mulx{q (%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src]]}\n\t"
        "lea{q 24(%[src]), %[src]| %[src], [%[src] + 24]}\n\t"
        "lea{q -40(%[dst]), %[dst]| %[dst], [%[dst] - 40]}\n\t"
        "jmp .Lb3%=\n\t"

        ".Ll4%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "lea{q 32(%[src]), %[src]| %[src], [%[src] + 32]}\n\t"
        "lea{q -32(%[dst]), %[dst]| %[dst], [%[dst] - 32]}\n\t"
        "jmp .Lb4%=\n\t"

        ".Ll5%=:\n\t"
        "mulx{q (%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src]]}\n\t"
        "lea{q 40(%[src]), %[src]| %[src], [%[src] + 40]}\n\t"
        "lea{q -24(%[dst]), %[dst]| %[dst], [%[dst] - 24]}\n\t"
        "jmp .Lb5%=\n\t"

        ".Ll6%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "lea{q -16(%[src]), %[src]| %[src], [%[src] - 16]}\n\t"
        "lea{q -16(%[dst]), %[dst]| %[dst], [%[dst] - 16]}\n\t"
        "jmp .Lb6%=\n\t"

        ".Ll7%=:\n\t"
        "mulx{q (%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src]]}\n\t"
        "lea{q -8(%[src]), %[src]| %[src], [%[src] - 8]}\n\t"
        "lea{q -8(%[dst]), %[dst]| %[dst], [%[dst] - 8]}\n\t"
        "jmp .Lb7%=\n\t"

        ".Ld1%=:\n\t"
        "adc{q (%[dst]), %[r8]| [%[dst]], %[r8]}\n\t"
        "sbb{q $-1, %[r9]| %[r9], -1}\n\t"
        "mov{q %[r8], (%[dst])| [%[dst]], %[r8]}\n\t"
        "jmp .Ldone%=\n\t"

        ".Ll1%=:\n\t"
        "mulx{q (%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src]]}\n\t"
        "not %[r8]\n\t"
        "jrcxz .Ld1%=\n\t"
        "lea{q 8(%[src]), %[src]| %[src], [%[src] + 8]}\n\t"
        "lea{q 8(%[dst]), %[dst]| %[dst], [%[dst] + 8]}\n\t"

        ".align 32\n\t"
        ".Lloop%=:\n\t"

        ".Lb1%=:\n\t"
        "mulx{q (%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src]]}\n\t"
        "adcx{q -8(%[dst]), %[r8]| %[r8], [%[dst] - 8]}\n\t"
        "adox{q %[r9], %[r10]| %[r10], %[r9]}\n\t"
        "mov{q %[r8], -8(%[dst])| [%[dst] - 8], %[r8]}\n\t"

        ".Lb0%=:\n\t"
        "not %[r10]\n\t"
        "mulx{q 8(%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src] + 8]}\n\t"
        "lea{q -1(%[rcx]), %[rcx]| %[rcx], [%[rcx] - 1]}\n\t"
        "adox{q %[r11], %[r8]| %[r8], %[r11]}\n\t"
        "adcx{q (%[dst]), %[r10]| %[r10], [%[dst]]}\n\t"
        "mov{q %[r10], (%[dst])| [%[dst]], %[r10]}\n\t"

        ".Lb7%=:\n\t"
        "not %[r8]\n\t"
        "mulx{q 16(%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src] + 16]}\n\t"
        "adcx{q 8(%[dst]), %[r8]| %[r8], [%[dst] + 8]}\n\t"
        "adox{q %[r9], %[r10]| %[r10], %[r9]}\n\t"
        "mov{q %[r8], 8(%[dst])| [%[dst] + 8], %[r8]}\n\t"

        ".Lb6%=:\n\t"
        "not %[r10]\n\t"
        "mulx{q 24(%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src] + 24]}\n\t"
        "lea{q 64(%[src]), %[src]| %[src], [%[src] + 64]}\n\t"
        "adox{q %[r11], %[r8]| %[r8], %[r11]}\n\t"
        "adcx{q 16(%[dst]), %[r10]| %[r10], [%[dst] + 16]}\n\t"
        "mov{q %[r10], 16(%[dst])| [%[dst] + 16], %[r10]}\n\t"

        ".Lb5%=:\n\t"
        "not %[r8]\n\t"
        "mulx{q -32(%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src] - 32]}\n\t"
        "adcx{q 24(%[dst]), %[r8]| %[r8], [%[dst] + 24]}\n\t"
        "adox{q %[r9], %[r10]| %[r10], %[r9]}\n\t"
        "mov{q %[r8], 24(%[dst])| [%[dst] + 24], %[r8]}\n\t"

        ".Lb4%=:\n\t"
        "mulx{q -24(%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src] - 24]}\n\t"
        "not %[r10]\n\t"
        "adox{q %[r11], %[r8]| %[r8], %[r11]}\n\t"
        "adcx{q 32(%[dst]), %[r10]| %[r10], [%[dst] + 32]}\n\t"
        "mov{q %[r10], 32(%[dst])| [%[dst] + 32], %[r10]}\n\t"

        ".Lb3%=:\n\t"
        "not %[r8]\n\t"
        "mulx{q -16(%[src]), %[r10], %[r11]| %[r11], %[r10], [%[src] - 16]}\n\t"
        "adcx{q 40(%[dst]), %[r8]| %[r8], [%[dst] + 40]}\n\t"
        "adox{q %[r9], %[r10]| %[r10], %[r9]}\n\t"
        "mov{q %[r8], 40(%[dst])| [%[dst] + 40], %[r8]}\n\t"

        ".Lb2%=:\n\t"
        "not %[r10]\n\t"
        "mulx{q -8(%[src]), %[r8], %[r9]| %[r9], %[r8], [%[src] - 8]}\n\t"
        "adox{q %[r11], %[r8]| %[r8], %[r11]}\n\t"
        "adcx{q 48(%[dst]), %[r10]| %[r10], [%[dst] + 48]}\n\t"
        "not %[r8]\n\t"
        "mov{q %[r10], 48(%[dst])| [%[dst] + 48], %[r10]}\n\t"

        "lea{q 64(%[dst]), %[dst]| %[dst], [%[dst] + 64]}\n\t"

        "jrcxz .Lloop_out%=\n\t"
        "jmp .Lloop%=\n\t"
        ".Lloop_out%=:\n\t"

        "adcx{q -8(%[dst]), %[r8]| %[r8], [%[dst] - 8]}\n\t"
        "mov{q %[r8], -8(%[dst])| [%[dst] - 8], %[r8]}\n\t"
        "adox{q %[rcx], %[r9]| %[r9], %[rcx]}\n\t"
        "sbb{q $-1, %[r9]| %[r9], -1}\n\t"

        ".Ldone%=:"

        : [dst] "+&r"(dst), [src] "+&r"(src), [rcx] "+&c"(rcx), [r8] "=&r"(r8),
          [r9] "=&r"(r9), [r10] "+&r"(r10), [r11] "=&r"(r11)
        : "d"(rdx)
        : "cc", "memory");

    WJR_ASSUME(rcx == 0);

    return r9;
}

#endif

#if WJR_HAS_BUILTIN(ASM_ADDLSH_N)
#define WJR_ADDSUB_I 1
// WJR_ADDSUB_I :
// 0 : SUB
// 1 : ADD

#ifndef WJR_ADDSUB_I
#error "abort"
#endif

#define WJR_addsub WJR_PP_BOOL_IF(WJR_ADDSUB_I, add, rsb)
#define WJR_adcsbb WJR_PP_BOOL_IF(WJR_ADDSUB_I, adc, sbb)

inline uint64_t WJR_PP_CONCAT(asm_, WJR_PP_CONCAT(WJR_addsub, lsh_n))(
    uint64_t *dst, const uint64_t *src0, const uint64_t *src1, size_t n, uint64_t cl) {
    WJR_ASSERT(cl != 0);

    size_t rcx = n / 8;
    uint64_t tcl = 64 - cl;
    uint64_t r8, r9 = n, r10, r11;

    asm volatile(
        "and{l $7, %k[r9]| %k[r9], 7}\n\t"
        "lea{q| %[r8], [rip +} .Llookup%={(%%rip), %[r8]|]}\n\t"
        "movs{lq (%[r8], %[r9], 4), %[r9]|xd %[r9], DWORD PTR [%[r8] + "
        "%[r9] * 4]}\n\t"
        "lea{q (%[r8], %[r9], 1), %[r9]| %[r9], [%[r9] + %[r8]]}\n\t"
        "jmp{q *%[r9]| %[r9]}\n\t"

        ".align 8\n\t"
        ".Llookup%=:\n\t"
        ".long .Ll0%=-.Llookup%=\n\t"
        ".long .Ll1%=-.Llookup%=\n\t"
        ".long .Ll2%=-.Llookup%=\n\t"
        ".long .Ll3%=-.Llookup%=\n\t"
        ".long .Ll4%=-.Llookup%=\n\t"
        ".long .Ll5%=-.Llookup%=\n\t"
        ".long .Ll6%=-.Llookup%=\n\t"
        ".long .Ll7%=-.Llookup%=\n\t"
        ".align 16\n\t"

        ".Ll0%=:\n\t"
        "mov{q (%[src1]), %[r8]| %[r8], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r8], %[r11]| %[r11], %[r8], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r11]| %[r11], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r8], %[r8]| %[r8], %[r8], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r9]| %[r9], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r9], %[r10]| %[r10], %[r9], %[cl]}\n\t"
        "mov{q %[r11], (%[dst])| [%[dst]], %[r11]}\n\t"
        "jmp .Lb0%=\n\t"

        ".Ld1%=:\n\t"
        "mov{q %[r10], (%[dst])| [%[dst]], %[r10]}\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q %[rcx], %[r11]| %[r11], %[rcx]}\n\t"
        "jmp .Ldone%=\n\t"

        ".Ll1%=:\n\t"
        "mov{q (%[src1]), %[r11]| %[r11], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r11], %[r10]| %[r10], %[r11], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r10]| %[r10], [%[src0]]}\n\t"
        "jrcxz .Ld1%=\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r8]| %[r8], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r8], %[r9]| %[r9], %[r8], %[cl]}\n\t"
        "mov{q %[r10], (%[dst])| [%[dst]], %[r10]}\n\t"
        "lea{q 8(%[src0]), %[src0]| %[src0], [%[src0] + 8]}\n\t"
        "lea{q 8(%[src1]), %[src1]| %[src1], [%[src1] + 8]}\n\t"
        "lea{q 8(%[dst]), %[dst]| %[dst], [%[dst] + 8]}\n\t"
        "jmp .Lb1%=\n\t"

        ".Ll3%=:\n\t"
        "mov{q (%[src1]), %[r9]| %[r9], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r9], %[r8]| %[r8], %[r9], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r8]| %[r8], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r9], %[r9]| %[r9], %[r9], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r10]| %[r10], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r10], %[r11]| %[r11], %[r10], %[cl]}\n\t"
        "mov{q %[r8], (%[dst])| [%[dst]], %[r8]}\n\t"
        "lea{q -40(%[src0]), %[src0]| %[src0], [%[src0] - 40]}\n\t"
        "lea{q -40(%[src1]), %[src1]| %[src1], [%[src1] - 40]}\n\t"
        "lea{q -40(%[dst]), %[dst]| %[dst], [%[dst] - 40]}\n\t"
        "jmp .Lb3%=\n\t"

        ".Ll4%=:\n\t"
        "mov{q (%[src1]), %[r8]| %[r8], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r8], %[r11]| %[r11], %[r8], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r11]| %[r11], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r8], %[r8]| %[r8], %[r8], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r9]| %[r9], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r9], %[r10]| %[r10], %[r9], %[cl]}\n\t"
        "mov{q %[r11], (%[dst])| [%[dst]], %[r11]}\n\t"
        "lea{q -32(%[src0]), %[src0]| %[src0], [%[src0] - 32]}\n\t"
        "lea{q -32(%[src1]), %[src1]| %[src1], [%[src1] - 32]}\n\t"
        "lea{q -32(%[dst]), %[dst]| %[dst], [%[dst] - 32]}\n\t"
        "jmp .Lb4%=\n\t"

        ".Ll5%=:\n\t"
        "mov{q (%[src1]), %[r11]| %[r11], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r11], %[r10]| %[r10], %[r11], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r10]| %[r10], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r8]| %[r8], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r8], %[r9]| %[r9], %[r8], %[cl]}\n\t"
        "mov{q %[r10], (%[dst])| [%[dst]], %[r10]}\n\t"
        "lea{q -24(%[src0]), %[src0]| %[src0], [%[src0] - 24]}\n\t"
        "lea{q -24(%[src1]), %[src1]| %[src1], [%[src1] - 24]}\n\t"
        "lea{q -24(%[dst]), %[dst]| %[dst], [%[dst] - 24]}\n\t"
        "jmp .Lb5%=\n\t"

        ".Ll6%=:\n\t"
        "mov{q (%[src1]), %[r10]| %[r10], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r10], %[r9]| %[r9], %[r10], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r9]| %[r9], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r10], %[r10]| %[r10], %[r10], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r11]| %[r11], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r11], %[r8]| %[r8], %[r11], %[cl]}\n\t"
        "mov{q %[r9], (%[dst])| [%[dst]], %[r9]}\n\t"
        "lea{q -16(%[src0]), %[src0]| %[src0], [%[src0] - 16]}\n\t"
        "lea{q -16(%[src1]), %[src1]| %[src1], [%[src1] - 16]}\n\t"
        "lea{q -16(%[dst]), %[dst]| %[dst], [%[dst] - 16]}\n\t"
        "jmp .Lb6%=\n\t"

        ".Ll7%=:\n\t"
        "mov{q (%[src1]), %[r9]| %[r9], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r9], %[r8]| %[r8], %[r9], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r8]| %[r8], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r9], %[r9]| %[r9], %[r9], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r10]| %[r10], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r10], %[r11]| %[r11], %[r10], %[cl]}\n\t"
        "mov{q %[r8], (%[dst])| [%[dst]], %[r8]}\n\t"
        "lea{q -8(%[src0]), %[src0]| %[src0], [%[src0] - 8]}\n\t"
        "lea{q -8(%[src1]), %[src1]| %[src1], [%[src1] - 8]}\n\t"
        "lea{q -8(%[dst]), %[dst]| %[dst], [%[dst] - 8]}\n\t"
        "jmp .Lb7%=\n\t"

        ".Ld2%=:\n\t"
        "lea{q (%[r8], %[r10]), %[r10]| %[r10], [%[r10] + %[r8]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 8(%[src0]), %[r10]| %[r10], [%[src0] + 8]}\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q %[rcx], %[r11]| %[r11], %[rcx]}\n\t"
        "mov{q %[r10], 8(%[dst])| [%[dst] + 8], %[r10]}\n\t"
        "jmp .Ldone%=\n\t"

        ".Ll2%=:\n\t"
        "mov{q (%[src1]), %[r10]| %[r10], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r10], %[r9]| %[r9], %[r10], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r9]| %[r9], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r10], %[r10]| %[r10], %[r10], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r11]| %[r11], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r11], %[r8]| %[r8], %[r11], %[cl]}\n\t"
        "mov{q %[r9], (%[dst])| [%[dst]], %[r9]}\n\t"
        "jrcxz .Ld2%=\n\t"
        "lea{q 16(%[src0]), %[src0]| %[src0], [%[src0] + 16]}\n\t"
        "lea{q 16(%[src1]), %[src1]| %[src1], [%[src1] + 16]}\n\t"
        "lea{q 16(%[dst]), %[dst]| %[dst], [%[dst] + 16]}\n\t"

        ".align 32\n\t"
        ".Lloop%=:\n\t"

        ".Lb2%=:\n\t"
        "lea{q (%[r8], %[r10]), %[r10]| %[r10], [%[r10] + %[r8]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q -8(%[src0]), %[r10]| %[r10], [%[src0] - 8]}\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        "mov{q (%[src1]), %[r8]| %[r8], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r8], %[r9]| %[r9], %[r8], %[cl]}\n\t"
        "mov{q %[r10], -8(%[dst])| [%[dst] - 8], %[r10]}\n\t"

        ".Lb1%=:\n\t"
        "lea{q (%[r9], %[r11]), %[r11]| %[r11], [%[r11] + %[r9]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r11]| %[r11], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r8], %[r8]| %[r8], %[r8], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r9]| %[r9], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r9], %[r10]| %[r10], %[r9], %[cl]}\n\t"
        "mov{q %[r11], (%[dst])| [%[dst]], %[r11]}\n\t"

        ".Lb0%=:\n\t"
        "lea{q (%[r10], %[r8]), %[r8]| %[r8], [%[r8] + %[r10]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 8(%[src0]), %[r8]| %[r8], [%[src0] + 8]}\n\t"
        "shrx{q %[tcl], %[r9], %[r9]| %[r9], %[r9], %[tcl]}\n\t"
        "mov{q 16(%[src1]), %[r10]| %[r10], [%[src1] + 16]}\n\t"
        "shlx{q %[cl], %[r10], %[r11]| %[r11], %[r10], %[cl]}\n\t"
        "mov{q %[r8], 8(%[dst])| [%[dst] + 8], %[r8]}\n\t"

        "lea{q -1(%[rcx]), %[rcx]| %[rcx], [%[rcx] - 1]}\n\t"

        ".Lb7%=:\n\t"
        "lea{q (%[r11], %[r9]), %[r9]| %[r9], [%[r9] + %[r11]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 16(%[src0]), %[r9]| %[r9], [%[src0] + 16]}\n\t"
        "shrx{q %[tcl], %[r10], %[r10]| %[r10], %[r10], %[tcl]}\n\t"
        "mov{q 24(%[src1]), %[r11]| %[r11], [%[src1] + 24]}\n\t"
        "shlx{q %[cl], %[r11], %[r8]| %[r8], %[r11], %[cl]}\n\t"
        "mov{q %[r9], 16(%[dst])| [%[dst] + 16], %[r9]}\n\t"

        ".Lb6%=:\n\t"
        "lea{q (%[r8], %[r10]), %[r10]| %[r10], [%[r10] + %[r8]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 24(%[src0]), %[r10]| %[r10], [%[src0] + 24]}\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        "mov{q 32(%[src1]), %[r8]| %[r8], [%[src1] + 32]}\n\t"
        "shlx{q %[cl], %[r8], %[r9]| %[r9], %[r8], %[cl]}\n\t"
        "mov{q %[r10], 24(%[dst])| [%[dst] + 24], %[r10]}\n\t"

        ".Lb5%=:\n\t"
        "lea{q (%[r9], %[r11]), %[r11]| %[r11], [%[r11] + %[r9]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 32(%[src0]), %[r11]| %[r11], [%[src0] + 32]}\n\t"
        "shrx{q %[tcl], %[r8], %[r8]| %[r8], %[r8], %[tcl]}\n\t"
        "mov{q 40(%[src1]), %[r9]| %[r9], [%[src1] + 40]}\n\t"
        "shlx{q %[cl], %[r9], %[r10]| %[r10], %[r9], %[cl]}\n\t"
        "mov{q %[r11], 32(%[dst])| [%[dst] + 32], %[r11]}\n\t"

        ".Lb4%=:\n\t"
        "lea{q (%[r10], %[r8]), %[r8]| %[r8], [%[r8] + %[r10]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 40(%[src0]), %[r8]| %[r8], [%[src0] + 40]}\n\t"
        "shrx{q %[tcl], %[r9], %[r9]| %[r9], %[r9], %[tcl]}\n\t"
        "mov{q 48(%[src1]), %[r10]| %[r10], [%[src1] + 48]}\n\t"
        "shlx{q %[cl], %[r10], %[r11]| %[r11], %[r10], %[cl]}\n\t"
        "mov{q %[r8], 40(%[dst])| [%[dst] + 40], %[r8]}\n\t"

        ".Lb3%=:\n\t"
        "lea{q (%[r11], %[r9]), %[r9]| %[r9], [%[r9] + %[r11]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 48(%[src0]), %[r9]| %[r9], [%[src0] + 48]}\n\t"
        "shrx{q %[tcl], %[r10], %[r10]| %[r10], %[r10], %[tcl]}\n\t"
        "mov{q 56(%[src1]), %[r11]| %[r11], [%[src1] + 56]}\n\t"
        "shlx{q %[cl], %[r11], %[r8]| %[r8], %[r11], %[cl]}\n\t"
        "mov{q %[r9], 48(%[dst])| [%[dst] + 48], %[r9]}\n\t"

        "lea{q 64(%[src0]), %[src0]| %[src0], [%[src0] + 64]}\n\t"
        "lea{q 64(%[src1]), %[src1]| %[src1], [%[src1] + 64]}\n\t"
        "lea{q 64(%[dst]), %[dst]| %[dst], [%[dst] + 64]}\n\t"

        "jrcxz .Lloop_out%=\n\t"
        "jmp .Lloop%=\n\t"
        ".Lloop_out%=:\n\t"

        "lea{q (%[r8], %[r10]), %[r10]| %[r10], [%[r10] + %[r8]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q -8(%[src0]), %[r10]| %[r10], [%[src0] - 8]}\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q %[rcx], %[r11]| %[r11], %[rcx]}\n\t"
        "mov{q %[r10], -8(%[dst])| [%[dst] - 8], %[r10]}\n\t"

        ".Ldone%=:"

        : [dst] "+&r"(dst), [src0] "+&r"(src0), [src1] "+&r"(src1), [rcx] "+&c"(rcx),
          [r8] "=&r"(r8), [r9] "+&r"(r9), [r10] "=&r"(r10), [r11] "=&r"(r11)
        : [cl] "r"(cl), [tcl] "r"(tcl)
        : "cc", "memory");

    WJR_ASSERT_ASSUME(rcx == 0);

    return r11;
}

#undef WJR_adcsbb
#undef WJR_addcsubc

#undef WJR_ADDSUB_I
#endif

#if WJR_HAS_BUILTIN(ASM_RSBLSH_N)
#define WJR_ADDSUB_I 0
// WJR_ADDSUB_I :
// 0 : SUB
// 1 : ADD

#ifndef WJR_ADDSUB_I
#error "abort"
#endif

#define WJR_addsub WJR_PP_BOOL_IF(WJR_ADDSUB_I, add, rsb)
#define WJR_adcsbb WJR_PP_BOOL_IF(WJR_ADDSUB_I, adc, sbb)

inline uint64_t WJR_PP_CONCAT(asm_, WJR_PP_CONCAT(WJR_addsub, lsh_n))(
    uint64_t *dst, const uint64_t *src0, const uint64_t *src1, size_t n, uint64_t cl) {
    WJR_ASSERT(cl != 0);

    size_t rcx = n / 8;
    uint64_t tcl = 64 - cl;
    uint64_t r8, r9 = n, r10, r11;

    asm volatile(
        "and{l $7, %k[r9]| %k[r9], 7}\n\t"
        "lea{q| %[r8], [rip +} .Llookup%={(%%rip), %[r8]|]}\n\t"
        "movs{lq (%[r8], %[r9], 4), %[r9]|xd %[r9], DWORD PTR [%[r8] + "
        "%[r9] * 4]}\n\t"
        "lea{q (%[r8], %[r9], 1), %[r9]| %[r9], [%[r9] + %[r8]]}\n\t"
        "jmp{q *%[r9]| %[r9]}\n\t"

        ".align 8\n\t"
        ".Llookup%=:\n\t"
        ".long .Ll0%=-.Llookup%=\n\t"
        ".long .Ll1%=-.Llookup%=\n\t"
        ".long .Ll2%=-.Llookup%=\n\t"
        ".long .Ll3%=-.Llookup%=\n\t"
        ".long .Ll4%=-.Llookup%=\n\t"
        ".long .Ll5%=-.Llookup%=\n\t"
        ".long .Ll6%=-.Llookup%=\n\t"
        ".long .Ll7%=-.Llookup%=\n\t"
        ".align 16\n\t"

        ".Ll0%=:\n\t"
        "mov{q (%[src1]), %[r8]| %[r8], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r8], %[r11]| %[r11], %[r8], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r11]| %[r11], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r8], %[r8]| %[r8], %[r8], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r9]| %[r9], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r9], %[r10]| %[r10], %[r9], %[cl]}\n\t"
        "mov{q %[r11], (%[dst])| [%[dst]], %[r11]}\n\t"
        "jmp .Lb0%=\n\t"

        ".Ld1%=:\n\t"
        "mov{q %[r10], (%[dst])| [%[dst]], %[r10]}\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q %[rcx], %[r11]| %[r11], %[rcx]}\n\t"
        "jmp .Ldone%=\n\t"

        ".Ll1%=:\n\t"
        "mov{q (%[src1]), %[r11]| %[r11], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r11], %[r10]| %[r10], %[r11], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r10]| %[r10], [%[src0]]}\n\t"
        "jrcxz .Ld1%=\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r8]| %[r8], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r8], %[r9]| %[r9], %[r8], %[cl]}\n\t"
        "mov{q %[r10], (%[dst])| [%[dst]], %[r10]}\n\t"
        "lea{q 8(%[src0]), %[src0]| %[src0], [%[src0] + 8]}\n\t"
        "lea{q 8(%[src1]), %[src1]| %[src1], [%[src1] + 8]}\n\t"
        "lea{q 8(%[dst]), %[dst]| %[dst], [%[dst] + 8]}\n\t"
        "jmp .Lb1%=\n\t"

        ".Ll3%=:\n\t"
        "mov{q (%[src1]), %[r9]| %[r9], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r9], %[r8]| %[r8], %[r9], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r8]| %[r8], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r9], %[r9]| %[r9], %[r9], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r10]| %[r10], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r10], %[r11]| %[r11], %[r10], %[cl]}\n\t"
        "mov{q %[r8], (%[dst])| [%[dst]], %[r8]}\n\t"
        "lea{q -40(%[src0]), %[src0]| %[src0], [%[src0] - 40]}\n\t"
        "lea{q -40(%[src1]), %[src1]| %[src1], [%[src1] - 40]}\n\t"
        "lea{q -40(%[dst]), %[dst]| %[dst], [%[dst] - 40]}\n\t"
        "jmp .Lb3%=\n\t"

        ".Ll4%=:\n\t"
        "mov{q (%[src1]), %[r8]| %[r8], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r8], %[r11]| %[r11], %[r8], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r11]| %[r11], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r8], %[r8]| %[r8], %[r8], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r9]| %[r9], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r9], %[r10]| %[r10], %[r9], %[cl]}\n\t"
        "mov{q %[r11], (%[dst])| [%[dst]], %[r11]}\n\t"
        "lea{q -32(%[src0]), %[src0]| %[src0], [%[src0] - 32]}\n\t"
        "lea{q -32(%[src1]), %[src1]| %[src1], [%[src1] - 32]}\n\t"
        "lea{q -32(%[dst]), %[dst]| %[dst], [%[dst] - 32]}\n\t"
        "jmp .Lb4%=\n\t"

        ".Ll5%=:\n\t"
        "mov{q (%[src1]), %[r11]| %[r11], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r11], %[r10]| %[r10], %[r11], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r10]| %[r10], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r8]| %[r8], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r8], %[r9]| %[r9], %[r8], %[cl]}\n\t"
        "mov{q %[r10], (%[dst])| [%[dst]], %[r10]}\n\t"
        "lea{q -24(%[src0]), %[src0]| %[src0], [%[src0] - 24]}\n\t"
        "lea{q -24(%[src1]), %[src1]| %[src1], [%[src1] - 24]}\n\t"
        "lea{q -24(%[dst]), %[dst]| %[dst], [%[dst] - 24]}\n\t"
        "jmp .Lb5%=\n\t"

        ".Ll6%=:\n\t"
        "mov{q (%[src1]), %[r10]| %[r10], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r10], %[r9]| %[r9], %[r10], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r9]| %[r9], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r10], %[r10]| %[r10], %[r10], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r11]| %[r11], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r11], %[r8]| %[r8], %[r11], %[cl]}\n\t"
        "mov{q %[r9], (%[dst])| [%[dst]], %[r9]}\n\t"
        "lea{q -16(%[src0]), %[src0]| %[src0], [%[src0] - 16]}\n\t"
        "lea{q -16(%[src1]), %[src1]| %[src1], [%[src1] - 16]}\n\t"
        "lea{q -16(%[dst]), %[dst]| %[dst], [%[dst] - 16]}\n\t"
        "jmp .Lb6%=\n\t"

        ".Ll7%=:\n\t"
        "mov{q (%[src1]), %[r9]| %[r9], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r9], %[r8]| %[r8], %[r9], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r8]| %[r8], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r9], %[r9]| %[r9], %[r9], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r10]| %[r10], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r10], %[r11]| %[r11], %[r10], %[cl]}\n\t"
        "mov{q %[r8], (%[dst])| [%[dst]], %[r8]}\n\t"
        "lea{q -8(%[src0]), %[src0]| %[src0], [%[src0] - 8]}\n\t"
        "lea{q -8(%[src1]), %[src1]| %[src1], [%[src1] - 8]}\n\t"
        "lea{q -8(%[dst]), %[dst]| %[dst], [%[dst] - 8]}\n\t"
        "jmp .Lb7%=\n\t"

        ".Ld2%=:\n\t"
        "lea{q (%[r8], %[r10]), %[r10]| %[r10], [%[r10] + %[r8]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 8(%[src0]), %[r10]| %[r10], [%[src0] + 8]}\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q %[rcx], %[r11]| %[r11], %[rcx]}\n\t"
        "mov{q %[r10], 8(%[dst])| [%[dst] + 8], %[r10]}\n\t"
        "jmp .Ldone%=\n\t"

        ".Ll2%=:\n\t"
        "mov{q (%[src1]), %[r10]| %[r10], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r10], %[r9]| %[r9], %[r10], %[cl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r9]| %[r9], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r10], %[r10]| %[r10], %[r10], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r11]| %[r11], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r11], %[r8]| %[r8], %[r11], %[cl]}\n\t"
        "mov{q %[r9], (%[dst])| [%[dst]], %[r9]}\n\t"
        "jrcxz .Ld2%=\n\t"
        "lea{q 16(%[src0]), %[src0]| %[src0], [%[src0] + 16]}\n\t"
        "lea{q 16(%[src1]), %[src1]| %[src1], [%[src1] + 16]}\n\t"
        "lea{q 16(%[dst]), %[dst]| %[dst], [%[dst] + 16]}\n\t"

        ".align 32\n\t"
        ".Lloop%=:\n\t"

        ".Lb2%=:\n\t"
        "lea{q (%[r8], %[r10]), %[r10]| %[r10], [%[r10] + %[r8]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q -8(%[src0]), %[r10]| %[r10], [%[src0] - 8]}\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        "mov{q (%[src1]), %[r8]| %[r8], [%[src1]]}\n\t"
        "shlx{q %[cl], %[r8], %[r9]| %[r9], %[r8], %[cl]}\n\t"
        "mov{q %[r10], -8(%[dst])| [%[dst] - 8], %[r10]}\n\t"

        ".Lb1%=:\n\t"
        "lea{q (%[r9], %[r11]), %[r11]| %[r11], [%[r11] + %[r9]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q (%[src0]), %[r11]| %[r11], [%[src0]]}\n\t"
        "shrx{q %[tcl], %[r8], %[r8]| %[r8], %[r8], %[tcl]}\n\t"
        "mov{q 8(%[src1]), %[r9]| %[r9], [%[src1] + 8]}\n\t"
        "shlx{q %[cl], %[r9], %[r10]| %[r10], %[r9], %[cl]}\n\t"
        "mov{q %[r11], (%[dst])| [%[dst]], %[r11]}\n\t"

        ".Lb0%=:\n\t"
        "lea{q (%[r10], %[r8]), %[r8]| %[r8], [%[r8] + %[r10]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 8(%[src0]), %[r8]| %[r8], [%[src0] + 8]}\n\t"
        "shrx{q %[tcl], %[r9], %[r9]| %[r9], %[r9], %[tcl]}\n\t"
        "mov{q 16(%[src1]), %[r10]| %[r10], [%[src1] + 16]}\n\t"
        "shlx{q %[cl], %[r10], %[r11]| %[r11], %[r10], %[cl]}\n\t"
        "mov{q %[r8], 8(%[dst])| [%[dst] + 8], %[r8]}\n\t"

        "lea{q -1(%[rcx]), %[rcx]| %[rcx], [%[rcx] - 1]}\n\t"

        ".Lb7%=:\n\t"
        "lea{q (%[r11], %[r9]), %[r9]| %[r9], [%[r9] + %[r11]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 16(%[src0]), %[r9]| %[r9], [%[src0] + 16]}\n\t"
        "shrx{q %[tcl], %[r10], %[r10]| %[r10], %[r10], %[tcl]}\n\t"
        "mov{q 24(%[src1]), %[r11]| %[r11], [%[src1] + 24]}\n\t"
        "shlx{q %[cl], %[r11], %[r8]| %[r8], %[r11], %[cl]}\n\t"
        "mov{q %[r9], 16(%[dst])| [%[dst] + 16], %[r9]}\n\t"

        ".Lb6%=:\n\t"
        "lea{q (%[r8], %[r10]), %[r10]| %[r10], [%[r10] + %[r8]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 24(%[src0]), %[r10]| %[r10], [%[src0] + 24]}\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        "mov{q 32(%[src1]), %[r8]| %[r8], [%[src1] + 32]}\n\t"
        "shlx{q %[cl], %[r8], %[r9]| %[r9], %[r8], %[cl]}\n\t"
        "mov{q %[r10], 24(%[dst])| [%[dst] + 24], %[r10]}\n\t"

        ".Lb5%=:\n\t"
        "lea{q (%[r9], %[r11]), %[r11]| %[r11], [%[r11] + %[r9]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 32(%[src0]), %[r11]| %[r11], [%[src0] + 32]}\n\t"
        "shrx{q %[tcl], %[r8], %[r8]| %[r8], %[r8], %[tcl]}\n\t"
        "mov{q 40(%[src1]), %[r9]| %[r9], [%[src1] + 40]}\n\t"
        "shlx{q %[cl], %[r9], %[r10]| %[r10], %[r9], %[cl]}\n\t"
        "mov{q %[r11], 32(%[dst])| [%[dst] + 32], %[r11]}\n\t"

        ".Lb4%=:\n\t"
        "lea{q (%[r10], %[r8]), %[r8]| %[r8], [%[r8] + %[r10]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 40(%[src0]), %[r8]| %[r8], [%[src0] + 40]}\n\t"
        "shrx{q %[tcl], %[r9], %[r9]| %[r9], %[r9], %[tcl]}\n\t"
        "mov{q 48(%[src1]), %[r10]| %[r10], [%[src1] + 48]}\n\t"
        "shlx{q %[cl], %[r10], %[r11]| %[r11], %[r10], %[cl]}\n\t"
        "mov{q %[r8], 40(%[dst])| [%[dst] + 40], %[r8]}\n\t"

        ".Lb3%=:\n\t"
        "lea{q (%[r11], %[r9]), %[r9]| %[r9], [%[r9] + %[r11]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q 48(%[src0]), %[r9]| %[r9], [%[src0] + 48]}\n\t"
        "shrx{q %[tcl], %[r10], %[r10]| %[r10], %[r10], %[tcl]}\n\t"
        "mov{q 56(%[src1]), %[r11]| %[r11], [%[src1] + 56]}\n\t"
        "shlx{q %[cl], %[r11], %[r8]| %[r8], %[r11], %[cl]}\n\t"
        "mov{q %[r9], 48(%[dst])| [%[dst] + 48], %[r9]}\n\t"

        "lea{q 64(%[src0]), %[src0]| %[src0], [%[src0] + 64]}\n\t"
        "lea{q 64(%[src1]), %[src1]| %[src1], [%[src1] + 64]}\n\t"
        "lea{q 64(%[dst]), %[dst]| %[dst], [%[dst] + 64]}\n\t"

        "jrcxz .Lloop_out%=\n\t"
        "jmp .Lloop%=\n\t"
        ".Lloop_out%=:\n\t"

        "lea{q (%[r8], %[r10]), %[r10]| %[r10], [%[r10] + %[r8]]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q -8(%[src0]), %[r10]| %[r10], [%[src0] - 8]}\n\t"
        "shrx{q %[tcl], %[r11], %[r11]| %[r11], %[r11], %[tcl]}\n\t"
        WJR_PP_STR(WJR_adcsbb) "{q %[rcx], %[r11]| %[r11], %[rcx]}\n\t"
        "mov{q %[r10], -8(%[dst])| [%[dst] - 8], %[r10]}\n\t"

        ".Ldone%=:"

        : [dst] "+&r"(dst), [src0] "+&r"(src0), [src1] "+&r"(src1), [rcx] "+&c"(rcx),
          [r8] "=&r"(r8), [r9] "+&r"(r9), [r10] "=&r"(r10), [r11] "=&r"(r11)
        : [cl] "r"(cl), [tcl] "r"(tcl)
        : "cc", "memory");

    WJR_ASSERT_ASSUME(rcx == 0);

    return r11;
}

#undef WJR_adcsbb
#undef WJR_addcsubc

#undef WJR_ADDSUB_I
#endif

} // namespace wjr

#endif // WJR_X86_MATH_MUL_HPP__
#endif

namespace wjr {

template <typename T>
WJR_INTRINSIC_CONSTEXPR T fallback_mul(T a, T b, T &hi) {
    constexpr auto nd = std::numeric_limits<T>::digits;
    using T2 = uint_t<nd * 2>;
    T2 x = static_cast<T2>(a) * b;
    hi = x >> nd;
    return static_cast<T>(x);
}

WJR_INTRINSIC_CONSTEXPR_E uint64_t fallback_mul64(uint64_t a, uint64_t b, uint64_t &hi) {
    uint64_t ah = a >> 32;
    uint64_t al = a & 0xFFFFFFFF;
    uint64_t bh = b >> 32;
    uint64_t bl = b & 0xFFFFFFFF;

    uint64_t rh = ah * bh;
    uint64_t rm0 = ah * bl;
    uint64_t rm1 = al * bh;
    uint64_t rl = al * bl;

    __add_128(rl, rh, rl, rh, rm0 << 32, rm0 >> 32);
    __add_128(rl, rh, rl, rh, rm1 << 32, rm1 >> 32);

    hi = rh;
    return rl;
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E T mul(T a, T b, T &hi) {
    constexpr auto nd = std::numeric_limits<T>::digits;

    if ((WJR_BUILTIN_CONSTANT_P(a == 0) && a == 0) ||
        (WJR_BUILTIN_CONSTANT_P(b == 0) && b == 0)) {
        hi = 0;
        return 0;
    }

    if (WJR_BUILTIN_CONSTANT_P(a == 1) && a == 1) {
        hi = 0;
        return b;
    }

    if (WJR_BUILTIN_CONSTANT_P(b == 1) && b == 1) {
        hi = 0;
        return a;
    }

    if constexpr (nd < 64) {
        return fallback_mul(a, b, hi);
    } else {

#if WJR_HAS_BUILTIN(MUL64)
        if (is_constant_evaluated()
#if WJR_HAS_BUILTIN(ASM_MUL64)
            || (WJR_BUILTIN_CONSTANT_P(a) && WJR_BUILTIN_CONSTANT_P(b))
#endif
        ) {
            return fallback_mul64(a, b, hi);
        }

#if WJR_HAS_BUILTIN(ASM_MUL64)
        // mov b to rax, then mul a
        // instead of mov a to rax, mov b to register, then mul
        if (WJR_BUILTIN_CONSTANT_P(b)) {
            return builtin_mul64(b, a, hi);
        }
#endif

        return builtin_mul64(a, b, hi);
#else
        return fallback_mul64(a, b, hi);
#endif
    }
}

template <typename T>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E T fallback_mulhi(T a, T b) {
    T hi = 0;
    (void)fallback_mul(a, b, hi);
    return hi;
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR_E T mulhi(T a, T b) {
    T ret = 0;
    (void)mul(a, b, ret);
    return ret;
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_CONST WJR_INTRINSIC_CONSTEXPR T mullo(T a, T b) {
    return a * b;
}

#if WJR_HAS_BUILTIN(__builtin_mul_overflow)
#define WJR_HAS_BUILTIN_MUL_OVERFLOW WJR_HAS_DEF
#endif

template <typename T>
WJR_INTRINSIC_CONSTEXPR_E bool fallback_mul_overflow(T a, T b, T &ret) {
    T hi;
    ret = mul(a, b, hi);
    return hi != 0;
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E bool mul_overflow(type_identity_t<T> a, type_identity_t<T> b,
                                            T &ret) {
#if WJR_HAS_BUILTIN(MUL_OVERFLOW)
    if (is_constant_evaluated() ||
        (WJR_BUILTIN_CONSTANT_P(a) && WJR_BUILTIN_CONSTANT_P(b))) {
        return fallback_mul_overflow(a, b, ret);
    }

    return __builtin_mul_overflow(a, b, &ret);
#else
    return fallback_mul_overflow(a, b, ret);
#endif
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR T fallback_mul_1(T *dst, const T *src, size_t n, T ml) {
    T lo = 0, hi = 0;
    T c_in = 0;

    for (size_t i = 0; i < n; ++i) {
        lo = mul(src[i], ml, hi);
        dst[i] = addc<T>(lo, c_in, 0, c_in);
        c_in += hi;
    }

    return c_in;
}

/*
require :
1. n >= 1
2. WJR_IS_SAME_OR_INCR_P(dst, n, src, n)
*/
template <typename T>
WJR_INTRINSIC_CONSTEXPR_E T mul_1(T *dst, const T *src, size_t n, type_identity_t<T> ml) {
    static_assert(std::is_same_v<T, uint64_t>, "only support uint64_t now");
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src, n));

    if (WJR_BUILTIN_CONSTANT_P(ml == 1) && ml == 1) {
        if (src != dst) {
            std::copy(src, src + n, dst);
        }

        return 0;
    }

    if (WJR_BUILTIN_CONSTANT_P(is_zero_or_single_bit(ml)) && is_zero_or_single_bit(ml)) {
        if (ml == 0) {
            set_n(dst, 0, n);
            return 0;
        }

        unsigned int k = ctz(ml);
        return lshift_n(dst, src, n, k);
    }

#if WJR_HAS_BUILTIN(ASM_MUL_1)
    if (is_constant_evaluated()) {
        return fallback_mul_1(dst, src, n, ml);
    }

    return asm_mul_1(dst, src, n, ml);
#else
    return fallback_mul_1(dst, src, n, ml);
#endif
}

// dst = src0 + (src1 << cl)
template <typename T>
WJR_INTRINSIC_CONSTEXPR_E T addlsh_n(T *dst, const T *src0, const T *src1, size_t n,
                                     type_identity_t<T> cl);

// dst = (src1 << cl) - src0
template <typename T>
WJR_INTRINSIC_CONSTEXPR_E T rsblsh_n(T *dst, const T *src0, const T *src1, size_t n,
                                     type_identity_t<T> cl);

template <typename T>
WJR_INTRINSIC_CONSTEXPR T fallback_addmul_1(T *dst, const T *src, size_t n, T ml) {
    T lo = 0, hi = 0;
    T o_in = 0, c_in = 0;

    for (size_t i = 0; i < n; ++i) {
        lo = mul(src[i], ml, hi);
        lo = addc<T>(lo, c_in, 0, c_in);
        dst[i] = addc<T>(lo, dst[i], 0, o_in);
        c_in += hi + o_in;
    }

    return c_in;
}

/*
require :
1. n >= 1
2. WJR_IS_SAME_OR_INCR_P(dst, n, src, n)
*/
template <typename T>
WJR_INTRINSIC_CONSTEXPR_E T addmul_1(T *dst, const T *src, size_t n,
                                     type_identity_t<T> ml) {
    static_assert(std::is_same_v<T, uint64_t>, "only support uint64_t now");
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src, n));

    if (WJR_BUILTIN_CONSTANT_P(ml == 1) && ml == 1) {
        return addc_n(dst, dst, src, n);
    }

    if (WJR_BUILTIN_CONSTANT_P(is_zero_or_single_bit(ml)) && is_zero_or_single_bit(ml)) {
        if (ml == 0) {
            return 0;
        }

        unsigned int c = ctz(ml);
        return addlsh_n(dst, dst, src, n, c);
    }

#if WJR_HAS_BUILTIN(ASM_ADDMUL_1)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_addmul_1(dst, src, n, ml);
        }

        return asm_addmul_1(dst, src, n, ml);
    } else {
        return fallback_addmul_1(dst, src, n, ml);
    }
#else
    return fallback_addmul_1(dst, src, n, ml);
#endif
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR T fallback_submul_1(T *dst, const T *src, size_t n, T ml) {
    T lo = 0, hi = 0;
    T o_in = 0, c_in = 0;

    for (size_t i = 0; i < n; ++i) {
        lo = mul(src[i], ml, hi);
        lo = addc<T>(lo, c_in, 0, c_in);
        dst[i] = subc<T>(dst[i], lo, 0, o_in);
        c_in += hi + o_in;
    }

    return c_in;
}

/*
require :
1. n >= 1
2. WJR_IS_SAME_OR_INCR_P(dst, n, src, n)
*/
template <typename T>
WJR_INTRINSIC_CONSTEXPR_E T submul_1(T *dst, const T *src, size_t n,
                                     type_identity_t<T> ml) {
    static_assert(std::is_same_v<T, uint64_t>, "only support uint64_t now");
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src, n));

    if (WJR_BUILTIN_CONSTANT_P(ml == 0) && ml == 0) {
        return 0;
    }

    if (WJR_BUILTIN_CONSTANT_P(ml == 1) && ml == 1) {
        return subc_n(dst, dst, src, n);
    }

#if WJR_HAS_BUILTIN(ASM_SUBMUL_1)
    if (is_constant_evaluated()) {
        return fallback_submul_1(dst, src, n, ml);
    }

    return asm_submul_1(dst, src, n, ml);
#else
    return fallback_submul_1(dst, src, n, ml);
#endif
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR T fallback_addlsh_n(T *dst, const T *src0, const T *src1,
                                            size_t n, type_identity_t<T> cl) {
    T tcl = std::numeric_limits<T>::digits - cl;
    T lo = 0, hi = 0;
    T c_in = 0, x = 0;

    for (size_t i = 0; i < n; ++i) {
        lo = src1[i] << cl;
        hi = src1[i] >> tcl;

        lo += x;
        dst[i] = addc<T>(lo, src0[i], c_in, c_in);
        x = hi;
    }

    return x + c_in;
}

/*
dst = src0 + (src1 << cl);

require :
1. n >= 1
2. WJR_IS_SAME_OR_INCR_P(dst, n, src, n)
3. WJR_IS_SAME_OR_INCR_P(sdt, n, src1, n)
*/
template <typename T>
WJR_INTRINSIC_CONSTEXPR_E T addlsh_n(T *dst, const T *src0, const T *src1, size_t n,
                                     type_identity_t<T> cl) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_ASSUME(cl < std::numeric_limits<T>::digits);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src0, n));
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src1, n));

    if (WJR_UNLIKELY(cl == 0)) {
        return addc_n(dst, src0, src1, n);
    }

#if WJR_HAS_BUILTIN(ASM_ADDLSH_N)
    if (is_constant_evaluated()) {
        return fallback_addlsh_n(dst, src0, src1, n, cl);
    }

    return asm_addlsh_n(dst, src0, src1, n, cl);
#else
    return fallback_addlsh_n(dst, src0, src1, n, cl);
#endif
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR T fallback_rsblsh_n(T *dst, const T *src0, const T *src1,
                                            size_t n, type_identity_t<T> cl) {
    T tcl = std::numeric_limits<T>::digits - cl;
    T lo = 0, hi = 0;
    T c_in = 0, x = 0;

    for (size_t i = 0; i < n; ++i) {
        lo = src1[i] << cl;
        hi = src1[i] >> tcl;

        lo += x;
        dst[i] = subc<T>(lo, src0[i], c_in, c_in);
        x = hi;
    }

    return x - c_in;
}

/*
dst = (src1 << cl) - src0;

require :
1. n >= 1
2. WJR_IS_SAME_OR_INCR_P(dst, n, src, n)
3. WJR_IS_SAME_OR_INCR_P(sdt, n, src1, n)
*/
template <typename T>
WJR_INTRINSIC_CONSTEXPR_E T rsblsh_n(T *dst, const T *src0, const T *src1, size_t n,
                                     type_identity_t<T> cl) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_ASSUME(cl < std::numeric_limits<T>::digits);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src0, n));
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src1, n));

    if (WJR_UNLIKELY(cl == 0)) {
        return T{0} - subc_n(dst, src1, src0, n);
    }

#if WJR_HAS_BUILTIN(ASM_RSBLSH_N)
    if (is_constant_evaluated()) {
        return fallback_rsblsh_n(dst, src0, src1, n, cl);
    }

    return asm_rsblsh_n(dst, src0, src1, n, cl);
#else
    return fallback_rsblsh_n(dst, src0, src1, n, cl);
#endif
}

template <typename T, T maxn = in_place_max>
WJR_INTRINSIC_CONSTEXPR_E T try_addmul_1(T *dst, const T *src, size_t n,
                                         type_identity_t<T> ml,
                                         std::integral_constant<T, maxn> = {}) {
    static_assert(std::is_same_v<T, uint64_t>, "only support uint64_t now");
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_INCR_P(dst, n, src, n));

    WJR_ASSERT_ASSUME(ml <= maxn);

    if constexpr (maxn == 0) {
        return 0;
    }

    if (ml == 0) {
        return 0;
    }

    if constexpr (maxn == 1) {
        return addc_n(dst, dst, src, n);
    } else {
        if (ml == 1) {
            return addc_n(dst, dst, src, n);
        }

        if constexpr (maxn <= 3) {
            // addlsh is slightly faster than addmul
            if constexpr (maxn == 2) {
                return addlsh_n(dst, dst, src, n, 1);
            } else {
                if (ml == 2) {
                    return addlsh_n(dst, dst, src, n, 1);
                }

                return addmul_1(dst, src, n, ml);
            }
        } else {
            return addmul_1(dst, src, n, ml);
        }
    }
}

inline constexpr size_t toom22_mul_threshold = WJR_TOOM22_MUL_THRESHOLD;
inline constexpr size_t toom33_mul_threshold = WJR_TOOM33_MUL_THRESHOLD;
inline constexpr size_t toom44_mul_threshold = WJR_TOOM44_MUL_THRESHOLD;
inline constexpr size_t toom32_to_toom43_mul_threshold =
    WJR_TOOM32_TO_TOOM43_MUL_THRESHOLD;
inline constexpr size_t toom32_to_toom53_mul_threshold =
    WJR_TOOM32_TO_TOOM53_MUL_THRESHOLD;
inline constexpr size_t toom42_to_toom53_mul_threshold =
    WJR_TOOM42_TO_TOOM53_MUL_THRESHOLD;
inline constexpr size_t toom2_sqr_threshold = WJR_TOOM2_SQR_THRESHOLD;
inline constexpr size_t toom3_sqr_threshold = WJR_TOOM3_SQR_THRESHOLD;
inline constexpr size_t toom4_sqr_threshold = WJR_TOOM4_SQR_THRESHOLD;

// only toom22 is optimized to inline
enum class __mul_mode : uint8_t {
    toom22 = 0x00,
    toom33 = 0x01,
    toom44 = 0x02,
    all = 0x03,
};

template <__mul_mode mode, bool reserved, typename T>
WJR_INTRINSIC_INLINE void
__mul_s_impl(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
             std::conditional_t<reserved, T *, in_place_empty_t> mal);

template <__mul_mode mode, typename T>
WJR_INTRINSIC_INLINE void __mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n,
                                  const T *src1, size_t m, T *stk);

template <typename T>
WJR_INTRINSIC_INLINE void mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n,
                                const T *src1, size_t m);

template <typename T>
WJR_INTRINSIC_INLINE void mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n,
                                const T *src1, size_t m, T *stk);

template <__mul_mode mode, bool reserved, typename T>
WJR_INTRINSIC_INLINE void
__mul_n_impl(T *WJR_RESTRICT dst, const T *src0, const T *src1, size_t n,
             std::conditional_t<reserved, T *, in_place_empty_t> mal);

template <__mul_mode mode, typename T>
WJR_INTRINSIC_INLINE void __mul_n(T *WJR_RESTRICT dst, const T *src0, const T *src1,
                                  size_t n, T *stk);

template <__mul_mode mode, typename T, T m0 = in_place_max, T m1 = in_place_max>
WJR_INTRINSIC_INLINE void
__mul_n(T *WJR_RESTRICT dst, const T *src0, const T *src1, size_t n, T *stk, T &c_out,
        type_identity_t<T> cf0, type_identity_t<T> cf1,
        std::integral_constant<T, m0> = {}, std::integral_constant<T, m1> = {});

template <typename T>
WJR_INTRINSIC_INLINE void mul_n(T *WJR_RESTRICT dst, const T *src0, const T *src1,
                                size_t n);

template <typename T>
WJR_INTRINSIC_INLINE void mul_n(T *WJR_RESTRICT dst, const T *src0, const T *src1,
                                size_t n, T *stk);

template <typename T>
void basecase_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1,
                    size_t m);

template <typename T>
void basecase_sqr(T *WJR_RESTRICT dst, const T *src, size_t n);

template <__mul_mode mode, bool reserved, typename T>
WJR_INTRINSIC_INLINE void
__sqr_impl(T *WJR_RESTRICT dst, const T *src, size_t n,
           std::conditional_t<reserved, T *, in_place_empty_t> mal);

template <typename T>
WJR_INTRINSIC_INLINE void __sqr(T *WJR_RESTRICT dst, const T *src, size_t n, T *stk);

template <__mul_mode mode, typename T, T m = in_place_max>
WJR_INTRINSIC_INLINE void __sqr(T *WJR_RESTRICT dst, const T *src, size_t n, T *stk,
                                T &c_out, type_identity_t<T> cf,
                                std::integral_constant<T, m> x = {});

template <typename T>
WJR_INTRINSIC_INLINE void sqr(T *WJR_RESTRICT dst, const T *src, size_t n);

template <typename T>
WJR_INTRINSIC_INLINE void sqr(T *WJR_RESTRICT dst, const T *src, size_t n, T *stk);

/*
 all toom-cook need to ensure rn + rm >= l to reserve memory
 for toom-cook-u-v
 1. (v-1) * n + g <= u * m
    g = (u-1)^2 + (u-1)*v
    e.g.
    TOOM-COOK32 :
    g = 8
    n + 8 <= 3 * m
 2. v * n >= (u - 1) * m + g
    g = (v-1)^2 + (v-1)*u
    e.g.
    TOOM-COOK32 :
    g = 4
    2 * n >= 2 * m + 4 => n >= m + 2
*/

/*
 l = ceIl(n/2)
 stk usage : l * 2
*/
template <typename T>
void toom22_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk);

extern template void toom22_mul_s<uint64_t>(uint64_t *WJR_RESTRICT dst,
                                            const uint64_t *src0, size_t n,
                                            const uint64_t *src1, size_t m,
                                            uint64_t *stk);

template <typename T>
void toom2_sqr(T *WJR_RESTRICT dst, const T *src, size_t n, T *stk);

extern template void toom2_sqr<uint64_t>(uint64_t *WJR_RESTRICT dst, const uint64_t *src,
                                         size_t n, uint64_t *stk);

/*
 l = max(ceil(n/3), ceil(m/2))
 stk usage : l * 4
*/
template <typename T>
void toom32_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk);

extern template void toom32_mul_s<uint64_t>(uint64_t *WJR_RESTRICT dst,
                                            const uint64_t *src0, size_t n,
                                            const uint64_t *src1, size_t m,
                                            uint64_t *stk);

template <typename T>
struct toom_interpolation_5p_struct;

template <typename T>
void toom_interpolation_5p_s(T *WJR_RESTRICT dst, T *w1p, size_t l, size_t rn, size_t rm,
                             toom_interpolation_5p_struct<T> &&flag);

extern template void
toom_interpolation_5p_s<uint64_t>(uint64_t *WJR_RESTRICT dst, uint64_t *w1p, size_t l,
                                  size_t rn, size_t rm,
                                  toom_interpolation_5p_struct<uint64_t> &&flag);

/*
 l = max(ceil(n/4), ceil(m/2))
 stk usage : l * 4
*/
template <typename T>
void toom42_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk);

extern template void toom42_mul_s<uint64_t>(uint64_t *WJR_RESTRICT dst,
                                            const uint64_t *src0, size_t n,
                                            const uint64_t *src1, size_t m,
                                            uint64_t *stk);

/*
 l = ceil(n/3)
 stk usage : l * 4
*/
template <typename T>
void toom33_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk);

extern template void toom33_mul_s<uint64_t>(uint64_t *WJR_RESTRICT dst,
                                            const uint64_t *src0, size_t n,
                                            const uint64_t *src1, size_t m,
                                            uint64_t *stk);

template <typename T>
void toom3_sqr(T *WJR_RESTRICT dst, const T *src, size_t n, T *stk);

extern template void toom3_sqr<uint64_t>(uint64_t *WJR_RESTRICT dst, const uint64_t *src,
                                         size_t n, uint64_t *stk);

template <typename T>
struct toom_interpolation_6p_struct;

template <typename T>
void toom_interpolation_6p_s(T *WJR_RESTRICT dst, T *w1p, size_t l, size_t rn, size_t rm,
                             toom_interpolation_6p_struct<T> &&flag);

extern template void
toom_interpolation_6p_s<uint64_t>(uint64_t *WJR_RESTRICT dst, uint64_t *w1p, size_t l,
                                  size_t rn, size_t rm,
                                  toom_interpolation_6p_struct<uint64_t> &&flag);

/*
 l = max(ceil(n/4), ceil(m/3))
 stk usage : l * 6
*/
template <typename T>
void toom43_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk);

extern template void toom43_mul_s<uint64_t>(uint64_t *WJR_RESTRICT dst,
                                            const uint64_t *src0, size_t n,
                                            const uint64_t *src1, size_t m,
                                            uint64_t *stk);

template <typename T>
struct toom_interpolation_7p_struct;

template <typename T>
void toom_interpolation_7p_s(T *WJR_RESTRICT dst, T *w1p, size_t l, size_t rn, size_t rm,
                             toom_interpolation_7p_struct<T> &&flag);

extern template void
toom_interpolation_7p_s<uint64_t>(uint64_t *WJR_RESTRICT dst, uint64_t *w1p, size_t l,
                                  size_t rn, size_t rm,
                                  toom_interpolation_7p_struct<uint64_t> &&flag);

/*
 l = max(ceil(n/5), ceil(m/3))
 stk usage : l * 6
*/
template <typename T>
void toom53_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk);

extern template void toom53_mul_s<uint64_t>(uint64_t *WJR_RESTRICT dst,
                                            const uint64_t *src0, size_t n,
                                            const uint64_t *src1, size_t m,
                                            uint64_t *stk);

/*
 l = ceil(n/4)
 stk usage : l * 6
*/
template <typename T>
void toom44_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk);

extern template void toom44_mul_s<uint64_t>(uint64_t *WJR_RESTRICT dst,
                                            const uint64_t *src0, size_t n,
                                            const uint64_t *src1, size_t m,
                                            uint64_t *stk);

template <typename T>
void toom4_sqr(T *WJR_RESTRICT dst, const T *src, size_t n, T *stk);

extern template void toom4_sqr<uint64_t>(uint64_t *WJR_RESTRICT dst, const uint64_t *src,
                                         size_t n, uint64_t *stk);

struct __mul_s_unique_stack_allocator {
    template <typename... Args>
    constexpr __mul_s_unique_stack_allocator(Args &&...) {}
};

template <typename P, typename T, typename U>
WJR_INTRINSIC_INLINE P *__mul_s_allocate(WJR_MAYBE_UNUSED T &mal,
                                         WJR_MAYBE_UNUSED U &alloc, size_t n) {
    if constexpr (std::is_same_v<remove_cvref_t<U>, __mul_s_unique_stack_allocator>) {
        static_assert(std::is_pointer_v<T>, "");
        (void)(alloc);
        P *ret = mal;
        mal += n;
        return ret;
    } else {
        (void)(mal);
        return static_cast<P *>(alloc.allocate(n));
    }
}

template <bool reserved, typename T>
void __toom22_mul_s_impl(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1,
                         size_t m,
                         std::conditional_t<reserved, T *, in_place_empty_t> mal) {
    WJR_ASSERT_ASSUME(m >= 1);
    WJR_ASSERT_ASSUME(n >= m);
    WJR_ASSERT_L1(WJR_IS_SEPARATE_P(dst, n + m, src0, n));
    WJR_ASSERT_L1(WJR_IS_SEPARATE_P(dst, n + m, src1, m));

    using unique_alloc =
        std::conditional_t<reserved, __mul_s_unique_stack_allocator,
                           unique_stack_allocator<wjr::math_details::stack_alloc_object>>;

    if (m < toom22_mul_threshold) {
        return basecase_mul_s(dst, src0, n, src1, m);
    }

    unique_alloc stkal(math_details::stack_alloc);

    if (n >= 3 * m) {
        T *tmp = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (4 * m));
        T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (6 * m + 67));

        toom42_mul_s(dst, src0, 2 * m, src1, m, stk);
        n -= 2 * m;
        src0 += 2 * m;
        dst += 2 * m;

        T cf = 0;

        while (n >= 3 * m) {
            toom42_mul_s(tmp, src0, 2 * m, src1, m, stk);
            n -= 2 * m;
            src0 += 2 * m;

            cf = addc_n(dst, dst, tmp, m, cf);
            std::copy(tmp + m, tmp + 3 * m, dst + m);
            cf = addc_1(dst + m, dst + m, 2 * m, 0, cf);

            dst += 2 * m;
        }

        if (4 * n < 5 * m) {
            toom22_mul_s(tmp, src0, n, src1, m, stk);
        } else if (4 * n < 7 * m) {
            toom32_mul_s(tmp, src0, n, src1, m, stk);
        } else {
            toom42_mul_s(tmp, src0, n, src1, m, stk);
        }

        cf = addc_n(dst, dst, tmp, m, cf);
        std::copy(tmp + m, tmp + m + n, dst + m);
        cf = addc_1(dst + m, dst + m, n, 0, cf);
        WJR_ASSERT(cf == 0);
    } else {
        T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (6 * m + 67));

        if (4 * n < 5 * m) {
            toom22_mul_s(dst, src0, n, src1, m, stk);
        } else if (4 * n < 7 * m) {
            toom32_mul_s(dst, src0, n, src1, m, stk);
        } else {
            toom42_mul_s(dst, src0, n, src1, m, stk);
        }
    }

    return;
}

template <bool reserved, typename T>
void __noinline_mul_s_impl(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1,
                           size_t m,
                           std::conditional_t<reserved, T *, in_place_empty_t> mal) {
    WJR_ASSERT_ASSUME(m >= 1);
    WJR_ASSERT_ASSUME(n >= m);
    WJR_ASSERT_L1(WJR_IS_SEPARATE_P(dst, n + m, src0, n));
    WJR_ASSERT_L1(WJR_IS_SEPARATE_P(dst, n + m, src1, m));

    using unique_alloc =
        std::conditional_t<reserved, __mul_s_unique_stack_allocator,
                           unique_stack_allocator<wjr::math_details::stack_alloc_object>>;

    if (m < toom22_mul_threshold) {
        return basecase_mul_s(dst, src0, n, src1, m);
    }

    unique_alloc stkal(math_details::stack_alloc);

    if (m < toom33_mul_threshold) {
        if (n >= 3 * m) {
            T *tmp = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (4 * m));
            T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (6 * m + 67));

            toom42_mul_s(dst, src0, 2 * m, src1, m, stk);
            n -= 2 * m;
            src0 += 2 * m;
            dst += 2 * m;

            T cf = 0;

            while (n >= 3 * m) {
                toom42_mul_s(tmp, src0, 2 * m, src1, m, stk);
                n -= 2 * m;
                src0 += 2 * m;

                cf = addc_n(dst, dst, tmp, m, cf);
                std::copy(tmp + m, tmp + 3 * m, dst + m);
                cf = addc_1(dst + m, dst + m, 2 * m, 0, cf);

                dst += 2 * m;
            }

            if (4 * n < 5 * m) {
                toom22_mul_s(tmp, src0, n, src1, m, stk);
            } else if (4 * n < 7 * m) {
                toom32_mul_s(tmp, src0, n, src1, m, stk);
            } else {
                toom42_mul_s(tmp, src0, n, src1, m, stk);
            }

            cf = addc_n(dst, dst, tmp, m, cf);
            std::copy(tmp + m, tmp + m + n, dst + m);
            cf = addc_1(dst + m, dst + m, n, 0, cf);
            WJR_ASSERT(cf == 0);
        } else {
            T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (6 * m + 67));

            if (4 * n < 5 * m) {
                toom22_mul_s(dst, src0, n, src1, m, stk);
            } else if (4 * n < 7 * m) {
                toom32_mul_s(dst, src0, n, src1, m, stk);
            } else {
                toom42_mul_s(dst, src0, n, src1, m, stk);
            }
        }

        return;
    }

    if (m < toom44_mul_threshold || 3 * n + 21 > 4 * m) {
        if (n >= 3 * m) {
            T *tmp = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (4 * m));
            T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (9 * m + 288));

            toom42_mul_s(dst, src0, 2 * m, src1, m, stk);
            n -= 2 * m;
            src0 += 2 * m;
            dst += 2 * m;

            T cf = 0;

            while (n >= 3 * m) {
                toom42_mul_s(tmp, src0, 2 * m, src1, m, stk);
                n -= 2 * m;
                src0 += 2 * m;

                cf = addc_n(dst, dst, tmp, m, cf);
                std::copy(tmp + m, tmp + 3 * m, dst + m);
                cf = addc_1(dst + m, dst + m, 2 * m, 0, cf);

                dst += 2 * m;
            }

            __mul_s_impl<__mul_mode::all, true>(tmp, src0, n, src1, m, stk);

            cf = addc_n(dst, dst, tmp, m, cf);
            std::copy(tmp + m, tmp + m + n, dst + m);
            cf = addc_1(dst + m, dst + m, n, 0, cf);
            WJR_ASSERT(cf == 0);
        } else {
            T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (9 * m + 288));
            if (6 * n < 7 * m) {
                toom33_mul_s(dst, src0, n, src1, m, stk);
            } else if (2 * n < 3 * m) {
                if (m < toom32_to_toom43_mul_threshold) {
                    toom32_mul_s(dst, src0, n, src1, m, stk);
                } else {
                    toom43_mul_s(dst, src0, n, src1, m, stk);
                }
            } else if (6 * n < 11 * m) {
                if (4 * n < 7 * m) {
                    if (m < toom32_to_toom53_mul_threshold) {
                        toom32_mul_s(dst, src0, n, src1, m, stk);
                    } else {
                        toom53_mul_s(dst, src0, n, src1, m, stk);
                    }
                } else {
                    if (m < toom42_to_toom53_mul_threshold) {
                        toom42_mul_s(dst, src0, n, src1, m, stk);
                    } else {
                        toom53_mul_s(dst, src0, n, src1, m, stk);
                    }
                }
            } else {
                toom42_mul_s(dst, src0, n, src1, m, stk);
            }
        }

        return;
    }

    T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (9 * m + 288));
    toom44_mul_s(dst, src0, n, src1, m, stk);
    return;
}

extern template void __noinline_mul_s_impl<true, uint64_t>(uint64_t *WJR_RESTRICT dst,
                                                           const uint64_t *src0, size_t n,
                                                           const uint64_t *src1, size_t m,
                                                           uint64_t *mal);

extern template void
__noinline_mul_s_impl<false, uint64_t>(uint64_t *WJR_RESTRICT dst, const uint64_t *src0,
                                       size_t n, const uint64_t *src1, size_t m,
                                       in_place_empty_t mal);

template <__mul_mode mode, bool reserved, typename T>
WJR_INTRINSIC_INLINE void
__mul_s_impl(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
             std::conditional_t<reserved, T *, in_place_empty_t> mal) {
    static_assert((int)__mul_mode::toom22 == 0, "");
    if (WJR_BUILTIN_CONSTANT_P(n == m) && n == m) {
        return __mul_n_impl<mode, reserved>(dst, src0, src1, n, mal);
    }

    if constexpr (mode == __mul_mode::toom22) {
        return __toom22_mul_s_impl<reserved>(dst, src0, n, src1, m, mal);
    } else {
        return __noinline_mul_s_impl<reserved>(dst, src0, n, src1, m, mal);
    }
}

template <__mul_mode mode, typename T>
WJR_INTRINSIC_INLINE void __mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n,
                                  const T *src1, size_t m, T *stk) {
    return __mul_s_impl<mode, true>(dst, src0, n, src1, m, stk);
}

template <typename T>
WJR_INTRINSIC_INLINE void mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n,
                                const T *src1, size_t m) {
    return __mul_s_impl<__mul_mode::all, false>(dst, src0, n, src1, m, in_place_empty);
}

template <typename T>
WJR_INTRINSIC_INLINE void mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n,
                                const T *src1, size_t m, T *stk) {
    return __mul_s_impl<__mul_mode::all, true>(dst, src0, n, src1, m, stk);
}

template <__mul_mode mode, bool reserved, typename T>
WJR_INTRINSIC_INLINE void
__inline_mul_n_impl(T *WJR_RESTRICT dst, const T *src0, const T *src1, size_t n,
                    std::conditional_t<reserved, T *, in_place_empty_t> mal) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SEPARATE_P(dst, n * 2, src0, n));
    WJR_ASSERT_L1(WJR_IS_SEPARATE_P(dst, n * 2, src1, n));

    using unique_alloc =
        std::conditional_t<reserved, __mul_s_unique_stack_allocator,
                           unique_stack_allocator<wjr::math_details::stack_alloc_object>>;

    if (n < toom22_mul_threshold) {
        return basecase_mul_s(dst, src0, n, src1, n);
    }

    unique_alloc stkal(math_details::stack_alloc);

    if (mode <= __mul_mode::toom22 || n < toom33_mul_threshold) {
        T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (6 * n + 67));
        return toom22_mul_s(dst, src0, n, src1, n, stk);
    }

    T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (9 * n + 288));
    return toom33_mul_s(dst, src0, n, src1, n, stk);
}

template <bool reserved, typename T>
void __noinline_mul_n_impl(T *WJR_RESTRICT dst, const T *src0, const T *src1, size_t n,
                           std::conditional_t<reserved, T *, in_place_empty_t> mal) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SEPARATE_P(dst, n * 2, src0, n));
    WJR_ASSERT_L1(WJR_IS_SEPARATE_P(dst, n * 2, src1, n));

    using unique_alloc =
        std::conditional_t<reserved, __mul_s_unique_stack_allocator,
                           unique_stack_allocator<wjr::math_details::stack_alloc_object>>;

    if (n < toom22_mul_threshold) {
        return basecase_mul_s(dst, src0, n, src1, n);
    }

    unique_alloc stkal(math_details::stack_alloc);

    if (n < toom33_mul_threshold) {
        T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (6 * n + 67));
        return toom22_mul_s(dst, src0, n, src1, n, stk);
    }

    if (n < toom44_mul_threshold) {
        T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (9 * n + 288));
        return toom33_mul_s(dst, src0, n, src1, n, stk);
    }

    T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (9 * n + 288));
    return toom44_mul_s(dst, src0, n, src1, n, stk);
}

template <__mul_mode mode, bool reserved, typename T>
WJR_INTRINSIC_INLINE void
__mul_n_impl(T *WJR_RESTRICT dst, const T *src0, const T *src1, size_t n,
             std::conditional_t<reserved, T *, in_place_empty_t> mal) {
    if (WJR_BUILTIN_CONSTANT_P(src0 == src1) && src0 == src1) {
        return __sqr_impl<mode, reserved>(dst, src0, n, mal);
    }

    if constexpr (mode <= __mul_mode::toom33) {
        return __inline_mul_n_impl<mode, reserved, T>(dst, src0, src1, n, mal);
    } else {
        return __noinline_mul_n_impl<reserved>(dst, src0, src1, n, mal);
    }
}

template <__mul_mode mode, typename T>
WJR_INTRINSIC_INLINE void __mul_n(T *WJR_RESTRICT dst, const T *src0, const T *src1,
                                  size_t n, T *stk) {
    return __mul_n_impl<mode, true>(dst, src0, src1, n, stk);
}

template <__mul_mode mode, typename T, T m0, T m1>
void __mul_n(T *WJR_RESTRICT dst, const T *src0, const T *src1, size_t n, T *stk,
             T &c_out, type_identity_t<T> cf0, type_identity_t<T> cf1,
             std::integral_constant<T, m0> x0, std::integral_constant<T, m1> x1) {
    WJR_ASSERT_ASSUME(cf0 <= m0);
    WJR_ASSERT_ASSUME(cf1 <= m1);
    __mul_n<mode>(dst, src0, src1, n, stk);
    if constexpr (m0 == 0 || m1 == 0) {
        c_out = 0;
    } else if constexpr (m0 == 1 || m1 == 1) {
        if constexpr (m0 == 1 && m1 == 1) {
            c_out = cf0 && cf1;
        } else if constexpr (m0 == 1) {
            c_out = cf0 ? cf1 : 0;
        } else {
            c_out = cf1 ? cf0 : 0;
        }
    } else {
        c_out = cf0 * cf1;
    }
    c_out += try_addmul_1(dst + n, src1, n, cf0, x0);
    c_out += try_addmul_1(dst + n, src0, n, cf1, x1);
}

template <typename T>
WJR_INTRINSIC_INLINE void mul_n(T *WJR_RESTRICT dst, const T *src0, const T *src1,
                                size_t n) {
    return __mul_n_impl<__mul_mode::all, false>(dst, src0, src1, n, in_place_empty);
}

template <typename T>
WJR_INTRINSIC_INLINE void mul_n(T *WJR_RESTRICT dst, const T *src0, const T *src1,
                                size_t n, T *stk) {
    return __mul_n_impl<__mul_mode::all, true>(dst, src0, src1, n, stk);
}

template <__mul_mode mode, bool reserved, typename T>
WJR_INTRINSIC_INLINE void
__inline_sqr_impl(T *WJR_RESTRICT dst, const T *src, size_t n,
                  std::conditional_t<reserved, T *, in_place_empty_t> mal) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SEPARATE_P(dst, n * 2, src, n));

    using unique_alloc =
        std::conditional_t<reserved, __mul_s_unique_stack_allocator,
                           unique_stack_allocator<wjr::math_details::stack_alloc_object>>;

    if (n < toom2_sqr_threshold) {
        return basecase_sqr(dst, src, n);
    }

    unique_alloc stkal(math_details::stack_alloc);

    if (mode <= __mul_mode::toom22 || n < toom3_sqr_threshold) {
        T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (6 * n + 67));
        return toom2_sqr(dst, src, n, stk);
    }

    T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (9 * n + 288));
    return toom3_sqr(dst, src, n, stk);
}

template <bool reserved, typename T>
void __noinline_sqr_impl(T *WJR_RESTRICT dst, const T *src, size_t n,
                         std::conditional_t<reserved, T *, in_place_empty_t> mal) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SEPARATE_P(dst, n * 2, src, n));

    using unique_alloc =
        std::conditional_t<reserved, __mul_s_unique_stack_allocator,
                           unique_stack_allocator<wjr::math_details::stack_alloc_object>>;

    if (n < toom2_sqr_threshold) {
        return basecase_sqr(dst, src, n);
    }

    unique_alloc stkal(math_details::stack_alloc);

    if (n < toom3_sqr_threshold) {
        T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (6 * n + 67));
        return toom2_sqr(dst, src, n, stk);
    }

    if (n < toom4_sqr_threshold) {
        T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (9 * n + 288));
        return toom3_sqr(dst, src, n, stk);
    }

    T *stk = __mul_s_allocate<T>(mal, stkal, sizeof(T) * (9 * n + 288));
    return toom4_sqr(dst, src, n, stk);
}

template <__mul_mode mode, bool reserved, typename T>
WJR_INTRINSIC_INLINE void
__sqr_impl(T *WJR_RESTRICT dst, const T *src, size_t n,
           std::conditional_t<reserved, T *, in_place_empty_t> mal) {
    if constexpr (mode <= __mul_mode::toom33) {
        return __inline_sqr_impl<mode, reserved>(dst, src, n, mal);
    } else {
        return __noinline_sqr_impl<reserved>(dst, src, n, mal);
    }
}

template <__mul_mode mode, typename T>
WJR_INTRINSIC_INLINE void __sqr(T *WJR_RESTRICT dst, const T *src, size_t n, T *stk) {
    return __sqr_impl<mode, true>(dst, src, n, stk);
}

template <__mul_mode mode, typename T, T m>
void __sqr(T *WJR_RESTRICT dst, const T *src, size_t n, T *stk, T &c_out,
           type_identity_t<T> cf, std::integral_constant<T, m>) {
    WJR_ASSERT_ASSUME(cf <= m);
    __sqr<mode>(dst, src, n, stk);
    if constexpr (m == 0) {
        c_out = 0;
    } else if constexpr (m == 1) {
        c_out = cf;
    } else {
        c_out = cf * cf;
    }

    constexpr auto m2 = m <= ((uint32_t)in_place_max) ? m * 2 : m;

    c_out += try_addmul_1(dst + n, src, n, 2 * cf, std::integral_constant<T, m2>{});
}

template <typename T>
WJR_INTRINSIC_INLINE void sqr(T *WJR_RESTRICT dst, const T *src, size_t n) {
    __sqr_impl<__mul_mode::all, false>(dst, src, n, in_place_empty);
}

template <typename T>
WJR_INTRINSIC_INLINE void sqr(T *WJR_RESTRICT dst, const T *src, size_t n, T *stk) {
    __sqr_impl<__mul_mode::all, true>(dst, src, n, stk);
}

template <typename T>
void fallback_basecase_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1,
                             size_t m) {
    dst[n] = mul_1(dst, src0, n, src1[0]);
    for (size_t i = 1; i < m; ++i) {
        ++dst;
        dst[n] = addmul_1(dst, src0, n, src1[i]);
    }
}

/*
require :
1. m >= 1
2. n >= m
3. WJR_IS_SAME_OR_SEPARATE_P(dst, n + m, src0, n)
4. WJR_IS_SAME_OR_SEPARATE_P(dst, n + m, src1, m)
*/
template <typename T>
void basecase_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1,
                    size_t m) {
    WJR_ASSERT_ASSUME(m >= 1);
    WJR_ASSERT_ASSUME(n >= m);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_SEPARATE_P(dst, n + m, src0, n));
    WJR_ASSERT_L1(WJR_IS_SAME_OR_SEPARATE_P(dst, n + m, src1, m));

#if WJR_HAS_BUILTIN(ASM_BASECASE_MUL_S)
    return asm_basecase_mul_s(dst, src0, n, src1, m);
#else
    return fallback_basecase_mul_s(dst, src0, n, src1, m);
#endif
}

template <typename T>
void basecase_sqr(T *WJR_RESTRICT dst, const T *src, size_t n) {
#if WJR_HAS_BUILTIN(ASM_BASECASE_SQR)
    return asm_basecase_sqr(dst, src, n);
#else
    return basecase_mul_s(dst, src, n, src, n);
#endif
}

template <typename T>
void toom22_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk) {
    // [u0(l), u1(rn)] (n)
    // [v0(l), v1(rm)] (m)

    // (u0 - u1) * (v0 - v1)

    WJR_ASSERT_ASSUME(m <= n && n < 2 * m);

    const size_t l = (n + 1) / 2;
    const size_t rn = n - l;
    const size_t rm = m - l;

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);
    WJR_ASSERT_ASSUME(0 < rm && rm <= l);
    WJR_ASSERT_ASSUME(rn + rm >= l);
    WJR_ASSERT_ASSUME(l - rn <= 1);

    const auto u0 = src0;
    const auto u1 = src0 + l;

    const auto v0 = src1;
    const auto v1 = src1 + l;

    const auto p0 = dst;
    const auto p1 = dst + l;
    const auto p2 = dst + l * 2;
    const auto p3 = dst + l * 3;

    const auto wp = stk;
    stk += l * 2;

    bool f = 0;

    do {
        ssize_t p;
        p = abs_subc_s(p0, u0, l, u1, rn);
        if (p < 0) {
            f ^= 1;
        } else if (WJR_UNLIKELY(p == 0)) {
            goto ZERO;
        }

        p = abs_subc_s(p1, v0, l, v1, rm);
        if (p < 0) {
            f ^= 1;
        } else if (WJR_UNLIKELY(p == 0)) {
            goto ZERO;
        }

        __mul_n<__mul_mode::toom22>(wp, p0, p1, l, stk);
        break;
    ZERO:
        set_n(wp, 0, l * 2);
        break;
    } while (0);

    __mul_n<__mul_mode::toom22>(p0, u0, v0, l, stk);
    __mul_s<__mul_mode::toom22>(p2, u1, rn, v1, rm, stk);

    T cf = 0, cf2 = 0;

    cf = addc_n(p2, p1, p2, l);
    cf2 = cf + addc_n(p1, p0, p2, l);
    cf += addc_sz(p2, p2, l, p3, rn + rm - l);

    if (!f) {
        cf -= subc_n(p1, p1, wp, l * 2);
    } else {
        cf += addc_n(p1, p1, wp, l * 2);
    }

    cf2 = addc_1(p2, p2, l, cf2);
    if (WJR_LIKELY(rn + rm != l)) {
        cf = addc_1(p3, p3, rn + rm - l, cf, cf2);
    }
    WJR_ASSERT(cf == 0);
}

template <typename T>
void toom2_sqr(T *WJR_RESTRICT dst, const T *src, size_t n, T *stk) {
    const size_t l = (n + 1) / 2;
    const size_t rn = n - l;

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);
    WJR_ASSERT_ASSUME(rn * 2 >= l);

    const auto u0 = src;
    const auto u1 = src + l;

    const auto p0 = dst;
    const auto p1 = dst + l;
    const auto p2 = dst + l * 2;
    const auto p3 = dst + l * 3;

    const auto wp = stk;
    stk += l * 2;

    do {
        ssize_t p = abs_subc_s(p0, u0, l, u1, rn);
        if (WJR_UNLIKELY(p == 0)) {
            goto ZERO;
        }

        __sqr<__mul_mode::toom22>(wp, p0, l, stk);
        break;
    ZERO:
        set_n(wp, 0, l * 2);
        break;
    } while (0);

    __sqr<__mul_mode::toom22>(p0, u0, l, stk);
    __sqr<__mul_mode::toom22>(p2, u1, rn, stk);

    T cf = 0, cf2 = 0;

    cf = addc_n(p2, p1, p2, l);
    cf2 = cf + addc_n(p1, p0, p2, l);
    cf += addc_sz(p2, p2, l, p3, rn * 2 - l);

    cf -= subc_n(p1, p1, wp, l * 2);

    cf2 = addc_1(p2, p2, l, cf2);
    if (WJR_LIKELY(rn * 2 != l)) {
        cf = addc_1(p3, p3, rn * 2 - l, cf, cf2);
    }
    WJR_ASSERT(cf == 0);
}

template <typename T>
void toom32_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk) {
    WJR_ASSERT_ASSUME(m + 2 <= n && n + 8 <= 3 * m);

    const size_t l = (2 * n >= 3 * m ? (n + 2) / 3 : (m + 1) / 2);
    const size_t rn = n - l * 2;
    const size_t rm = m - l;
    const size_t maxr = std::max(rn, rm);

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);
    WJR_ASSERT_ASSUME(0 < rm && rm <= l);
    WJR_ASSERT_ASSUME(rn + rm >= l);

    const auto u0p = src0;
    const auto u1p = src0 + l;
    const auto u2p = src0 + l * 2;

    const auto v0p = src1;
    const auto v1p = src1 + l;

    const auto w0p = dst;
    const auto w1p = stk;
    const auto w2p = stk + l * 2;
    const auto w3p = dst + l * 3;

    stk += l * 4;

    T cf = 0;
    T cf0 = 0, cf1 = 0, cf2 = 0, cf3 = 0;
    bool neg0 = 0, neg3 = 0;

    // W0 = U0 + U2 : (non-negative)
    cf0 = addc_s(w0p, u0p, l, u2p, rn);

    // W3 = V0 + V1 : (non-negative) v(1)
    cf3 = addc_s(w3p, v0p, l, v1p, rm);
    WJR_ASSERT_ASSUME(cf3 <= 1);

    // W2 = W0 + U1 : (non-negative) u(1)
    cf2 = cf0 + addc_n(w2p, w0p, u1p, l);
    WJR_ASSERT_ASSUME(cf2 <= 2);

    // W1 = W2 * W3 : (non-negative) f(1) = r1
    __mul_n<__mul_mode::toom33, T, 2, 1>(w1p, w2p, w3p, l, stk, cf1, cf2, cf3);

    // W0 = W0 - U1 : u(-1)
    {
        ssize_t p = abs_subc_n(w0p, w0p, u1p, l, cf0, cf0, 0);
        neg0 = p < 0;
    }
    WJR_ASSERT_ASSUME(cf0 <= 1);

    // W3 = V0 - V1 : v(-1)
    {
        ssize_t p = abs_subc_s(w3p, v0p, l, v1p, rm);
        neg3 = p < 0;
    }

    // W2 = W0 * W3 : f(-1) = r2
    neg0 ^= neg3;
    __mul_n<__mul_mode::toom33, T, 1, 0>(w2p, w0p, w3p, l, stk, cf2, cf0, 0);

    // W0 = U0 * V0 : (non-negative) f(0) = r0
    __mul_n<__mul_mode::toom33>(w0p, u0p, v0p, l, stk);

    // W3 = U2 * V1 : (non-negative) f(inf) = r3
    if (rn >= rm) {
        __mul_s<__mul_mode::toom33>(w3p, u2p, rn, v1p, rm, stk);
    } else {
        __mul_s<__mul_mode::toom33>(w3p, v1p, rm, u2p, rn, stk);
    }

    // W1 = (W1 - W2) >> 1 : (non-negative) (f(1) - f(-1)) / 2
    {
        if (!neg0) {
            cf1 = cf1 - cf2 - subc_n(w1p, w1p, w2p, l * 2);
        } else {
            cf1 = cf1 + cf2 + addc_n(w1p, w1p, w2p, l * 2);
        }

        rshift_n(w1p, w1p, l * 2, 1, cf1);
        cf1 >>= 1;
    }

    // W2 = (W1 + W2) - W0 : (non-negative) r2
    {
        if (!neg0) {
            cf2 = cf1 + cf2 + addc_n(w2p, w1p, w2p, l * 2);
        } else {
            cf2 = cf1 - cf2 - subc_n(w2p, w1p, w2p, l * 2);
        }

        cf2 -= subc_n(w2p, w2p, w0p, l * 2);
        if (l != maxr) {
            WJR_ASSERT(cf2 == 0);
            cf2 = w2p[l + maxr];
        }
    }

    // W1 = W1 - W3 : (non-negative) r1
    cf1 -= subc_s(w1p, w1p, l * 2, w3p, rn + rm);

    // W = W3*x^3+W2*x^2+W1*x+W0
    cf = addc_n(w0p + l, w0p + l, w1p, l);
    cf = addc_n(dst + l * 2, w1p + l, w2p, l, cf);
    cf = addc_n(w3p, w3p, w2p + l, maxr, cf);
    cf2 += addc_1(w3p, w3p, maxr, cf1);
    cf = addc_1(w3p + maxr, w3p + maxr, (rn + rm) - maxr, cf2, cf);
    WJR_ASSERT(cf == 0);
}

template <typename T>
struct toom_interpolation_5p_struct {
    bool neg1;
    T cf1;
    T cf2;
    T cf3;
};

template <typename T>
void toom_interpolation_5p_s(T *WJR_RESTRICT dst, T *w1p, size_t l, size_t rn, size_t rm,
                             toom_interpolation_5p_struct<T> &&flag) {
    /*
     W0 = f(0);
     W1 = f(-1);
     W2 = f(1);
     W3 = f(2);
     W4 = f(inf);
    */

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);
    WJR_ASSERT_ASSUME(0 < rm && rm <= l);
    WJR_ASSERT_ASSUME(rn + rm >= l);

    const size_t maxr = std::max(rn, rm);

    const auto w0p = dst;
    const auto w2p = w0p + l * 2;
    const auto w3p = w1p + l * 2;
    const auto w4p = w0p + l * 4;

    T cf;
    auto [neg1, cf1, cf2, cf3] = flag;

    // W3 = (W3 - W1) / 3 : (non-negative) (f(2) - f(-1)) / 3
    {
        if (!neg1) {
            cf3 -= cf1 + subc_n(w3p, w3p, w1p, l * 2);
        } else {
            cf3 += cf1 + addc_n(w3p, w3p, w1p, l * 2);
        }

        divexact_by3(w3p, w3p, l * 2);
        cf3 /= 3;
    }

    // W1 = (W2 - W1) >> 1 : (non-negative) (f(1) - f(-1)) / 2
    {
        if (!neg1) {
            cf1 = cf2 - cf1 - subc_n(w1p, w2p, w1p, l * 2);
        } else {
            cf1 = cf2 + cf1 + addc_n(w1p, w2p, w1p, l * 2);
        }

        rshift_n(w1p, w1p, l * 2, 1, cf1);
        cf1 >>= 1;
    }

    // W2 = W2 - W0 : (non-negative) f(1) - f(0)
    cf2 -= subc_n(w2p, w2p, w0p, l * 2);

    // W3 = ((W3 - W2) >> 1) - (W4 << 1) : (non-negative) r3
    {
        cf3 -= cf2 + subc_n(w3p, w3p, w2p, l * 2);

        rshift_n(w3p, w3p, l * 2, 1, cf3);
        cf3 >>= 1;

        cf = submul_1(w3p, w4p, rn + rm, 2);
        if (l * 2 != (rn + rm)) {
            cf3 -= subc_1(w3p + rn + rm, w3p + rn + rm, l * 2 - (rn + rm), cf);
            if (l != maxr) {
                WJR_ASSERT(cf3 == 0);
                cf3 = w3p[l + maxr];
            }
        } else {
            cf3 -= cf;
        }
    }

    // W2 = W2 - W1 : (non-negative) f(1) / 2 - f(0) + f(-1) / 2
    cf2 -= cf1 + subc_n(w2p, w2p, w1p, l * 2);

    // W3 = W4 * x + W3 : r4 * x + r3
    cf = addc_n(w4p, w4p, w3p + l, maxr);
    cf = addc_1(w4p + maxr, w4p + maxr, rn + rm - maxr, cf3, cf);
    WJR_ASSERT(cf == 0);

    // W1 = W2 * x + W1 :
    cf = addc_n(w2p, w2p, w1p + l, l);
    cf2 += addc_1(w2p + l, w2p + l, l, cf1, cf);

    // W1 = W1 - W3 : // r2 * x + r1
    cf = subc_n(w1p, w1p, w3p, l);
    cf2 -= subc_s(w2p, w2p, l * 2, w4p, rn + rm, cf);

    // W  = W3*x^3 + W1*x + W0
    cf = addc_n(dst + l, dst + l, w1p, l);
    cf = addc_1(dst + l * 2, dst + l * 2, l, cf);
    cf = addc_n(dst + l * 3, dst + l * 3, w3p, l, cf);
    cf = addc_1(dst + l * 4, dst + l * 4, rn + rm, cf2, cf);
    WJR_ASSERT(cf == 0);
}

template <typename T>
void toom42_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk) {
    /*
     W0 = f(0);
     W1 = f(-1);
     W2 = f(1);
     W3 = f(2);
     W4 = f(inf);

     W0 = U0 * V0;
     W1 = (U0 - U1 + U2 - U3) * (V0 - v1);
     W2 = (U0 + U1 + U2 + U3) * (V0 + V1);
     W3 = (U0 + 2U1 + 4U2 + 8U3) * (V0 + 2V1);
     W4 = U3 * V1;
    */

    /*
     7 add/sub + 3 addlsh

     Not implemented according to the following steps

     T0 = U0 + U2;
     T2 = U1 + U3;
     T1 = T0 - T2; u(-1)
     T0 = T0 + T2; u(1)
     W3 = V0 - V1; v(-1)
     T2 = V0 + V1; v(1)
     W1 = T1 * W3; f(-1)
     W2 = T0 * T2; f(1)
     T0 = ((2U3 + U2) << 1 + U1) << 1 + U0; u(2)
     T2 = T0 + V1; v(2)
     W3 = T0 * T2; f(2)
     W0 = U0 * V0;
     W4 = U3 * V1;
    */

    WJR_ASSERT_ASSUME(3 * m + 5 <= 2 * n && n + 15 <= 4 * m);

    const size_t l = (n >= 2 * m ? (n + 3) / 4 : (m + 1) / 2);
    const size_t rn = n - l * 3;
    const size_t rm = m - l;

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);
    WJR_ASSERT_ASSUME(0 < rm && rm <= l);
    WJR_ASSERT_ASSUME(rn + rm >= l);

    const auto u0p = src0;
    const auto u1p = src0 + l;
    const auto u2p = src0 + l * 2;
    const auto u3p = src0 + l * 3;

    const auto v0p = src1;
    const auto v1p = src1 + l;

    const auto w0p = dst;
    const auto w1p = stk;
    const auto w2p = w0p + l * 2;
    const auto w3p = stk + l * 2;
    const auto w4p = w0p + l * 4;

    stk += l * 4;

    T cf0 = 0, cf1 = 0, cf2 = 0, cf3 = 0, cf4 = 0;
    bool neg2 = 0, neg3 = 0;

    // W0 = U0 + U2 : (non-negative)
    cf0 = addc_n(w0p, u0p, u2p, l);

    // W4 = V0 + V1 : (non-negative) v(1)
    cf4 = addc_s(w4p, v0p, l, v1p, rm);
    WJR_ASSERT_ASSUME(cf4 <= 1);

    // W1 = U1 + U3 : (non-negative)
    cf1 = addc_s(w1p, u1p, l, u3p, rn);

    // W3 = W0 - W1 : u(-1)
    {
        ssize_t p = abs_subc_n(w3p, w0p, w1p, l, cf3, cf0, cf1);
        neg3 = p < 0;
    }
    WJR_ASSERT_ASSUME(cf3 <= 1);

    // W2 = V0 - V1 : v(-1)
    {
        ssize_t p = abs_subc_s(w2p, v0p, l, v1p, rm);
        neg2 = p < 0;
    }

    // W0 = W0 + W1 : (non-negative) u(1)
    cf0 += cf1 + addc_n(w0p, w0p, w1p, l);
    WJR_ASSERT_ASSUME(cf0 <= 3);

    // W1 = W3 * W2 : f(-1)
    neg2 ^= neg3;
    __mul_n<__mul_mode::toom33, T, 1, 0>(w1p, w3p, w2p, l, stk, cf1, cf3, 0);

    // W2 = W0 * W4 : (non-negative) f(1)
    __mul_n<__mul_mode::toom33, T, 3, 1>(w2p, w0p, w4p, l, stk, cf2, cf0, cf4);

    // W0 = U0 +(U1 +(U2 + U3<<1)<<1)<<1 : (non-negative) u(2)
    {
        cf0 = addlsh_n(w0p, u2p, u3p, rn, 1);
        if (l != rn) {
            cf0 = addc_1(w0p + rn, u2p + rn, l - rn, cf0);
        }

        cf0 += cf0 + addlsh_n(w0p, u1p, w0p, l, 1);
        cf0 += cf0 + addlsh_n(w0p, u0p, w0p, l, 1);
        WJR_ASSERT_ASSUME(cf0 <= 14);
    }

    // W4 = W4 + V1 : (non-negative) v(2)
    cf4 += addc_s(w4p, w4p, l, v1p, rm);
    WJR_ASSERT_ASSUME(cf4 <= 2);

    // W3 = W0 * W4 : (non-negative) f(2)
    __mul_n<__mul_mode::toom33, T, in_place_max, 2>(w3p, w0p, w4p, l, stk, cf3, cf0, cf4);

    // W0 = U0 * V0 : (non-negative) f(0) = r0
    __mul_n<__mul_mode::toom33>(w0p, u0p, v0p, l, stk);

    // W4 = U3 * V1 : (non-negative) f(inf) = r4
    if (rn >= rm) {
        __mul_s<__mul_mode::toom33>(w4p, u3p, rn, v1p, rm, stk);
    } else {
        __mul_s<__mul_mode::toom33>(w4p, v1p, rm, u3p, rn, stk);
    }

    return toom_interpolation_5p_s(dst, w1p, l, rn, rm,
                                   toom_interpolation_5p_struct<T>{neg2, cf1, cf2, cf3});
}

template <typename T>
void toom33_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk) {
    WJR_ASSERT_ASSUME(2 * n + 10 <= 3 * m);

    const size_t l = (n + 2) / 3;
    const size_t rn = n - l * 2;
    const size_t rm = m - l * 2;

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);
    WJR_ASSERT_ASSUME(0 < rm && rm <= l);
    WJR_ASSERT_ASSUME(rn + rm >= l);

    const auto u0p = src0;
    const auto u1p = src0 + l;
    const auto u2p = src0 + l * 2;

    const auto v0p = src1;
    const auto v1p = src1 + l;
    const auto v2p = src1 + l * 2;

    const auto w0p = dst;
    const auto w1p = stk;
    const auto w2p = w0p + l * 2;
    const auto w3p = stk + l * 2;
    const auto w4p = w0p + l * 4;

    stk += l * 4;

    T cf0 = 0, cf1 = 0, cf2 = 0, cf3 = 0, cf4 = 0;
    bool neg2 = 0, neg3 = 0;

    // W0 = U0 + U2 : (non-negative)
    cf0 = addc_s(w0p, u0p, l, u2p, rn);
    // W4 = V0 + V2 : (non-negative)
    cf4 = addc_s(w4p, v0p, l, v2p, rm);

    // W3 = W0 - U1 : u(-1)
    {
        ssize_t p = abs_subc_n(w3p, w0p, u1p, l, cf3, cf0, 0);
        neg3 = p < 0;
        WJR_ASSERT_ASSUME(cf3 <= 1);
    }

    // W2 = W4 - V1 : v(-1)
    {
        ssize_t p = abs_subc_n(w2p, w4p, v1p, l, cf2, cf4, 0);
        neg2 = p < 0;
        WJR_ASSERT_ASSUME(cf2 <= 1);
    }

    // W0 = W0 + U1 : (non-negative) u(1)
    cf0 += addc_n(w0p, w0p, u1p, l);
    WJR_ASSERT_ASSUME(cf0 <= 2);

    // W4 = W4 + V1 : (non-negative) v(1)
    cf4 += addc_n(w4p, w4p, v1p, l);
    WJR_ASSERT_ASSUME(cf4 <= 2);

    // W1 = W3 * W2 : f(-1)
    neg2 ^= neg3;
    __mul_n<__mul_mode::toom33, T, 1, 1>(w1p, w3p, w2p, l, stk, cf1, cf3, cf2);

    // W2 = W0 * W4 : (non-negative) f(1)
    __mul_n<__mul_mode::toom33, T, 2, 2>(w2p, w0p, w4p, l, stk, cf2, cf0, cf4);

    // W0 = (W0 + U2) << 1 - U0 : (non-negative) u(2)
    cf0 += addc_s(w0p, w0p, l, u2p, rn);
    cf0 += cf0 + rsblsh_n(w0p, u0p, w0p, l, 1);
    WJR_ASSERT_ASSUME(cf0 <= 6);

    // W4 = (W4 + V2) << 1 - V0 : (non-negative) v(2)
    cf4 += addc_s(w4p, w4p, l, v2p, rm);
    cf4 += cf4 + rsblsh_n(w4p, v0p, w4p, l, 1);
    WJR_ASSERT_ASSUME(cf4 <= 6);

    // W3 = W0 * W4 : (non-negative) f(2)
    __mul_n<__mul_mode::toom33>(w3p, w0p, w4p, l, stk, cf3, cf0, cf4);

    // W0 = U0 * V0 : (non-negative) f(0) = r0
    __mul_n<__mul_mode::toom33>(w0p, u0p, v0p, l, stk);

    // W4 = U2 * V2 : (non-negative) f(inf) = r4
    __mul_s<__mul_mode::toom33>(w4p, u2p, rn, v2p, rm, stk);

    return toom_interpolation_5p_s(dst, w1p, l, rn, rm,
                                   toom_interpolation_5p_struct<T>{neg2, cf1, cf2, cf3});
}

template <typename T>
void toom3_sqr(T *WJR_RESTRICT dst, const T *src, size_t n, T *stk) {
    WJR_ASSERT_ASSUME(10 <= n);

    const size_t l = (n + 2) / 3;
    const size_t rn = n - l * 2;

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);

    const auto u0p = src;
    const auto u1p = src + l;
    const auto u2p = src + l * 2;

    const auto w0p = dst;
    const auto w1p = stk;
    const auto w2p = w0p + l * 2;
    const auto w3p = stk + l * 2;
    const auto w4p = w0p + l * 4;

    stk += l * 4;

    T cf0 = 0, cf1 = 0, cf2 = 0, cf3 = 0;

    // W0 = U0 + U2 : (non-negative)
    cf0 = addc_s(w0p, u0p, l, u2p, rn);

    // W3 = W0 - U1 : u(-1)
    {
        (void)abs_subc_n(w3p, w0p, u1p, l, cf3, cf0, 0);
        WJR_ASSERT_ASSUME(cf3 <= 1);
    }

    // W0 = W0 + U1 : (non-negative) u(1)
    cf0 += addc_n(w0p, w0p, u1p, l);
    WJR_ASSERT_ASSUME(cf0 <= 2);

    // W1 = W3 * W3 : f(-1)
    __sqr<__mul_mode::toom33, T, 1>(w1p, w3p, l, stk, cf1, cf3);

    // W2 = W0 * W9 : (non-negative) f(1)
    __sqr<__mul_mode::toom33, T, 2>(w2p, w0p, l, stk, cf2, cf0);

    // W0 = (W0 + U2) << 1 - U0 : (non-negative) u(2)
    cf0 += addc_s(w0p, w0p, l, u2p, rn);
    cf0 += cf0 + rsblsh_n(w0p, u0p, w0p, l, 1);
    WJR_ASSERT_ASSUME(cf0 <= 6);

    // W3 = W0 * W0 : (non-negative) f(2)
    __sqr<__mul_mode::toom33>(w3p, w0p, l, stk, cf3, cf0);

    // W0 = U0 * U0 : (non-negative) f(0) = r0
    __sqr<__mul_mode::toom33>(w0p, u0p, l, stk);

    // W4 = U2 * U2 : (non-negative) f(inf) = r4
    __sqr<__mul_mode::toom33>(w4p, u2p, rn, stk);

    return toom_interpolation_5p_s(dst, w1p, l, rn, rn,
                                   toom_interpolation_5p_struct<T>{0, cf1, cf2, cf3});
}

template <typename T>
struct toom_interpolation_6p_struct {
    uint8_t neg1 : 1;
    uint8_t neg3 : 1;
    T cf1;
    T cf2;
    T cf3;
    T cf4;
};

template <typename T>
void toom_interpolation_6p_s(T *WJR_RESTRICT dst, T *w1p, size_t l, size_t rn, size_t rm,
                             toom_interpolation_6p_struct<T> &&flag) {
    /*
     W0 = f(0);
     W1 = f(-1);
     W2 = f(1);
     W3 = f(-2);
     W4 = f(2);
     W5 = f(inf)
    */

    /*
     11 add/sub + 4 shift + 2 divby3 + 1 submul

     W3 = (W3 - W4) >> 1;
     W4 = W4 + W3;
     W4 = W4 - W0;
     W1 = W2 - W1;
     W2 = W2 - W0;
     W3 = W3 + W1;
     W1 >>= 1;
     W3 = W3 / -6; => W3 = W3 / -2 / 3;
     W2 = W2 - W1;
     W3 = W3 - 4 * W5;
     W1 = W1 - W3;
     W4 = W4 >> 2;
     W4 = W4 - W2;
     W4 = W4 / 3;

     W4 = W5 * x + W4;
     W2 = W3 * x + W2;
     W2 = W2 - W4;
     W = W4 * x ^ 4 + W2 * x ^ 2 + W1 * x + W0;
    */

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);
    WJR_ASSERT_ASSUME(0 < rm && rm <= l);
    WJR_ASSERT_ASSUME(rn + rm >= l);

    const size_t maxr = std::max(rn, rm);

    const auto w0p = dst;
    const auto w2p = w0p + l * 2;
    const auto w3p = w1p + l * 2;
    const auto w4p = w1p + l * 4;
    const auto w5p = w0p + l * 5;

    T cf;
    auto [neg1, neg3, cf1, cf2, cf3, cf4] = flag;

    // W3 = (W3 - W4) >> 1;
    {
        if (!neg3) {
            cf3 = cf4 - cf3 - subc_n(w3p, w4p, w3p, l * 2);
        } else {
            cf3 += cf4 + addc_n(w3p, w3p, w4p, l * 2);
        }

        (void)rshift_n(w3p, w3p, l * 2, 1, cf3);
        cf3 >>= 1;
    }

    // W4 = W4 + W3;
    cf4 -= cf3 + subc_n(w4p, w4p, w3p, l * 2);

    // W4 = W4 - W0;
    cf4 -= subc_n(w4p, w4p, w0p, l * 2);

    // W1 = W2 - W1;
    if (!neg1) {
        cf1 = cf2 - cf1 - subc_n(w1p, w2p, w1p, l * 2);
    } else {
        cf1 = cf2 + cf1 + addc_n(w1p, w2p, w1p, l * 2);
    }

    // W2 = W2 - W0;
    cf2 -= subc_n(w2p, w2p, w0p, l * 2);

    // W3 = W3 + W1;
    cf3 -= cf1 + subc_n(w3p, w3p, w1p, l * 2);

    // W1 >>= 1;
    (void)rshift_n(w1p, w1p, l * 2, 1, cf1);
    cf1 >>= 1;

    // W3 = W3 / -6; = > W3 = W3 / -2 / 3;
    {
        (void)rshift_n(w3p, w3p, l * 2, 1, cf3);
        divexact_by3(w3p, w3p, l * 2);
        cf3 /= 6;
    }

    // W2 = W2 - W1;
    cf2 -= cf1 + subc_n(w2p, w2p, w1p, l * 2);

    // W3 = W3 - 4 * W5;
    {
        cf = submul_1(w3p, w5p, rn + rm, 4);
        if (rn + rm != l * 2) {
            cf3 -= subc_1(w3p + rn + rm, w3p + rn + rm, l * 2 - (rn + rm), cf);
        } else {
            cf3 -= cf;
        }
    }

    // W1 = W1 - W3;
    cf1 -= cf3 + subc_n(w1p, w1p, w3p, l * 2);

    // W4 = W4 >> 2;
    rshift_n(w4p, w4p, l * 2, 2, cf4);
    cf4 >>= 2;

    // W4 = W4 - W2;
    cf4 -= cf2 + subc_n(w4p, w4p, w2p, l * 2);

    // W4 = W4 / 3;
    divexact_by3(w4p, w4p, l * 2);
    if (l != maxr) {
        cf4 = w4p[l + maxr];
    } else {
        cf4 /= 3;
    }

    // W4 = W5 * x + W4;
    cf = addc_n(w5p, w5p, w4p + l, maxr);
    cf = addc_1(w5p + maxr, w5p + maxr, rn + rm - maxr, cf4, cf);
    WJR_ASSERT(cf == 0);

    // W2 = W3 * x + W2;
    cf = addc_n(w3p, w3p, w2p + l, l);
    cf3 += addc_1(w3p + l, w3p + l, l, cf2, cf);

    // W2 = W2 - W4;
    cf = subc_n(w2p, w2p, w4p, l);
    cf3 -= subc_s(dst + l * 3, w3p, l * 2, w5p, rn + rm, cf);

    // W = W4 * x ^ 4 + W2 * x ^ 2 + W1 * x + W0;
    cf = addc_n(dst + l, dst + l, w1p, l * 2);
    cf = addc_1(dst + l * 3, dst + l * 3, l, cf1, cf);
    cf = addc_n(dst + l * 4, dst + l * 4, w4p, l, cf);
    cf = addc_1(dst + l * 5, dst + l * 5, rn + rm, cf3, cf);
    WJR_ASSERT(cf == 0);
}

template <typename T>
void toom43_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk) {
    /*
     W0 = f(0);
     W1 = f(-1);
     W2 = f(1);
     W3 = f(-2);
     W4 = f(2);
     W5 = f(inf);

     W0 = U0 * V0;
     W1 = (U0 - U1 + U2 - U3) * (V0 - V1 + V2);
     W2 = (U0 + U1 + U2 + U3) * (V0 + V1 + V2);
     W3 = (U0 - 2U1 + 4U2 - 8U3) * (V0 - 2V2 + 4V3);
     W4 = (U0 + 2U1 + 4U2 + 8U3) * (V0 + 2V2 + 4V3);
     W5 = U3 * V2;
    */

    /*
        dst :
        |--- l * 2 ---|--- l * 2 ---|- l --|--- rn+rm ---|
        W0             W2            T2    W5
        |- l --|- l --|
        T0      T1
        stk :
        |--- l * 2 ---|--- l * 2 ---|--- l * 2 ---|
        W1             W3            W4
     */

    /*
     11 add/sub + 4 addlsh/rsblsh + 1 shift

     W3 = U0 + U2;
     W4 = U1 + U3;
     T0 = W3 - W4; u(-1)
     T1 = W3 + W4; u(1)
     W3 = V0 + V2;
     W4 = W3 - V1; v(-1)
     W1 = T0 * W4; f(-1)
     W5 = W3 + V1; v(1)
     W2 = T1 * W5; f(1)
     W4 = (W4 + V2) << 1 - V0; v(-2)
     W5 = (W5 + V2) << 1 - V0; v(2)
     T0 = U0 + 4U2;
     T1 = (U1 + 4U3) << 1;
     T2 = T0 - T1; u(-2)
     W3 = T2 * W4; f(-2)
     T2 = T0 + T1; u(2)
     W4 = T2 * W5; f(2)
     W0 = U0 * V0;
     W5 = U3 * V2;
     */

    WJR_ASSERT_ASSUME(3 * m + 8 <= 3 * n && 2 * n + 18 <= 4 * m);

    const size_t l = (3 * n >= 4 * m ? (n + 3) / 4 : (m + 2) / 3);
    const size_t rn = n - l * 3;
    const size_t rm = m - l * 2;

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);
    WJR_ASSERT_ASSUME(0 < rm && rm <= l);
    WJR_ASSERT_ASSUME(rn + rm >= l);

    const auto u0p = src0;
    const auto u1p = u0p + l;
    const auto u2p = u0p + l * 2;
    const auto u3p = u0p + l * 3;

    const auto v0p = src1;
    const auto v1p = v0p + l;
    const auto v2p = v0p + l * 2;

    const auto w0p = dst;
    const auto w1p = stk;
    const auto w2p = w0p + l * 2;
    const auto w3p = stk + l * 2;
    const auto w4p = stk + l * 4;
    const auto w5p = w0p + l * 5;

    const auto t0p = w0p;
    const auto t1p = t0p + l;
    const auto t2p = w0p + l * 4;

    stk += l * 6;

    // Hand it over to the compiler to optimize reusable variables
    T cf1 = 0, cf2 = 0, cf3 = 0, cf4 = 0, cf5 = 0;
    T cft0 = 0, cft1 = 0, cft2 = 0;
    // Reusable
    // not point to real W0...W5 or T0...T2
    bool neg0 = 0, neg1 = 0, neg2 = 0;

    //  W3 = U0 + U2;
    cf3 = addc_n(w3p, u0p, u2p, l);

    //  W4 = U1 + U3;
    cf4 = addc_s(w4p, u1p, l, u3p, rn);

    //  T0 = W3 - W4; u(-1)
    {
        ssize_t p = abs_subc_n(t0p, w3p, w4p, l, cft0, cf3, cf4);
        neg0 = p < 0;
        WJR_ASSERT_ASSUME(cft0 <= 1);
    }

    //  T1 = W3 + W4; u(1)
    cft1 = cf3 + cf4 + addc_n(t1p, w3p, w4p, l);
    WJR_ASSERT_ASSUME(cft1 <= 3);

    //  W3 = V0 + V2;
    cf3 = addc_s(w3p, v0p, l, v2p, rm);

    //  W4 = W3 - V1; v(-1)
    {
        ssize_t p = abs_subc_n(w4p, w3p, v1p, l, cf4, cf3, 0);
        neg1 = p < 0;
        WJR_ASSERT_ASSUME(cf4 <= 1);
    }

    //  W1 = T0 * W4; f(-1)
    neg0 ^= neg1;
    __mul_n<__mul_mode::toom33, T, 1, 1>(w1p, t0p, w4p, l, stk, cf1, cft0, cf4);

    //  W5 = W3 + V1; v(1)
    cf5 = cf3 + addc_n(w5p, w3p, v1p, l);
    WJR_ASSERT_ASSUME(cf5 <= 2);

    //  W2 = T1 * W5; f(1)
    __mul_n<__mul_mode::toom33, T, 3, 2>(w2p, t1p, w5p, l, stk, cf2, cft1, cf5);

    //  W4 = (W4 + V2) << 1 - V0; v(-2)
    {
        if (!neg1) {
            cf4 += addc_s(w4p, w4p, l, v2p, rm);
        } else {
            WJR_ASSERT_ASSUME(cf4 == 0);
            ssize_t p = abs_subc_s(w4p, w4p, l, v2p, rm);
            neg1 = p > 0;
        }

        if (!neg1) {
            // W4 maybe less than V0
            // use lshfit + abs_sub instead of rsblsh_n
            cf4 += cf4 + lshift_n(w4p, w4p, l, 1);
            {
                ssize_t p = abs_subc_n(w4p, w4p, v0p, l, cf4, cf4, 0);
                neg1 = p < 0;
            }
        } else {
            WJR_ASSERT_ASSUME(cf4 == 0);
            cf4 += addlsh_n(w4p, v0p, w4p, l, 1);
        }
        WJR_ASSERT_ASSUME(cf4 <= 4);
    }

    //  W5 = (W5 + V2) << 1 - V0; v(2)
    cf5 += addc_s(w5p, w5p, l, v2p, rm);
    cf5 += cf5 + rsblsh_n(w5p, v0p, w5p, l, 1);
    WJR_ASSERT_ASSUME(cf5 <= 6);

    //  T0 = U0 + 4U2;
    cft0 = addlsh_n(t0p, u0p, u2p, l, 2);
    WJR_ASSERT_ASSUME(cft0 <= 4);

    //  T1 = (U1 + 4U3) << 1;
    cft1 = addlsh_n(t1p, u1p, u3p, rn, 2);
    if (l != rn) {
        cft1 = addc_1(t1p + rn, u1p + rn, l - rn, cft1);
    }
    cft1 += cft1 + lshift_n(t1p, t1p, l, 1);
    WJR_ASSERT_ASSUME(cft1 <= 9);

    //  T2 = T0 - T1; u(-2)
    {
        ssize_t p = abs_subc_n(t2p, t0p, t1p, l, cft2, cft0, cft1);
        neg2 = p < 0;
        WJR_ASSERT_ASSUME(cft2 <= 9);
    }

    //  W3 = T2 * W4; f(-2)
    neg1 ^= neg2;
    __mul_n<__mul_mode::toom33>(w3p, t2p, w4p, l, stk, cf3, cft2, cf4);

    //  T2 = T0 + T1; u(2)
    cft2 = cft0 + cft1 + addc_n(t2p, t0p, t1p, l);
    WJR_ASSERT_ASSUME(cft2 <= 14);

    //  W4 = T2 * W5; f(2)
    __mul_n<__mul_mode::toom33>(w4p, t2p, w5p, l, stk, cf4, cft2, cf5);

    //  W0 = U0 * V0;
    __mul_n<__mul_mode::toom33>(w0p, u0p, v0p, l, stk);

    //  W5 = U3 * V2;
    if (rn >= rm) {
        __mul_s<__mul_mode::toom33>(w5p, u3p, rn, v2p, rm, stk);
    } else {
        __mul_s<__mul_mode::toom33>(w5p, v2p, rm, u3p, rn, stk);
    }

    toom_interpolation_6p_s(
        dst, w1p, l, rn, rm,
        toom_interpolation_6p_struct<T>{neg0, neg1, cf1, cf2, cf3, cf4});
}

template <typename T>
struct toom_interpolation_7p_struct {
    uint8_t neg1 : 1;
    uint8_t neg3 : 1;
    T cf1;
    T cf2;
    T cf3;
    T cf4;
    T cf5;
};

template <typename T>
void toom_interpolation_7p_s(T *WJR_RESTRICT dst, T *w1p, size_t l, size_t rn, size_t rm,
                             toom_interpolation_7p_struct<T> &&flag) {
    /*
     W0 = f(0);
     W1 = f(-2);
     W2 = f(1);
     W3 = f(-1);
     W4 = f(2);
     W5 = 64 * f(1/2);
     W6 = f(inf);
    */

    /*
    [
    W0   1,  0,  0,  0,  0,  0,  0
    W1   1, -2,  4, -8, 16,-32, 64
    W2   1,  1,  1,  1,  1,  1,  1
    W3   1, -1,  1, -1,  1, -1,  1
    W4   1,  2,  4,  8, 16, 32, 64
    W5  64, 32, 16,  8,  4,  2,  1
    W6   0,  0,  0,  0,  0,  0,  1
    ]

     W5 = W5 + W4
     W1 =(W4 - W1)/2
     W4 = W4 - W0
     W4 =(W4 - W1)/4 - W6*16
     W3 =(W2 - W3)/2
     W2 = W2 - W3

     [
    W0   1,  0,  0,  0,  0,  0,  0
    W1   0,  2,  0,  8,  0, 32,  0
    W2   1,  0,  1,  0,  1,  0,  1
    W3   0,  1,  0,  1,  0,  1,  0
    W4   0,  0,  1,  0,  4,  0,  0
    W5  65, 34, 20, 16, 20, 34, 65
    W6   0,  0,  0,  0,  0,  0,  1
    ]

     W5 = W5 - W2*65      May be negative.
     W2 = W2 - W6 - W0
     W5 =(W5 + W2*45)/2   Now >= 0 again.
     W4 =(W4 - W2)/3
     W2 = W2 - W4

     [
    W0   1,  0,  0,  0,  0,  0,  0
    W1   0,  2,  0,  8,  0, 32,  0
    W2   0,  0,  1,  0,  0,  0,  0
    W3   0,  1,  0,  1,  0,  1,  0
    W4   0,  0,  0,  0,  1,  0,  0
    W5   0, 17,  0,  8,  0, 17,  0
    W6   0,  0,  0,  0,  0,  0,  1
    ]

     W1 = W5 - W1         May be negative.
     W5 =(W5 - W3*8)/9
     W3 = W3 - W5
     W1 =(W1/15 + W5)/2   Now >= 0 again.
     W5 = W5 - W1

     [
    W0   1,  0,  0,  0,  0,  0,  0
    W1   0,  1,  0,  0,  0,  0,  0
    W2   0,  0,  1,  0,  0,  0,  0
    W3   0,  0,  0,  1,  0,  0,  0
    W4   0,  0,  0,  0,  1,  0,  0
    W5   0,  0,  0,  0,  0,  1,  0
    W6   0,  0,  0,  0,  0,  0,  1
    ]

    */

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);
    WJR_ASSERT_ASSUME(0 < rm && rm <= l);
    WJR_ASSERT_ASSUME(rn + rm >= l);

    const size_t maxr = std::max(rn, rm);

    const auto w0p = dst;
    const auto w2p = w0p + l * 2;
    const auto w3p = w1p + l * 2;
    const auto w4p = w0p + l * 4;
    const auto w5p = w1p + l * 4;
    const auto w6p = w0p + l * 6;

    T cf = 0;
    auto [neg1, neg3, cf1, cf2, cf3, cf4, cf5] = flag;

    //  W5 = W5 + W4
    cf5 += cf4 + addc_n(w5p, w5p, w4p, l * 2);

    //  W1 =(W4 - W1)/2
    if (!neg1) {
        WJR_ASSERT(cf4 >= cf1);
        cf1 = cf4 - cf1 - subc_n(w1p, w4p, w1p, l * 2);
    } else {
        cf1 += cf4 + addc_n(w1p, w4p, w1p, l * 2);
    }
    (void)rshift_n(w1p, w1p, l * 2, 1, cf1);
    cf1 >>= 1;

    //  W4 = W4 - W0
    cf4 -= subc_n(w4p, w4p, w0p, l * 2);

    //  W4 =(W4 - W1)/4 - W6*16
    cf4 -= cf1 + subc_n(w4p, w4p, w1p, l * 2);
    (void)rshift_n(w4p, w4p, l * 2, 2, cf4);
    cf4 >>= 2;
    cf = submul_1(w4p, w6p, rn + rm, 16);
    if (rn + rm != l * 2) {
        cf4 -= subc_1(w4p + rn + rm, w4p + rn + rm, l * 2 - (rn + rm), cf);
    } else {
        cf4 -= cf;
    }

    //  W3 =(W2 - W3)/2
    if (!neg3) {
        WJR_ASSERT(cf2 >= cf3);
        cf3 = cf2 - cf3 - subc_n(w3p, w2p, w3p, l * 2);
    } else {
        cf3 += cf2 + addc_n(w3p, w2p, w3p, l * 2);
    }
    (void)rshift_n(w3p, w3p, l * 2, 1, cf3);
    cf3 >>= 1;

    //  W2 = W2 - W3
    WJR_ASSERT(cf2 >= cf3);
    cf2 -= cf3 + subc_n(w2p, w2p, w3p, l * 2);

    //  W5 = W5 - W2*65      May be negative.
    cf5 -= cf2 * 65 + submul_1(w5p, w2p, l * 2, 65);

    //  W2 = W2 - W6 - W0
    cf2 -= subc_n(w2p, w2p, w0p, l * 2);
    cf2 -= subc_s(w2p, w2p, l * 2, w6p, rn + rm);

    //  W5 =(W5 + W2*45)/2   Now >= 0 again.
    cf5 += cf2 * 45 + addmul_1(w5p, w2p, l * 2, 45);
    (void)rshift_n(w5p, w5p, l * 2, 1, cf5);
    cf5 >>= 1;

    //  W4 =(W4 - W2)/3
    cf4 -= cf2 + subc_n(w4p, w4p, w2p, l * 2);
    divexact_by3(w4p, w4p, l * 2);
    cf4 /= 3;

    //  W2 = W2 - W4
    cf2 -= cf4 + subc_n(w2p, w2p, w4p, l * 2);

    //  W1 = W5 - W1         May be negative.
    {
        ssize_t p = abs_subc_n(w1p, w5p, w1p, l * 2, cf1, cf5, cf1);
        neg1 = p < 0;
    }

    //  W5 =(W5 - W3*8)/9
    cf5 -= cf3 * 8 + submul_1(w5p, w3p, l * 2, 8);
    divexact_byc(w5p, w5p, l * 2, std::integral_constant<T, 9>{});
    cf5 /= 9;

    //  W3 = W3 - W5
    cf3 -= cf5 + subc_n(w3p, w3p, w5p, l * 2);

    //  W1 =(W1/15 + W5)/2   Now >= 0 again.
    divexact_by15(w1p, w1p, l * 2);
    cf1 /= 15;
    if (!neg1) {
        cf1 += cf5 + addc_n(w1p, w1p, w5p, l * 2);
    } else {
        cf1 = cf5 - cf1 - subc_n(w1p, w5p, w1p, l * 2);
    }
    rshift_n(w1p, w1p, l * 2, 1, cf1);
    cf1 >>= 1;

    //  W5 = W5 - W1
    cf5 -= cf1 + subc_n(w5p, w5p, w1p, l * 2);
    if (l != maxr) {
        cf5 = w5p[l + maxr];
    }

    cf = addc_n(dst + l, dst + l, w1p, l * 2);
    cf = addc_n(dst + l * 3, dst + l * 3, w3p, l, cf);
    cf2 += addc_1(dst + l * 3, dst + l * 3, l, cf1);
    cf = addc_n(dst + l * 4, dst + l * 4, w3p + l, l, cf);
    cf3 += addc_1(dst + l * 4, dst + l * 4, l, cf2);
    cf = addc_n(dst + l * 5, dst + l * 5, w5p, l, cf);
    cf4 += addc_1(dst + l * 5, dst + l * 5, l, cf3);
    cf = addc_n(dst + l * 6, dst + l * 6, w5p + l, maxr, cf);
    cf5 += addc_1(dst + l * 6, dst + l * 6, maxr, cf4);
    cf = addc_1(dst + l * 6 + maxr, dst + l * 6 + maxr, (rn + rm) - maxr, cf5, cf);
    WJR_ASSERT(cf == 0);
}

template <typename T>
void toom53_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk) {
    /*
     W0 = f(0);
     W1 = f(-2);
     W2 = f(1);
     W3 = f(-1);
     W4 = f(2);
     W5 = 64 * f(1/2);
     W6 = f(inf);

     W0 = U0 * V0;
     W1 = (U0 - 2U1 + 4U2 + 8U3 - 16U4) * (V0 - 2V1 + 4V2);
     W2 = (U0 + U1 + U2 + U3 + U4) * (V0 + V1 + V2);
     W3 = (U0 - U1 + U2 - U3 + U4) * (V0 - V1 + V2);
     W4 = (U0 + 2U1 + 4U2 + 8U3 + 16U4) * (V0 + 2V1 + 4V2);
     W5 = (16U0 + 8U1 + 4U2 + 2U3 + U4) * (4V0 + 2V1 + V2);
     W6 = U4 * V2;
    */

    /*
          dst :
          |--- l * 2 ---|--- l * 2 ---|--- l * 2 ---|--- rn+rm ---|
          W0             W2            W4            W6
          |- l --|- l --|                           |- l --|
          T0      T1                                 T2
          stk :
          |--- l * 2 ---|--- l * 2 ---|--- l * 2 ---|
          W1             W3            W5
     */

    /*
     11 add/sub + 11 addlsh/rsblsh + 1 shift

     T0 = U0 + U2 + U4;
     T2 = U1 + U3;
     T1 = T0 - T2; u(-1)
     T0 = T0 + T2; u(1)
     W5 = V0 + V2;
     T2 = W5 + V1; v(1)
     W5 = W5 - V1; v(-1)
     W2 = T0 * T2; f(1)
     W3 = T1 * W5; f(-1)
     W5 = (W5 + V2) << 1 - V0; v(-2)
     T2 = W5 + 4V1; v(2)
     T1 = U0 + (U2 + 4U4) << 2;
     W1 = (U1 + 4U3) << 1;
     T0 = T1 + W1; u(2)
     T1 = T1 - W1; u(-2)
     W1 = T1 * W5; f(-2)
     W4 = T0 * T2; f(2)
     T0 = (((2U0 + U1) << 1 + U2) << 1 + U3) << 1 + U4; 16 * u(1/2)
     T1 = (2V0 + V1) << 1 + V2; 4 * v(1/2)
     W5 = T0 * T1; 64 * f(1/2)
     W0 = U0 * V0; f(0)
     W6 = U4 * V2; f(inf)
    */

    WJR_ASSERT_ASSUME(4 * m + 14 <= 3 * n && 2 * n + 28 <= 5 * m);

    const size_t l = 3 * n >= 5 * m ? (n + 4) / 5 : (m + 2) / 3;
    const size_t rn = n - l * 4;
    const size_t rm = m - l * 2;

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);
    WJR_ASSERT_ASSUME(0 < rm && rm <= l);
    WJR_ASSERT_ASSUME(rn + rm >= l);

    const auto u0p = src0;
    const auto u1p = src0 + l;
    const auto u2p = src0 + l * 2;
    const auto u3p = src0 + l * 3;
    const auto u4p = src0 + l * 4;

    const auto v0p = src1;
    const auto v1p = src1 + l;
    const auto v2p = src1 + l * 2;

    const auto w0p = dst;
    const auto w1p = stk;
    const auto w2p = w0p + l * 2;
    const auto w3p = stk + l * 2;
    const auto w4p = w0p + l * 4;
    const auto w5p = stk + l * 4;
    const auto w6p = w0p + l * 6;

    const auto t0p = w0p;
    const auto t1p = w0p + l;
    const auto t2p = w6p;

    stk += l * 6;

    T cf = 0;
    T cf1 = 0, cf2 = 0, cf3 = 0, cf4 = 0, cf5 = 0;
    T cft0 = 0, cft1 = 0, cft2 = 0;
    bool neg0 = 0, neg1 = 0, neg2 = 0;

    //  T0 = U0 + U2 + U4;
    cft0 = addc_n(t0p, u0p, u2p, l);
    cft0 += addc_s(t0p, t0p, l, u4p, rn);

    //  T2 = U1 + U3;
    cft2 = addc_n(t2p, u1p, u3p, l);

    //  T1 = T0 - T2; u(-1)
    {
        ssize_t p = abs_subc_n(t1p, t0p, t2p, l, cft1, cft0, cft2);
        neg0 = p < 0;
        WJR_ASSERT_ASSUME(cft1 <= 2);
    }

    //  T0 = T0 + T2; u(1)
    cft0 += cft2 + addc_n(t0p, t0p, t2p, l);
    WJR_ASSERT_ASSUME(cft0 <= 4);

    //  W5 = V0 + V2;
    cf5 = addc_s(w5p, v0p, l, v2p, rm);

    //  T2 = W5 + V1; v(1)
    cft2 = cf5 + addc_n(t2p, w5p, v1p, l);
    WJR_ASSERT_ASSUME(cft2 <= 2);

    //  W5 = W5 - V1; v(-1)
    {
        ssize_t p = abs_subc_n(w5p, w5p, v1p, l, cf5, cf5, 0);
        neg1 = p < 0;
        WJR_ASSERT_ASSUME(cf5 <= 1);
    }

    //  W2 = T0 * T2; f(1)
    __mul_n<__mul_mode::toom44, T, 4, 2>(w2p, t0p, t2p, l, stk, cf2, cft0, cft2);

    //  W3 = T1 * W5; f(-1)
    neg0 ^= neg1;
    __mul_n<__mul_mode::toom44, T, 2, 1>(w3p, t1p, w5p, l, stk, cf3, cft1, cf5);

    //  W5 = (W5 + V2) << 1 - V0; v(-2)
    if (!neg1) {
        cf5 += addc_s(w5p, w5p, l, v2p, rm);
    } else {
        WJR_ASSERT_ASSUME(cf5 == 0);
        ssize_t p = abs_subc_s(w5p, w5p, l, v2p, rm);
        neg1 = p > 0;
    }

    if (!neg1) {
        cf5 += cf5 + lshift_n(w5p, w5p, l, 1);
        {
            ssize_t p = abs_subc_n(w5p, w5p, v0p, l, cf5, cf5, 0);
            neg1 = p < 0;
        }
    } else {
        WJR_ASSERT_ASSUME(cf5 == 0);
        cf5 += addlsh_n(w5p, v0p, w5p, l, 1);
    }
    WJR_ASSERT_ASSUME(cf5 <= 4);

    //  T2 = W5 + 4V1; v(2)
    if (!neg1) {
        cft2 = cf5 + addlsh_n(t2p, w5p, v1p, l, 2);
    } else {
        cft2 = rsblsh_n(t2p, w5p, v1p, l, 2) - cf5;
    }
    WJR_ASSERT_ASSUME(cft2 <= 6);

    //  T1 = U0 + (U2 + 4U4) << 2;
    cft1 = addlsh_n(t1p, u2p, u4p, rn, 2);
    if (l != rn) {
        cft1 = addc_1(t1p + rn, u2p + rn, l - rn, cft1);
    }
    cft1 = cft1 * 4 + addlsh_n(t1p, u0p, t1p, l, 2);

    //  W1 = (U1 + 4U3) << 1;
    cf1 = addlsh_n(w1p, u1p, u3p, l, 2);
    cf1 += cf1 + lshift_n(w1p, w1p, l, 1);

    //  T0 = T1 + W1; u(2)
    cft0 = cft1 + cf1 + addc_n(t0p, t1p, w1p, l);
    WJR_ASSERT_ASSUME(cft0 <= 30);

    //  T1 = T1 - W1; u(-2)
    {
        ssize_t p = abs_subc_n(t1p, t1p, w1p, l, cft1, cft1, cf1);
        neg2 = p < 0;
    }
    WJR_ASSERT_ASSUME(cft1 <= 20);

    //  W1 = T1 * W5; f(-2)
    neg1 ^= neg2;
    __mul_n<__mul_mode::toom44, T, in_place_max, 4>(w1p, t1p, w5p, l, stk, cf1, cft1,
                                                    cf5);

    //  W4 = T0 * T2; f(2)
    __mul_n<__mul_mode::toom44, T, in_place_max, 6>(w4p, t0p, t2p, l, stk, cf4, cft0,
                                                    cft2);

    //  T0 = (((2U0 + U1) << 1 + U2) << 1 + U3) << 1 + U4; 16 * u(1/2)
    cft0 = addlsh_n(t0p, u1p, u0p, l, 1);
    cft0 += cft0 + addlsh_n(t0p, u2p, t0p, l, 1);
    cft0 += cft0 + addlsh_n(t0p, u3p, t0p, l, 1);
    cf = addlsh_n(t0p, u4p, t0p, rn, 1);
    if (l != rn) {
        cft0 += cft0 + lshift_n(t0p + rn, t0p + rn, l - rn, 1);
        cft0 += addc_1(t0p + rn, t0p + rn, l - rn, cf);
    } else {
        cft0 += cft0 + cf;
    }
    WJR_ASSERT_ASSUME(cft0 <= 30);

    //  T1 = (2V0 + V1) << 1 + V2; 4 * v(1/2)
    cft1 = addlsh_n(t1p, v1p, v0p, l, 1);
    cf = addlsh_n(t1p, v2p, t1p, rm, 1);
    if (l != rm) {
        cft1 += cft1 + lshift_n(t1p + rm, t1p + rm, l - rm, 1);
        cft1 += addc_1(t1p + rm, t1p + rm, l - rm, cf);
    } else {
        cft1 += cft1 + cf;
    }
    WJR_ASSERT_ASSUME(cft1 <= 6);

    //  W5 = T0 * T1; 64 * f(1/2)
    __mul_n<__mul_mode::toom44>(w5p, t0p, t1p, l, stk, cf5, cft0, cft1);

    //  W0 = U0 * V0; f(0)
    __mul_n<__mul_mode::toom44>(w0p, u0p, v0p, l, stk);

    //  W6 = U4 * V2; f(inf)
    if (rn >= rm) {
        __mul_s<__mul_mode::toom44>(w6p, u4p, rn, v2p, rm, stk);
    } else {
        __mul_s<__mul_mode::toom44>(w6p, v2p, rm, u4p, rn, stk);
    }

    toom_interpolation_7p_s(
        dst, w1p, l, rn, rm,
        toom_interpolation_7p_struct<T>{neg1, neg0, cf1, cf2, cf3, cf4, cf5});
}

template <typename T>
void toom44_mul_s(T *WJR_RESTRICT dst, const T *src0, size_t n, const T *src1, size_t m,
                  T *stk) {
    /*
     W0 = f(0);
     W1 = f(-2);
     W2 = f(1);
     W3 = f(-1);
     W4 = f(2);
     W5 = 64 * f(1/2);
     W6 = f(inf);

     W0 = U0 * V0;
     W1 = (U0 - 2U1 + 4U2 + 8U3) * (V0 - 2V1 + 4V2 - 8V3);
     W2 = (U0 + U1 + U2 + U3) * (V0 + V1 + V2 + V3);
     W3 = (U0 - U1 + U2 - U3) * (V0 - V1 + V2 - V3);
     W4 = (U0 + 2U1 + 4U2 + 8U3) * (V0 + 2V1 + 4V2 + 8V3);
     W5 = (8U0 + 4U1 + 2U2 + U3) * (8V0 + 4V1 + 2V2 + V3);
     W6 = U3 * V3;
    */

    /*
         dst :
         |--- l * 2 ---|--- l * 2 ---|--- l * 2 ---|--- rn+rm ---|
         W0             W2            W4            W6
         |- l --|- l --|                           |- l --|
         T0      T1                                 T2
         stk :
         |--- l * 2 ---|--- l * 2 ---|--- l * 2 ---|
         W1             W3            W5
    */

    /*
     12 add/sub + 10 addlsh + 2shift

     T0 = U0 + U2;
     T2 = U1 + U3;
     T1 = T0 - T2; u(-1)
     T0 = T0 + T2; u(1)
     W1 = V0 + V2;
     T2 = V1 + V3;
     W4 = W1 + T2; v(1)
     W1 = W1 - T2; v(-1)
     W2 = T0 * W4; f(1)
     W3 = T1 * W1; f(-1)
     T0 = U0 + 4U2;
     T2 = (U1 + 4U3) << 1;
     T1 = T0 - T2; u(-2)
     T0 = T0 + T2; u(2)
     W5 = V0 + 4V2;
     T2 = (V1 + 4V3) << 1;
     W1 = W5 + T2; v(2)
     W5 = W5 - T2; v(-2)
     W4 = T0 * W1; f(2)
     W1 = T1 * W5; f(-2)
     T0 = ((2U0 + U1) << 1 + U2) << 1 + U3; 8 * u(1/2)
     T1 = ((2V0 + V1) << 1 + V2) << 1 + V3; 8 * v(1/2)
     W5 = T0 * T1; 64 * f(1/2)
     W0 = U0 * V0; f(0)
     W6 = U3 * V3; f(inf)
    */

    WJR_ASSERT_ASSUME(3 * n + 21 <= 4 * m);

    const size_t l = (n + 3) / 4;
    const size_t rn = n - l * 3;
    const size_t rm = m - l * 3;

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);
    WJR_ASSERT_ASSUME(0 < rm && rm <= l);
    WJR_ASSERT_ASSUME(rn + rm >= l);

    const auto u0p = src0;
    const auto u1p = src0 + l;
    const auto u2p = src0 + l * 2;
    const auto u3p = src0 + l * 3;

    const auto v0p = src1;
    const auto v1p = src1 + l;
    const auto v2p = src1 + l * 2;
    const auto v3p = src1 + l * 3;

    const auto w0p = dst;
    const auto w1p = stk;
    const auto w2p = w0p + l * 2;
    const auto w3p = stk + l * 2;
    const auto w4p = w0p + l * 4;
    const auto w5p = stk + l * 4;
    const auto w6p = w0p + l * 6;

    const auto t0p = w0p;
    const auto t1p = w0p + l;
    const auto t2p = w6p;

    stk += l * 6;

    T cf = 0;
    T cf1 = 0, cf2 = 0, cf3 = 0, cf4 = 0, cf5 = 0;
    T cft0 = 0, cft1 = 0, cft2 = 0;
    bool neg0 = 0, neg1 = 0, neg2 = 0;

    //  T0 = U0 + U2;
    cft0 = addc_n(t0p, u0p, u2p, l);

    //  T2 = U1 + U3;
    cft2 = addc_s(t2p, u1p, l, u3p, rn);

    //  T1 = T0 - T2; u(-1)
    {
        ssize_t p = abs_subc_n(t1p, t0p, t2p, l, cft1, cft0, cft2);
        neg0 = p < 0;
        WJR_ASSERT_ASSUME(cft1 <= 1);
    }

    //  T0 = T0 + T2; u(1)
    cft0 += cft2 + addc_n(t0p, t0p, t2p, l);
    WJR_ASSERT_ASSUME(cft0 <= 3);

    //  W1 = V0 + V2;
    cf1 = addc_n(w1p, v0p, v2p, l);

    //  T2 = V1 + V3;
    cft2 = addc_s(t2p, v1p, l, v3p, rm);

    //  W4 = W1 + T2; v(1)
    cf4 = cf1 + cft2 + addc_n(w4p, w1p, t2p, l);
    WJR_ASSERT_ASSUME(cf4 <= 3);

    //  W1 = W1 - T2; v(-1)
    {
        ssize_t p = abs_subc_n(w1p, w1p, t2p, l, cf1, cf1, cft2);
        neg1 = p < 0;
        WJR_ASSERT_ASSUME(cf1 <= 1);
    }

    //  W2 = T0 * W4; f(1)
    __mul_n<__mul_mode::toom44, T, 3, 3>(w2p, t0p, w4p, l, stk, cf2, cft0, cf4);

    //  W3 = T1 * W1; f(-1)
    neg0 ^= neg1;
    __mul_n<__mul_mode::toom44, T, 1, 1>(w3p, t1p, w1p, l, stk, cf3, cft1, cf1);

    //  T0 = U0 + 4U2;
    cft0 = addlsh_n(t0p, u0p, u2p, l, 2);
    WJR_ASSERT_ASSUME(cft0 <= 4);

    //  T2 = (U1 + 4U3) << 1;
    cft2 = addlsh_n(t2p, u1p, u3p, rn, 2);
    if (l != rn) {
        cft2 = addc_1(t2p + rn, u1p + rn, l - rn, cft2);
    }
    cft2 += cft2 + lshift_n(t2p, t2p, l, 1);
    WJR_ASSERT_ASSUME(cft2 <= 9);

    //  T1 = T0 - T2; u(-2)
    {
        ssize_t p = abs_subc_n(t1p, t0p, t2p, l, cft1, cft0, cft2);
        neg1 = p < 0;
        WJR_ASSERT_ASSUME(cft1 <= 9);
    }

    //  T0 = T0 + T2; u(2)
    cft0 += cft2 + addc_n(t0p, t0p, t2p, l);
    WJR_ASSERT_ASSUME(cft0 <= 14);

    //  W5 = V0 + 4V2;
    cf5 = addlsh_n(w5p, v0p, v2p, l, 2);
    WJR_ASSERT_ASSUME(cf5 <= 4);

    //  T2 = (V1 + 4V3) << 1;
    cft2 = addlsh_n(t2p, v1p, v3p, rm, 2);
    if (l != rm) {
        cft2 = addc_1(t2p + rm, v1p + rm, l - rm, cft2);
    }
    cft2 += cft2 + lshift_n(t2p, t2p, l, 1);
    WJR_ASSERT_ASSUME(cft2 <= 9);

    //  W1 = W5 + T2; v(2)
    cf1 = cf5 + cft2 + addc_n(w1p, w5p, t2p, l);
    WJR_ASSERT_ASSUME(cf1 <= 14);

    //  W5 = W5 - T2; v(-2)
    {
        ssize_t p = abs_subc_n(w5p, w5p, t2p, l, cf5, cf5, cft2);
        neg2 = p < 0;
        WJR_ASSERT_ASSUME(cf5 <= 9);
    }

    //  W4 = T0 * W1; f(2)
    __mul_n<__mul_mode::toom44>(w4p, t0p, w1p, l, stk, cf4, cft0, cf1);

    //  W1 = T1 * W5; f(-2)
    neg1 ^= neg2;
    __mul_n<__mul_mode::toom44>(w1p, t1p, w5p, l, stk, cf1, cft1, cf5);

    //  T0 = ((2U0 + U1) << 1 + U2) << 1 + U3; 8 * u(1/2)
    cft0 = addlsh_n(t0p, u1p, u0p, l, 1);
    cft0 += cft0 + addlsh_n(t0p, u2p, t0p, l, 1);
    cf = addlsh_n(t0p, u3p, t0p, rn, 1);
    if (l != rn) {
        cft0 += cft0 + lshift_n(t0p + rn, t0p + rn, l - rn, 1);
        cft0 += addc_1(t0p + rn, t0p + rn, l - rn, cf);
    } else {
        cft0 += cft0 + cf;
    }
    WJR_ASSERT_ASSUME(cft0 <= 14);

    //  T1 = ((2V0 + V1) << 1 + V2) << 1 + V3; 8 * v(1/2)
    cft1 = addlsh_n(t1p, v1p, v0p, l, 1);
    cft1 += cft1 + addlsh_n(t1p, v2p, t1p, l, 1);
    cf = addlsh_n(t1p, v3p, t1p, rm, 1);
    if (l != rm) {
        cft1 += cft1 + lshift_n(t1p + rm, t1p + rm, l - rm, 1);
        cft1 += addc_1(t1p + rm, t1p + rm, l - rm, cf);
    } else {
        cft1 += cft1 + cf;
    }
    WJR_ASSERT_ASSUME(cft1 <= 14);

    //  W5 = T0 * T1; 64 * f(1/2)
    __mul_n<__mul_mode::toom44>(w5p, t0p, t1p, l, stk, cf5, cft0, cft1);

    //  W0 = U0 * V0; f(0)
    __mul_n<__mul_mode::toom44>(w0p, u0p, v0p, l, stk);

    //  W6 = U3 * V3; f(inf)
    if (rn >= rm) {
        __mul_s<__mul_mode::toom44>(w6p, u3p, rn, v3p, rm, stk);
    } else {
        __mul_s<__mul_mode::toom44>(w6p, v3p, rm, u3p, rn, stk);
    }

    toom_interpolation_7p_s(
        dst, w1p, l, rn, rm,
        toom_interpolation_7p_struct<T>{neg1, neg0, cf1, cf2, cf3, cf4, cf5});
}

template <typename T>
void toom4_sqr(T *WJR_RESTRICT dst, const T *src, size_t n, T *stk) {
    /*
     W0 = f(0);
     W1 = f(-2);
     W2 = f(1);
     W3 = f(-1);
     W4 = f(2);
     W5 = 64 * f(1/2);
     W6 = f(inf);

     W0 = U0 ^ 2;
     W1 = (U0 - 2U1 + 4U2 + 8U3) ^ 2;
     W2 = (U0 + U1 + U2 + U3) ^ 2;
     W3 = (U0 - U1 + U2 - U3) ^ 2;
     W4 = (U0 + 2U1 + 4U2 + 8U3) ^ 2;
     W5 = (8U0 + 4U1 + 2U2 + U3) ^ 2;
     W6 = U3 ^ 2;
    */

    /*
         dst :
         |--- l * 2 ---|--- l * 2 ---|--- l * 2 ---|--- rn+rm ---|
         W0             W2            W4            W6
         |- l --|- l --|                           |- l --|
         T0      T1                                 T2
         stk :
         |--- l * 2 ---|--- l * 2 ---|--- l * 2 ---|
         W1             W3            W5
    */

    /*
     12 add/sub + 10 addlsh + 2shift

     T0 = U0 + U2;
     T2 = U1 + U3;
     T1 = T0 - T2; u(-1)
     T0 = T0 + T2; u(1)
     W2 = T0 ^ 2; f(1)
     W3 = T1 ^ 2; f(-1)
     T0 = U0 + 4U2;
     T2 = (U1 + 4U3) << 1;
     T1 = T0 - T2; u(-2)
     T0 = T0 + T2; u(2)
     W4 = T0 ^ 2; f(2)
     W1 = T1 ^ 2; f(-2)
     T0 = ((2U0 + U1) << 1 + U2) << 1 + U3; 8 * u(1/2)
     W5 = T0 ^ 2; 64 * f(1/2)
     W0 = U0 ^ 2; f(0)
     W6 = U3 ^ 2; f(inf)
    */

    WJR_ASSERT_ASSUME(21 <= n);

    const size_t l = (n + 3) / 4;
    const size_t rn = n - l * 3;

    WJR_ASSERT_ASSUME(0 < rn && rn <= l);
    WJR_ASSERT_ASSUME(rn * 2 >= l);

    const auto u0p = src;
    const auto u1p = src + l;
    const auto u2p = src + l * 2;
    const auto u3p = src + l * 3;

    const auto w0p = dst;
    const auto w1p = stk;
    const auto w2p = w0p + l * 2;
    const auto w3p = stk + l * 2;
    const auto w4p = w0p + l * 4;
    const auto w5p = stk + l * 4;
    const auto w6p = w0p + l * 6;

    const auto t0p = w0p;
    const auto t1p = w0p + l;
    const auto t2p = w6p;

    stk += l * 6;

    T cf = 0;
    T cf1 = 0, cf2 = 0, cf3 = 0, cf4 = 0, cf5 = 0;
    T cft0 = 0, cft1 = 0, cft2 = 0;

    //  T0 = U0 + U2;
    cft0 = addc_n(t0p, u0p, u2p, l);

    //  T2 = U1 + U3;
    cft2 = addc_s(t2p, u1p, l, u3p, rn);

    //  T1 = T0 - T2; u(-1)
    {
        (void)abs_subc_n(t1p, t0p, t2p, l, cft1, cft0, cft2);
        WJR_ASSERT_ASSUME(cft1 <= 1);
    }

    //  T0 = T0 + T2; u(1)
    cft0 += cft2 + addc_n(t0p, t0p, t2p, l);
    WJR_ASSERT_ASSUME(cft0 <= 3);

    //  W2 = T0 ^ 2; f(1)
    __sqr<__mul_mode::toom44, T, 3>(w2p, t0p, l, stk, cf2, cft0);

    //  W3 = T1 ^ 2; f(-1)
    __sqr<__mul_mode::toom44, T, 1>(w3p, t1p, l, stk, cf3, cft1);

    //  T0 = U0 + 4U2;
    cft0 = addlsh_n(t0p, u0p, u2p, l, 2);
    WJR_ASSERT_ASSUME(cft0 <= 4);

    //  T2 = (U1 + 4U3) << 1;
    cft2 = addlsh_n(t2p, u1p, u3p, rn, 2);
    if (l != rn) {
        cft2 = addc_1(t2p + rn, u1p + rn, l - rn, cft2);
    }
    cft2 += cft2 + lshift_n(t2p, t2p, l, 1);
    WJR_ASSERT_ASSUME(cft2 <= 9);

    //  T1 = T0 - T2; u(-2)
    {
        (void)abs_subc_n(t1p, t0p, t2p, l, cft1, cft0, cft2);
        WJR_ASSERT_ASSUME(cft1 <= 9);
    }

    //  T0 = T0 + T2; u(2)
    cft0 += cft2 + addc_n(t0p, t0p, t2p, l);
    WJR_ASSERT_ASSUME(cft0 <= 14);

    //  W4 = T0 ^ 2; f(2)
    __sqr<__mul_mode::toom44>(w4p, t0p, l, stk, cf4, cft0);

    //  W1 = T1 * W5; f(-2)
    __sqr<__mul_mode::toom44>(w1p, t1p, l, stk, cf1, cft1);

    //  T0 = ((2U0 + U1) << 1 + U2) << 1 + U3; 8 * u(1/2)
    cft0 = addlsh_n(t0p, u1p, u0p, l, 1);
    cft0 += cft0 + addlsh_n(t0p, u2p, t0p, l, 1);
    cf = addlsh_n(t0p, u3p, t0p, rn, 1);
    if (l != rn) {
        cft0 += cft0 + lshift_n(t0p + rn, t0p + rn, l - rn, 1);
        cft0 += addc_1(t0p + rn, t0p + rn, l - rn, cf);
    } else {
        cft0 += cft0 + cf;
    }
    WJR_ASSERT_ASSUME(cft0 <= 14);

    //  W5 = T0 ^ 2; 64 * f(1/2)
    __sqr<__mul_mode::toom44>(w5p, t0p, l, stk, cf5, cft0);

    //  W0 = U0 * V0; f(0)
    __sqr<__mul_mode::toom44>(w0p, u0p, l, stk);

    //  W6 = U3 * V3; f(inf)
    __sqr<__mul_mode::toom44>(w6p, u3p, rn, stk);

    toom_interpolation_7p_s(
        dst, w1p, l, rn, rn,
        toom_interpolation_7p_struct<T>{0, 0, cf1, cf2, cf3, cf4, cf5});
}

} // namespace wjr

#endif // WJR_MATH_MUL_HPP__

#if defined(WJR_X86)
#ifndef WJR_X86_MATH_DIVIDER_HPP__
#define WJR_X86_MATH_DIVIDER_HPP__

// Already included

namespace wjr {

#if WJR_HAS_FEATURE(GCC_STYLE_INLINE_ASM)
#define WJR_HAS_BUILTIN_ASM_DIV2BY1_ADJUST WJR_HAS_DEF

#if defined(WJR_COMPILER_CLANG) && !WJR_HAS_CLANG(13, 0, 0)
#define WJR_HAS_BUILTIN_ASM_DIV2BY1_ADJUST_BRANCH WJR_HAS_DEF
#endif

#endif

#if WJR_HAS_BUILTIN(ASM_DIV2BY1_ADJUST)

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_INLINE void asm_div2by1_adjust(T rax, T div, T &r8, T &rdx) {
    T r9 = r8 + div;
    asm("cmp{q %[rax], %[r8]| %[r8], %[rax]}\n\t"
        "cmovb{q %[r8], %[r9]| %[r9], %[r8]}\n\t"
        "adc{q $-1, %[rdx]| %[rdx], -1}"
        : [r9] "+r"(r9), [r8] "+r"(r8), [rdx] "+r"(rdx)
        : [rax] "r"(rax)
        : "cc", "memory");
    r8 = r9;
}

#endif

#if WJR_HAS_BUILTIN(ASM_DIV2BY1_ADJUST_BRANCH)

template <typename T, WJR_REQUIRES(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_INLINE void asm_div2by1_adjust_branch(T div, T & lo) {
    asm("sub %1, %0" : "+r"(lo) : "r"(div));
}

#endif

} // namespace wjr

#endif // WJR_X86_MATH_DIVIDER_HPP__
#endif

namespace wjr {

namespace math_details {

inline constexpr std::array<uint16_t, 0x100> div2by1_u64_lookup = {
    0x7fd, 0x7f5, 0x7ed, 0x7e5, 0x7dd, 0x7d5, 0x7ce, 0x7c6, 0x7bf, 0x7b7, 0x7b0, 0x7a8,
    0x7a1, 0x79a, 0x792, 0x78b, 0x784, 0x77d, 0x776, 0x76f, 0x768, 0x761, 0x75b, 0x754,
    0x74d, 0x747, 0x740, 0x739, 0x733, 0x72c, 0x726, 0x720, 0x719, 0x713, 0x70d, 0x707,
    0x700, 0x6fa, 0x6f4, 0x6ee, 0x6e8, 0x6e2, 0x6dc, 0x6d6, 0x6d1, 0x6cb, 0x6c5, 0x6bf,
    0x6ba, 0x6b4, 0x6ae, 0x6a9, 0x6a3, 0x69e, 0x698, 0x693, 0x68d, 0x688, 0x683, 0x67d,
    0x678, 0x673, 0x66e, 0x669, 0x664, 0x65e, 0x659, 0x654, 0x64f, 0x64a, 0x645, 0x640,
    0x63c, 0x637, 0x632, 0x62d, 0x628, 0x624, 0x61f, 0x61a, 0x616, 0x611, 0x60c, 0x608,
    0x603, 0x5ff, 0x5fa, 0x5f6, 0x5f1, 0x5ed, 0x5e9, 0x5e4, 0x5e0, 0x5dc, 0x5d7, 0x5d3,
    0x5cf, 0x5cb, 0x5c6, 0x5c2, 0x5be, 0x5ba, 0x5b6, 0x5b2, 0x5ae, 0x5aa, 0x5a6, 0x5a2,
    0x59e, 0x59a, 0x596, 0x592, 0x58e, 0x58a, 0x586, 0x583, 0x57f, 0x57b, 0x577, 0x574,
    0x570, 0x56c, 0x568, 0x565, 0x561, 0x55e, 0x55a, 0x556, 0x553, 0x54f, 0x54c, 0x548,
    0x545, 0x541, 0x53e, 0x53a, 0x537, 0x534, 0x530, 0x52d, 0x52a, 0x526, 0x523, 0x520,
    0x51c, 0x519, 0x516, 0x513, 0x50f, 0x50c, 0x509, 0x506, 0x503, 0x500, 0x4fc, 0x4f9,
    0x4f6, 0x4f3, 0x4f0, 0x4ed, 0x4ea, 0x4e7, 0x4e4, 0x4e1, 0x4de, 0x4db, 0x4d8, 0x4d5,
    0x4d2, 0x4cf, 0x4cc, 0x4ca, 0x4c7, 0x4c4, 0x4c1, 0x4be, 0x4bb, 0x4b9, 0x4b6, 0x4b3,
    0x4b0, 0x4ad, 0x4ab, 0x4a8, 0x4a5, 0x4a3, 0x4a0, 0x49d, 0x49b, 0x498, 0x495, 0x493,
    0x490, 0x48d, 0x48b, 0x488, 0x486, 0x483, 0x481, 0x47e, 0x47c, 0x479, 0x477, 0x474,
    0x472, 0x46f, 0x46d, 0x46a, 0x468, 0x465, 0x463, 0x461, 0x45e, 0x45c, 0x459, 0x457,
    0x455, 0x452, 0x450, 0x44e, 0x44b, 0x449, 0x447, 0x444, 0x442, 0x440, 0x43e, 0x43b,
    0x439, 0x437, 0x435, 0x432, 0x430, 0x42e, 0x42c, 0x42a, 0x428, 0x425, 0x423, 0x421,
    0x41f, 0x41d, 0x41b, 0x419, 0x417, 0x414, 0x412, 0x410, 0x40e, 0x40c, 0x40a, 0x408,
    0x406, 0x404, 0x402, 0x400};

// invert of (2 * i + 1) mod 256
inline constexpr std::array<uint64_t, 0x80> divexact1_lookup = {
    0x01, 0xAB, 0xCD, 0xB7, 0x39, 0xA3, 0xC5, 0xEF, 0xF1, 0x1B, 0x3D, 0xA7, 0x29,
    0x13, 0x35, 0xDF, 0xE1, 0x8B, 0xAD, 0x97, 0x19, 0x83, 0xA5, 0xCF, 0xD1, 0xFB,
    0x1D, 0x87, 0x09, 0xF3, 0x15, 0xBF, 0xC1, 0x6B, 0x8D, 0x77, 0xF9, 0x63, 0x85,
    0xAF, 0xB1, 0xDB, 0xFD, 0x67, 0xE9, 0xD3, 0xF5, 0x9F, 0xA1, 0x4B, 0x6D, 0x57,
    0xD9, 0x43, 0x65, 0x8F, 0x91, 0xBB, 0xDD, 0x47, 0xC9, 0xB3, 0xD5, 0x7F, 0x81,
    0x2B, 0x4D, 0x37, 0xB9, 0x23, 0x45, 0x6F, 0x71, 0x9B, 0xBD, 0x27, 0xA9, 0x93,
    0xB5, 0x5F, 0x61, 0x0B, 0x2D, 0x17, 0x99, 0x03, 0x25, 0x4F, 0x51, 0x7B, 0x9D,
    0x07, 0x89, 0x73, 0x95, 0x3F, 0x41, 0xEB, 0x0D, 0xF7, 0x79, 0xE3, 0x05, 0x2F,
    0x31, 0x5B, 0x7D, 0xE7, 0x69, 0x53, 0x75, 0x1F, 0x21, 0xCB, 0xED, 0xD7, 0x59,
    0xC3, 0xE5, 0x0F, 0x11, 0x3B, 0x5D, 0xC7, 0x49, 0x33, 0x55, 0xFF};

} // namespace math_details

template <typename T>
class div2by1_divider_noshift {
    static_assert(std::is_same_v<T, uint64_t>, "Currently only support uint64_t");

public:
    div2by1_divider_noshift() = default;
    div2by1_divider_noshift(const div2by1_divider_noshift &) = default;
    div2by1_divider_noshift &operator=(const div2by1_divider_noshift &) = default;
    ~div2by1_divider_noshift() = default;

    explicit div2by1_divider_noshift(T divisor) : m_divisor(divisor) {
        m_value = reciprocal(divisor);
    }

    constexpr div2by1_divider_noshift(T divisor, T value)
        : m_divisor(divisor), m_value(value) {}

    constexpr T get_divisor() const { return m_divisor; }
    constexpr T get_value() const { return m_value; }

    constexpr bool is_zero_or_single_bit() const { return m_divisor == (1ull << 63); }

    WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR20 T divide(T lo, T &hi) const {
        return divide(m_divisor, m_value, lo, hi);
    }

    WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR20 static T divide(T divisor, T value, T lo,
                                                            T &hi) {
        WJR_ASSERT_ASSUME_L2(__has_high_bit(divisor));

        if (WJR_BUILTIN_CONSTANT_P(lo == 0) && lo == 0) {
            return divide_lo0(divisor, value, lo, hi);
        }

        return basic_divide(divisor, value, lo, hi);
    }

    WJR_CONST WJR_CONSTEXPR_E static T reciprocal(T d);

private:
    WJR_INTRINSIC_CONSTEXPR static void fallback_div2by1_adjust(T rax, T div, T &r8,
                                                                T &rdx) {
        const T r9 = r8 + div;
        bool f = r8 < rax;
        r8 = f ? r8 : r9;
        rdx += -1 + f;
    }

    // see fallback_div2by1_adjust
    WJR_INTRINSIC_CONSTEXPR20 static void div2by1_adjust(T rax, T div, T &r8, T &rdx) {
#if WJR_HAS_BUILTIN(ASM_DIV2BY1_ADJUST)
        if (is_constant_evaluated()) {
            return fallback_div2by1_adjust(rax, div, r8, rdx);
        }

        return asm_div2by1_adjust(rax, div, r8, rdx);
#else
        return fallback_div2by1_adjust(rax, div, r8, rdx);
#endif
    }

    WJR_INTRINSIC_CONSTEXPR20 static T basic_divide(T divisor, T value, T lo, T &hi) {
        const T hi1 = hi + 1;

        T rax, rdx;

        rax = mul(hi, value, rdx);
        __add_128(rax, rdx, rax, rdx, lo, hi1);

        lo -= mullo(rdx, divisor);

        div2by1_adjust(rax, divisor, lo, rdx);

        if (WJR_UNLIKELY(lo >= divisor)) {
#if !WJR_HAS_BUILTIN(ASM_DIV2BY1_ADJUST_BRANCH)
            lo -= divisor;
#else
            // low version clang may have some performance issue here
            // so we use asm to avoid use cmov
            asm_div2by1_adjust_branch(divisor, lo);
#endif
            ++rdx;
        }

        hi = lo;
        return rdx;
    }

    WJR_INTRINSIC_CONSTEXPR20 static T divide_lo0(T divisor, T value, T lo, T &hi) {
        const T hi1 = hi + 1;
        T rax, rdx;

        rax = mul(hi, value, rdx);
        rdx += hi1;

        lo -= mullo(rdx, divisor);

        div2by1_adjust(rax, divisor, lo, rdx);

        hi = lo;
        return rdx;
    }

protected:
    T m_divisor;
    T m_value;
};

template <typename T>
WJR_CONST WJR_CONSTEXPR_E T div2by1_divider_noshift<T>::reciprocal(T d) {
    WJR_ASSERT_ASSUME_L1(__has_high_bit(d));

    uint64_t d40 = 0, d63 = 0;
    uint32_t v0 = 0;
    uint64_t v1 = 0, v2 = 0, v3 = 0, v4 = 0;
    uint64_t t0 = 0, t1 = 0;

    // 40 bit
    d40 = (d >> 24) + 1;
    // 63 bit
    d63 = (d + 1) >> 1;
    // 11 bit
    v0 = math_details::div2by1_u64_lookup[((d >> 55) - 0x100)];
    // 22 bit
    v1 = (v0 << 11) - (mullo<uint64_t>(mullo<uint32_t>(v0, v0), d40) >> 40) - 1;

    t1 = mulhi<uint64_t>(v1 << 17, (1ull << 60) - mullo<uint64_t>(v1, d40));
    // 35 bit
    v2 = (v1 << 13) + t1;

    t0 = 0 - mul<uint64_t>(v2, d63, t1);
    if (d & 1) {
        t0 += v2 >> 1;
    }

    v3 = (v2 << 31) + (mulhi<uint64_t>(t0, v2) >> 1);

    v1 = v3 + 1;

    if (WJR_UNLIKELY(v1 == 0)) {
        v4 = ~(d * 2);
    } else {
        v4 = v3 - mulhi<uint64_t>(v1, d) - d;
    }

    return v4;
}

template <typename T>
class div2by1_divider : public div2by1_divider_noshift<T> {
private:
    using Mybase = div2by1_divider_noshift<T>;
    // disable member divide function of Mybase
    using Mybase::divide;
    using Mybase::m_divisor;
    using Mybase::m_value;

public:
    static_assert(std::is_same_v<T, uint64_t>, "Currently only support uint64_t");

    div2by1_divider() = default;
    div2by1_divider(const div2by1_divider &) = default;
    div2by1_divider &operator=(const div2by1_divider &) = default;
    ~div2by1_divider() = default;

    constexpr explicit div2by1_divider(const Mybase &base) : Mybase(base), m_shift(0) {}

    WJR_INTRINSIC_CONSTEXPR_E explicit div2by1_divider(T divisor) {
        m_divisor = divisor;
        initialize();
    }

    constexpr div2by1_divider(T divisor, T value, unsigned int shift)
        : Mybase(divisor, value), m_shift(shift) {}

    constexpr unsigned int get_shift() const { return m_shift; }

    constexpr const Mybase &get_base() const { return *this; }

    // enable static divide function of Mybase
    // This function needs to ensure that lo and hi have been adjusted
    WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR20 static T divide(T divisor, T value, T lo,
                                                            T &hi) {
        return Mybase::divide(divisor, value, lo, hi);
    }

private:
    // make sure m_shift/one_single_bit(divisor) can be inlined
    WJR_INTRINSIC_CONSTEXPR_E void initialize() {
        if (WJR_UNLIKELY(!__has_high_bit(m_divisor))) {
            m_shift = clz(m_divisor);
            m_divisor <<= m_shift;

            WJR_ASSUME(m_shift != 0);
        } else {
            WJR_ASSUME(m_shift == 0);
        }

        WJR_ASSUME(__has_high_bit(m_divisor));

        if (WJR_UNLIKELY(m_divisor == 1ull << 63)) {
            m_value = -1;
            return;
        }

        m_value = Mybase::reciprocal(m_divisor);
    }

    unsigned int m_shift = 0;
};

template <typename T>
class div3by2_divider_noshift {
public:
    static_assert(std::is_same_v<T, uint64_t>, "");

    div3by2_divider_noshift() = default;
    div3by2_divider_noshift(const div3by2_divider_noshift &) = default;
    div3by2_divider_noshift &operator=(const div3by2_divider_noshift &) = default;
    ~div3by2_divider_noshift() = default;

    WJR_INTRINSIC_CONSTEXPR_E div3by2_divider_noshift(T d0, T d1)
        : m_divisor0(d0), m_divisor1(d1) {
        m_value = reciprocal(d0, d1);
    }

    WJR_INTRINSIC_CONSTEXPR div3by2_divider_noshift(T d0, T d1, T value)
        : m_divisor0(d0), m_divisor1(d1), m_value(value) {}

    constexpr T get_divisor0() const { return m_divisor0; }
    constexpr T get_divisor1() const { return m_divisor1; }
    constexpr T get_value() const { return m_value; }

    WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR20 T divide(T u0, T &u1, T &u2) const {
        return divide(m_divisor0, m_divisor1, m_value, u0, u1, u2);
    }

    WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR20 static T divide(T divisor0, T divisor1,
                                                            T value, T u0, T &u1, T &u2);

    WJR_CONST WJR_CONSTEXPR_E static T reciprocal(T d0, T d1);

protected:
    T m_divisor0 = 0;
    T m_divisor1 = 0;
    T m_value = 0;
};

template <typename T>
WJR_INTRINSIC_CONSTEXPR20 T div3by2_divider_noshift<T>::divide(T divisor0, T divisor1,
                                                               T value, T u0, T &u1,
                                                               T &u2) {
    WJR_ASSERT_ASSUME_L2(__has_high_bit(divisor1));

    T q1, q0;
    q0 = mul<T>(value, u2, q1);
    __add_128(q0, q1, q0, q1, u1, u2);

    T r1, r0;
    r1 = u1 - mullo<T>(q1, divisor1);
    T t1;
    r0 = mul<T>(divisor0, q1, t1);

    __sub_128(r0, r1, u0, r1, r0, t1);
    __sub_128(r0, r1, r0, r1, divisor0, divisor1);
    ++q1;

    if (r1 >= q0) {
        --q1;
        __add_128(r0, r1, r0, r1, divisor0, divisor1);
    }

    if (WJR_UNLIKELY(__less_equal_128(divisor0, divisor1, r0, r1))) {
        ++q1;
        __sub_128(r0, r1, r0, r1, divisor0, divisor1);
    }

    u1 = r0;
    u2 = r1;
    return q1;
}

template <typename T>
WJR_CONST WJR_CONSTEXPR_E T div3by2_divider_noshift<T>::reciprocal(T d0, T d1) {
    WJR_ASSERT_ASSUME_L1(__has_high_bit(d1));

    T v = div2by1_divider<T>::reciprocal(d1);
    T p = mullo<T>(d1, v);
    p += d0;
    if (p < d0) {
        --v;
        if (p >= d1) {
            --v;
            p -= d1;
        }
        p -= d1;
    }

    T t0 = 0, t1 = 0;
    t0 = mul<T>(d0, v, t1);
    p += t1;
    if (p < t1) {
        --v;
        if (__less_equal_128(d0, d1, t0, p)) {
            --v;
        }
    }

    return v;
}

template <typename T>
class div3by2_divider : public div3by2_divider_noshift<T> {
    using Mybase = div3by2_divider_noshift<T>;
    using Mybase::divide;
    using Mybase::m_divisor0;
    using Mybase::m_divisor1;
    using Mybase::m_value;

public:
    static_assert(std::is_same_v<T, uint64_t>, "");

    div3by2_divider() = default;
    div3by2_divider(const div3by2_divider &) = default;
    div3by2_divider &operator=(const div3by2_divider &) = default;
    ~div3by2_divider() = default;

    constexpr explicit div3by2_divider(const Mybase &base) : Mybase(base), m_shift(0) {}

    WJR_INTRINSIC_CONSTEXPR_E div3by2_divider(T d0, T d1) {
        m_divisor0 = d0;
        m_divisor1 = d1;
        initialize();
    }

    WJR_INTRINSIC_CONSTEXPR div3by2_divider(T d0, T d1, T value, unsigned int shift)
        : Mybase(d0, d1, value), m_shift(shift) {}

    constexpr unsigned int get_shift() const { return m_shift; }

    constexpr const Mybase &get_base() const { return *this; }

    WJR_NODISCARD WJR_INTRINSIC_CONSTEXPR20 static T divide(T divisor0, T divisor1,
                                                            T value, T u0, T &u1, T &u2) {
        return Mybase::divide(divisor0, divisor1, value, u0, u1, u2);
    }

private:
    WJR_INTRINSIC_CONSTEXPR_E void initialize() {
        if (WJR_UNLIKELY(!__has_high_bit(m_divisor1))) {
            m_shift = clz(m_divisor1);
            m_divisor1 = shld(m_divisor1, m_divisor0, m_shift);
            m_divisor0 <<= m_shift;

            WJR_ASSUME(m_shift != 0);
        } else {
            WJR_ASSUME(m_shift == 0);
        }

        WJR_ASSUME(__has_high_bit(m_divisor1));

        m_value = Mybase::reciprocal(m_divisor0, m_divisor1);
    }

    unsigned int m_shift = 0;
};

// divexact1_divider
// m_value is invert of m_divisor mod 2 ^ N
// m_divisor is odd
template <typename T>
class divexact1_divider {
public:
    static_assert(std::is_same_v<T, uint64_t>, "Currently only support uint64_t");

    divexact1_divider() = default;
    divexact1_divider(const divexact1_divider &) = default;
    divexact1_divider &operator=(const divexact1_divider &) = default;
    ~divexact1_divider() = default;

    WJR_INTRINSIC_CONSTEXPR_E explicit divexact1_divider(T divisor) : m_divisor(divisor) {
        initialize();
    }

    constexpr divexact1_divider(T divisor, T value, unsigned int shift)
        : m_divisor(divisor), m_value(value), m_shift(shift) {}

    constexpr T get_divisor() const { return m_divisor; }
    constexpr T get_value() const { return m_value; }
    constexpr unsigned int get_shift() const { return m_shift; }

    constexpr bool is_zero_or_single_bit() const { return m_divisor == 1; }

    WJR_CONST constexpr static T reciprocal(T divisor) {
        T inv = math_details::divexact1_lookup[(divisor & 0xFF) >> 1];
        inv = inv * (2 - inv * divisor);
        inv = inv * (2 - inv * divisor);
        inv = inv * (2 - inv * divisor);
        return inv;
    }

private:
    WJR_INTRINSIC_CONSTEXPR_E void initialize() {
        if (WJR_UNLIKELY(!(m_divisor & 1))) {
            m_shift = ctz(m_divisor);
            m_divisor >>= m_shift;

            WJR_ASSUME(m_shift != 0);
        } else {
            WJR_ASSUME(m_shift == 0);
        }

        WJR_ASSUME((m_divisor & 1) != 0);

        m_value = reciprocal(m_divisor);
    }

    T m_divisor = 0;
    T m_value = 0;
    unsigned int m_shift = 0;
};

} // namespace wjr

#endif // WJR_MATH_DIVIDER_HPP__

#if defined(WJR_X86)
#ifndef WJR_X86_MATH_DIV_HPP__
#define WJR_X86_MATH_DIV_HPP__

// Already included

#ifndef WJR_X86
#error "x86 required"
#endif

namespace wjr {

#if WJR_HAS_FEATURE(GCC_STYLE_INLINE_ASM)
#define WJR_HAS_BUILTIN_ASM_DIVEXACT_DBM1C WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(ASM_DIVEXACT_DBM1C)

// TODO : optimize pipeline
inline uint64_t asm_divexact_dbm1c(uint64_t *dst, const uint64_t *src, size_t n,
                                   uint64_t bd, uint64_t h) {
    uint64_t r8 = h, r9 = n, r10, r11 = static_cast<uint32_t>(n);

    src += r9;
    dst += r9;
    r9 = -r9;

    asm volatile(
        "and{l $3, %k[r11]| %k[r11], 3}\n\t"
        "je .Lb0%=\n\t"
        "lea{q -4(%[r9], %[r11], 1), %[r9]| %[r9], [%[r9] + %[r11] * 1 - 4]}\n\t"
        "cmp{l $2, %k[r11]| %k[r11], 2}\n\t"
        "jb .Lb1%=\n\t"
        "je .Lb2%=\n\t"
        "jmp .Lb3%=\n\t"

        ".Lloop%=:\n\t"

        ".Lb0%=:\n\t"
        "mulx{q (%[src], %[r9], 8), %[r10], %[r11]| %[r11], %[r10], [%[src] + %[r9] * 8]}\n\t"
        "sub{q %[r10], %[r8]| %[r8], %[r10]}\n\t"
        "mov{q %[r8], (%[dst], %[r9], 8)| [%[dst] + %[r9] * 8], %[r8]}\n\t"
        "sbb{q %[r11], %[r8]| %[r8], %[r11]}\n\t"

        ".Lb3%=:\n\t"
        "mulx{q 8(%[src], %[r9], 8), %[r10], %[r11]| %[r11], %[r10], [%[src] + %[r9] * 8 + 8]}\n\t"
        "sub{q %[r10], %[r8]| %[r8], %[r10]}\n\t"
        "mov{q %[r8], 8(%[dst], %[r9], 8)| [%[dst] + %[r9] * 8 + 8], %[r8]}\n\t"
        "sbb{q %[r11], %[r8]| %[r8], %[r11]}\n\t"

        ".Lb2%=:\n\t"
        "mulx{q 16(%[src], %[r9], 8), %[r10], %[r11]| %[r11], %[r10], [%[src] + %[r9] * 8 + 16]}\n\t"
        "sub{q %[r10], %[r8]| %[r8], %[r10]}\n\t"
        "mov{q %[r8], 16(%[dst], %[r9], 8)| [%[dst] + %[r9] * 8 + 16], %[r8]}\n\t"
        "sbb{q %[r11], %[r8]| %[r8], %[r11]}\n\t"

        ".Lb1%=:\n\t"
        "mulx{q 24(%[src], %[r9], 8), %[r10], %[r11]| %[r11], %[r10], [%[src] + %[r9] * 8 + 24]}\n\t"
        "sub{q %[r10], %[r8]| %[r8], %[r10]}\n\t"
        "mov{q %[r8], 24(%[dst], %[r9], 8)| [%[dst] + %[r9] * 8 + 24], %[r8]}\n\t"
        "sbb{q %[r11], %[r8]| %[r8], %[r11]}\n\t"

        "add $4, %[r9]\n\t"
        "jne .Lloop%=\n\t"

        : [dst] "+&r"(dst), [src] "+&r"(src), [r8] "+&r"(r8), [r9] "+&r"(r9),
          [r10] "=&r"(r10), [r11] "+&r"(r11)
        : "d"(bd)
        : "cc", "memory");

    return r8;
}

#endif

} // namespace wjr

#endif // WJR_X86_MATH_DIV_HPP__
#endif

namespace wjr {

/*
 TODO :
 1. __div_constant_128
 2. __mod_constant_128
 3. __div_qr_constant_128
 4. optimize constant divisor of div_qr_1
 1, 2, 3, 4: constant numbers that can be divisible by (uint64_t)(-1),
*/

inline uint64_t
div128by64to64_noshift(uint64_t &rem, uint64_t lo, uint64_t hi,
                       const wjr::div2by1_divider_noshift<uint64_t> &divider) {
    const uint64_t result = divider.divide(lo, hi);
    rem = hi;
    return result;
}

inline uint64_t div128by64to64_shift(uint64_t &rem, uint64_t lo, uint64_t hi,
                                     const wjr::div2by1_divider<uint64_t> &divider) {
    const auto shift = divider.get_shift();
    hi = shld(hi, lo, shift);
    lo <<= shift;
    const uint64_t result = divider.get_base().divide(lo, hi);
    rem = hi >> shift;
    return result;
}

inline uint64_t div128by64to64_impl(uint64_t &rem, uint64_t lo, uint64_t hi,
                                    const wjr::div2by1_divider<uint64_t> &divider) {
    if (divider.get_shift() == 0) {
        return div128by64to64_noshift(rem, lo, hi, divider);
    }

    return div128by64to64_shift(rem, lo, hi, divider);
}

/*
 not optimize for divider that is power of 2,
 manually consider whether it needs to be optimized
*/
inline uint64_t div128by64to64(uint64_t &rem, uint64_t lo, uint64_t hi,
                               const div2by1_divider<uint64_t> &divider) {
    return div128by64to64_impl(rem, lo, hi, divider);
}

/*
 not optimize for divider that is power of 2,
 manually consider whether it needs to be optimized
*/
inline uint64_t div128by64to64(uint64_t &rem, uint64_t lo, uint64_t hi, uint64_t div) {
    return div128by64to64_impl(rem, lo, hi, wjr::div2by1_divider<uint64_t>(div));
}

inline tuple<uint64_t, uint64_t>
div128by64to128_noshift(uint64_t &rem, uint64_t lo, uint64_t hi,
                        const div2by1_divider_noshift<uint64_t> &divider) {
    const auto divisor = divider.get_divisor();
    uint64_t q0, q1 = 0;

    if (hi >= divisor) {
        q1 = 1;
        hi -= divisor;
    }

    q0 = divider.divide(lo, hi);
    rem = hi;
    return std::make_pair(q0, q1);
}

inline tuple<uint64_t, uint64_t>
div128by64to128_shift(uint64_t &rem, uint64_t lo, uint64_t hi,
                      const div2by1_divider<uint64_t> &divider) {
    const auto shift = divider.get_shift();
    uint64_t u0, u1, u2;
    uint64_t q0, q1;

    u2 = hi >> (64 - shift);
    u1 = shld(hi, lo, shift);
    u0 = lo << shift;

    const auto &div = divider.get_base();
    q1 = div.divide(u1, u2);
    q0 = div.divide(u0, u2);

    rem = u2 >> shift;
    return std::make_pair(q0, q1);
}

inline tuple<uint64_t, uint64_t>
div128by64to128_impl(uint64_t &rem, uint64_t lo, uint64_t hi,
                     const div2by1_divider<uint64_t> &divider) {
    if (divider.get_shift() == 0) {
        return div128by64to128_noshift(rem, lo, hi, divider);
    }

    return div128by64to128_shift(rem, lo, hi, divider);
}

/*
 not optimize for divider that is power of 2,
 manually consider whether it needs to be optimized
*/
inline tuple<uint64_t, uint64_t>
div128by64to128(uint64_t &rem, uint64_t lo, uint64_t hi,
                const div2by1_divider<uint64_t> &divider) {
    return div128by64to128_impl(rem, lo, hi, divider);
}

/*
 not optimize for divider that is power of 2,
 manually consider whether it needs to be optimized
*/
inline tuple<uint64_t, uint64_t> div128by64to128(uint64_t &rem, uint64_t lo, uint64_t hi,
                                                 uint64_t div) {
    return div128by64to128_impl(rem, lo, hi, div2by1_divider<uint64_t>(div));
}

// reference : https://ieeexplore.ieee.org/document/5487506
template <typename T>
WJR_CONSTEXPR20 T div_qr_1_noshift(T *dst, T &rem, const T *src, size_t n,
                                   const div2by1_divider_noshift<T> &div) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_DECR_P(dst, n - 1, src, n - 1));

    const T divisor = div.get_divisor();
    const T value = div.get_value();

    T qh = 0;
    T lo, hi;

    hi = src[n - 1];

    if (hi >= divisor) {
        hi -= divisor;
        qh = 1;
    }

    do {
        if (WJR_UNLIKELY(n == 1)) {
            break;
        }

        --n;

        do {
            lo = src[n - 1];
            dst[n - 1] = div.divide(divisor, value, lo, hi);
            --n;
        } while (WJR_LIKELY(n != 0));

    } while (0);

    rem = hi;
    return qh;
}

template <typename T>
WJR_CONSTEXPR20 T div_qr_1_shift(T *dst, T &rem, const T *src, size_t n,
                                 const div2by1_divider<T> &div) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT(div.get_shift() != 0);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_DECR_P(dst, n - 1, src, n - 1));

    const T divisor = div.get_divisor();
    const T value = div.get_value();
    const auto shift = div.get_shift();

    T qh;
    T lo, hi;

    T rbp = src[n - 1];
    --n;
    hi = rbp >> (64 - shift);

    do {
        if (WJR_UNLIKELY(n == 0)) {
            qh = div.divide(divisor, value, rbp << shift, hi);
            break;
        }

        lo = src[n - 1];
        qh = div.divide(divisor, value, shld(rbp, lo, shift), hi);
        rbp = lo;
        --n;

        if (WJR_LIKELY(n != 0)) {
            do {
                lo = src[n - 1];
                dst[n] = div.divide(divisor, value, shld(rbp, lo, shift), hi);
                rbp = lo;
                --n;
            } while (WJR_LIKELY(n != 0));
        }

        dst[0] = div.divide(divisor, value, rbp << shift, hi);
    } while (0);

    rem = hi >> shift;
    return qh;
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR20 T div_qr_1_impl(T *dst, T &rem, const T *src, size_t n,
                                          const div2by1_divider<T> &div) {
    if (div.get_shift() == 0) {
        return div_qr_1_noshift(dst, rem, src, n, div);
    }

    return div_qr_1_shift(dst, rem, src, n, div);
}

// return high quotient limb
template <typename T, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_CONSTEXPR20 void div_qr_1(T *dst, T &rem, const T *src, size_t n,
                                        const div2by1_divider<T> &div) {
    WJR_ASSERT_ASSUME(n >= 1);

    if (WJR_UNLIKELY(div.is_zero_or_single_bit())) {
        const unsigned int c = 63 - div.get_shift();
        rem = src[0] & ((1ull << c) - 1);
        (void)rshift_n(dst, src, n, c);
        return;
    }

    if (WJR_BUILTIN_CONSTANT_P(n == 2) && n == 2) {
        const auto [ax, dx] = div128by64to128(rem, src[0], src[1], div);
        dst[0] = ax;
        dst[1] = dx;
        return;
    }

    dst[n - 1] = div_qr_1_impl(dst, rem, src, n, div);
}

template <typename T, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_CONSTEXPR20 void div_qr_1(T *dst, T &rem, const T *src, size_t n,
                                        type_identity_t<T> div) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_ASSUME(div != 0);

    if (WJR_UNLIKELY(is_zero_or_single_bit(div))) {
        const unsigned int c = ctz(div);
        rem = src[0] & ((1ull << c) - 1);
        (void)rshift_n(dst, src, n, c);
        return;
    }

    if (WJR_UNLIKELY(n == 1)) {
        const T tmp = src[0];

        if (__has_high_bit(div)) {
            if (tmp >= div) {
                rem = tmp - div;
                dst[0] = 1;
                return;
            }
            rem = tmp;
            dst[0] = 0;
            return;
        }

        dst[0] = tmp / div;
        rem = tmp % div;
        return;
    }

    if (WJR_BUILTIN_CONSTANT_P(n == 2) && n == 2) {
        const auto [ax, dx] = div128by64to128(rem, src[0], src[1], div);
        dst[0] = ax;
        dst[1] = dx;
        return;
    }

    dst[n - 1] = div_qr_1_impl(dst, rem, src, n, div2by1_divider<T>(div));
}

template <typename T>
WJR_CONSTEXPR20 T div_qr_2_noshift(T *dst, T *rem, const T *src, size_t n,
                                   const div3by2_divider_noshift<T> &div) {
    WJR_ASSERT_ASSUME(n >= 2);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_DECR_P(dst, n - 2, src, n - 2));
    WJR_ASSERT_L1(WJR_IS_SEPARATE_P(dst, n - 2, rem, n - 2));

    const T divisor0 = div.get_divisor0();
    const T divisor1 = div.get_divisor1();
    const T value = div.get_value();

    T qh = 0;
    T u0, u1, u2;

    u2 = src[n - 1];
    u1 = src[n - 2];

    if (__less_equal_128(divisor0, divisor1, u1, u2)) {
        __sub_128(u1, u2, u1, u2, divisor0, divisor1);
        qh = 1;
    }

    do {
        if (WJR_UNLIKELY(n == 2)) {
            break;
        }

        n -= 2;

        do {
            u0 = src[n - 1];
            dst[n - 1] = div.divide(divisor0, divisor1, value, u0, u1, u2);
            --n;
        } while (WJR_LIKELY(n != 0));

    } while (0);

    rem[0] = u1;
    rem[1] = u2;
    return qh;
}

template <typename T>
WJR_CONSTEXPR20 T div_qr_2_shift(T *dst, T *rem, const T *src, size_t n,
                                 const div3by2_divider<T> &div) {
    WJR_ASSERT_ASSUME(n >= 2);
    WJR_ASSERT(div.get_shift() != 0);
    WJR_ASSERT_L1(WJR_IS_SAME_OR_DECR_P(dst, n - 2, src, n - 2));
    WJR_ASSERT_L1(WJR_IS_SEPARATE_P(dst, n - 2, rem, n - 2));

    const T divisor0 = div.get_divisor0();
    const T divisor1 = div.get_divisor1();
    const T value = div.get_value();
    const auto shift = div.get_shift();

    T qh;
    T u0, u1, u2;
    T rbp;

    rbp = src[n - 2];
    u2 = src[n - 1];
    u1 = shld(u2, rbp, shift);
    u2 >>= (64 - shift);

    n -= 2;

    do {
        if (WJR_UNLIKELY(n == 0)) {
            qh = div.divide(divisor0, divisor1, value, rbp << shift, u1, u2);
            break;
        }

        u0 = src[n - 1];
        qh = div.divide(divisor0, divisor1, value, shld(rbp, u0, shift), u1, u2);
        rbp = u0;
        --n;

        if (WJR_LIKELY(n != 0)) {
            do {
                u0 = src[n - 1];
                dst[n] =
                    div.divide(divisor0, divisor1, value, shld(rbp, u0, shift), u1, u2);
                rbp = u0;
                --n;
            } while (WJR_LIKELY(n != 0));
        }

        dst[0] = div.divide(divisor0, divisor1, value, rbp << shift, u1, u2);
    } while (0);

    rem[0] = shrd(u1, u2, shift);
    rem[1] = u2 >> shift;
    return qh;
}

template <typename T>
WJR_INTRINSIC_CONSTEXPR20 T div_qr_2_impl(T *dst, T *rem, const T *src, size_t n,
                                          const div3by2_divider<T> &div) {
    if (div.get_shift() == 0) {
        return div_qr_2_noshift(dst, rem, src, n, div);
    }

    return div_qr_2_shift(dst, rem, src, n, div);
}

template <typename T, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_CONSTEXPR20 void div_qr_2(T *dst, T *rem, const T *src, size_t n,
                                        const div3by2_divider<T> &div) {
    WJR_ASSERT_ASSUME(n >= 2);

    dst[n - 2] = div_qr_2_impl(dst, rem, src, n, div);
}

template <typename T, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_CONSTEXPR20 void div_qr_2(T *dst, T *rem, const T *src, size_t n,
                                        const T *div) {
    WJR_ASSERT_ASSUME(n >= 2);

    dst[n - 2] = div_qr_2_impl(dst, rem, src, n, div3by2_divider<T>(div[0], div[1]));
}

// reference : GMP
// return qh;
template <typename T>
T sb_div_qr_s(T *dst, T *src, size_t n, const T *div, size_t m, T dinv) {
    using divider = div3by2_divider<T>;
    constexpr T mask = in_place_max;

    WJR_ASSERT_ASSUME(m > 2);
    WJR_ASSERT_ASSUME(n >= m);
    WJR_ASSERT(__has_high_bit(div[m - 1]));

    T n1, n0;
    T d1, d0;
    T cy;
    T q;

    src += n;

    const T qh = reverse_compare_n(src - m, div, m) >= 0;
    if (qh != 0) {
        (void)subc_n(src - m, src - m, div, m);
    }

    dst += n - m;

    m -= 2;
    d1 = div[m + 1];
    d0 = div[m + 0];

    src -= 2;

    n1 = src[1];

    for (size_t i = n - (m + 2); i > 0; i--) {
        src--;
        if (WJR_UNLIKELY(n1 == d1) && src[1] == d0) {
            q = mask;
            submul_1(src - m, div, m + 2, q);
            n1 = src[1];
        } else {
            n0 = src[1];
            q = divider::divide(d0, d1, dinv, src[0], n0, n1);

            cy = submul_1(src - m, div, m, q);
            cy = __subc_cc_128(n0, n1, n0, n1, cy, 0, 0);
            src[0] = n0;

            if (WJR_UNLIKELY(cy != 0)) {
                n1 += d1 + addc_n(src - m, src - m, div, m + 1);
                q--;
            }
        }

        *--dst = q;
    }

    src[1] = n1;

    return qh;
}

extern template uint64_t sb_div_qr_s<uint64_t>(uint64_t *dst, uint64_t *src, size_t n,
                                               const uint64_t *div, size_t m,
                                               uint64_t dinv);

inline constexpr size_t dc_div_qr_threshold = WJR_DC_DIV_QR_THRESHOLD;

template <typename T>
T dc_div4by2_qr(T *dst, T *src, const T *div, size_t m, T dinv, T *stk) {
    size_t lo, hi;
    T cy, qh, ql;

    lo = m >> 1; /* floor(n/2) */
    hi = m - lo; /* ceil(n/2) */

    if (hi < dc_div_qr_threshold) {
        qh = sb_div_qr_s(dst + lo, src + 2 * lo, 2 * hi, div + lo, hi, dinv);
    } else {
        qh = dc_div4by2_qr(dst + lo, src + 2 * lo, div + lo, hi, dinv, stk);
    }

    mul_s(stk, dst + lo, hi, div, lo);

    cy = subc_n(src + lo, src + lo, stk, m);
    if (qh != 0) {
        cy += subc_n(src + m, src + m, div, lo);
    }

    while (cy != 0) {
        qh -= subc_1(dst + lo, dst + lo, hi, 1);
        cy -= addc_n(src + lo, src + lo, div, m);
    }

    if (lo < dc_div_qr_threshold) {
        ql = sb_div_qr_s(dst, src + hi, 2 * lo, div + hi, lo, dinv);
    } else {
        ql = dc_div4by2_qr(dst, src + hi, div + hi, lo, dinv, stk);
    }

    mul_s(stk, div, hi, dst, lo);

    cy = subc_n(src, src, stk, m);
    if (ql != 0) {
        cy += subc_n(src + lo, src + lo, div, hi);
    }

    while (cy != 0) {
        (void)subc_1(dst, dst, lo, 1);
        cy -= addc_n(src, src, div, m);
    }

    return qh;
}

template <typename T>
T dc_div_qr_s(T *dst, T *src, size_t n, const T *div, size_t m, T dinv) {
    WJR_ASSERT(m >= 6);
    WJR_ASSERT(n - m >= 3);
    WJR_ASSERT(__has_high_bit(div[m - 1]));

    size_t qn;
    T qh, cy;

    unique_stack_allocator stkal(math_details::stack_alloc);
    const auto tp = static_cast<T *>(stkal.allocate(sizeof(T) * m));

    qn = n - m;
    dst += qn;
    src += n;
    div += m;

    if (qn > m) {
        /* Reduce qn mod m without division, optimizing small operations.  */
        if (qn >= 8 * m) {
            qn %= m;
            if (qn == 0) {
                qn = m;
            }
        } else {
            do {
                qn -= m;
            } while (qn > m);
        }

        dst -= qn; /* point at low limb of next quotient block */
        src -= qn; /* point in the middle of partial remainder */

        /* Perform the typically smaller block first.  */
        if (qn == 1) {
            T q, n1, n0, d1, d0;

            /* Handle qh up front, for simplicity. */
            qh = reverse_compare_n(src - m + 1, div - m, m) >= 0;
            if (qh) {
                (void)subc_n(src - m + 1, src - m + 1, div - m, m);
            }

            /* A single iteration of schoolbook: One 3/2 division,
               followed by the bignum update and adjustment. */
            n1 = src[0];
            n0 = src[-1];
            d1 = div[-1];
            d0 = div[-2];

            WJR_ASSERT(n1 < d1 || (n1 == d1 && n0 <= d0));

            if (WJR_UNLIKELY(n1 == d1) && n0 == d0) {
                q = in_place_max;
                cy = submul_1(src - m, div - m, m, q);
            } else {
                q = wjr::div3by2_divider<T>::divide(d0, d1, dinv, src[-2], n0, n1);

                if (m > 2) {
                    T cy;
                    cy = submul_1(src - m, div - m, m - 2, q);
                    cy = __subc_cc_128(n0, n1, n0, n1, cy, 0, 0);
                    src[-2] = n0;

                    if (WJR_UNLIKELY(cy != 0)) {
                        n1 += d1 + addc_n(src - m, src - m, div - m, m - 1);
                        qh -= (q == 0);
                        --q;
                    }
                } else {
                    src[-2] = n0;
                }

                src[-1] = n1;
            }
            dst[0] = q;
        } else {
            /* Do a 2qn / qn division */
            if (qn == 2) {
                qh = div_qr_2_noshift(dst, src - 2, src - 2, 4,
                                      div3by2_divider_noshift<T>(div[-2], div[-1], dinv));
            } else if (qn < dc_div_qr_threshold) {
                qh = sb_div_qr_s(dst, src - qn, 2 * qn, div - qn, qn, dinv);
            } else {
                qh = dc_div4by2_qr(dst, src - qn, div - qn, qn, dinv, tp);
            }

            if (qn != m) {
                if (qn > m - qn) {
                    mul_s(tp, dst, qn, div - m, m - qn);
                } else {
                    mul_s(tp, div - m, m - qn, dst, qn);
                }

                cy = subc_n(src - m, src - m, tp, m);
                if (qh != 0) {
                    cy += subc_n(src - m + qn, src - m + qn, div - m, m - qn);
                }

                while (cy != 0) {
                    qh -= subc_1(dst, dst, qn, 1);
                    cy -= addc_n(src - m, src - m, div - m, m);
                }
            }
        }

        qn = n - m - qn;
        do {
            dst -= m;
            src -= m;
            dc_div4by2_qr(dst, src - m, div - m, m, dinv, tp);
            qn -= m;
        } while (qn > 0);
    } else {
        dst -= qn; /* point at low limb of next quotient block */
        src -= qn; /* point in the middle of partial remainder */

        if (qn < dc_div_qr_threshold) {
            qh = sb_div_qr_s(dst, src - qn, 2 * qn, div - qn, qn, dinv);
        } else {
            qh = dc_div4by2_qr(dst, src - qn, div - qn, qn, dinv, tp);
        }

        if (qn != m) {
            if (qn > m - qn) {
                mul_s(tp, dst, qn, div - m, m - qn);
            } else {
                mul_s(tp, div - m, m - qn, dst, qn);
            }

            cy = subc_n(src - m, src - m, tp, m);
            if (qh != 0) {
                cy += subc_n(src - m + qn, src - m + qn, div - m, m - qn);
            }

            while (cy != 0) {
                qh -= subc_1(dst, dst, qn, 1);
                cy -= addc_n(src - m, src - m, div - m, m);
            }
        }
    }

    return qh;
}

extern template uint64_t dc_div_qr_s<uint64_t>(uint64_t *dst, uint64_t *src, size_t n,
                                               const uint64_t *div, size_t m,
                                               uint64_t dinv);

template <typename T>
void div_qr_s(T *dst, T *rem, const T *src, size_t n, const T *div, size_t m) {
    WJR_ASSERT_ASSUME(m >= 1);
    WJR_ASSERT_ASSUME(n >= m);

    switch (m) {
    case 0: {
        WJR_UNREACHABLE();
        break;
    }
    case 1: {
        return div_qr_1(dst, rem[0], src, n, div[0]);
    }
    case 2: {
        return div_qr_2(dst, rem, src, n, div);
    }
    default: {
        break;
    }
    }

    unsigned int adjust = src[n - 1] >= div[m - 1];
    if (n + adjust >= 2 * m) {
        T *sp;
        T *dp;

        dst[n - m] = 0;

        const auto shift = clz(div[m - 1]);
        const size_t alloc = n + 1 + (shift != 0 ? m : 0);
        unique_stack_allocator stkal(math_details::stack_alloc);
        auto stk = static_cast<T *>(stkal.allocate(sizeof(T) * alloc));
        sp = stk;

        if (shift != 0) {
            dp = stk + (n + 1);
            (void)lshift_n(dp, div, m, shift);
            sp[n] = lshift_n(sp, src, n, shift);
        } else {
            dp = const_cast<T *>(div);
            std::copy_n(src, n, sp);
            sp[n] = 0;
        }

        n += adjust;

        const auto dinv = div3by2_divider<T>::reciprocal(dp[m - 2], dp[m - 1]);

        if (m < dc_div_qr_threshold) {
            sb_div_qr_s(dst, sp, n, dp, m, dinv);
        } else {
            dc_div_qr_s(dst, sp, n, dp, m, dinv);
        }

        rshift_n(rem, sp, m, shift);
        return;
    }

    // 2 * m > n + adjust

    size_t qn = n - m;
    dst[qn] = 0;
    qn += adjust;

    if (qn == 0) {
        std::copy_n(src, m, rem);
        return;
    }

    T *sp, *dp;
    size_t st;

    st = m - qn; // st = m - qn = 2 * m - n + adjust > 2 * adjust

    const auto shift = clz(div[m - 1]);

    const size_t alloc = 2 * qn + (shift != 0 ? qn : 0);
    unique_stack_allocator stkal(math_details::stack_alloc);
    auto stk = static_cast<T *>(stkal.allocate(sizeof(T) * alloc));
    sp = stk;

    if (shift != 0) {
        dp = stk + 2 * qn;
        (void)lshift_n(dp, div + st, qn, shift, div[st - 1]);
        if (adjust) {
            sp[2 * qn - 1] =
                lshift_n(sp, src + n - 2 * qn + 1, 2 * qn - 1, shift, src[n - 2 * qn]);
        } else {
            (void)lshift_n(sp, src + n - 2 * qn, 2 * qn, shift, src[n - 2 * qn - 1]);
        }
    } else {
        dp = const_cast<T *>(div + st);
        if (adjust) {
            std::copy_n(src + n - 2 * qn + 1, 2 * qn - 1, sp);
            sp[2 * qn - 1] = 0;
        } else {
            std::copy_n(src + n - 2 * qn, 2 * qn, sp);
        }
    }

    if (qn == 1) {
        const auto dinv = div2by1_divider<T>::reciprocal(dp[0]);
        auto hi = sp[1];
        dst[0] = div2by1_divider<T>::divide(dp[0], dinv, sp[0], hi);
        sp[0] = hi;
    } else if (qn == 2) {
        const auto lo = dp[0];
        const auto hi = dp[1];
        const auto dinv = div3by2_divider<T>::reciprocal(lo, hi);
        // maybe inlined ?
        div_qr_2_noshift(dst, sp, sp, 4, div3by2_divider_noshift<T>(lo, hi, dinv));
    } else {
        const auto lo = dp[qn - 2];
        const auto hi = dp[qn - 1];
        const auto dinv = div3by2_divider<T>::reciprocal(lo, hi);
        if (qn < dc_div_qr_threshold) {
            sb_div_qr_s(dst, sp, 2 * qn, dp, qn, dinv);
        } else {
            dc_div_qr_s(dst, sp, 2 * qn, dp, qn, dinv);
        }
    }

    WJR_ASSUME(st >= 1);

    auto stk2 = static_cast<T *>(stkal.allocate(sizeof(T) * m));
    auto rp = stk2;

    unsigned int cf;

    if (!shift) {
        if (qn >= st) {
            mul_s(rp, dst, qn, div, st);
        } else {
            mul_s(rp, div, st, dst, qn);
        }
        cf = subc_n(rem, src, rp, st);
    } else {
        constexpr auto digits = std::numeric_limits<T>::digits;
        const T fix = rshift_n(sp, sp, qn, shift);
        const T mask = (1ull << (digits - shift)) - 1;

        if (st != 1) {
            if (qn >= st - 1) {
                mul_s(rp, dst, qn, div, st - 1);
            } else {
                mul_s(rp, div, st - 1, dst, qn);
            }
            rp[m - 1] = addmul_1(rp + st - 1, dst, qn, div[st - 1] & mask);
            cf = subc_n(rem, src, rp, st - 1);
            rem[st - 1] = subc((src[st - 1] & mask) | fix, rp[st - 1], cf, cf);
        } else {
            rp[m - 1] = mul_1(rp, dst, qn, div[0] & mask);
            rem[0] = subc((src[0] & mask) | fix, rp[0], 0, cf);
        }
    }

    cf = subc_n(rem + st, sp, rp + st, qn, cf);

    while (cf != 0) {
        (void)subc_1(dst, dst, qn, 1);
        cf -= addc_n(rem, rem, div, m);
    }

    return;
}

extern template void div_qr_s<uint64_t>(uint64_t *dst, uint64_t *rem, const uint64_t *src,
                                        size_t n, const uint64_t *div, size_t m);

template <typename T>
WJR_CONSTEXPR_E T fallback_divexact_dbm1c(T *dst, const T *src, size_t n, T bd, T h) {
    T a = 0, p0 = 0, p1 = 0, cf = 0;

    // GCC can't optimize well
    WJR_UNROLL(4)
    for (size_t i = 0; i < n; i++) {
        a = src[i];
        p0 = mul(a, bd, p1);
        h = subc(h, p0, 0, cf);
        dst[i] = h;
        h -= p1 + cf;
    }

    return h;
}

template <typename T, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_CONSTEXPR_E T divexact_dbm1c(T *dst, const T *src, size_t n, T bd, T h) {
#if WJR_HAS_BUILTIN(ASM_DIVEXACT_DBM1C)
    if (is_constant_evaluated()) {
        return fallback_divexact_dbm1c(dst, src, n, bd, h);
    }

    return asm_divexact_dbm1c(dst, src, n, bd, h);
#else
    return fallback_divexact_dbm1c(dst, src, n, bd, h);
#endif
}

template <typename T, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_CONSTEXPR_E void divexact_by3(T *dst, const T *src, size_t n) {
    constexpr T max = in_place_max;
    (void)divexact_dbm1c<T>(dst, src, n, max / 3, 0);
}

template <typename T, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_CONSTEXPR_E void divexact_by5(T *dst, const T *src, size_t n) {
    constexpr T max = in_place_max;
    (void)divexact_dbm1c<T>(dst, src, n, max / 5, 0);
}

template <typename T, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_CONSTEXPR_E void divexact_by15(T *dst, const T *src, size_t n) {
    constexpr T max = in_place_max;
    (void)divexact_dbm1c<T>(dst, src, n, max / 15, 0);
}

template <typename T, T c, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_CONSTEXPR_E void divexact_byc(T *dst, const T *src, size_t n,
                                  std::integral_constant<T, c>) {

    // cost : divexact_dbm1c * 2 + shift * 1 <= divexact_1

    constexpr auto __is_fast = [](auto cr) {
        constexpr T r = get_place_index_v<remove_cvref_t<decltype(cr)>>;
        return c % r == 0 && is_zero_or_single_bit(c / r);
    };

    auto __resolve = [dst, n](auto cr) {
        constexpr T r = get_place_index_v<remove_cvref_t<decltype(cr)>>;
        if constexpr (c >= r) {
            constexpr auto p = fallback_ctz(c / r);
            if constexpr (p != 0) {
                rshift_n(dst, dst, n, p);
            } else {
                (void)(dst);
                (void)(n);
            }
        }
    };

    if constexpr (__is_fast(std::in_place_index<1>)) {
        __resolve(std::in_place_index<1>);
    } else if constexpr (__is_fast(std::in_place_index<3>)) {
        divexact_by3(dst, src, n);
        __resolve(std::in_place_index<3>);
    } else if constexpr (__is_fast(std::in_place_index<5>)) {
        divexact_by5(dst, src, n);
        __resolve(std::in_place_index<5>);
    } else if constexpr (__is_fast(std::in_place_index<15>)) {
        divexact_by15(dst, src, n);
        __resolve(std::in_place_index<15>);
    } else if constexpr (__is_fast(std::in_place_index<9>)) {
        divexact_by3(dst, src, n);
        divexact_by3(dst, dst, n);
        __resolve(std::in_place_index<9>);
    } else if constexpr (__is_fast(std::in_place_index<25>)) {
        divexact_by5(dst, src, n);
        divexact_by5(dst, dst, n);
        __resolve(std::in_place_index<25>);
    } else if constexpr (__is_fast(std::in_place_index<45>)) {
        divexact_by3(dst, src, n);
        divexact_by15(dst, dst, n);
        __resolve(std::in_place_index<45>);
    } else if constexpr (__is_fast(std::in_place_index<75>)) {
        divexact_by5(dst, src, n);
        divexact_by15(dst, dst, n);
        __resolve(std::in_place_index<75>);
    } else if constexpr (__is_fast(std::in_place_index<225>)) {
        divexact_by15(dst, src, n);
        divexact_by15(dst, dst, n);
        __resolve(std::in_place_index<225>);
    } else {
        constexpr auto shift = fallback_ctz(c);
        using divider_t = divexact1_divider<T>;
        constexpr auto divisor = c >> shift;
        constexpr auto value = divider_t::reciprocal(divisor);
        constexpr auto divider = divider_t(divisor, value, shift);
        divexact_1(dst, src, n, divider);
    }
}

// reference : ftp://ftp.risc.uni-linz.ac.at/pub/techreports/1992/92-35.ps.gz
template <typename T>
WJR_CONSTEXPR_E void fallback_divexact_1_noshift(T *dst, const T *src, size_t n,
                                                 const divexact1_divider<T> &div) {
    uint64_t divisor = div.get_divisor();
    uint64_t value = div.get_value();

    uint64_t rdx = 0, r10 = 0;
    uint64_t cf = 0;
    size_t idx = 0;

    --n;

    if (WJR_LIKELY(n != 0)) {
        do {
            r10 = src[idx];
            r10 = subc(r10, rdx, cf, cf);
            r10 = mullo(r10, value);
            dst[idx] = r10;
            ++idx;
            rdx = mulhi(r10, divisor);
        } while (WJR_LIKELY(idx != n));
    }

    r10 = src[n];
    r10 -= rdx + cf;
    r10 = mullo(r10, value);
    dst[n] = r10;
    return;
}

template <typename T>
WJR_CONSTEXPR_E void fallback_divexact_1_shift(T *dst, const T *src, size_t n,
                                               const divexact1_divider<T> &div) {
    const uint64_t divisor = div.get_divisor();
    const uint64_t value = div.get_value();
    const auto shift = div.get_shift();

    uint64_t rdx = 0, r10 = 0;
    uint64_t cf = 0;
    size_t idx = 0;

    --n;
    r10 = src[0];

    if (WJR_LIKELY(n != 0)) {
        do {
            uint64_t r11 = src[idx + 1];
            r10 = shrd(r10, r11, shift);
            r10 = subc(r10, rdx, cf, cf);
            r10 = mullo(r10, value);
            dst[idx] = r10;
            ++idx;
            rdx = mulhi(r10, divisor);
            r10 = r11;
        } while (WJR_LIKELY(idx != n));
    }

    r10 = r10 >> shift;
    r10 -= rdx + cf;
    r10 = mullo(r10, value);
    dst[n] = r10;
    return;
}

template <typename T>
WJR_CONSTEXPR_E void fallback_divexact_1(T *dst, const T *src, size_t n,
                                         const divexact1_divider<T> &div) {
    if (div.get_shift() == 0) {
        return fallback_divexact_1_noshift(dst, src, n, div);
    }

    return fallback_divexact_1_shift(dst, src, n, div);
}

template <typename T, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_CONSTEXPR_E void divexact_1(T *dst, const T *src, size_t n,
                                          const divexact1_divider<T> &div) {
    WJR_ASSERT_ASSUME(n >= 1);

    if (WJR_UNLIKELY(div.is_zero_or_single_bit())) {
        unsigned int c = div.get_shift();
        (void)rshift_n(dst, src, n, c);
        return;
    }

    return fallback_divexact_1(dst, src, n, div);
}

template <typename T, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_INTRINSIC_CONSTEXPR_E void divexact_1(T *dst, const T *src, size_t n,
                                          type_identity_t<T> div) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_ASSUME(div != 0);

    if (WJR_UNLIKELY(is_zero_or_single_bit(div))) {
        unsigned int c = ctz(div);
        (void)rshift_n(dst, src, n, c);
        return;
    }

    return fallback_divexact_1(dst, src, n, divexact1_divider<T>(div));
}

template <typename T>
WJR_PURE WJR_CONSTEXPR20 T mod_1_noshift(const T *src, size_t n,
                                         const div2by1_divider_noshift<T> &div) {
    WJR_ASSERT_ASSUME(n >= 1);

    const T divisor = div.get_divisor();
    const T value = div.get_value();

    T lo, hi;

    hi = src[n - 1];

    if (hi >= divisor) {
        hi -= divisor;
    }

    do {
        if (WJR_UNLIKELY(n == 1)) {
            break;
        }

        --n;

        do {
            lo = src[n - 1];
            (void)div.divide(divisor, value, lo, hi);
            --n;
        } while (WJR_LIKELY(n != 0));

    } while (0);

    return hi;
}

template <typename T>
WJR_PURE WJR_CONSTEXPR20 T mod_1_shift(const T *src, size_t n,
                                       const div2by1_divider<T> &div) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT(div.get_shift() != 0);

    const T divisor = div.get_divisor();
    const T value = div.get_value();
    const auto shift = div.get_shift();

    T lo, hi;

    T rbp = src[n - 1];
    --n;
    hi = rbp >> (64 - shift);

    do {
        if (WJR_UNLIKELY(n == 0)) {
            (void)div.divide(divisor, value, rbp << shift, hi);
            break;
        }

        lo = src[n - 1];
        (void)div.divide(divisor, value, shld(rbp, lo, shift), hi);
        rbp = lo;
        --n;

        if (WJR_LIKELY(n != 0)) {
            do {
                lo = src[n - 1];
                (void)div.divide(divisor, value, shld(rbp, lo, shift), hi);
                rbp = lo;
                --n;
            } while (WJR_LIKELY(n != 0));
        }

        (void)div.divide(divisor, value, rbp << shift, hi);
    } while (0);

    return hi >> shift;
}

template <typename T>
WJR_PURE WJR_INTRINSIC_CONSTEXPR20 T mod_1_impl(const T *src, size_t n,
                                                const div2by1_divider<T> &div) {
    if (div.get_shift() == 0) {
        return mod_1_noshift(src, n, div);
    }

    return mod_1_shift(src, n, div);
}

template <typename T, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_CONSTEXPR20 T mod_1(const T *src, size_t n, const div2by1_divider<T> &div) {
    WJR_ASSERT_ASSUME(n >= 1);

    if (WJR_UNLIKELY(div.is_zero_or_single_bit())) {
        const unsigned int c = 63 - div.get_shift();
        return src[0] & ((1ull << c) - 1);
    }

    if (WJR_BUILTIN_CONSTANT_P(n == 2) && n == 2) {
        uint64_t rem;
        (void)div128by64to128(rem, src[0], src[1], div);
        return rem;
    }

    return mod_1_impl(src, n, div);
}

template <typename T, WJR_REQUIRES_I(std::is_same_v<T, uint64_t>)>
WJR_CONSTEXPR20 T mod_1(const T *src, size_t n, type_identity_t<T> div) {
    WJR_ASSERT_ASSUME(n >= 1);
    WJR_ASSERT_ASSUME(div != 0);

    if (WJR_UNLIKELY(is_zero_or_single_bit(div))) {
        const unsigned int c = ctz(div);
        return src[0] & ((1ull << c) - 1);
    }

    if (WJR_UNLIKELY(n == 1)) {
        const T tmp = src[0];

        if (__has_high_bit(div)) {
            if (tmp >= div) {
                return tmp - div;
            }

            return tmp;
        }

        return tmp % div;
    }

    if (WJR_BUILTIN_CONSTANT_P(n == 2) && n == 2) {
        uint64_t rem;
        (void)div128by64to128(rem, src[0], src[1], div);
        return rem;
    }

    return mod_1_impl(src, n, div2by1_divider<T>(div));
}

} // namespace wjr

#endif // WJR_MATH_DIV_HPP__
#ifndef WJR_MATH_PRECOMPUTE_CHARS_CONVERT_HPP__
#define WJR_MATH_PRECOMPUTE_CHARS_CONVERT_HPP__

// Already included

namespace wjr {

struct precompute_chars_convert_16n_t {
    uint64_t big_base;
    size_t n;
    int digits_in_one_base;
    int digits_in_sixteen_base;
    uint64_t arr[16];
};

struct precompute_chars_convert_t {
    const uint64_t *ptr;
    size_t n;
    size_t shift;
    size_t digits_in_base;
    unsigned int base;
};

extern const std::array<const precompute_chars_convert_16n_t *, 37>
    precompute_chars_convert_16n_ptr;

extern precompute_chars_convert_t *
precompute_chars_convert(precompute_chars_convert_t *pre, size_t n, unsigned int base,
                         uint64_t *table_mem);

} // namespace wjr

#endif // WJR_MATH_PRECOMPUTE_CHARS_CONVERT_HPP__
// Already included

#if defined(WJR_X86)
#ifndef WJR_X86_MATH_CONVERT_HPP__
#define WJR_X86_MATH_CONVERT_HPP__

// Already included
// Already included

#ifndef WJR_X86
#error "x86 required"
#endif

namespace wjr {

#if WJR_HAS_SIMD(SSE4_1) && WJR_HAS_SIMD(X86_SIMD)
#define WJR_HAS_BUILTIN_TO_CHARS_UNROLL_8_FAST WJR_HAS_DEF

#define WJR_HAS_BUILTIN_FROM_CHARS_UNROLL_8_FAST WJR_HAS_DEF
#define WJR_HAS_BUILTIN_FROM_CHARS_UNROLL_16_FAST WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(TO_CHARS_UNROLL_8_FAST)

namespace to_chars_details {

const static __m128i mul10p4 = simd_cast<uint32_t, __m128i_t>(3518437209);
const static __m128i mul10p4x = simd_cast<uint32_t, __m128i_t>(10000);
const static __m128i mul10p2 = sse::set1_epi16(5243);
const static __m128i mul10p2x = sse::set1_epi16(100);
const static __m128i mul10p1 = sse::set1_epi16((short)52429u);
const static __m128i mul10p1x = sse::set1_epi16(10);

const static __m128i shuf =
    sse::setr_epi8(0, 8, 4, 12, 2, 10, 6, 14, 1, 1, 1, 1, 1, 1, 1, 1);

} // namespace to_chars_details

#endif

#if WJR_HAS_BUILTIN(TO_CHARS_UNROLL_8_FAST)

inline uint64_t builtin_to_chars_unroll_8_fast_10(uint32_t in) {

    __m128i x = simd_cast<uint32_t, __m128i_t>(in);
    __m128i q, r;

    q = _mm_mul_epu32(x, to_chars_details::mul10p4);
    q = _mm_srli_epi64(q, 45);

    r = _mm_sub_epi32(x, _mm_mul_epu32(q, to_chars_details::mul10p4x));
    x = _mm_packus_epi32(q, r);

    q = _mm_mulhi_epu16(x, to_chars_details::mul10p2);
    q = _mm_srli_epi16(q, 3);

    r = _mm_sub_epi16(x, _mm_mullo_epi16(q, to_chars_details::mul10p2x));
    x = _mm_packus_epi16(q, r);

    q = _mm_mulhi_epu16(x, to_chars_details::mul10p1);
    q = _mm_srli_epi16(q, 3);

    r = _mm_sub_epi16(x, _mm_mullo_epi16(q, to_chars_details::mul10p1x));
    x = _mm_packus_epi16(q, r);

    return simd_cast<__m128i_t, uint64_t>(sse::shuffle_epi8(x, to_chars_details::shuf));
}

template <uint64_t Base>
uint64_t builtin_to_chars_unroll_8_fast(uint32_t in) {
    if constexpr (Base == 10) {
        return builtin_to_chars_unroll_8_fast_10(in);
    } else {
        static_assert(Base == 10, "");
    }
}

template <uint64_t Base>
void builtin_to_chars_unroll_8_fast(void *ptr, uint32_t in, char_converter_t) {
    const uint64_t x = builtin_to_chars_unroll_8_fast<Base>(in) + 0x3030303030303030ull;
    write_memory<uint64_t>(ptr, x);
}

template <uint64_t Base>
void builtin_to_chars_unroll_8_fast(void *ptr, uint32_t in, origin_converter_t) {
    const uint64_t x = builtin_to_chars_unroll_8_fast<Base>(in);
    write_memory<uint64_t>(ptr, x);
}

#endif

#if WJR_HAS_BUILTIN(FROM_CHARS_UNROLL_8_FAST) ||                                         \
    WJR_HAS_BUILTIN(FROM_CHARS_UNROLL_16_FAST)

namespace from_chars_details {

/// @private
template <uint64_t Base>
inline constexpr uint64_t __base2 = Base * Base;

/// @private
template <uint64_t Base>
inline constexpr uint64_t __base4 = __base2<Base> * __base2<Base>;

/// @private
template <uint64_t Base>
inline constexpr uint64_t __base8 = __base4<Base> * __base4<Base>;

/// @private
template <uint64_t Base>
const static __m128i mulp1x = sse::setr_epi8(Base, 1, Base, 1, Base, 1, Base, 1, Base, 1,
                                             Base, 1, Base, 1, Base, 1);

/// @private
template <uint64_t Base>
const static __m128i mulp2x = sse::setr_epi16(__base2<Base>, 1, __base2<Base>, 1,
                                              __base2<Base>, 1, __base2<Base>, 1);

/// @private
template <uint64_t Base>
const static __m128i mulp4x = sse::setr_epi16(__base4<Base>, 1, __base4<Base>, 1,
                                              __base4<Base>, 1, __base4<Base>, 1);

/// @private
template <uint64_t Base>
const static __m128i baseu8 =
    sse::setr_epi8(Base, Base, Base, Base, Base, Base, Base, 0xff, 0xff, 0xff, 0xff, 0xff,
                   0xff, 0xff, 0xff, 0xff);

/// @private
static __m128i ascii = sse::set1_epi8(0x30);

} // namespace from_chars_details

#endif

#if WJR_HAS_BUILTIN(FROM_CHARS_UNROLL_8_FAST)

template <uint64_t Base>
uint32_t builtin_from_chars_unroll_8_fast(__m128i in) {
    const __m128i t1 = _mm_maddubs_epi16(in, from_chars_details::mulp1x<Base>);
    const __m128i t2 = _mm_madd_epi16(t1, from_chars_details::mulp2x<Base>);
    const __m128i t3 = _mm_packus_epi32(t2, t2);
    const __m128i t4 = _mm_madd_epi16(t3, from_chars_details::mulp4x<Base>);

    return simd_cast<__m128i_t, uint32_t>(t4);
}

template <uint64_t Base>
uint32_t builtin_from_chars_unroll_8_fast(const void *ptr, char_converter_t) {
    static_assert(Base <= 10, "");
    const __m128i in = _mm_sub_epi8(sse::loadu_si64(ptr), from_chars_details::ascii);
    return builtin_from_chars_unroll_8_fast<Base>(in);
}

template <uint64_t Base>
uint32_t builtin_from_chars_unroll_8_fast(const void *ptr, origin_converter_t) {
    static_assert(Base <= 10, "");
    const __m128i in = sse::loadu_si64(ptr);
    return builtin_from_chars_unroll_8_fast<Base>(in);
}

#endif

#if WJR_HAS_BUILTIN(FROM_CHARS_UNROLL_16_FAST)

template <uint64_t Base>
uint64_t builtin_from_chars_unroll_16_fast(__m128i in) {
    const __m128i t1 = _mm_maddubs_epi16(in, from_chars_details::mulp1x<Base>);
    const __m128i t2 = _mm_madd_epi16(t1, from_chars_details::mulp2x<Base>);
    const __m128i t3 = _mm_packus_epi32(t2, t2);
    const __m128i t4 = _mm_madd_epi16(t3, from_chars_details::mulp4x<Base>);

    const uint64_t val = simd_cast<__m128i_t, uint64_t>(t4);
    const uint32_t lo = val;
    const uint32_t hi = val >> 32;

    return lo * from_chars_details::__base8<Base> + hi;
}

template <uint64_t Base>
uint64_t builtin_from_chars_unroll_16_fast(const void *ptr, char_converter_t) {
    static_assert(Base <= 10, "");
    const __m128i in =
        _mm_sub_epi8(sse::loadu((__m128i *)(ptr)), from_chars_details::ascii);
    return builtin_from_chars_unroll_16_fast<Base>(in);
}

template <uint64_t Base>
uint64_t builtin_from_chars_unroll_16_fast(const void *ptr, origin_converter_t) {
    static_assert(Base <= 10, "");
    const __m128i in = sse::loadu((__m128i *)(ptr));
    return builtin_from_chars_unroll_16_fast<Base>(in);
}

#endif

} // namespace wjr

#endif // WJR_X86_MATH_CONVERT_HPP__
#endif

namespace wjr {

inline constexpr size_t dc_bignum_to_chars_threshold = WJR_DC_BIGNUM_TO_CHARS_THRESHOLD;
inline constexpr size_t dc_bignum_to_chars_precompute_threshold =
    WJR_DC_BIGNUM_TO_CHARS_THRESHOLD;

inline constexpr size_t dc_bignum_from_chars_threshold =
    WJR_DC_BIGNUM_FROM_CHARS_THRESHOLD;
inline constexpr size_t dc_bignum_from_chars_precompute_threshold =
    WJR_DC_BIGNUM_FROM_CHARS_PRECOMPUTE_THRESHOLD;

inline constexpr auto div2by1_divider_noshift_of_big_base_10 =
    div2by1_divider_noshift<uint64_t>(10'000'000'000'000'000'000ull,
                                      15'581'492'618'384'294'730ull);

namespace convert_details {

WJR_CONST constexpr bool __isspace(uint8_t ch) { return char_converter.from(ch) == 64; }

template <typename T>
struct __is_fast_convert_value
    : std::conjunction<std::is_trivial<T>, std::bool_constant<sizeof(T) == 1>> {};

template <typename T>
inline constexpr bool __is_fast_convert_value_v = __is_fast_convert_value<T>::value;

template <typename Iter, typename = void>
struct __is_fast_convert_iterator_helper : std::false_type {};

template <typename Iter>
struct __is_fast_convert_iterator_helper<Iter,
                                         std::enable_if_t<is_contiguous_iterator_v<Iter>>>
    : __is_fast_convert_value<iterator_contiguous_value_t<Iter>> {};

template <typename Iter>
struct __is_fast_convert_iterator : __is_fast_convert_iterator_helper<Iter> {};

/**
 * @brief Iterator concept that can be used in fast_convert.
 *
 * @details The iterator must be contiguous iterator and the value_type must be
 * trivial and sizeof(value_type) == 1. Cast to_address(iter) to uint8_t*(to_chars)/const
 * uint8_t*(from_chars) in fast_convert.
 *
 */
template <typename Iter>
inline constexpr bool __is_fast_convert_iterator_v =
    __is_fast_convert_iterator<Iter>::value;

template <typename Value, typename Converter>
struct __is_valid_converter
    : std::disjunction<std::conjunction<std::is_same<Converter, char_converter_t>,
                                        is_nonbool_integral<Value>>,
                       std::conjunction<std::is_same<Converter, origin_converter_t>,
                                        is_nonbool_unsigned_integral<Value>>> {};

template <typename Value, typename Converter>
inline constexpr bool __is_valid_converter_v =
    __is_valid_converter<Value, Converter>::value;

WJR_REGISTER_HAS_TYPE(to_chars_fast_fn_fast_conv,
                      Base::__fast_conv(std::declval<void *>(), std::declval<Args>()...),
                      Base);

WJR_REGISTER_HAS_TYPE(from_chars_fast_fn_fast_conv,
                      Base::__fast_conv(std::declval<const void *>(),
                                        std::declval<Args>()...),
                      Base);

template <typename Iter, typename = void>
struct fast_buffer {
private:
    using value_type = iterator_value_t<Iter>;

public:
    using type =
        std::conditional_t<__is_fast_convert_value_v<value_type>, value_type, char>;
};

// back_inserter or inserter
template <typename Iter>
struct fast_buffer<Iter, std::void_t<decltype(std::declval<std::enable_if_t<
                                                  is_any_insert_iterator_v<Iter>>>())>> {
private:
    using value_type = typename Iter::container_type::value_type;

public:
    using type =
        std::conditional_t<__is_fast_convert_value_v<value_type>, value_type, char>;
};

template <typename Iter>
using fast_buffer_t = typename fast_buffer<Iter>::type;

template <typename Container>
struct __fast_container_inserter_test {
private:
    using traits_type = container_traits<Container>;

public:
    static constexpr int value =
        traits_type::is_trivially_contiguous_v &&
                has_container_resize_v<Container, size_t>
            ? (has_container_resize_v<Container, size_t, dctor_t> ? 2 : 1)
            : 0;

    static_assert(value != 2 || has_container_append_v<Container, size_t, dctor_t>, "");
};

template <typename Iter, typename = void>
struct __is_fast_container_inserter {
    static constexpr int value = 0;
};

template <typename Iter>
struct __is_fast_container_inserter<
    Iter,
    std::void_t<
        decltype(std::declval<std::enable_if_t<is_any_insert_iterator_v<Iter>>>(),
                 __fast_container_inserter_test<typename Iter::container_type>::value)>> {
private:
    using container_type = typename Iter::container_type;

public:
    static constexpr int value =
        __is_fast_convert_value_v<typename container_type::value_type> &&
                is_trivially_allocator_constructible_v<
                    typename container_type::allocator_type>
            ? __fast_container_inserter_test<container_type>::value
            : 0;
};

template <typename Iter>
inline constexpr int is_fast_container_inserter_v =
    __is_fast_container_inserter<Iter>::value;

} // namespace convert_details

// require operator() of Converter is constexpr
template <typename Converter, uint64_t Base, int Unroll>
class __char_converter_table_t {
    static constexpr uint64_t pw2 = Unroll == 2 ? Base * Base : Base * Base * Base * Base;

public:
    constexpr __char_converter_table_t() : table() {
        for (uint64_t i = 0, j = 0; i < pw2; ++i, j += Unroll) {
            int x = i;
            for (int k = Unroll - 1; ~k; --k) {
                table[j + k] = Converter::to(x % Base);
                x /= Base;
            }
        }
    }

    WJR_CONST constexpr char operator[](unsigned int idx) const { return table[idx]; }

    WJR_CONST constexpr const char *data() const { return table.data(); }

private:
    std::array<char, pw2 * Unroll> table;
};

template <typename Converter, uint64_t Base, int Unroll>
inline constexpr __char_converter_table_t<Converter, Base, Unroll>
    __char_converter_table{};

template <uint64_t Base>
class __to_chars_unroll_2_fast_fn_impl_base {
public:
    template <typename Converter>
    WJR_INTRINSIC_INLINE static void __fast_conv(void *ptr, uint32_t val, Converter) {
        auto str = (char *)ptr;
        if constexpr (Base * Base <= 16) {
            constexpr auto &table = __char_converter_table<Converter, Base, 4>;
            std::memcpy(str, table.data() + val * 4 + 2, 2);
        } else {
            constexpr auto &table = __char_converter_table<Converter, Base, 2>;
            std::memcpy(str, table.data() + val * 2, 2);
        }
    }
};

template <uint64_t Base>
class __to_chars_unroll_2_fast_fn_impl {};

template <>
class __to_chars_unroll_2_fast_fn_impl<2>
    : public __to_chars_unroll_2_fast_fn_impl_base<2> {};

template <>
class __to_chars_unroll_2_fast_fn_impl<8>
    : public __to_chars_unroll_2_fast_fn_impl_base<8> {};

template <>
class __to_chars_unroll_2_fast_fn_impl<10>
    : public __to_chars_unroll_2_fast_fn_impl_base<10> {};

template <>
class __to_chars_unroll_2_fast_fn_impl<16>
    : public __to_chars_unroll_2_fast_fn_impl_base<16> {};

template <uint64_t Base>
class __to_chars_unroll_4_fast_fn_impl_base {
public:
    template <typename Converter>
    WJR_INTRINSIC_INLINE static void __fast_conv(void *ptr, uint32_t val, Converter) {
        auto str = (char *)ptr;
        if constexpr (Base * Base <= 16) {
            constexpr auto &table = __char_converter_table<Converter, Base, 4>;
            std::memcpy(str, table.data() + val * 4, 4);
        } else {
            constexpr auto &table = __char_converter_table<Converter, Base, 2>;
            constexpr auto Base2 = Base * Base;
            const uint32_t hi = val / Base2;
            const uint32_t lo = val % Base2;

            std::memcpy(str, table.data() + hi * 2, 2);
            std::memcpy(str + 2, table.data() + lo * 2, 2);
        }
    }
};

template <uint64_t Base>
class __to_chars_unroll_4_fast_fn_impl {};

template <>
class __to_chars_unroll_4_fast_fn_impl<2>
    : public __to_chars_unroll_4_fast_fn_impl_base<2> {};

template <>
class __to_chars_unroll_4_fast_fn_impl<8>
    : public __to_chars_unroll_4_fast_fn_impl_base<8> {};

template <>
class __to_chars_unroll_4_fast_fn_impl<10>
    : public __to_chars_unroll_4_fast_fn_impl_base<10> {};

template <>
class __to_chars_unroll_4_fast_fn_impl<16>
    : public __to_chars_unroll_4_fast_fn_impl_base<16> {};

template <uint64_t Base>
class __to_chars_unroll_8_fast_fn_impl_base {
#if WJR_HAS_BUILTIN(TO_CHARS_UNROLL_8_FAST)
public:
    template <typename Converter>
    WJR_INTRINSIC_INLINE static void __fast_conv(void *ptr, uint64_t val,
                                                 Converter conv) {
        builtin_to_chars_unroll_8_fast<Base>(ptr, val, conv);
    }
#endif
};

template <uint64_t Base>
class __to_chars_unroll_8_fast_fn_impl {};

template <>
class __to_chars_unroll_8_fast_fn_impl<10>
    : public __to_chars_unroll_8_fast_fn_impl_base<10> {};

template <uint64_t Base>
class __to_chars_unroll_2_fn : public __to_chars_unroll_2_fast_fn_impl<Base> {
    using Mybase = __to_chars_unroll_2_fast_fn_impl<Base>;

public:
    template <typename Converter>
    WJR_INTRINSIC_INLINE void operator()(uint8_t *ptr, uint32_t val,
                                         Converter conv) const {
        if constexpr (convert_details::has_to_chars_fast_fn_fast_conv_v<Mybase, uint32_t,
                                                                        Converter>) {
            Mybase::__fast_conv(ptr, val, conv);
        } else {
            ptr[0] = conv.template to<Base>(val / Base);
            ptr[1] = conv.template to<Base>(val % Base);
        }
    }
};

template <uint64_t Base>
inline constexpr __to_chars_unroll_2_fn<Base> __to_chars_unroll_2{};

template <uint64_t Base>
class __to_chars_unroll_4_fn_impl : public __to_chars_unroll_4_fast_fn_impl<Base> {
    using Mybase = __to_chars_unroll_4_fast_fn_impl<Base>;

public:
    template <typename Converter>
    WJR_INTRINSIC_INLINE void operator()(uint8_t *ptr, uint32_t val,
                                         Converter conv) const {
        if constexpr (convert_details::has_to_chars_fast_fn_fast_conv_v<Mybase, uint32_t,
                                                                        Converter>) {
            Mybase::__fast_conv(ptr, val, conv);
        } else {
            constexpr auto Base2 = Base * Base;
            __to_chars_unroll_2<Base>(ptr, val / Base2, conv);
            __to_chars_unroll_2<Base>(ptr + 2, val % Base2, conv);
        }
    }
};

template <uint64_t Base>
inline constexpr __to_chars_unroll_4_fn_impl<Base> __to_chars_unroll_4{};

template <uint64_t Base>
class __to_chars_unroll_8_fn_impl : public __to_chars_unroll_8_fast_fn_impl<Base> {
    using Mybase = __to_chars_unroll_8_fast_fn_impl<Base>;

public:
    template <typename Converter>
    WJR_INTRINSIC_INLINE void operator()(uint8_t *ptr, uint64_t val,
                                         Converter conv) const {
        if constexpr (convert_details::has_to_chars_fast_fn_fast_conv_v<Mybase, uint64_t,
                                                                        Converter>) {
            Mybase::__fast_conv(ptr, val, conv);
        } else {
            constexpr auto Base4 = Base * Base * Base * Base;
            __to_chars_unroll_4<Base>(ptr, val / Base4, conv);
            __to_chars_unroll_4<Base>(ptr + 4, val % Base4, conv);
        }
    }
};

template <uint64_t Base>
inline constexpr __to_chars_unroll_8_fn_impl<Base> __to_chars_unroll_8{};

template <uint64_t Base>
class __from_chars_unroll_4_fast_fn_impl_base {
protected:
    WJR_CONST WJR_INTRINSIC_INLINE static uint32_t __fast_conv(uint32_t val) {
        constexpr uint32_t Base2 = Base * Base;

        constexpr uint32_t mask = 0x00FF00FF;
        constexpr uint32_t mul = 1 + (Base2 << 16);
        val = (val * Base) + (val >> 8);
        val = ((val & mask) * mul) >> 16;
        return uint32_t(val);
    }

public:
    WJR_PURE WJR_INTRINSIC_INLINE static uint32_t __fast_conv(const void *ptr,
                                                              char_converter_t) {
        return __fast_conv(read_memory<uint32_t>(ptr) - 0x30303030u);
    }

    WJR_PURE WJR_INTRINSIC_INLINE static uint32_t __fast_conv(const void *ptr,
                                                              origin_converter_t) {
        return __fast_conv(read_memory<uint32_t>(ptr));
    }
};

template <uint64_t Base>
class __from_chars_unroll_8_fast_fn_impl_base {
protected:
    WJR_CONST WJR_INTRINSIC_INLINE static uint32_t __fast_conv(uint64_t val) {
        constexpr uint64_t Base2 = Base * Base;
        constexpr uint64_t Base4 = Base2 * Base2;
        constexpr uint64_t Base6 = Base4 * Base2;

        constexpr uint64_t mask = 0x000000FF000000FF;
        constexpr uint64_t mul1 = Base2 + (Base6 << 32);
        constexpr uint64_t mul2 = 1 + (Base4 << 32);

        val = (val * Base) + (val >> 8);
        val = (((val & mask) * mul1) + (((val >> 16) & mask) * mul2)) >> 32;
        return val;
    }

public:
    WJR_PURE WJR_INTRINSIC_INLINE static uint32_t
    __fast_conv(const void *ptr, WJR_MAYBE_UNUSED char_converter_t conv) {
#if WJR_HAS_BUILTIN(FROM_CHARS_UNROLL_8_FAST)
        return builtin_from_chars_unroll_8_fast<Base>(ptr, conv);
#else
        return __fast_conv(read_memory<uint64_t>(ptr) - 0x3030303030303030ull);
#endif
    }

    WJR_PURE WJR_INTRINSIC_INLINE static uint32_t
    __fast_conv(const void *ptr, WJR_MAYBE_UNUSED origin_converter_t conv) {
#if WJR_HAS_BUILTIN(FROM_CHARS_UNROLL_8_FAST)
        return builtin_from_chars_unroll_8_fast<Base>(ptr, conv);
#else
        return __fast_conv(read_memory<uint64_t>(ptr));
#endif
    }
};

template <uint64_t Base>
class __from_chars_unroll_16_fast_fn_impl_base {
#if WJR_HAS_BUILTIN(FROM_CHARS_UNROLL_16_FAST)
public:
    template <typename Converter>
    WJR_PURE WJR_INTRINSIC_INLINE static uint64_t __fast_conv(const void *ptr,
                                                              Converter conv) {
        return builtin_from_chars_unroll_16_fast<Base>(ptr, conv);
    }
#endif
};

template <uint64_t Base>
class __from_chars_unroll_4_fast_fn_impl {};

template <>
class __from_chars_unroll_4_fast_fn_impl<2>
    : public __from_chars_unroll_4_fast_fn_impl_base<2> {};

template <>
class __from_chars_unroll_4_fast_fn_impl<8>
    : public __from_chars_unroll_4_fast_fn_impl_base<8> {};

template <>
class __from_chars_unroll_4_fast_fn_impl<10>
    : public __from_chars_unroll_4_fast_fn_impl_base<10> {};

template <uint64_t Base>
class __from_chars_unroll_8_fast_fn_impl {};

template <>
class __from_chars_unroll_8_fast_fn_impl<2>
    : public __from_chars_unroll_8_fast_fn_impl_base<2> {};

template <>
class __from_chars_unroll_8_fast_fn_impl<8>
    : public __from_chars_unroll_8_fast_fn_impl_base<8> {};

template <>
class __from_chars_unroll_8_fast_fn_impl<10>
    : public __from_chars_unroll_8_fast_fn_impl_base<10> {};

template <uint64_t Base>
class __from_chars_unroll_16_fast_fn_impl {};

template <>
class __from_chars_unroll_16_fast_fn_impl<2>
    : public __from_chars_unroll_16_fast_fn_impl_base<2> {};

template <>
class __from_chars_unroll_16_fast_fn_impl<8>
    : public __from_chars_unroll_16_fast_fn_impl_base<8> {};

template <>
class __from_chars_unroll_16_fast_fn_impl<10>
    : public __from_chars_unroll_16_fast_fn_impl_base<10> {};

template <uint64_t Base>
class __from_chars_unroll_4_fn : public __from_chars_unroll_4_fast_fn_impl<Base> {
    using Mybase = __from_chars_unroll_4_fast_fn_impl<Base>;

public:
    template <typename Converter>
    WJR_PURE WJR_INTRINSIC_INLINE uint64_t operator()(const uint8_t *ptr,
                                                      Converter conv) const {
        if constexpr (convert_details::has_from_chars_fast_fn_fast_conv_v<Mybase,
                                                                          Converter>) {
            return Mybase::__fast_conv(ptr, conv);
        } else {
            uint64_t value = 0;
            value = conv.template from<Base>(*ptr++);
            value = value * Base + conv.template from<Base>(*ptr++);
            value = value * Base + conv.template from<Base>(*ptr++);
            return value * Base + conv.template from<Base>(*ptr++);
        }
    }
};

template <uint64_t Base>
inline constexpr __from_chars_unroll_4_fn<Base> __from_chars_unroll_4{};

template <uint64_t Base>
class __from_chars_unroll_8_fn : public __from_chars_unroll_8_fast_fn_impl<Base> {
    using Mybase = __from_chars_unroll_8_fast_fn_impl<Base>;

public:
    template <typename Converter>
    WJR_PURE WJR_INTRINSIC_INLINE uint64_t operator()(const uint8_t *ptr,
                                                      Converter conv) const {
        if constexpr (convert_details::has_from_chars_fast_fn_fast_conv_v<Mybase,
                                                                          Converter>) {
            return Mybase::__fast_conv(ptr, conv);
        } else {
            constexpr uint64_t Base4 = Base * Base * Base * Base;
            return __from_chars_unroll_4<Base>(ptr, conv) * Base4 +
                   __from_chars_unroll_4<Base>(ptr + 4, conv);
        }
    }
};

template <uint64_t Base>
inline constexpr __from_chars_unroll_8_fn<Base> __from_chars_unroll_8{};

template <uint64_t Base>
class __from_chars_unroll_16_fn : public __from_chars_unroll_16_fast_fn_impl<Base> {
    using Mybase = __from_chars_unroll_16_fast_fn_impl<Base>;

public:
    template <typename Converter>
    WJR_PURE WJR_INTRINSIC_INLINE uint64_t operator()(const uint8_t *ptr,
                                                      Converter conv) const {
        if constexpr (convert_details::has_from_chars_fast_fn_fast_conv_v<Mybase,
                                                                          Converter>) {
            return Mybase::__fast_conv(ptr, conv);
        } else {
            constexpr uint64_t Base4 = Base * Base * Base * Base;
            constexpr uint64_t Base8 = Base4 * Base4;
            return __from_chars_unroll_8<Base>(ptr, conv) * Base8 +
                   __from_chars_unroll_8<Base>(ptr + 8, conv);
        }
    }
};

template <uint64_t Base>
inline constexpr __from_chars_unroll_16_fn<Base> __from_chars_unroll_16{};

template <typename UnsignedValue>
constexpr int fallback_count_digits10(UnsignedValue n) {
    int count = 1;

    if (WJR_UNLIKELY(n >= 1000)) {
        do {
            n /= 10000;
            count += 4;
        } while (n >= 1000);

        if (n == 0) {
            return count;
        }
    }

    if (n < 10) {
        return count;
    }

    if (n < 100) {
        return count + 1;
    }

    return count + 3;
}

inline int builtin_count_digits10_u32(uint32_t n) {
#define WJR_INC(T) (((sizeof(#T) - 1ull) << 32) - T)
    static constexpr uint64_t table[] = {
        WJR_INC(0),          WJR_INC(0),          WJR_INC(0),          // 8
        WJR_INC(10),         WJR_INC(10),         WJR_INC(10),         // 64
        WJR_INC(100),        WJR_INC(100),        WJR_INC(100),        // 512
        WJR_INC(1000),       WJR_INC(1000),       WJR_INC(1000),       // 4096
        WJR_INC(10000),      WJR_INC(10000),      WJR_INC(10000),      // 32k
        WJR_INC(100000),     WJR_INC(100000),     WJR_INC(100000),     // 256k
        WJR_INC(1000000),    WJR_INC(1000000),    WJR_INC(1000000),    // 2048k
        WJR_INC(10000000),   WJR_INC(10000000),   WJR_INC(10000000),   // 16M
        WJR_INC(100000000),  WJR_INC(100000000),  WJR_INC(100000000),  // 128M
        WJR_INC(1000000000), WJR_INC(1000000000), WJR_INC(1000000000), // 1024M
        WJR_INC(1000000000), WJR_INC(1000000000)                       // 4B
    };
    const auto inc = table[clz(n | 1) ^ 31];
    return static_cast<int>((n + inc) >> 32);
#undef WJR_INC
}

inline int builtin_count_digits10_u64(uint64_t n) {
#define WJR_POWERS_OF_10(factor)                                                         \
    factor * 10, (factor)*100, (factor)*1000, (factor)*10000, (factor)*100000,           \
        (factor)*1000000, (factor)*10000000, (factor)*100000000, (factor)*1000000000
    static constexpr uint8_t bsr2log10[] = {
        1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  4,  5,  5,  5,
        6,  6,  6,  7,  7,  7,  7,  8,  8,  8,  9,  9,  9,  10, 10, 10,
        10, 11, 11, 11, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 15, 15,
        15, 16, 16, 16, 16, 17, 17, 17, 18, 18, 18, 19, 19, 19, 19, 20};
    const auto t = bsr2log10[clz(n | 1) ^ 63];
    static constexpr const uint64_t zero_or_powers_of_10[] = {
        0, 0, WJR_POWERS_OF_10(1U), WJR_POWERS_OF_10(1000000000ULL),
        10000000000000000000ULL};
    return t - (n < zero_or_powers_of_10[t]);
#undef WJR_POWERS_OF_10
}

template <typename T>
WJR_CONSTEXPR_E int count_digits10_impl(T n) {
    if (is_constant_evaluated() || WJR_BUILTIN_CONSTANT_P(n)) {
        return fallback_count_digits10(n);
    }

    if constexpr (sizeof(T) <= sizeof(uint32_t)) {
        return builtin_count_digits10_u32(static_cast<uint32_t>(n));
    } else {
        return builtin_count_digits10_u64(static_cast<uint64_t>(n));
    }
}

template <uint64_t Base>
struct count_digits_fn {};

template <uint64_t Base>
inline constexpr count_digits_fn<Base> count_digits{};

template <>
struct count_digits_fn<2> {
    template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
    WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int operator()(T n) const {
        return bit_width(n);
    }
};

template <>
struct count_digits_fn<8> {
    template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
    WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int operator()(T n) const {
        return __ceil_div(to_unsigned(bit_width(n)), 3);
    }
};

template <>
struct count_digits_fn<16> {
    template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
    WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int operator()(T n) const {
        return __ceil_div(to_unsigned(bit_width(n)), 4);
    }
};

template <>
struct count_digits_fn<1> {
    template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
    WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int operator()(T n, int bits) const {
        return (bit_width(n) + bits - 1) / bits;
    }
};

template <>
struct count_digits_fn<10> {
    template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
    WJR_CONST WJR_INTRINSIC_CONSTEXPR_E int operator()(T n) const {
        const int ret = count_digits10_impl(n);
        WJR_ASSUME(1 <= ret && ret <= std::numeric_limits<T>::digits10 + 1);
        return ret;
    }
};

template <typename Iter>
struct to_chars_result {
    Iter ptr;
    std::errc ec;

    friend bool operator==(const to_chars_result &lhs, const to_chars_result &rhs) {
        return lhs.ptr == rhs.ptr && lhs.ec == rhs.ec;
    }

    constexpr explicit operator bool() const { return ec == std::errc{}; }
};

template <typename Iter = const char *>
struct from_chars_result {
    Iter ptr;
    std::errc ec;

    friend bool operator==(const from_chars_result &lhs, const from_chars_result &rhs) {
        return lhs.ptr == rhs.ptr && lhs.ec == rhs.ec;
    }

    constexpr explicit operator bool() const { return ec == std::errc{}; }
};

// Base :
// 0 : dynamic base
// 1 : base is power of two
template <uint64_t Base>
class __unsigned_to_chars_backward_unchecked_fn {};

template <uint64_t Base>
inline constexpr __unsigned_to_chars_backward_unchecked_fn<Base>
    __unsigned_to_chars_backward_unchecked{};

template <>
class __unsigned_to_chars_backward_unchecked_fn<2> {
    template <typename UnsignedValue, typename Converter>
    static uint8_t *fn(uint8_t *ptr, int n, UnsignedValue x, Converter conv) {
        constexpr auto nd = std::numeric_limits<UnsignedValue>::digits;
        WJR_ASSERT_L1(x != 0);
        WJR_ASSERT_ASSUME(1 <= n && n <= nd);
        (void)(nd);

        if (WJR_UNLIKELY(n >= 4)) {
            do {
                __to_chars_unroll_4<2>(ptr - 4, x & 0x0f, conv);
                ptr -= 4;
                x >>= 4;
                n -= 4;
            } while (WJR_LIKELY(n >= 4));

            if (n == 0) {
                return ptr;
            }
        }

        switch (n) {
        case 3: {
            *--ptr = conv.template to<2>(x & 1);
            x >>= 1;
            WJR_FALLTHROUGH;
        }
        case 2: {
            __to_chars_unroll_2<2>(ptr - 2, x, conv);
            ptr -= 2;
            break;
        }
        case 1: {
            *--ptr = conv.template to<2>(x);
            break;
        }
        default: {
            WJR_UNREACHABLE();
            break;
        }
        }

        return ptr;
    }

public:
    template <typename UnsignedValue, typename Converter,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    uint8_t *operator()(uint8_t *ptr, int n, UnsignedValue x, Converter conv) const {
        return fn(ptr, n, x, conv);
    }
};

template <>
class __unsigned_to_chars_backward_unchecked_fn<8> {
    template <typename UnsignedValue, typename Converter>
    static uint8_t *fn(uint8_t *ptr, int n, UnsignedValue x, Converter conv) {
        constexpr auto nd = std::numeric_limits<UnsignedValue>::digits;
        WJR_ASSERT_L1(x != 0);
        WJR_ASSERT_ASSUME(1 <= n && n <= (nd + 2) / 3);

        if constexpr (nd >= 16) {
            if (WJR_UNLIKELY(n >= 4)) {
                do {
                    __to_chars_unroll_4<8>(ptr - 4, x & 0x0fff, conv);
                    ptr -= 4;
                    x >>= 12;
                    n -= 4;
                } while (WJR_LIKELY(n >= 4));

                if (n == 0) {
                    return ptr;
                }
            }
        }

        switch (n) {
        case 3: {
            *--ptr = conv.template to<8>(x & 0x07);
            x >>= 3;
            WJR_FALLTHROUGH;
        }
        case 2: {
            __to_chars_unroll_2<8>(ptr - 2, x, conv);
            ptr -= 2;
            break;
        }
        case 1: {
            *--ptr = conv.template to<8>(x);
            break;
        }
        default: {
            WJR_UNREACHABLE();
            break;
        }
        }

        return ptr;
    }

public:
    template <typename UnsignedValue, typename Converter,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    uint8_t *operator()(uint8_t *ptr, int n, UnsignedValue x, Converter conv) const {
        return fn(ptr, n, x, conv);
    }
};

template <>
class __unsigned_to_chars_backward_unchecked_fn<16> {
    template <typename UnsignedValue, typename Converter>
    static uint8_t *fn(uint8_t *ptr, int n, UnsignedValue x, Converter conv) {
        constexpr auto nd = std::numeric_limits<UnsignedValue>::digits;
        WJR_ASSERT_L1(x != 0);
        WJR_ASSERT_ASSUME(1 <= n && n <= (nd + 3) / 4);

        if constexpr (nd >= 16) {
            if (WJR_UNLIKELY(n >= 4)) {
                do {
                    __to_chars_unroll_4<16>(ptr - 4, x & 0xffff, conv);
                    ptr -= 4;
                    x >>= 16;
                    n -= 4;
                } while (WJR_LIKELY(n >= 4));

                if (n == 0) {
                    return ptr;
                }
            }
        }

        switch (n) {
        case 3: {
            *--ptr = conv.to(x & 0x0f);
            x >>= 4;
            WJR_FALLTHROUGH;
        }
        case 2: {
            __to_chars_unroll_2<16>(ptr - 2, x, conv);
            ptr -= 2;
            break;
        }
        case 1: {
            *--ptr = conv.to(x);
            break;
        }
        default: {
            WJR_UNREACHABLE();
            break;
        }
        }

        return ptr;
    }

public:
    template <typename UnsignedValue, typename Converter,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    uint8_t *operator()(uint8_t *ptr, int n, UnsignedValue x, Converter conv) const {
        return fn(ptr, n, x, conv);
    }
};

template <>
class __unsigned_to_chars_backward_unchecked_fn<1> {
private:
    template <typename UnsignedValue, typename Converter>
    static uint8_t *fn(uint8_t *ptr, int n, UnsignedValue x, int bits, Converter conv) {
        WJR_ASSERT_L1(x != 0);
        WJR_ASSERT_ASSUME(1 <= n && n <= std::numeric_limits<UnsignedValue>::digits);

        const unsigned int mask = (1u << bits) - 1;

        do {
            *--ptr = conv.to(x & mask);
            x >>= bits;
            --n;
        } while (WJR_LIKELY(n != 0));

        return ptr;
    }

public:
    template <typename UnsignedValue, typename Converter,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    uint8_t *operator()(uint8_t *ptr, int n, UnsignedValue x, int bits,
                        Converter conv) const {
        return fn(ptr, n, x, bits, conv);
    }
};

template <>
class __unsigned_to_chars_backward_unchecked_fn<10> {
private:
    template <typename UnsignedValue, typename Converter,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    static uint8_t *fn(uint8_t *ptr, UnsignedValue val, Converter conv) {
        WJR_ASSERT_ASSUME(val != 0);

        if (WJR_LIKELY(val >= 100)) {
            do {
                __to_chars_unroll_2<10>(ptr - 2, val % 100, conv);
                ptr -= 2;
                val /= 100;
            } while (WJR_LIKELY(val >= 100));
        }

        if (val < 10) {
            *--ptr = conv.template to<10>(val);
            return ptr;
        }

        __to_chars_unroll_2<10>(ptr - 2, val, conv);
        ptr -= 2;
        return ptr;
    }

public:
    template <typename UnsignedValue, typename Converter,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    uint8_t *operator()(uint8_t *ptr, UnsignedValue val, Converter conv) const {
        return fn(ptr, val, conv);
    }
};

template <typename Value, typename IBase, typename Converter>
uint8_t *__fast_to_chars_backward_unchecked_impl(uint8_t *ptr, Value val, IBase ibase,
                                                 Converter conv) {
    if (WJR_UNLIKELY(val == 0)) {
        *--ptr = conv.template to<1>(0);
        return ptr;
    }

    auto uVal = to_unsigned(val);
    int sign = 0;

    if constexpr (std::is_signed_v<Value>) {
        if (val < 0) {
            sign = 1;
            uVal = -val;
        }
    }

    const unsigned int base = ibase;

    switch (base) {
    case 2: {
        ptr = __unsigned_to_chars_backward_unchecked<2>(ptr, count_digits<2>(uVal), uVal,
                                                        conv);
        break;
    }
    case 8: {
        ptr = __unsigned_to_chars_backward_unchecked<8>(ptr, count_digits<8>(uVal), uVal,
                                                        conv);
        break;
    }
    case 16: {
        ptr = __unsigned_to_chars_backward_unchecked<16>(ptr, count_digits<16>(uVal),
                                                         uVal, conv);
        break;
    }
    case 4:
    case 32: {
        const int bits = base == 4 ? 2 : 5;
        ptr = __unsigned_to_chars_backward_unchecked<1>(ptr, count_digits<1>(uVal, bits),
                                                        uVal, bits, conv);
        break;
    }
    case 10: {
        ptr = __unsigned_to_chars_backward_unchecked<10>(ptr, uVal, conv);
        break;
    }
    default: {
        WJR_UNREACHABLE();
        return ptr;
    }
    }

    if constexpr (std::is_signed_v<Value>) {
        if (sign) {
            *--ptr = '-';
        }
    }

    return ptr;
}

template <typename Iter, typename Value, typename IBase, typename Converter>
Iter __to_chars_backward_unchecked_impl(Iter first, Value val, IBase ibase,
                                        Converter conv) {
    const auto __ptr = reinterpret_cast<uint8_t *>((to_address)(first));
    const auto __end = __fast_to_chars_backward_unchecked_impl(__ptr, val, ibase, conv);
    return first + std::distance(__ptr, __end);
}

/**
 * @brief Convert an unsigned integer to a string in reverse order without checking
 * buf size.
 *
 * @details Only use fast_convert mode.
 *
 */
template <typename Iter, typename Value, typename BaseType = unsigned int,
          BaseType IBase = 10, typename Converter = char_converter_t,
          WJR_REQUIRES(convert_details::__is_fast_convert_iterator_v<Iter>
                           &&convert_details::__is_valid_converter_v<Value, Converter>)>
Iter to_chars_backward_unchecked(Iter first, Value val,
                                 std::integral_constant<BaseType, IBase> = {},
                                 Converter conv = {}) {
    return __to_chars_backward_unchecked_impl(
        first, val, std::integral_constant<unsigned int, IBase>(), conv);
}

/**
 * @brief Convert an unsigned integer to a string in reverse order without checking
 * buf size.
 *
 *
 */
template <typename Iter, typename Value, typename Converter = char_converter_t,
          WJR_REQUIRES(convert_details::__is_fast_convert_iterator_v<Iter>
                           &&convert_details::__is_valid_converter_v<Value, Converter>)>
Iter to_chars_backward_unchecked(Iter first, Value val, unsigned int base,
                                 Converter conv = {}) {
    if (WJR_BUILTIN_CONSTANT_P(base)) {
        switch (base) {
        case 2: {
            return to_chars_backward_unchecked(
                first, val, std::integral_constant<unsigned int, 2>(), conv);
        }
        case 8: {
            return to_chars_backward_unchecked(
                first, val, std::integral_constant<unsigned int, 8>(), conv);
        }
        case 16: {
            return to_chars_backward_unchecked(
                first, val, std::integral_constant<unsigned int, 16>(), conv);
        }
        case 10: {
            return to_chars_backward_unchecked(
                first, val, std::integral_constant<unsigned int, 10>(), conv);
        }
        default: {
            break;
        }
        }
    }

    return __to_chars_backward_unchecked_impl(first, val, base, conv);
}

template <typename Value, typename IBase, typename Converter>
to_chars_result<uint8_t *> __fast_to_chars_impl(uint8_t *first, uint8_t *last, Value val,
                                                IBase ibase, Converter conv) {
    if (WJR_UNLIKELY(val == 0)) {
        if (WJR_UNLIKELY(first == last)) {
            return {last, std::errc::value_too_large};
        }

        *first++ = conv.template to<1>(0);
        return {first, std::errc{}};
    }

    auto uVal = to_unsigned(val);

    if constexpr (std::is_signed_v<Value>) {
        if (val < 0) {
            if (WJR_UNLIKELY(first == last)) {
                return {last, std::errc::value_too_large};
            }

            *first++ = '-';
            uVal = -val;
        }
    }

    const unsigned int base = ibase;
    const auto size = std::distance(first, last);

#define WJR_TO_CHARS_VALIDATE_IMPL(BASE, DIGITS, CALL)                                   \
    const int n = count_digits<BASE> DIGITS;                                             \
    if (WJR_LIKELY(n <= size)) {                                                         \
        first += n;                                                                      \
        (void)__unsigned_to_chars_backward_unchecked<BASE>(                              \
            first, WJR_PP_QUEUE_EXPAND(CALL), conv);                                     \
        return {first, std::errc{}};                                                     \
    }                                                                                    \
    return { last, std::errc::value_too_large }

    switch (base) {
    case 2: {
        WJR_TO_CHARS_VALIDATE_IMPL(2, (uVal), (n, uVal));
    }
    case 8: {
        WJR_TO_CHARS_VALIDATE_IMPL(8, (uVal), (n, uVal));
    }
    case 16: {
        WJR_TO_CHARS_VALIDATE_IMPL(16, (uVal), (n, uVal));
    }
    case 4:
    case 32: {
        const int bits = base == 4 ? 2 : 5;
        WJR_TO_CHARS_VALIDATE_IMPL(1, (uVal, bits), (n, uVal, bits));
    }
    case 10: {
        WJR_TO_CHARS_VALIDATE_IMPL(10, (uVal), (uVal));
    }
    default: {
        WJR_UNREACHABLE();
        return {last, std::errc::invalid_argument};
    }
    }

#undef WJR_TO_CHARS_VALIDATE_IMPL
}

template <typename Iter, typename Value, typename IBase, typename Converter>
to_chars_result<Iter> __fallback_to_chars_impl(Iter first, Iter last, Value val,
                                               IBase ibase, Converter conv) {
    constexpr auto is_signed = std::is_signed_v<Value>;
    constexpr auto base_2_table = std::numeric_limits<Value>::digits;
    constexpr auto base_10_table = std::numeric_limits<Value>::digits10 + 1;
    constexpr auto is_random_access = is_random_access_iterator_v<Iter>;

    if (WJR_UNLIKELY(val == 0)) {
        if (WJR_UNLIKELY(first == last)) {
            return {last, std::errc::value_too_large};
        }

        *first++ = conv.template to<1>(0);
        return {first, std::errc{}};
    }

    auto uVal = to_unsigned(val);
    int sign = 0;

    if constexpr (is_signed) {
        if constexpr (is_random_access) {
            if (val < 0) {
                if (WJR_UNLIKELY(first == last)) {
                    return {last, std::errc::value_too_large};
                }

                sign = 1;
                uVal = -val;
            }
        } else {
            if (val < 0) {
                if (WJR_UNLIKELY(first == last)) {
                    return {last, std::errc::value_too_large};
                }

                *first++ = '-';
                uVal = -val;
            }
        }
    }

    const unsigned int base = ibase;

#define WJR_TO_CHARS_VALIDATE_IMPL(BASE, TABLE, CALL)                                    \
    if constexpr (is_random_access) {                                                    \
        const auto size = std::distance(first, last);                                    \
        WJR_PP_QUEUE_EXPAND(                                                             \
            WJR_PP_BOOL_IF(WJR_PP_NE(BASE, 10),                                          \
                           (                                                             \
                               if constexpr (is_signed) {                                \
                                   if (WJR_UNLIKELY(n + sign > size)) {                  \
                                       return {last, std::errc::value_too_large};        \
                                   }                                                     \
                               } else {                                                  \
                                   if (WJR_UNLIKELY(n > size)) {                         \
                                       return {last, std::errc::value_too_large};        \
                                   }                                                     \
                               }),                                                       \
                           ()))                                                          \
                                                                                         \
        convert_details::fast_buffer_t<Iter> buffer[TABLE + is_signed];                  \
        const auto __end = buffer + TABLE + is_signed;                                   \
        auto __ptr = (convert_details::fast_buffer_t<Iter> *)                            \
            __unsigned_to_chars_backward_unchecked<BASE>(                                \
                (uint8_t *)__end, WJR_PP_QUEUE_EXPAND(CALL), conv);                      \
                                                                                         \
        WJR_PP_QUEUE_EXPAND(                                                             \
            WJR_PP_BOOL_IF(WJR_PP_EQ(BASE, 10),                                          \
                           (                                                             \
                               const auto n = __end - __ptr;                             \
                                                                                         \
                               if constexpr (is_signed) {                                \
                                   if (WJR_UNLIKELY(n + sign > size)) {                  \
                                       return {last, std::errc::value_too_large};        \
                                   }                                                     \
                               } else {                                                  \
                                   if (WJR_UNLIKELY(n > size)) {                         \
                                       return {last, std::errc::value_too_large};        \
                                   }                                                     \
                               }),                                                       \
                           ()))                                                          \
                                                                                         \
        if constexpr (is_signed) {                                                       \
            if (sign) {                                                                  \
                *--__ptr = '-';                                                          \
            }                                                                            \
        }                                                                                \
                                                                                         \
        return wjr::copy(__ptr, __end, first);                                           \
    } else {                                                                             \
        convert_details::fast_buffer_t<Iter> buffer[TABLE];                              \
        const auto __end = buffer + TABLE;                                               \
        auto __ptr = (convert_details::fast_buffer_t<Iter> *)                            \
            __unsigned_to_chars_backward_unchecked<BASE>(                                \
                (uint8_t *)__end, WJR_PP_QUEUE_EXPAND(CALL), conv);                      \
                                                                                         \
        do {                                                                             \
            if (WJR_UNLIKELY(first == last)) {                                           \
                return {last, std::errc::value_too_large};                               \
            }                                                                            \
                                                                                         \
            *first++ = *__ptr++;                                                         \
        } while (__ptr != __end);                                                        \
                                                                                         \
        return {first, std::errc{}};                                                     \
    }

    switch (base) {
    case 2: {
        const int n = count_digits<2>(uVal);
        WJR_TO_CHARS_VALIDATE_IMPL(2, base_2_table, (n, uVal));
    }
    case 8: {
        const int n = count_digits<8>(uVal);
        WJR_TO_CHARS_VALIDATE_IMPL(8, (base_2_table + 2) / 3, (n, uVal));
    }
    case 16: {
        const int n = count_digits<16>(uVal);
        WJR_TO_CHARS_VALIDATE_IMPL(16, (base_2_table + 3) / 4, (n, uVal));
    }
    case 4:
    case 32: {
        const int bits = base == 4 ? 2 : 5;
        const int n = count_digits<1>(uVal, bits);
        WJR_TO_CHARS_VALIDATE_IMPL(1, (base_2_table + bits - 1) / bits, (n, uVal, bits));
    }
    case 10: {
        WJR_TO_CHARS_VALIDATE_IMPL(10, base_10_table, (uVal));
    }
    default: {
        WJR_UNREACHABLE();
        return {last, std::errc::invalid_argument};
    }
    }

#undef WJR_TO_CHARS_VALIDATE_IMPL
}

template <typename Iter, typename Value, typename IBase, typename Converter>
to_chars_result<Iter> __to_chars_impl(Iter first, Iter last, Value val, IBase ibase,
                                      Converter conv) {
    if constexpr (convert_details::__is_fast_convert_iterator_v<Iter>) {
        const auto __first = reinterpret_cast<uint8_t *>((to_address)(first));
        const auto __last = reinterpret_cast<uint8_t *>((to_address)(last));
        const auto __result = __fast_to_chars_impl(__first, __last, val, ibase, conv);
        return {first + std::distance(__first, __result.ptr), __result.ec};
    } else {
        return __fallback_to_chars_impl(first, last, val, ibase, conv);
    }
}

template <typename Value, typename IBase, typename Converter>
uint8_t *__fast_to_chars_unchecked_impl(uint8_t *ptr, Value val, IBase ibase,
                                        Converter conv) {
    if (WJR_UNLIKELY(val == 0)) {
        *ptr++ = conv.template to<1>(0);
        return ptr;
    }

    auto uVal = to_unsigned(val);

    if constexpr (std::is_signed_v<Value>) {
        if (val < 0) {
            *ptr++ = '-';
            uVal = -val;
        }
    }

    const unsigned int base = ibase;

    switch (base) {
    case 2: {
        const int n = count_digits<2>(uVal);
        ptr += n;
        (void)__unsigned_to_chars_backward_unchecked<2>(ptr, n, uVal, conv);
        return ptr;
    }
    case 8: {
        const int n = count_digits<8>(uVal);
        ptr += n;
        (void)__unsigned_to_chars_backward_unchecked<8>(ptr, n, uVal, conv);
        return ptr;
    }
    case 16: {
        const int n = count_digits<16>(uVal);
        ptr += n;
        (void)__unsigned_to_chars_backward_unchecked<16>(ptr, n, uVal, conv);
        return ptr;
    }
    case 4:
    case 32: {
        const int bits = base == 4 ? 2 : 5;
        const int n = count_digits<1>(uVal, bits);
        ptr += n;
        (void)__unsigned_to_chars_backward_unchecked<1>(ptr, n, uVal, bits, conv);
        return ptr;
    }
    case 10: {
        const int n = count_digits<10>(uVal);
        ptr += n;
        (void)__unsigned_to_chars_backward_unchecked<10>(ptr, uVal, conv);
        return ptr;
    }
    default: {
        WJR_UNREACHABLE();
        return ptr;
    }
    }
}

template <typename Iter, typename Value, typename IBase, typename Converter>
Iter __fallback_to_chars_unchecked_impl(Iter ptr, Value val, IBase ibase,
                                        Converter conv) {
    constexpr auto is_signed = std::is_signed_v<Value>;
    constexpr auto base_2_table = std::numeric_limits<Value>::digits;
    constexpr auto base_10_table = std::numeric_limits<Value>::digits10 + 1;

    if (WJR_UNLIKELY(val == 0)) {
        *ptr++ = conv.template to<1>(0);
        return ptr;
    }

    auto uVal = to_unsigned(val);
    int sign = 0;

    if constexpr (is_signed) {
        if (val < 0) {
            sign = 1;
            uVal = -val;
        }
    }

    const unsigned int base = ibase;

#define WJR_TO_CHARS_IMPL(BASE, TABLE, CALL)                                             \
    constexpr auto __fast_container_inserter_v =                                         \
        convert_details::is_fast_container_inserter_v<Iter>;                             \
    if constexpr (__fast_container_inserter_v != 0) {                                    \
        WJR_PP_BOOL_IF(WJR_PP_EQ(BASE, 10), const int n = count_digits<10>(uVal), );     \
        auto &cont = get_inserter_container(ptr);                                        \
        if constexpr (__fast_container_inserter_v == 1) {                                \
            resize(cont, cont.size() + n + sign);                                        \
        } else {                                                                         \
            append(cont, n + sign, dctor);                                               \
        }                                                                                \
        const auto __end = (to_address)(cont.data() + cont.size());                      \
        auto __ptr = (convert_details::fast_buffer_t<Iter> *)                            \
            __unsigned_to_chars_backward_unchecked<BASE>(                                \
                (uint8_t *)__end, WJR_PP_QUEUE_EXPAND(CALL), conv);                      \
                                                                                         \
        if constexpr (is_signed) {                                                       \
            if (sign) {                                                                  \
                *--__ptr = '-';                                                          \
            }                                                                            \
        }                                                                                \
                                                                                         \
        return ptr;                                                                      \
    } else {                                                                             \
        convert_details::fast_buffer_t<Iter> buffer[TABLE + is_signed];                  \
        const auto __end = buffer + TABLE + is_signed;                                   \
        auto __ptr = (convert_details::fast_buffer_t<Iter> *)                            \
            __unsigned_to_chars_backward_unchecked<BASE>(                                \
                (uint8_t *)__end, WJR_PP_QUEUE_EXPAND(CALL), conv);                      \
                                                                                         \
        if constexpr (is_signed) {                                                       \
            if (sign) {                                                                  \
                *--__ptr = '-';                                                          \
            }                                                                            \
        }                                                                                \
                                                                                         \
        return wjr::copy(__ptr, __end, ptr);                                             \
    }

    switch (base) {
    case 2: {
        const int n = count_digits<2>(uVal);
        WJR_TO_CHARS_IMPL(2, base_2_table, (n, uVal));
    }
    case 8: {
        const int n = count_digits<8>(uVal);
        WJR_TO_CHARS_IMPL(8, (base_2_table + 2) / 3, (n, uVal));
    }
    case 16: {
        const int n = count_digits<16>(uVal);
        WJR_TO_CHARS_IMPL(16, (base_2_table + 3) / 4, (n, uVal));
    }
    case 4:
    case 32: {
        const int bits = base == 4 ? 2 : 5;
        const int n = count_digits<1>(uVal, bits);
        WJR_TO_CHARS_IMPL(1, (base_2_table + bits - 1) / bits, (n, uVal, bits));
    }
    case 10: {
        WJR_TO_CHARS_IMPL(10, base_10_table, (uVal));
    }
    default: {
        WJR_UNREACHABLE();
        return ptr;
    }
    }

#undef WJR_TO_CHARS_IMPL
}

template <typename Iter, typename Value, typename IBase, typename Converter>
Iter __to_chars_unchecked_impl(Iter ptr, Value val, IBase ibase, Converter conv) {
    if constexpr (convert_details::__is_fast_convert_iterator_v<Iter>) {
        const auto __ptr = reinterpret_cast<uint8_t *>((to_address)(ptr));
        const auto __result = __fast_to_chars_unchecked_impl(__ptr, val, ibase, conv);
        return ptr + std::distance(__ptr, __result);
    } else {
        return __fallback_to_chars_unchecked_impl(ptr, val, ibase, conv);
    }
}
/**
 * @brief Convert an unsigned integer to a string with checking buf size.
 *
 *
 * @return to_chars_result<Iter> If the conversion is successful, return {ans,
 * std::errc{}}. Otherwise, return {last, std::errc::value_too_large}.
 *
 */
template <typename Iter, typename Value, typename BaseType = unsigned int,
          BaseType IBase = 10, typename Converter = char_converter_t,
          WJR_REQUIRES(convert_details::__is_valid_converter_v<Value, Converter>)>
to_chars_result<Iter> to_chars(Iter ptr, Iter last, Value val,
                               std::integral_constant<BaseType, IBase> = {},
                               Converter conv = {}) {
    return __to_chars_impl(ptr, last, val, std::integral_constant<unsigned int, IBase>(),
                           conv);
}

/**
 * @brief Convert an unsigned integer to a string with checking buf size.
 *
 * @return to_chars_result<Iter> If the conversion is successful, return {ans,
 * std::errc{}}. Otherwise, return {last, std::errc::value_too_large}.
 *
 */
template <typename Iter, typename Value, typename Converter = char_converter_t,
          WJR_REQUIRES(convert_details::__is_valid_converter_v<Value, Converter>)>
to_chars_result<Iter> to_chars(Iter ptr, Iter last, Value val, unsigned int base,
                               Converter conv = {}) {
    if (WJR_BUILTIN_CONSTANT_P(base)) {
        switch (base) {
        case 2: {
            return to_chars(ptr, last, val, std::integral_constant<unsigned int, 2>(),
                            conv);
        }
        case 8: {
            return to_chars(ptr, last, val, std::integral_constant<unsigned int, 8>(),
                            conv);
        }
        case 16: {
            return to_chars(ptr, last, val, std::integral_constant<unsigned int, 16>(),
                            conv);
        }
        case 10: {
            return to_chars(ptr, last, val, std::integral_constant<unsigned int, 10>(),
                            conv);
        }
        default: {
            break;
        }
        }
    }

    return __to_chars_impl(ptr, last, val, base, conv);
}

/**
 * @brief Convert an unsigned integer to a string without checking buf size.
 *
 * @details Iter can be any output iterator. Support fast_convert mode and fallback mode.
 * \n fast_convert mode : \n fast_convert mode is used when
 * __is_fast_convert_iterator_v<Iter> is true. \n caclulate the number of digits and
 * convert the integer to a string in reverse order. \n fallback mode : \n use buffer to
 * store the result and use @ref wjr::copy to copy the result to the output iterator. \n
 *
 */
template <typename Iter, typename Value, typename BaseType = unsigned int,
          BaseType IBase = 10, typename Converter = char_converter_t,
          WJR_REQUIRES(convert_details::__is_valid_converter_v<Value, Converter>)>
Iter to_chars_unchecked(Iter ptr, Value val, std::integral_constant<BaseType, IBase> = {},
                        Converter conv = {}) {
    return __to_chars_unchecked_impl(ptr, val,
                                     std::integral_constant<unsigned int, IBase>(), conv);
}

/**
 * @brief Convert an unsigned integer to a string without checking buf size.
 *
 * @tparam Iter The iterator type. Must be random access iterator.
 * @tparam Value The value type. If Converter is origin_converter_t, Value must be
 * non-bool unsigned integral type. Otherwise, Value must be non-bool integral type.
 *
 */
template <typename Iter, typename Value, typename Converter = char_converter_t,
          WJR_REQUIRES(convert_details::__is_valid_converter_v<Value, Converter>)>
Iter to_chars_unchecked(Iter ptr, Value val, unsigned int base, Converter conv = {}) {
    if (WJR_BUILTIN_CONSTANT_P(base)) {
        switch (base) {
        case 2: {
            return to_chars_unchecked(ptr, val, std::integral_constant<unsigned int, 2>(),
                                      conv);
        }
        case 8: {
            return to_chars_unchecked(ptr, val, std::integral_constant<unsigned int, 8>(),
                                      conv);
        }
        case 16: {
            return to_chars_unchecked(ptr, val,
                                      std::integral_constant<unsigned int, 16>(), conv);
        }
        case 10: {
            return to_chars_unchecked(ptr, val,
                                      std::integral_constant<unsigned int, 10>(), conv);
        }
        default: {
            break;
        }
        }
    }

    return __to_chars_unchecked_impl(ptr, val, base, conv);
}

template <typename Converter>
size_t __biginteger_to_chars_2_impl(uint8_t *first, const uint64_t *up, size_t n,
                                    Converter conv) {
    WJR_ASSERT_L1(up[n - 1] != 0);
    WJR_ASSERT_ASSUME(n >= 2);

    uint64_t x = up[n - 1];
    --n;
    const int pc = clz(x);
    const int hbits = 64 - pc;
    WJR_ASSUME(1 <= hbits && hbits <= 64);

    size_t len = hbits + 64 * n;
    first += len;

    do {
        x = *up;

        for (int i = 0; i < 8; ++i) {
            __to_chars_unroll_8<2>(first - 8, x & 0xff, conv);
            first -= 8;
            x >>= 8;
        }

        ++up;
        --n;
    } while (n);
    x = *up;

    (void)__unsigned_to_chars_backward_unchecked<2>(first, hbits, x, conv);
    return len;
}

template <typename Converter>
size_t __biginteger_to_chars_8_impl(uint8_t *first, const uint64_t *up, size_t n,
                                    Converter conv) {
    WJR_ASSERT_L1(up[n - 1] != 0);
    WJR_ASSERT_ASSUME(n >= 2);

    uint64_t x = up[n - 1];
    --n;
    const int pc = clz(x);
    int hbits = 64 - pc;
    WJR_ASSUME(1 <= hbits && hbits <= 64);

    const size_t len = (hbits + 64 * n + 2) / 3;
    first += len;

    int rest = 0;
    unsigned int last = 0;

    do {
        x = *up;

        switch (rest) {
        case 0: {
            rest = 2;
            break;
        }
        case 2: {
            __to_chars_unroll_2<8>(first - 2, last | ((x & 0x03) << 4), conv);
            first -= 2;
            x >>= 2;
            rest = 4;
            break;
        }
        case 4: {
            __to_chars_unroll_2<8>(first - 2, last | ((x & 0x0f) << 2), conv);
            first -= 2;
            x >>= 4;
            rest = 0;
            break;
        }
        default: {
            WJR_UNREACHABLE();
            break;
        }
        }

        __to_chars_unroll_8<8>(first - 8, x & 0xff'ffff, conv);
        x >>= 24;
        __to_chars_unroll_8<8>(first - 16, x & 0xff'ffff, conv);
        x >>= 24;
        __to_chars_unroll_4<8>(first - 20, x & 0x0fff, conv);
        x >>= 12;
        first -= 20;

        last = x;

        ++up;
        --n;
    } while (n);
    x = *up;

    switch (rest) {
    case 0: {
        break;
    }
    case 2: {
        __to_chars_unroll_2<8>(first - 2, last | ((x & 0x03) << 4), conv);
        first -= 2;
        if (hbits <= 2) {
            goto DONE;
        }
        hbits -= 2;
        x >>= 2;
        break;
    }
    case 4: {
        if (WJR_UNLIKELY(hbits == 1)) {
            *--first = conv.template to<8>(x << 2 | last);
            goto DONE;
        }

        __to_chars_unroll_2<8>(first - 2, last | ((x & 0x0f) << 2), conv);
        first -= 2;
        if (hbits <= 4) {
            goto DONE;
        }
        hbits -= 4;
        x >>= 4;
        break;
    }
    default: {
        WJR_UNREACHABLE();
        break;
    }
    }

    if (WJR_LIKELY(hbits + 2 >= 12)) {
        do {
            __to_chars_unroll_4<8>(first - 4, x & 0x0fff, conv);
            first -= 4;
            x >>= 12;
            hbits -= 12;
        } while (WJR_LIKELY(hbits + 2 >= 12));
    }

    switch ((hbits + 2) / 3) {
    case 3: {
        *--first = conv.template to<8>(x & 0x07);
        x >>= 3;
        WJR_FALLTHROUGH;
    }
    case 2: {
        __to_chars_unroll_2<8>(first - 2, x, conv);
        break;
    }
    case 1: {
        *--first = conv.template to<8>(x);
        WJR_FALLTHROUGH;
    }
    case 0: {
        break;
    }
    default: {
        WJR_UNREACHABLE();
        break;
    }
    }

DONE:
    return len;
}

template <typename Converter>
size_t __biginteger_to_chars_16_impl(uint8_t *first, const uint64_t *up, size_t n,
                                     Converter conv) {
    WJR_ASSERT_L1(up[n - 1] != 0);
    WJR_ASSERT_ASSUME(n >= 2);

    uint64_t x = up[n - 1];
    --n;
    const int pc = clz(x);
    int hbits = 64 - pc;
    WJR_ASSUME(1 <= hbits && hbits <= 64);
    hbits = (hbits + 3) / 4;

    const size_t len = hbits + 16 * n;
    first += len;

    do {
        x = *up;

        __to_chars_unroll_8<16>(first - 8, x & 0xffff'ffff, conv);
        __to_chars_unroll_8<16>(first - 16, x >> 32, conv);
        first -= 16;

        ++up;
        --n;
    } while (n);
    x = *up;

    (void)__unsigned_to_chars_backward_unchecked<16>(first, hbits, x, conv);

    return len;
}

template <typename Converter>
size_t __biginteger_to_chars_power_of_two_impl(uint8_t *first, const uint64_t *up,
                                               size_t n, unsigned int base,
                                               Converter conv) {
    WJR_ASSERT_L1(up[n - 1] != 0);
    WJR_ASSERT_ASSUME(n >= 2);

    const int bits = ctz(base);
    const unsigned int mask = (1u << bits) - 1;

    uint64_t x = up[n - 1];
    --n;
    const int pc = clz(x);
    const int hbits = 64 - pc;
    WJR_ASSUME(1 <= hbits && hbits <= 64);

    const size_t len = (hbits + 64 * n + bits - 1) / bits;
    first += len;

    int rest = 0;
    unsigned int last = 0;

    do {
        x = *up;

        if (rest) {
            int fix = bits - rest;
            unsigned int val = ((x & ((1u << fix) - 1)) << rest) | last;
            x >>= fix;
            rest = 64 - fix;
            *--first = conv.to(val);
        } else {
            rest = 64;
        }

        do {
            *--first = conv.to(x & mask);
            x >>= bits;
            rest -= bits;
        } while (rest >= bits);

        last = x;

        ++up;
        --n;
    } while (n);
    x = *up;

    WJR_ASSERT_ASSUME(rest < bits);

    if (WJR_UNLIKELY(rest != 0)) {
        int fix = bits - rest;
        unsigned int val = ((x & ((1u << fix) - 1)) << rest) | last;
        x >>= fix;
        *--first = conv.to(val);
        rest = hbits - fix;
        if (WJR_UNLIKELY(rest == 0)) {
            goto DONE;
        }
    } else {
        rest = hbits;
    }

    do {
        *--first = conv.to(x & mask);
        x >>= bits;
        rest -= bits;
    } while (WJR_LIKELY(rest > 0));

DONE:
    return len;
}

template <typename Converter>
uint8_t *basecase_to_chars_10(uint8_t *buf, uint64_t *up, size_t n, Converter conv) {
    do {
        if (WJR_UNLIKELY(n == 1)) {
            return __unsigned_to_chars_backward_unchecked<10>(buf, up[0], conv);
        }

        uint64_t q, rem;

        q = div_qr_1_noshift(up, rem, up, n, div2by1_divider_noshift_of_big_base_10);
        n -= q == 0;
        if (q != 0) {
            up[n - 1] = q;
        }

        __to_chars_unroll_8<10>(buf - 8, rem % 1'0000'0000, conv);
        rem /= 1'0000'0000;
        __to_chars_unroll_8<10>(buf - 16, rem % 1'0000'0000, conv);
        rem /= 1'0000'0000;
        __to_chars_unroll_2<10>(buf - 18, rem % 100, conv);
        rem /= 100;
        buf[-19] = conv.template to<10>(rem);

        buf -= 19;
    } while (n);

    return buf;
}

template <typename Converter>
uint8_t *basecase_to_chars(uint8_t *first, size_t len, uint64_t *up, size_t n,
                           unsigned int base, Converter conv) {
    constexpr size_t buf_len = dc_bignum_to_chars_precompute_threshold * 64 * 7 / 11;
    uint8_t buf[buf_len];
    uint8_t *end = buf + buf_len;
    uint8_t *start;

    if (WJR_LIKELY(base == 10)) {
        start = basecase_to_chars_10(end, up, n, conv);
    } else {
        start = end;
    }

    size_t rlen = end - start;
    if (len > rlen) {
        first = std::fill_n(first, len - rlen, conv.template to<1>(0));
    }

    return std::copy(start, end, first);
}

template <typename Converter>
uint8_t *dc_to_chars(uint8_t *first, size_t len, uint64_t *up, size_t n,
                     precompute_chars_convert_t *pre, uint64_t *stk, Converter conv) {
    WJR_ASSERT_ASSUME(n >= 1);
    if (n < dc_bignum_to_chars_threshold) {
        return basecase_to_chars(first, len, up, n, pre->base, conv);
    } else {
        const auto pp = pre->ptr;
        const auto pn = pre->n;
        const auto ps = pre->shift;

        WJR_ASSERT((pn + ps) * 5 >= n * 2);

        if (n < pn + ps || (n == pn + ps && reverse_compare_n(up + ps, pp, pn) < 0)) {
            return dc_to_chars(first, len, up, n, pre - 1, stk, conv);
        }

        const auto pd = pre->digits_in_base;
        auto qp = stk;

        div_qr_s(qp, up + ps, up + ps, n - ps, pp, pn);

        size_t qn = n - pn - ps;
        qn += qp[qn] != 0;

        if (len != 0) {
            len = len - pd;
        }

        pre -= qn * 2 <= n;

        first = dc_to_chars(first, len, qp, qn, pre, stk + qn, conv);
        first = dc_to_chars(first, pd, up, pn + ps, pre, stk, conv);
        return first;
    }
}

template <typename Converter>
uint8_t *__biginteger_basecase_to_chars(uint8_t *first, const uint64_t *up, size_t n,
                                        unsigned int base, Converter conv) {
    if (WJR_LIKELY(n < dc_bignum_to_chars_precompute_threshold)) {
        uint64_t upbuf[dc_bignum_to_chars_precompute_threshold];
        std::copy_n(up, n, upbuf);
        return basecase_to_chars(first, 0, upbuf, n, base, conv);
    }

    precompute_chars_convert_t pre[64 - 3];

    unique_stack_allocator stkal(math_details::stack_alloc);
    auto stk =
        static_cast<uint64_t *>(stkal.allocate((n * 18 / 5 + 192) * sizeof(uint64_t)));
    const auto __up = stk;
    std::copy_n(up, n, __up);
    stk += n;
    const auto mpre = precompute_chars_convert(pre, n, base, stk);
    stk += n * 8 / 5 + 128;
    return dc_to_chars(first, 0, __up, n, mpre, stk, conv);
}

template <typename Converter>
uint8_t *__fast_biginteger_large_to_chars_impl(uint8_t *first, const uint64_t *up,
                                               size_t n, unsigned int base,
                                               Converter conv) {

    switch (base) {
    case 2: {
        return first + __biginteger_to_chars_2_impl(first, up, n, conv);
    }
    case 8: {
        return first + __biginteger_to_chars_8_impl(first, up, n, conv);
    }
    case 16: {
        return first + __biginteger_to_chars_16_impl(first, up, n, conv);
    }
    case 4:
    case 32: {
        return first + __biginteger_to_chars_power_of_two_impl(first, up, n, base, conv);
    }
    default: {
        break;
    }
    }

    return __biginteger_basecase_to_chars(first, up, n, base, conv);
}

template <typename Iter, typename Converter>
Iter __fallback_biginteger_large_to_chars_impl(Iter ptr, const uint64_t *up, size_t n,
                                               unsigned int base, Converter conv) {
#define WJR_BIGINTEGER_TO_CHARS_IMPL(BASE, NAME, TAIL, SIZE, CALL)                       \
    constexpr auto __fast_container_inserter_v =                                         \
        convert_details::is_fast_container_inserter_v<Iter>;                             \
    if constexpr (__fast_container_inserter_v != 0) {                                    \
        auto &cont = get_inserter_container(ptr);                                        \
        const auto __presize = cont.size();                                              \
        if constexpr (__fast_container_inserter_v == 1) {                                \
            resize(cont, __presize + SIZE);                                              \
        } else {                                                                         \
            append(cont, SIZE, dctor);                                                   \
        }                                                                                \
        const auto __ptr = (uint8_t *)(to_address)(cont.data()) + __presize;             \
        const auto __size = NAME(__ptr, WJR_PP_QUEUE_EXPAND(CALL), conv) TAIL;           \
        WJR_ASSERT((size_t)__size <= SIZE);                                              \
        if constexpr (__fast_container_inserter_v == 1) {                                \
            resize(cont, __presize + __size);                                            \
        } else {                                                                         \
            resize(cont, __presize + __size, wjr::dctor);                                \
        }                                                                                \
                                                                                         \
        return ptr;                                                                      \
    } else {                                                                             \
        unique_stack_allocator stkal(math_details::stack_alloc);                         \
        const auto __ptr = (uint8_t *)stkal.allocate(SIZE * sizeof(uint64_t));           \
        const auto __size = NAME(__ptr, WJR_PP_QUEUE_EXPAND(CALL), conv) TAIL;           \
                                                                                         \
        return wjr::copy_n((convert_details::fast_buffer_t<Iter> *)__ptr, __size, ptr);  \
    }

    switch (base) {
    case 2: {
        const size_t capacity = 64 * n;
        WJR_BIGINTEGER_TO_CHARS_IMPL(2, __biginteger_to_chars_2_impl, , capacity,
                                     (up, n));
    }
    case 8: {
        const size_t capacity = (64 * n + 2) / 3;
        WJR_BIGINTEGER_TO_CHARS_IMPL(8, __biginteger_to_chars_8_impl, , capacity,
                                     (up, n));
    }
    case 16: {
        const size_t capacity = (64 * n + 3) / 4;
        WJR_BIGINTEGER_TO_CHARS_IMPL(16, __biginteger_to_chars_16_impl, , capacity,
                                     (up, n));
    }
    case 4:
    case 32: {
        const int bits = base == 4 ? 2 : 5;
        const size_t capacity = (64 * n + bits - 1) / bits;
        WJR_BIGINTEGER_TO_CHARS_IMPL(base, __biginteger_to_chars_power_of_two_impl, ,
                                     capacity, (up, n, base));
    }
    default: {
        break;
    }
    }

    const size_t capacity = ((64 * n) * 4 + 12) / 13;
    WJR_BIGINTEGER_TO_CHARS_IMPL(base, __biginteger_basecase_to_chars, -__ptr, capacity,
                                 (up, n, base));

#undef WJR_BIGINTEGER_TO_CHARS_IMPL
}

template <typename Iter, typename Converter>
Iter __biginteger_to_chars_impl(Iter first, const uint64_t *up, size_t n,
                                unsigned int base, Converter conv) {
    if (n == 1) {
        return to_chars_unchecked(first, up[0], base, conv);
    }

    if constexpr (convert_details::__is_fast_convert_iterator_v<Iter>) {
        const auto __first = reinterpret_cast<uint8_t *>((to_address)(first));
        const auto __result =
            __fast_biginteger_large_to_chars_impl(__first, up, n, base, conv);
        return first + std::distance(__first, __result);
    } else {
        return __fallback_biginteger_large_to_chars_impl(first, up, n, base, conv);
    }
}

/**
 * @brief Convert a biginteger to a string by a given base.
 *
 * @tparam Iter Output iterator type
 * @param[out] first Output iterator
 * @param[in] up Pointer to the biginteger
 * @param[in] n Length of the biginteger
 * @param[in] base Base of the output string. Range: `[2, 36]`,
 * Only support 10 and power of two currently.
 * @return Output iterator after the conversion
 */
template <typename Iter, typename Converter = char_converter_t>
Iter biginteger_to_chars(Iter first, const uint64_t *up, size_t n, unsigned int base = 10,
                         Converter conv = {}) {
    WJR_ASSERT(base <= 36 && (is_zero_or_single_bit(base) || base == 10));
    WJR_ASSERT_ASSUME(up[n - 1] != 0);

    return __biginteger_to_chars_impl(first, up, n, base, conv);
}

template <uint64_t Base>
class __unsigned_from_chars_unchecked_fn {};

template <uint64_t Base>
inline constexpr __unsigned_from_chars_unchecked_fn<Base>
    __unsigned_from_chars_unchecked{};

template <>
class __unsigned_from_chars_unchecked_fn<2> {
    template <typename UnsignedValue, typename Converter>
    static void fn(const uint8_t *first, const uint8_t *last, UnsignedValue &val,
                   Converter conv) {
        constexpr auto nd = std::numeric_limits<UnsignedValue>::digits;

        auto n = std::distance(first, last);
        WJR_ASSERT_ASSUME(1 <= n && n <= nd);

        if constexpr (nd >= 16) {
            if (WJR_UNLIKELY(n >= 8)) {
                do {
                    val = (val << 8) + __from_chars_unroll_8<2>(first, conv);
                    first += 8;
                    n -= 8;
                } while (WJR_LIKELY(n >= 8));

                if (n == 0) {
                    return;
                }
            }
        } else if constexpr (nd == 8) {
            if (WJR_UNLIKELY(n == 8)) {
                val = __from_chars_unroll_8<2>(first, conv);
                first += 8;
                return;
            }
        }

        switch (n) {
        case 7: {
            val = (val << 1) + conv.template from<2>(*first++);
            WJR_FALLTHROUGH;
        }
        case 6: {
            val = (val << 1) + conv.template from<2>(*first++);
            WJR_FALLTHROUGH;
        }
        case 5: {
            val = (val << 1) + conv.template from<2>(*first++);
            WJR_FALLTHROUGH;
        }
        case 4: {
            val <<= 4;
            val += __from_chars_unroll_4<2>(first, conv);
            first += 4;
            break;
        }
        case 3: {
            val = (val << 1) + conv.template from<2>(*first++);
            WJR_FALLTHROUGH;
        }
        case 2: {
            val = (val << 1) + conv.template from<2>(*first++);
            WJR_FALLTHROUGH;
        }
        case 1: {
            val = (val << 1) + conv.template from<2>(*first++);
            break;
        }
        default: {
            WJR_UNREACHABLE();
            break;
        }
        }
    }

public:
    template <typename UnsignedValue, typename Converter,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    void operator()(const uint8_t *first, const uint8_t *last, UnsignedValue &val,
                    Converter conv) const {
        return fn(first, last, val, conv);
    }
};

template <>
class __unsigned_from_chars_unchecked_fn<8> {
    template <typename UnsignedValue, typename Converter>
    static void fn(const uint8_t *first, const uint8_t *last, UnsignedValue &val,
                   Converter conv) {
        constexpr auto nd = std::numeric_limits<UnsignedValue>::digits;

        auto n = std::distance(first, last);
        WJR_ASSERT_ASSUME(1 <= n && n <= (nd + 2) / 3);

        if constexpr (nd >= 32) {
            if (WJR_UNLIKELY(n >= 8)) {
                do {
                    val = (val << 24) + __from_chars_unroll_8<8>(first, conv);
                    first += 8;
                    n -= 8;
                } while (WJR_LIKELY(n >= 8));

                if (n == 0) {
                    return;
                }
            }
        }

        switch (n) {
        case 7: {
            val = (val << 3) + conv.template from<8>(*first++);
            WJR_FALLTHROUGH;
        }
        case 6: {
            val = (val << 3) + conv.template from<8>(*first++);
            WJR_FALLTHROUGH;
        }
        case 5: {
            val = (val << 3) + conv.template from<8>(*first++);
            WJR_FALLTHROUGH;
        }
        case 4: {
            val <<= 12;
            val += __from_chars_unroll_4<8>(first, conv);
            first += 4;
            break;
        }
        case 3: {
            val = (val << 3) + conv.template from<8>(*first++);
            WJR_FALLTHROUGH;
        }
        case 2: {
            val = (val << 3) + conv.template from<8>(*first++);
            WJR_FALLTHROUGH;
        }
        case 1: {
            val = (val << 3) + conv.template from<8>(*first++);
            break;
        }
        default: {
            WJR_UNREACHABLE();
            break;
        }
        }
    }

public:
    template <typename UnsignedValue, typename Converter,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    void operator()(const uint8_t *first, const uint8_t *last, UnsignedValue &val,
                    Converter conv) const {
        return fn(first, last, val, conv);
    }
};

template <>
class __unsigned_from_chars_unchecked_fn<16> {
    template <typename UnsignedValue, typename Converter>
    static void fn(const uint8_t *first, const uint8_t *last, UnsignedValue &val,
                   Converter conv) {
        constexpr auto nd = std::numeric_limits<UnsignedValue>::digits;

        auto n = std::distance(first, last);
        WJR_ASSERT_ASSUME(1 <= n && n <= (nd + 3) / 4);

        if constexpr (nd >= 64) {
            if (WJR_UNLIKELY(n >= 8)) {
                do {
                    val = (val << 32) + __from_chars_unroll_8<16>(first, conv);
                    first += 8;
                    n -= 8;
                } while (WJR_LIKELY(n >= 8));

                if (n == 0) {
                    return;
                }
            }
        } else if constexpr (nd == 32) {
            if (WJR_UNLIKELY(n == 8)) {
                val = __from_chars_unroll_8<16>(first, conv);
                first += 8;
                return;
            }
        }

        switch (n) {
        case 7: {
            val = (val << 4) + conv.template from<16>(*first++);
            WJR_FALLTHROUGH;
        }
        case 6: {
            val = (val << 4) + conv.template from<16>(*first++);
            WJR_FALLTHROUGH;
        }
        case 5: {
            val = (val << 4) + conv.template from<16>(*first++);
            WJR_FALLTHROUGH;
        }
        case 4: {
            val <<= 16;
            val += __from_chars_unroll_4<16>(first, conv);
            first += 4;
            break;
        }
        case 3: {
            val = (val << 4) + conv.template from<16>(*first++);
            WJR_FALLTHROUGH;
        }
        case 2: {
            val = (val << 4) + conv.template from<16>(*first++);
            WJR_FALLTHROUGH;
        }
        case 1: {
            val = (val << 4) + conv.template from<16>(*first++);
            break;
        }
        default: {
            WJR_UNREACHABLE();
            break;
        }
        }
    }

public:
    template <typename UnsignedValue, typename Converter,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    void operator()(const uint8_t *first, const uint8_t *last, UnsignedValue &val,
                    Converter conv) const {
        return fn(first, last, val, conv);
    }
};

template <>
class __unsigned_from_chars_unchecked_fn<1> {};

template <>
class __unsigned_from_chars_unchecked_fn<10> {
    template <typename UnsignedValue, typename Converter>
    static void fn(const uint8_t *first, const uint8_t *last, UnsignedValue &val,
                   Converter conv) {
        constexpr auto nd = std::numeric_limits<UnsignedValue>::digits10 + 1;

        auto n = std::distance(first, last);
        WJR_ASSUME(1 <= n && n <= nd);

        if constexpr (nd >= 8) {
            if (WJR_UNLIKELY(n >= 8)) {
                if (WJR_UNLIKELY(n >= 16)) {
                    val = __from_chars_unroll_16<10>(first, conv);
                    first += 16;
                    n -= 16;
                } else {
                    val = __from_chars_unroll_8<10>(first, conv);
                    first += 8;
                    n -= 8;
                }

                if (WJR_UNLIKELY(n == 0)) {
                    return;
                }
            }
        }

        switch (n) {
        case 7: {
            val = val * 10 + conv.template from<10>(*first++);
            WJR_FALLTHROUGH;
        }
        case 6: {
            val = val * 10 + conv.template from<10>(*first++);
            WJR_FALLTHROUGH;
        }
        case 5: {
            val = val * 10 + conv.template from<10>(*first++);
            WJR_FALLTHROUGH;
        }
        case 4: {
            val = (val * 10000) + __from_chars_unroll_4<10>(first, conv);
            break;
        }
        case 3: {
            val = val * 10 + conv.template from<10>(*first++);
            WJR_FALLTHROUGH;
        }
        case 2: {
            val = val * 10 + conv.template from<10>(*first++);
            WJR_FALLTHROUGH;
        }
        case 1: {
            val = val * 10 + conv.template from<10>(*first++);
            WJR_FALLTHROUGH;
        }
        case 0: {
            break;
        }
        default: {
            WJR_UNREACHABLE();
            break;
        }
        }
    }

public:
    template <typename UnsignedValue, typename Converter,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    void operator()(const uint8_t *first, const uint8_t *last, UnsignedValue &val,
                    Converter conv) const {
        return fn(first, last, val, conv);
    }
};

template <typename Value, typename IBase, typename Converter,
          WJR_REQUIRES(is_nonbool_integral_v<Value>)>
void __fast_from_chars_unchecked_impl(const uint8_t *first, const uint8_t *last,
                                      Value &val, IBase ibase, Converter conv) {
    int sign = 0;

    if constexpr (std::is_signed_v<Value>) {
        WJR_ASSERT(first != last);

        if (*first == '-') {
            ++first;
            sign = 1;
        }
    }

    std::make_unsigned_t<Value> uVal = 0;

    const unsigned int base = ibase;

    switch (base) {
    case 2: {
        __unsigned_from_chars_unchecked<2>(first, last, uVal, conv);
        break;
    }
    case 8: {
        __unsigned_from_chars_unchecked<8>(first, last, uVal, conv);
        break;
    }
    case 16: {
        __unsigned_from_chars_unchecked<16>(first, last, uVal, conv);
        break;
    }
    case 10: {
        __unsigned_from_chars_unchecked<10>(first, last, uVal, conv);
        break;
    }
    default: {
        WJR_UNREACHABLE();
        break;
    }
    }

    if (sign) {
        val = -static_cast<Value>(uVal);
    } else {
        val = static_cast<Value>(uVal);
    }
}

template <typename Iter, typename Value, typename IBase, typename Converter,
          WJR_REQUIRES(is_nonbool_integral_v<Value>)>
void __from_chars_unchecked_impl(Iter first, Iter last, Value &val, IBase ibase,
                                 Converter conv) {
    const auto __first = reinterpret_cast<const uint8_t *>((to_address)(first));
    const auto __last = reinterpret_cast<const uint8_t *>((to_address)(last));
    __fast_from_chars_unchecked_impl(__first, __last, val, ibase, conv);
}

template <typename Iter, typename Value, typename BaseType = unsigned int,
          BaseType IBase = 10, typename Converter = char_converter_t,
          WJR_REQUIRES(convert_details::__is_fast_convert_iterator_v<Iter>
                           &&convert_details::__is_valid_converter_v<Value, Converter>)>
void from_chars_unchecked(Iter first, Iter last, Value &val,
                          std::integral_constant<BaseType, IBase> = {},
                          Converter conv = {}) {
    __from_chars_unchecked_impl(first, last, val,
                                std::integral_constant<unsigned int, IBase>(), conv);
}

template <typename Iter, typename Value, typename Converter,
          WJR_REQUIRES(convert_details::__is_fast_convert_iterator_v<Iter>
                           &&convert_details::__is_valid_converter_v<Value, Converter>)>
void from_chars_unchecked(Iter first, Iter last, Value &val, unsigned int base,
                          Converter conv = {}) {
    if (WJR_BUILTIN_CONSTANT_P(base)) {
        switch (base) {
        case 2: {
            __from_chars_unchecked_impl(first, last, val,
                                        std::integral_constant<unsigned int, 2>(), conv);
            return;
        }
        case 8: {
            __from_chars_unchecked_impl(first, last, val,
                                        std::integral_constant<unsigned int, 8>(), conv);
            return;
        }
        case 16: {
            __from_chars_unchecked_impl(first, last, val,
                                        std::integral_constant<unsigned int, 16>(), conv);
            return;
        }
        case 10: {
            __from_chars_unchecked_impl(first, last, val,
                                        std::integral_constant<unsigned int, 10>(), conv);
            return;
        }
        default: {
            break;
        }
        }
    }

    __from_chars_unchecked_impl(first, last, val, base, conv);
}

template <uint64_t Base>
struct __unsigned_from_chars_fn {};

template <uint64_t Base>
inline constexpr __unsigned_from_chars_fn<Base> __unsigned_from_chars{};

template <>
struct __unsigned_from_chars_fn<2> {
    template <typename UnsignedValue, typename Converter,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    int operator()(const uint8_t *&first, const uint8_t *last, UnsignedValue &value,
                   Converter conv) const {
        constexpr auto nd = std::numeric_limits<UnsignedValue>::digits;
        constexpr auto zero = conv.template to<2>(0);
        constexpr auto one = conv.template to<2>(1);

        if (first == last) {
            return 2;
        }

        uint8_t ch = first[0];

        if (ch == one) {
        } else if (ch == zero) {
            do {
                ++first;
                if (first == last) {
                    return 1;
                }

                ch = first[0];
            } while (ch == zero);
            if (ch != one) {
                return 1;
            }
        } else {
            return 2;
        }

        const size_t n = std::distance(first, last);
        ch = 1;
        size_t idx = 0;

        do {
            value = value << 1 | ch;
            ++idx;
            if (idx == n) {
                break;
            }

            ch = conv.template from<2>(first[idx]);
        } while (ch < 2);

        first += idx;
        return idx <= nd;
    }
};

template <>
struct __unsigned_from_chars_fn<10> {
    template <typename UnsignedValue, typename Converter,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    int operator()(const uint8_t *&first, const uint8_t *last, UnsignedValue &value,
                   Converter conv) const {
        constexpr auto zero = conv.template to<10>(0);
        constexpr auto nine = conv.template to<10>(9);

        constexpr auto __try_matches = [](uint8_t &ch) {
            if constexpr (zero != 0) {
                return ch <= nine && !sub_overflow(ch, zero, ch);
            } else {
                return ch <= nine;
            }
        };

        constexpr auto __matches = [](uint8_t ch) {
            if constexpr (zero != 0) {
                return zero <= ch && ch <= nine;
            } else {
                return ch <= nine;
            }
        };

        if (first == last) {
            return 2;
        }

        uint8_t ch = *first;

        if (WJR_UNLIKELY(!__try_matches(ch))) {
            return 2;
        }

        do {
            ++first;
            if (WJR_UNLIKELY(mul_overflow(value, 10, value) ||
                             add_overflow(value, ch, value))) {
                while (first != last && __matches(*first)) {
                    ++first;
                }

                return 0;
            }

            if (first == last) {
                break;
            }

            ch = *first;
        } while (__try_matches(ch));

        return 1;
    }
};

template <typename Value, typename IBase, typename Converter,
          WJR_REQUIRES(is_nonbool_integral_v<Value>)>
from_chars_result<const uint8_t *> __fast_from_chars_impl(const uint8_t *first,
                                                          const uint8_t *last, Value &val,
                                                          IBase ibase, Converter conv) {
    constexpr auto is_signed = std::is_signed_v<Value>;

    int sign = 0;

    if constexpr (is_signed) {
        if (first != last && *first == '-') {
            ++first;
            sign = 1;
        }
    }

    using UnsignedValue = std::make_unsigned_t<Value>;

    UnsignedValue uVal = 0;

    const unsigned int base = ibase;
    from_chars_result<const uint8_t *> ret{first, std::errc{}};
    int valid;

    switch (base) {
    case 2: {
        valid = __unsigned_from_chars<2>(first, last, uVal, conv);
        break;
    }
    case 10: {
        valid = __unsigned_from_chars<10>(first, last, uVal, conv);
        break;
    }
    default: {
        return {first, std::errc::invalid_argument};
    }
    }

    if (WJR_UNLIKELY(valid == 2)) {
        ret.ec = std::errc::invalid_argument;
    } else {
        ret.ptr = first;
        if (!valid) {
            ret.ec = std::errc::result_out_of_range;
        } else {
            if constexpr (is_signed) {
                if (sign) {
                    if (uVal >
                        static_cast<UnsignedValue>(std::numeric_limits<Value>::min())) {
                        ret.ec = std::errc::result_out_of_range;
                    } else {
                        val = -static_cast<Value>(uVal);
                    }
                } else {
                    if (uVal >
                        static_cast<UnsignedValue>(std::numeric_limits<Value>::max())) {
                        ret.ec = std::errc::result_out_of_range;
                    } else {
                        val = static_cast<Value>(uVal);
                    }
                }
            } else {
                val = uVal;
            }
        }
    }

    return ret;
}

template <typename Value, typename IBase, typename Converter,
          WJR_REQUIRES(is_nonbool_integral_v<Value>)>
from_chars_result<const char *> __from_chars_impl(const char *first, const char *last,
                                                  Value &val, IBase ibase,
                                                  Converter conv) {
    const auto __first = reinterpret_cast<const uint8_t *>(first);
    const auto __last = reinterpret_cast<const uint8_t *>(last);
    const auto ret = __fast_from_chars_impl(__first, __last, val, ibase, conv);
    return {reinterpret_cast<const char *>(ret.ptr), ret.ec};
}

template <typename Value, typename BaseType = unsigned int, BaseType IBase = 10,
          typename Converter = char_converter_t,
          WJR_REQUIRES(convert_details::__is_valid_converter_v<Value, Converter>)>
from_chars_result<const char *>
from_chars(const char *first, const char *last, Value &val,
           std::integral_constant<BaseType, IBase> = {}, Converter conv = {}) {
    return __from_chars_impl(first, last, val,
                             std::integral_constant<unsigned int, IBase>(), conv);
}

template <typename Value, typename Converter = char_converter_t,
          WJR_REQUIRES(convert_details::__is_valid_converter_v<Value, Converter>)>
from_chars_result<const char *> from_chars(const char *first, const char *last,
                                           Value &val, unsigned int base,
                                           Converter conv = {}) {
    if (WJR_BUILTIN_CONSTANT_P(base)) {
        switch (base) {
        case 2: {
            return __from_chars_impl(first, last, val,
                                     std::integral_constant<unsigned int, 2>(), conv);
        }
        case 10: {
            return __from_chars_impl(first, last, val,
                                     std::integral_constant<unsigned int, 10>(), conv);
        }
        default: {
            break;
        }
        }
    }

    return __from_chars_impl(first, last, val, base, conv);
}

template <typename Converter>
size_t __biginteger_from_chars_2_impl(const uint8_t *first, size_t n, uint64_t *up,
                                      Converter conv) {
    const size_t hbits = (n - 1) % 64 + 1;
    const size_t len = (n - 1) / 64 + 1;

    uint64_t x = 0;
    up += len;

    __unsigned_from_chars_unchecked<2>(first, first + hbits, x, conv);
    first += hbits;

    *--up = x;

    size_t idx = len - 1;

    if (idx) {
        do {
            x = 0;

            for (int i = 0; i < 4; ++i) {
                x = (x << 16) + __from_chars_unroll_16<2>(first, conv);
                first += 16;
            }

            *--up = x;
        } while (WJR_LIKELY(--idx));
    }

    return len;
}

template <typename Converter>
size_t __biginteger_from_chars_8_impl(const uint8_t *first, size_t n, uint64_t *up,
                                      Converter conv) {
    size_t len = (n * 3 + 63) / 64;
    size_t lbits = (64 * (len - 1)) / 3;
    size_t rest = (64 * (len - 1)) % 3;
    const size_t hbits = n - lbits - 1;

    auto unroll = [conv](uint64_t &x, auto &first) {
        auto x0 = conv.template from<8>(first[0]);
        auto x1 = conv.template from<8>(first[1]);
        auto x2 = conv.template from<8>(first[2]);
        auto x3 = conv.template from<8>(first[3]);

        x = x << 12 | x0 << 9 | x1 << 6 | x2 << 3 | x3;
        first += 4;
    };

    uint64_t x = 0;
    up += len;
    size_t idx = len - 1;

    if (WJR_UNLIKELY(hbits == 0)) {
    } else {
        __unsigned_from_chars_unchecked<8>(first, first + hbits, x, conv);
        first += hbits;
    }

    uint64_t nx = conv.template from<8>(*first++);
    switch (rest) {
    case 0: {
        *--up = x << 3 | nx;
        x = 0;
        break;
    }
    case 1: {
        x = x << 2 | nx >> 1;
        if (WJR_UNLIKELY(x == 0)) {
            --len;
        }

        *--up = x;
        x = nx & 1;
        break;
    }
    case 2: {
        x = x << 1 | nx >> 2;
        if (WJR_UNLIKELY(x == 0)) {
            --len;
        }
        *--up = x;
        x = nx & 3;
        break;
    }
    default: {
        WJR_UNREACHABLE();
        break;
    }
    }

    if (idx) {
        do {
            for (int i = 0; i < 5; ++i) {
                unroll(x, first);
            }

            switch (rest) {
            case 0: {
                x = x << 3 | conv.template from<8>(*first++);
                uint64_t nx = conv.template from<8>(*first++);
                x = x << 1 | nx >> 2;
                *--up = x;
                x = nx & 3;
                rest = 2;
                break;
            }
            case 1: {
                x = x << 3 | conv.template from<8>(*first++);
                *--up = x;
                x = 0;
                rest = 0;
                break;
            }
            case 2: {
                uint64_t nx = conv.template from<8>(*first++);
                x = x << 2 | nx >> 1;
                *--up = x;
                x = nx & 1;
                rest = 1;
                break;
            }
            default: {
                WJR_UNREACHABLE();
                break;
            }
            }

        } while (WJR_LIKELY(--idx));
    }

    return len;
}

template <typename Converter>
size_t __biginteger_from_chars_16_impl(const uint8_t *first, size_t n, uint64_t *up,
                                       Converter conv) {
    const size_t hbits = (n - 1) % 16 + 1;
    size_t len = (n - 1) / 16 + 1;

    auto unroll = [conv](uint64_t &x, auto &first) {
        auto x0 = conv.template from<16>(first[0]);
        auto x1 = conv.template from<16>(first[1]);
        auto x2 = conv.template from<16>(first[2]);
        auto x3 = conv.template from<16>(first[3]);

        x = x << 16 | x0 << 12 | x1 << 8 | x2 << 4 | x3;
        first += 4;
    };

    uint64_t x = 0;
    up += len;

    __unsigned_from_chars_unchecked<16>(first, first + hbits, x, conv);
    first += hbits;

    *--up = x;

    size_t idx = len - 1;

    if (idx) {
        do {
            x = 0;

            for (int i = 0; i < 4; ++i) {
                unroll(x, first);
            }

            *--up = x;
        } while (WJR_LIKELY(--idx));
    }

    return len;
}

template <typename Converter>
size_t basecase_from_chars_10(const uint8_t *first, size_t n, uint64_t *up,
                              Converter conv) {
    uint64_t x = 0;

    if (n <= 19) {
        __unsigned_from_chars_unchecked<10>(first, first + n, x, conv);
        up[0] = x;

        return up[0] != 0;
    }

    size_t m = (n - 1) % 19 + 1;

    __unsigned_from_chars_unchecked<10>(first, first + m, x, conv);
    up[0] = x;

    first += m;
    n -= m;

    m = up[0] != 0;

    do {
        x = 0;

        x = __from_chars_unroll_16<10>(first, conv);
        first += 16;

        x = x * 10 + conv.template from<10>(*first++);
        x = x * 10 + conv.template from<10>(*first++);
        x = x * 10 + conv.template from<10>(*first++);

        uint64_t cf;

        if (WJR_LIKELY(m == 0)) {
            cf = x;
        } else {
            cf = mul_1(up, up, m, div2by1_divider_noshift_of_big_base_10.get_divisor());
            cf += addc_1(up, up, m, x);
        }

        if (WJR_LIKELY(cf != 0)) {
            up[m++] = cf;
        }

        n -= 19;
    } while (WJR_LIKELY(n != 0));

    return m;
}

template <typename Converter>
size_t basecase_from_chars(const uint8_t *first, size_t n, uint64_t *up,
                           unsigned int base, Converter conv) {
    if (base == 10) {
        return basecase_from_chars_10(first, n, up, conv);
    } else {
        return 0;
    }
}

template <typename Converter>
size_t dc_from_chars(const uint8_t *first, size_t n, uint64_t *up,
                     precompute_chars_convert_t *pre, uint64_t *stk, Converter conv) {
    const size_t lo = pre->digits_in_base;
    if (n <= lo) {
        if (n < dc_bignum_from_chars_threshold) {
            return basecase_from_chars(first, n, up, pre->base, conv);
        } else {
            return dc_from_chars(first, n, up, pre - 1, stk, conv);
        }
    }

    const size_t hi = n - lo;
    size_t hn, ln;

    if (hi < dc_bignum_from_chars_threshold) {
        hn = basecase_from_chars(first, hi, stk, pre->base, conv);
    } else {
        hn = dc_from_chars(first, hi, stk, pre - (lo * 2 >= n), up, conv);
    }

    const size_t ps = pre->shift;
    const size_t pn = pre->n;

    if (WJR_LIKELY(hn != 0)) {
        if (pn >= hn) {
            mul_s(up + ps, pre->ptr, pn, stk, hn);
        } else {
            mul_s(up + ps, stk, hn, pre->ptr, pn);
        }
        set_n(up, 0, ps);
    }

    std::advance(first, hi);
    if (lo < dc_bignum_from_chars_threshold) {
        ln = basecase_from_chars(first, lo, stk, pre->base, conv);
    } else {
        ln = dc_from_chars(first, lo, stk, pre - (lo * 2 >= n), stk + pn + ps + 1, conv);
    }

    WJR_ASSERT(ps + pn + 1 >= ln);

    if (WJR_LIKELY(hn != 0)) {
        if (WJR_LIKELY(ln != 0)) {
            auto cf = addc_s(up, up, ps + pn + hn, stk, ln);
            WJR_ASSERT(cf == 0);
            (void)(cf);
        }

        n = ps + pn + hn;
        return n - (up[n - 1] == 0);
    }

    if (WJR_LIKELY(ln != 0)) {
        std::copy_n(stk, ln, up);
    }

    return ln;
}

template <typename Converter>
uint64_t *__biginteger_from_chars_impl(const uint8_t *first, const uint8_t *last,
                                       uint64_t *up, unsigned int base,
                                       Converter conv = {}) {
    WJR_ASSERT(base <= 36 && (is_zero_or_single_bit(base) || base == 10));

    const size_t n = std::distance(first, last);

    if (is_zero_or_single_bit(base)) {
        switch (base) {
        case 2: {
            return up + __biginteger_from_chars_2_impl(first, n, up, conv);
        }
        case 8: {
            return up + __biginteger_from_chars_8_impl(first, n, up, conv);
        }
        case 16: {
            return up + __biginteger_from_chars_16_impl(first, n, up, conv);
        }
        default: {
            WJR_UNREACHABLE();
            return up;
        }
        }
    }

    if (WJR_LIKELY(n < dc_bignum_from_chars_precompute_threshold)) {
        return up + basecase_from_chars(first, n, up, base, conv);
    }

    const auto per_digits = precompute_chars_convert_16n_ptr[base]->digits_in_one_base;

    precompute_chars_convert_t pre[64 - 3];

    unique_stack_allocator stkal(math_details::stack_alloc);
    const size_t un = n / per_digits + 1;
    auto stk =
        static_cast<uint64_t *>(stkal.allocate((un * 16 / 5 + 192) * sizeof(uint64_t)));
    const auto mpre = precompute_chars_convert(pre, un, base, stk);
    stk += un * 8 / 5 + 128;
    return up + dc_from_chars(first, n, up, mpre, stk, conv);
}

/**
 * @brief Convert a string to a biginteger by a given base.
 *
 * @tparam Iter Input iterator type
 * @param[in] first Input iterator
 * @param[in] last Input iterator
 * @param[out] up Pointer to the biginteger
 * @param[in] base Base of the input string. Range: `[2, 36]`,
 * Only support 10 and power of two currently.
 * @return Pointer after the conversion
 */
template <typename Iter, typename Converter = char_converter_t,
          WJR_REQUIRES(convert_details::__is_fast_convert_iterator_v<Iter>)>
uint64_t *biginteger_from_chars(Iter first, Iter last, uint64_t *up,
                                unsigned int base = 10, Converter conv = {}) {
    WJR_ASSERT(base <= 36 && (is_zero_or_single_bit(base) || base == 10));

    const auto __first = reinterpret_cast<const uint8_t *>((to_address)(first));
    const auto __last = reinterpret_cast<const uint8_t *>((to_address)(last));

    return __biginteger_from_chars_impl(__first, __last, up, base, conv);
}

} // namespace wjr

#endif // WJR_MATH_CONVERT_HPP__
#ifndef WJR_MATH_COPY_HPP__
#define WJR_MATH_COPY_HPP__

// Already included

#if defined(WJR_X86)
#ifndef WJR_X86_MATH_COPY_HPP__
#define WJR_X86_MATH_COPY_HPP__

#include <algorithm>

// Already included

#ifndef WJR_X86
#error "x86 required"
#endif

namespace wjr {

#if WJR_HAS_SIMD(SSE2) && WJR_HAS_SIMD(X86_SIMD)
#define WJR_HAS_BUILTIN_COPY_N WJR_HAS_DEF
#define WJR_HAS_BUILTIN_COPY_C WJR_HAS_DEF
#define WJR_HAS_BUILTIN_COPY_BACKWARD_N WJR_HAS_DEF
#define WJR_HAS_BUILTIN_COPY_BACKWARD_C WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(COPY_C)

template <size_t N>
void builtin_copy_c_bytes_impl(const uint8_t *src, uint8_t *dst) {
    if constexpr (N == 2) {
        const auto x = read_memory<uint16_t>(src, endian::native);
        write_memory<uint16_t>(dst, x, endian::native);
        return;
    }

    if constexpr (N == 4) {
        const auto x = read_memory<uint32_t>(src, endian::native);
        write_memory<uint32_t>(dst, x, endian::native);
        return;
    }

    if constexpr (N == 8) {
        const auto x = read_memory<uint64_t>(src, endian::native);
        write_memory<uint64_t>(dst, x, endian::native);
        return;
    }

    if constexpr (N == 16) {
        const auto x = sse::loadu((__m128i *)(src));
        sse::storeu((__m128i *)(dst), x);
        return;
    }

    if constexpr (N == 32) {
#if WJR_HAS_SIMD(AVX2)
        const auto x = avx::loadu((__m256i *)(src));
        avx::storeu((__m256i *)(dst), x);
#else
        builtin_copy_c_bytes<16>(dst, src);
        builtin_copy_c_bytes<16>(dst + 16, src + 16);
#endif
        return;
    }

    WJR_UNREACHABLE();
}

template <size_t N, typename T>
void builtin_copy_c_bytes(const T *src, T *dst) {
    builtin_copy_c_bytes_impl<N * sizeof(T)>(reinterpret_cast<const uint8_t *>(src),
                                             reinterpret_cast<uint8_t *>(dst));
}

template <size_t N, typename T>
void builtin_copy_c(const T *src, T *dst) {
    if constexpr (N == 0) {
        return;
    }

    if constexpr (N == 1) {
        dst[0] = src[0];
        return;
    }

    if constexpr (N > 8) {
        std::copy(src, src + N, dst);
        return;
    }

    if constexpr (N == 2) {
        builtin_copy_c_bytes<2>(src, dst);
    } else if constexpr (N == 3) {
        builtin_copy_c_bytes<2>(src, dst);
        builtin_copy_c_bytes<1>(src + 2, dst + 2);
    } else if constexpr (N == 4) {
        builtin_copy_c_bytes<4>(src, dst);
    } else if constexpr (N == 5) {
        builtin_copy_c_bytes<4>(src, dst);
        builtin_copy_c_bytes<1>(src + 4, dst + 4);
    } else if constexpr (N == 6) {
        builtin_copy_c_bytes<4>(src, dst);
        builtin_copy_c_bytes<2>(src + 4, dst + 4);
    } else if constexpr (N == 7) {
        builtin_copy_c_bytes<4>(src, dst);
        builtin_copy_c_bytes<4>(src + 3, dst + 3);
    } else if constexpr (N == 8) {
        builtin_copy_c_bytes<8>(src, dst);
    }
}

#endif

#if WJR_HAS_BUILTIN(COPY_BACKWARD_C)

template <size_t N>
void builtin_copy_backward_c_bytes_impl(const uint8_t *s_last, uint8_t *d_last) {
    if constexpr (N == 2) {
        const auto x = read_memory<uint16_t>(s_last - 2, endian::native);
        write_memory<uint16_t>(d_last - 2, x, endian::native);
        return;
    }

    if constexpr (N == 4) {
        const auto x = read_memory<uint32_t>(s_last - 4, endian::native);
        write_memory<uint32_t>(d_last - 4, x, endian::native);
        return;
    }

    if constexpr (N == 8) {
        const auto x = read_memory<uint64_t>(s_last - 8, endian::native);
        write_memory<uint64_t>(d_last - 8, x, endian::native);
        return;
    }

    if constexpr (N == 16) {
        const auto x = sse::loadu((__m128i *)(s_last - 16));
        sse::storeu((__m128i *)(d_last - 16), x);
        return;
    }

    if constexpr (N == 32) {
#if WJR_HAS_SIMD(AVX2)
        const auto x = avx::loadu((__m256i *)(s_last - 32));
        avx::storeu((__m256i *)(d_last - 32), x);
#else
        builtin_copy_backward_c_bytes<16>(s_last, d_last);
        builtin_copy_backward_c_bytes<16>(s_last - 16, d_last - 16);
#endif
    }

    WJR_UNREACHABLE();
}

template <size_t N, typename T>
void builtin_copy_backward_c_bytes(const T *s_last, T *d_last) {
    builtin_copy_backward_c_bytes_impl<N * sizeof(T)>(
        reinterpret_cast<const uint8_t *>(s_last), reinterpret_cast<uint8_t *>(d_last));
}

template <size_t N, typename T>
void builtin_copy_backward_c(const T *s_last, T *d_last) {
    if constexpr (N == 0) {
        return;
    }

    if constexpr (N == 1) {
        d_last[-1] = s_last[-1];
        return;
    }

    if constexpr (N > 8) {
        std::copy_backward(s_last - N, s_last, d_last);
        return;
    }

    if constexpr (N == 2) {
        builtin_copy_backward_c_bytes<2>(s_last, d_last);
    } else if constexpr (N == 3) {
        builtin_copy_backward_c_bytes<2>(s_last, d_last);
        builtin_copy_backward_c_bytes<1>(s_last - 2, d_last - 2);
    } else if constexpr (N == 4) {
        builtin_copy_backward_c_bytes<4>(s_last, d_last);
    } else if constexpr (N == 5) {
        builtin_copy_backward_c_bytes<4>(s_last, d_last);
        builtin_copy_backward_c_bytes<1>(s_last - 4, d_last - 4);
    } else if constexpr (N == 6) {
        builtin_copy_backward_c_bytes<4>(s_last, d_last);
        builtin_copy_backward_c_bytes<2>(s_last - 4, d_last - 4);
    } else if constexpr (N == 7) {
        builtin_copy_backward_c_bytes<4>(s_last, d_last);
        builtin_copy_backward_c_bytes<4>(s_last - 3, d_last - 3);
    } else if constexpr (N == 8) {
        builtin_copy_backward_c_bytes<8>(s_last, d_last);
    }
}

#endif

} // namespace wjr

#endif // WJR_X86_MATH_COPY_HPP__
#endif

namespace wjr {

/// @private
template <typename InputIt, typename OutputIt>
struct __is_builtin_copy
    : std::conjunction<
          std::is_trivially_copyable<iterator_value_t<InputIt>>,
          std::is_same<iterator_value_t<InputIt>, iterator_value_t<OutputIt>>,
          is_contiguous_iterator<InputIt>, is_contiguous_iterator<OutputIt>> {};

/// @private
template <typename InputIt, typename OutputIt>
inline constexpr bool __is_builtin_copy_v = __is_builtin_copy<InputIt, OutputIt>::value;

template <size_t N, typename InputIt, typename OutputIt>
OutputIt fallback_copy_c(InputIt first, OutputIt dst) {
    return std::copy(first, first + N, dst);
}

template <size_t N, typename InputIt, typename OutputIt>
OutputIt copy_c(InputIt first, OutputIt dst) {
#if WJR_HAS_BUILTIN(COPY_C)
    if constexpr (__is_builtin_copy_v<InputIt, OutputIt>) {
        builtin_copy_c<N>(to_address(first), to_address(dst));
        return dst + N;
    } else {
        return fallback_copy_c<N>(first, dst);
    }

#else
    return fallback_copy_c<N>(first, dst);
#endif
}

template <size_t N, typename InputIt, typename OutputIt>
OutputIt fallback_copy_backward_c(InputIt s_last, OutputIt d_last) {
    return std::copy_backward(s_last - N, s_last, d_last);
}

template <size_t N, typename InputIt, typename OutputIt>
OutputIt copy_backward_c(InputIt s_last, OutputIt d_last) {
#if WJR_HAS_BUILTIN(COPY_BACKWARD_C)
    if constexpr (__is_builtin_copy_v<InputIt, OutputIt>) {
        builtin_copy_backward_c<N>(to_address(s_last), to_address(d_last));
        return d_last - N;
    } else {
        return fallback_copy_backward_c<N>(s_last, d_last);
    }
#else
    return fallback_copy_backward_c<N>(s_last, d_last);
#endif
}

} // namespace wjr

#endif // WJR_MATH_COPY_HPP__
#ifndef WJR_MATH_NEG_HPP__
#define WJR_MATH_NEG_HPP__

#ifndef WJR_MATH_NOT_HPP__
#define WJR_MATH_NOT_HPP__

// Already included

#if defined(WJR_X86)
#ifndef WJR_X86_MATH_NOT_HPP__
#define WJR_X86_MATH_NOT_HPP__

// Already included
// Already included

#ifndef WJR_X86
#error "x86 required"
#endif

namespace wjr {

#if WJR_HAS_SIMD(SSE2) && WJR_HAS_SIMD(X86_SIMD)
#define WJR_HAS_BUILTIN_COMPLEMENT_N WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(COMPLEMENT_N)

template <typename T>
WJR_COLD void large_builtin_complement_n(T *dst, const T *src, size_t n) {
    constexpr auto is_avx = WJR_HAS_SIMD(AVX2);

    using simd = std::conditional_t<is_avx, avx, sse>;
    using simd_int = typename simd::int_type;
    constexpr auto simd_width = simd::width();
    constexpr auto type_width = simd_width / 64;

    const uintptr_t ptr = reinterpret_cast<uintptr_t>(dst);
    WJR_ASSUME(ptr % sizeof(T) == 0);
    const size_t offset = __align_up_offset(ptr, 32) / sizeof(T);

    WJR_ASSUME(offset < 4);

    const auto y = sse::ones();

    switch (offset) {
    case 0: {
        break;
    }
    case 1: {
        dst[0] = ~src[0];

        ++dst;
        ++src;
        --n;
        break;
    }
    case 2: {
        sse::storeu((__m128i *)(dst), sse::Xor(sse::loadu((__m128i *)(src)), y));

        dst += 2;
        src += 2;
        n -= 2;
        break;
    }

    case 3: {
        sse::storeu((__m128i *)(dst), sse::Xor(sse::loadu((__m128i *)(src)), y));
        dst[2] = ~src[2];

        dst += 3;
        src += 3;
        n -= 3;
        break;
    }

    default: {
        WJR_UNREACHABLE();
    }
    }

    auto z = broadcast<__m128i_t, typename simd::int_tag_type>(y);

    size_t idx = 0;
    size_t m = n & (-type_width * 4);

    WJR_ASSUME(idx != m);

    do {
        auto x0 = simd::loadu((simd_int *)(src + idx));
        auto x1 = simd::loadu((simd_int *)(src + idx + type_width));
        auto x2 = simd::loadu((simd_int *)(src + idx + type_width * 2));
        auto x3 = simd::loadu((simd_int *)(src + idx + type_width * 3));

        simd::store((simd_int *)(dst + idx), simd::Xor(x0, z));
        simd::store((simd_int *)(dst + idx + type_width), simd::Xor(x1, z));
        simd::store((simd_int *)(dst + idx + type_width * 2), simd::Xor(x2, z));
        simd::store((simd_int *)(dst + idx + type_width * 3), simd::Xor(x3, z));

        idx += type_width * 4;
    } while (idx != m);

    if (WJR_UNLIKELY(n == m)) {
        return;
    }

    dst += m;
    src += m;
    n -= m;

    m = n / type_width;
    WJR_ASSUME(m < 4);

    switch (m) {
    case 3: {
        simd::store((simd_int *)(dst), simd::Xor(simd::loadu((simd_int *)(src)), z));
        WJR_FALLTHROUGH;
    }
    case 2: {
        simd::store((simd_int *)(dst + type_width * (m - 2)),
                    simd::Xor(simd::loadu((simd_int *)(src + type_width * (m - 2))), z));
        WJR_FALLTHROUGH;
    }
    case 1: {
        simd::store((simd_int *)(dst + type_width * (m - 1)),
                    simd::Xor(simd::loadu((simd_int *)(src + type_width * (m - 1))), z));
        WJR_FALLTHROUGH;
    }
    case 0: {
        break;
    }
    default: {
        WJR_UNREACHABLE();
    }
    }

    m = n & (-type_width);

    if (WJR_UNLIKELY(n == m)) {
        return;
    }

    WJR_ASSUME(n - m < 4);

    switch (n - m) {
    case 1: {
        dst[m] = ~src[m];
        break;
    }
    case 2: {
        sse::store((__m128i *)(dst + m), sse::Xor(sse::loadu((__m128i *)(src + m)), y));
        break;
    }

    case 3: {
        sse::store((__m128i *)(dst + m), sse::Xor(sse::loadu((__m128i *)(src + m)), y));
        dst[m + 2] = ~src[m + 2];
        break;
    }

    default: {
        WJR_UNREACHABLE();
    }
    }

    return;
}

template <typename T>
WJR_INTRINSIC_INLINE void builtin_complement_n(T *dst, const T *src, size_t n) {
    static_assert(sizeof(T) == 8, "");

    if (WJR_UNLIKELY(n < 4)) {
        switch (n) {
        case 3: {
            dst[0] = ~src[0];
            WJR_FALLTHROUGH;
        }
        case 2: {
            dst[n - 2] = ~src[n - 2];
            WJR_FALLTHROUGH;
        }
        case 1: {
            dst[n - 1] = ~src[n - 1];
            WJR_FALLTHROUGH;
        }
        case 0: {
            break;
        }
        }

        return;
    }

    if (WJR_UNLIKELY(n >= 35)) {
        // Can be aligned
        // TODO : Align those that cannot be aligned with T through uint8_t
        if (WJR_LIKELY(reinterpret_cast<uintptr_t>(dst) % sizeof(T) == 0)) {
            return large_builtin_complement_n(dst, src, n);
        }
    }

    size_t idx = 0;

    const auto y = sse::ones();

    if (n & 4) {
        auto x0 = sse::loadu((__m128i *)(src + idx));
        auto x1 = sse::loadu((__m128i *)(src + idx + 2));

        sse::storeu((__m128i *)(dst + idx), sse::Xor(x0, y));
        sse::storeu((__m128i *)(dst + idx + 2), sse::Xor(x1, y));

        idx += 4;
    }

    if (n & 2) {
        sse::storeu((__m128i *)(dst + idx),
                    sse::Xor(sse::loadu((__m128i *)(src + idx)), y));

        idx += 2;
    }

    if (n & 1) {
        dst[idx] = ~src[idx];

        ++idx;
    }

    if (WJR_UNLIKELY(idx == n)) {
        return;
    }

    WJR_ASSUME((n - idx) % 8 == 0);

    do {
        auto x0 = sse::loadu((__m128i *)(src + idx));
        auto x1 = sse::loadu((__m128i *)(src + idx + 2));
        auto x2 = sse::loadu((__m128i *)(src + idx + 4));
        auto x3 = sse::loadu((__m128i *)(src + idx + 6));

        sse::storeu((__m128i *)(dst + idx), sse::Xor(x0, y));
        sse::storeu((__m128i *)(dst + idx + 2), sse::Xor(x1, y));
        sse::storeu((__m128i *)(dst + idx + 4), sse::Xor(x2, y));
        sse::storeu((__m128i *)(dst + idx + 6), sse::Xor(x3, y));

        idx += 8;
    } while (idx != n);
}

#endif //

} // namespace wjr

#endif // WJR_X86_MATH_NOT_HPP__
#endif

namespace wjr {

template <typename T>
WJR_INTRINSIC_CONSTEXPR void fallback_complement_n(T *dst, const T *src, size_t n) {
    for (size_t i = 0; i < n; ++i) {
        dst[i] = ~src[i];
    }
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E void complement_n(T *dst, const T *src, size_t n) {
#if WJR_HAS_BUILTIN(COMPLEMENT_N)
    if constexpr (sizeof(T) == 8) {
        if (is_constant_evaluated()) {
            return fallback_complement_n(dst, src, n);
        }

        return builtin_complement_n(dst, src, n);
    } else {
        return fallback_complement_n(dst, src, n);
    }
#else
    return fallback_complement_n(dst, src, n);
#endif
}

} // namespace wjr

#endif // WJR_MATH_NOT_HPP__
// Already included

namespace wjr {

/*
  return true if src is all zero
  calculations : stable n instead of not + inc which maybe n * 2
*/
template <typename T>
WJR_INTRINSIC_CONSTEXPR_E bool negate_n(T *dst, const T *src, size_t n) {
    size_t idx = replace_find_not(dst, src, n, 0, 0);

    if (idx == n) {
        return true;
    }

    dst[idx] = -src[idx];
    complement_n(dst + idx + 1, src + idx + 1, n - idx - 1);
    return false;
}

} // namespace wjr

#endif // WJR_MATH_NEG_HPP__
#ifndef WJR_MATH_PREFIX_XOR_HPP__
#define WJR_MATH_PREFIX_XOR_HPP__

// Already included

#if defined(WJR_X86)
#ifndef WJR_X86_MATH_PREFIX_XOR_HPP__
#define WJR_X86_MATH_PREFIX_XOR_HPP__

// Already included
// Already included

namespace wjr {

#if WJR_HAS_SIMD(PCLMUL) && WJR_HAS_SIMD(X86_SIMD)
#define WJR_HAS_BUILTIN_PREFIX_XOR WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(PREFIX_XOR)

template <typename T>
T __builtin_prefix_xor(T x) {
    const __m128i __result =
        _mm_clmulepi64_si128((simd_cast<T, __m128i_t>(x)), sse::set1_epi8(0xFF), 0);
    return simd_cast<__m128i_t, T>(__result);
}

template <typename T>
T builtin_prefix_xor(T x) {
    constexpr auto nd = std::numeric_limits<T>::digits;
    if constexpr (nd < 32) {
        return static_cast<T>(__builtin_prefix_xor(static_cast<uint32_t>(x)));
    } else {
        return __builtin_prefix_xor(x);
    }
}

#endif

} // namespace wjr

#endif // WJR_X86_MATH_PREFIX_XOR_HPP__
#endif

namespace wjr {

template <typename T>
constexpr T fallback_prefix_xor(T x) {
    constexpr auto nd = std::numeric_limits<T>::digits;
    static_assert(nd <= 64, "Type T has more than 64 bits");

    x ^= x << 1;
    x ^= x << 2;
    x ^= x << 4;
    if constexpr (nd > 8) {
        x ^= x << 8;
    }
    if constexpr (nd > 16) {
        x ^= x << 16;
    }
    if constexpr (nd > 32) {
        x ^= x << 32;
    }

    return x;
}

template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T>)>
WJR_INTRINSIC_CONSTEXPR_E T prefix_xor(T x) {
#if WJR_HAS_BUILTIN(PREFIX_XOR)
    if (is_constant_evaluated() || WJR_BUILTIN_CONSTANT_P(x)) {
        return fallback_prefix_xor(x);
    }

    return builtin_prefix_xor(x);
#else
    return fallback_prefix_xor(x);
#endif
}

} // namespace wjr

#endif // WJR_MATH_PREFIX_XOR_HPP__
#ifndef WJR_MATH_RANDOM_HPP__
#define WJR_MATH_RANDOM_HPP__

#include <algorithm>
#include <random>

// Already included

namespace wjr {

template <typename Engine, typename = void>
struct __uniform_random_bit_generator_impl : std::false_type {};

template <typename Engine>
struct __uniform_random_bit_generator_impl<Engine,
                                           std::enable_if_t<has_invocable_v<Engine &>>>
    : std::conjunction<
          is_nonbool_unsigned_integral<std::invoke_result_t<Engine &>>,
          std::is_same<decltype(Engine::min()), std::invoke_result_t<Engine &>>,
          std::is_same<decltype(Engine::max()), std::invoke_result_t<Engine &>>> {};

template <typename Engine>
struct uniform_random_bit_generator : __uniform_random_bit_generator_impl<Engine> {};

template <typename Engine>
inline constexpr bool uniform_random_bit_generator_v =
    uniform_random_bit_generator<Engine>::value;

template <typename Engine>
struct biginteger_uniform_random_bit_generator
    : std::conjunction<
          __uniform_random_bit_generator_impl<Engine>,
          std::is_same<std::invoke_result_t<Engine &>, uint64_t>,
          std::bool_constant<Engine::min() == std::numeric_limits<uint64_t>::min()>,
          std::bool_constant<Engine::max() == std::numeric_limits<uint64_t>::max()>> {};

template <typename Engine>
inline constexpr bool biginteger_uniform_random_bit_generator_v =
    biginteger_uniform_random_bit_generator<Engine>::value;

template <typename Iter, typename Rand>
void random(Iter first, Iter last, Rand &&rd) {
    std::generate(first, last, std::ref(rd));
}

template <typename Iter, typename Rand>
void random_n(Iter First, size_t Count, Rand &&rd) {
    std::generate_n(First, Count, std::ref(rd));
}

} // namespace wjr

#endif // WJR_MATH_RANDOM_HPP__

#endif // WJR_MATH_HPP__
#ifndef WJR_SPAN_HPP__
#define WJR_SPAN_HPP__

#include <stdexcept>

// Already included
// Already included

namespace wjr {

/**
 * @brief A type representing a static-sized span.
 *
 * @tparam Extent The number of elements in the span.
 */
template <typename T, size_t Extent>
struct __span_static_storage {

    __span_static_storage() noexcept = default;
    __span_static_storage(const __span_static_storage &) noexcept = default;
    __span_static_storage &operator=(const __span_static_storage &) noexcept = default;

    __span_static_storage(T *p, WJR_MAYBE_UNUSED size_t s) noexcept : ptr(p) {
        WJR_ASSERT_L1(s == size);
    }

    T *ptr = nullptr;
    static constexpr size_t size = Extent;
};

/**
 * @brief A type representing a dynamic-sized span.
 */
template <typename T>
struct __span_dynamic_storage {

    __span_dynamic_storage() noexcept = default;
    __span_dynamic_storage(const __span_dynamic_storage &) noexcept = default;
    __span_dynamic_storage &operator=(const __span_dynamic_storage &) noexcept = default;

    __span_dynamic_storage(T *p, size_t s) noexcept : ptr(p), size(s) {}

    T *ptr = nullptr;
    size_t size = 0;
};

template <typename Iter, typename Elem>
struct __is_span_iterator
    : std::conjunction<is_contiguous_iterator<Iter>,
                       std::is_convertible<iterator_contiguous_pointer_t<Iter>, Elem *>> {
};

template <typename T, size_t Extent = dynamic_extent>
class span;

namespace span_details {

WJR_REGISTER_HAS_TYPE(data, std::data(std::declval<Container &>()), Container);
WJR_REGISTER_HAS_TYPE(size, std::size(std::declval<Container &>()), Container);

/// @private
template <typename T>
struct __is_std_array : std::false_type {};

/// @private
template <typename T, size_t N>
struct __is_std_array<std::array<T, N>> : std::true_type {};

/// @private
template <typename T>
inline constexpr bool __is_std_array_v = __is_std_array<T>::value;

template <typename T>
struct __is_span : std::false_type {};

template <typename T, size_t Extent>
struct __is_span<span<T, Extent>> : std::true_type {};

template <typename T>
inline constexpr bool __is_span_v = __is_span<T>::value;

/// @private
template <typename Container, typename = void>
struct __is_container_like : std::false_type {};

/// @private
template <typename Container>
struct __is_container_like<
    Container, std::enable_if_t<has_data_v<Container &> && has_size_v<Container &>>>
    : std::conjunction<
          std::negation<std::is_array<remove_cvref_t<Container>>>,
          std::negation<__is_std_array<remove_cvref_t<Container>>>,
          std::negation<__is_span<remove_cvref_t<Container>>>,
          std::is_pointer<decltype(std::data(std::declval<Container &>()))>> {};

/// @private
template <typename Container>
inline constexpr bool __is_container_like_v = __is_container_like<Container>::value;

template <typename Container, typename Elem, typename = void>
struct __is_span_like : std::false_type {};

template <typename Container, typename Elem>
struct __is_span_like<
    Container, Elem, std::enable_if_t<has_data_v<Container &> && has_size_v<Container &>>>
    : std::conjunction<
          __is_container_like<Container>,
          std::is_convertible<decltype(std::data(std::declval<Container &>())), Elem *>> {
};

template <typename Container, typename Elem>
inline constexpr bool __is_span_like_v = __is_span_like<Container, Elem>::value;

/// @private
template <typename T>
struct basic_span_traits {
    using value_type = std::remove_cv_t<T>;
    using difference_type = ptrdiff_t;
    using pointer = T *;
    using const_pointer = const T *;
    using reference = T &;
    using const_reference = const T &;
};

} // namespace span_details

/**
 * @class span
 *
 * @brief A view over a contiguous sequence of objectsd.
 *
 * @tparam Extent if Extent is `dynamic_extent`, the span is a runtime-sized view.
 * Otherwise, the span is a compile-time-sized view.
 */
template <typename T, size_t Extent>
class span {
    static constexpr bool __is_dynamic = Extent == dynamic_extent;
    using __storage = std::conditional_t<__is_dynamic, __span_dynamic_storage<T>,
                                         __span_static_storage<T, Extent>>;

    using IteratorTraits = span_details::basic_span_traits<T>;

public:
    using element_type = T;
    using value_type = std::remove_cv_t<T>;
    using size_type = size_t;
    using difference_type = ptrdiff_t;
    using pointer = T *;
    using const_pointer = const T *;
    using reference = T &;
    using const_reference = const T &;
    using iterator = contiguous_iterator_adapter<span, IteratorTraits>;
    using const_iterator = contiguous_const_iterator_adapter<span, IteratorTraits>;
    using reverse_iterator = std::reverse_iterator<iterator>;
    using const_reverse_iterator = std::reverse_iterator<const_iterator>;

    template <size_t Ex = Extent, WJR_REQUIRES(Ex == dynamic_extent || Ex == 0)>
    constexpr span() noexcept : storage() {}

    template <typename It,
              WJR_REQUIRES(__is_span_iterator<It, element_type>::value &&__is_dynamic)>
    constexpr span(It first, size_type count) noexcept
        : storage((to_address)(first), count) {}

    template <typename It,
              WJR_REQUIRES(__is_span_iterator<It, element_type>::value && !__is_dynamic)>
    constexpr explicit span(It first, size_type count) noexcept
        : storage((to_address)(first), count) {}

    template <typename It,
              WJR_REQUIRES(__is_span_iterator<It, element_type>::value &&__is_dynamic)>
    constexpr span(It first, It last) noexcept
        : storage((to_address)(first), static_cast<size_type>(last - first)) {}

    template <typename It,
              WJR_REQUIRES(__is_span_iterator<It, element_type>::value && !__is_dynamic)>
    constexpr explicit span(It first, It last) noexcept
        : storage((to_address)(first), static_cast<size_type>(last - first)) {}

    template <size_t N, WJR_REQUIRES((__is_dynamic || N == Extent))>
    constexpr span(type_identity_t<element_type> (&arr)[N]) noexcept
        : storage(std::data(arr), N) {}

    template <typename U, size_t N,
              WJR_REQUIRES((__is_dynamic || N == Extent) &&
                           std::is_convertible_v<U *, T *>)>
    constexpr span(std::array<U, N> &arr) noexcept
        : storage(std::data(arr), std::size(arr)) {}

    template <typename U, size_t N,
              WJR_REQUIRES((__is_dynamic || N == Extent) &&
                           std::is_convertible_v<const U *, T *>)>
    constexpr span(const std::array<U, N> &arr) noexcept
        : storage(std::data(arr), std::size(arr)) {}

    template <typename U, size_t N,
              WJR_REQUIRES((__is_dynamic || N == dynamic_extent || N == Extent) &&
                           std::is_convertible_v<U *, T *> && __is_dynamic)>
    constexpr span(const span<U, N> &source) noexcept
        : storage(source.data(), source.size()) {}

    template <typename U, size_t N,
              WJR_REQUIRES((__is_dynamic || N == dynamic_extent || N == Extent) &&
                           std::is_convertible_v<U *, T *> && !__is_dynamic)>
    constexpr explicit span(const span<U, N> &source) noexcept
        : storage(source.data(), source.size()) {}

    constexpr span(const span &other) noexcept = default;
    constexpr span &operator=(const span &other) noexcept = default;

    ~span() = default;

    WJR_PURE WJR_CONSTEXPR20 pointer begin_unsafe() noexcept { return data(); }
    WJR_PURE WJR_CONSTEXPR20 const_pointer begin_unsafe() const noexcept {
        return data();
    }
    WJR_PURE WJR_CONSTEXPR20 const_pointer cbegin_unsafe() const noexcept {
        return data();
    }

    WJR_PURE WJR_CONSTEXPR20 pointer end_unsafe() noexcept { return data() + size(); }
    WJR_PURE WJR_CONSTEXPR20 const_pointer end_unsafe() const noexcept {
        return data() + size();
    }
    WJR_PURE WJR_CONSTEXPR20 const_pointer cend_unsafe() const noexcept {
        return end_unsafe();
    }

private:
    WJR_PURE WJR_CONSTEXPR20 iterator __make_iterator(const_pointer ptr) const noexcept {
        return iterator(const_cast<pointer>(ptr), this);
    }

    WJR_PURE WJR_CONSTEXPR20 pointer __get_pointer(iterator ptr) const noexcept {
        ptr.check_same_container(this);
        return (to_address)(ptr);
    }

    WJR_PURE WJR_CONSTEXPR20 pointer __get_pointer(const_iterator ptr) const noexcept {
        ptr.check_same_container(this);
        return const_cast<pointer>((to_address)(ptr));
    }

public:
    constexpr iterator begin() noexcept { return __make_iterator(begin_unsafe()); }
    constexpr const_iterator begin() const noexcept {
        return __make_iterator(begin_unsafe());
    }
    constexpr const_iterator cbegin() const noexcept {
        return __make_iterator(begin_unsafe());
    }

    constexpr iterator end() noexcept { return __make_iterator(end_unsafe()); }
    constexpr const_iterator end() const noexcept {
        return __make_iterator(end_unsafe());
    }
    constexpr const_iterator cend() const noexcept {
        return __make_iterator(end_unsafe());
    }

    constexpr reverse_iterator rbegin() noexcept {
        return std::make_reverse_iterator(end());
    }
    constexpr reverse_iterator rbegin() const noexcept {
        return std::make_reverse_iterator(end());
    }
    constexpr const_reverse_iterator crbegin() const noexcept {
        return std::make_reverse_iterator(cend());
    }

    constexpr reverse_iterator rend() noexcept {
        return std::make_reverse_iterator(begin());
    }
    constexpr reverse_iterator rend() const noexcept {
        return std::make_reverse_iterator(begin());
    }
    constexpr const_reverse_iterator crend() const noexcept {
        return std::make_reverse_iterator(cbegin());
    }

    constexpr reference front() const {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECKER)
        WJR_ASSERT_LX(size() > 0, "basic_vector::front: empty");
#endif
        return *data();
    }
    constexpr reference back() const {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECKER)
        WJR_ASSERT_LX(size() > 0, "basic_vector::front: empty");
#endif
        return *(end_unsafe() - 1);
    }

    constexpr reference at(size_type pos) const {
        if (WJR_UNLIKELY(pos >= size())) {
            WJR_THROW(std::out_of_range("span at out of range"));
        }

        return data()[pos];
    }

    constexpr reference operator[](size_type pos) const {
#if WJR_HAS_DEBUG(CONTIGUOUS_ITERATOR_CHECKER)
        WJR_ASSERT_LX(pos < size(), "basic_vector::operator[]: out of range");
#endif
        return data()[pos];
    }

    constexpr pointer data() const { return storage.ptr; }
    constexpr size_type size() const { return storage.size; }
    constexpr size_type size_bytes() const { return size() * sizeof(element_type); }
    constexpr bool empty() const { return size() == 0; }

    template <size_t Count>
    constexpr span<element_type, Count> first() const {
        static_assert(Count <= Extent, "");

        return {begin(), Count};
    }

    constexpr span<element_type, dynamic_extent> first(size_type Count) const {
        WJR_ASSERT_L1(Count <= size());

        return {begin(), Count};
    }

    template <size_t Count>
    constexpr span<element_type, Count> last() const {
        static_assert(Count <= Extent, "");

        return {end() - Count, Count};
    }

    constexpr span<element_type, dynamic_extent> last(size_type Count) const {
        WJR_ASSERT_L1(Count <= size());

        return {data() - Count, Count};
    }

    template <size_t Offset, size_t Count = dynamic_extent>
    constexpr span<element_type, Count != dynamic_extent    ? Count
                                 : Extent != dynamic_extent ? Extent - Offset
                                                            : dynamic_extent>
    subspan() const {
        if constexpr (Extent != dynamic_extent) {
            static_assert(Offset <= Extent, "");
            static_assert(Count == dynamic_extent || Count <= Extent - Offset, "");
        } else {
            WJR_ASSERT_L1(Offset <= size());
            if constexpr (Count != dynamic_extent) {
                WJR_ASSERT_L1(Count <= size() - Offset);
            }
        }
        return {begin() + Offset, Count == dynamic_extent ? size() - Offset : Count};
    }

    constexpr span<element_type, dynamic_extent>
    subspan(size_type Offset, size_type Count = dynamic_extent) const {
        WJR_ASSERT_L1(Offset <= size());

        return {begin() + Offset, Count == dynamic_extent ? size() - Offset : Count};
    }

    // extension :

    /**
     * @brief Construct a span from a container.
     *
     * @details The container must have a `data()` member function that returns a @ref
     * __is_span_iterator. The container must also have a `size()` member function that
     * can be converted to `size_type`.
     *
     */
    template <typename Container, WJR_REQUIRES(span_details::__is_span_like_v<
                                               Container, element_type> &&__is_dynamic)>
    constexpr span(Container &&c) noexcept : storage(std::data(c), std::size(c)) {}

    /**
     * @brief Construct a span from a container.
     *
     * @details Like @ref span(Container &&), but the span is not dynamic-sized, so the
     * construct must be explicit.
     *
     */
    template <typename Container,
              WJR_REQUIRES(span_details::__is_span_like_v<Container, element_type> &&
                           !__is_dynamic)>
    constexpr explicit span(Container &&c) noexcept
        : storage(std::data(c), std::size(c)) {}

private:
    __storage storage;
};

template <typename T, size_t Extent>
span(T (&)[Extent]) -> span<T, Extent>;

template <typename T, size_t Size>
span(std::array<T, Size> &) -> span<T, Size>;

template <typename T, size_t Size>
span(const std::array<T, Size> &) -> span<const T, Size>;

template <typename It, typename End, WJR_REQUIRES(is_contiguous_iterator_v<It>)>
span(It, End) -> span<iterator_contiguous_value_t<It>>;

template <typename Container,
          WJR_REQUIRES(span_details::__is_container_like_v<Container>)>
span(Container &&) -> span<
    iterator_contiguous_value_t<decltype(std::data(std::declval<Container &>()))>>;

} // namespace wjr

#endif // WJR_SPAN_HPP__
// Already included

namespace wjr {

template <typename Storage>
struct is_biginteger_storage : std::false_type {};

template <typename Storage>
inline constexpr bool is_biginteger_storage_v = is_biginteger_storage<Storage>::value;

namespace biginteger_details {

inline uint32_t normalize(const uint64_t *ptr, uint32_t n) {
    return reverse_find_not_n(ptr, 0, n);
}

} // namespace biginteger_details

class default_biginteger_size_reference {
public:
    default_biginteger_size_reference() = delete;
    default_biginteger_size_reference(const default_biginteger_size_reference &) = delete;
    default_biginteger_size_reference(default_biginteger_size_reference &&) = default;
    default_biginteger_size_reference &
    operator=(const default_biginteger_size_reference &) = delete;
    default_biginteger_size_reference &
    operator=(default_biginteger_size_reference &&) = default;

    explicit default_biginteger_size_reference(int32_t &size) noexcept : m_size(&size) {}
    ~default_biginteger_size_reference() = default;

    default_biginteger_size_reference &operator=(uint32_t size) noexcept {
        *m_size = __fasts_negate_with<int32_t>(*m_size, size);
        return *this;
    }

    WJR_PURE operator uint32_t() const noexcept { return __fasts_abs(*m_size); }

    default_biginteger_size_reference &operator++() noexcept {
        *m_size = __fasts_increment(*m_size);
        return *this;
    }

    default_biginteger_size_reference &operator--() noexcept {
        *m_size = __fasts_decrement(*m_size);
        return *this;
    }

    default_biginteger_size_reference &operator+=(uint32_t size) noexcept {
        *m_size = __fasts_add(*m_size, size);
        return *this;
    }

    default_biginteger_size_reference &operator-=(uint32_t size) noexcept {
        *m_size = __fasts_sub(*m_size, size);
        return *this;
    }

private:
    int32_t *m_size;
};

template <>
struct __unref_wrapper_helper<default_biginteger_size_reference> {
    using type = uint32_t &;
};

/**
 * @brief data_type of biginteger
 *
 * @details View the data of biginteger. Used for type erasure. Manage memory allocation
 * and release on your own.
 *
 */
struct biginteger_data {
    WJR_PURE constexpr const uint64_t *data() const noexcept { return m_data; }
    WJR_PURE constexpr uint32_t size() const noexcept { return __fasts_abs(m_size); }

    WJR_PURE constexpr bool empty() const noexcept { return m_size == 0; }
    WJR_PURE constexpr bool is_negate() const noexcept { return m_size < 0; }

    WJR_PURE constexpr int32_t get_ssize() const noexcept { return m_size; }

    uint64_t *m_data = nullptr;
    int32_t m_size = 0;
    uint32_t m_capacity = 0;
};

/**
 * @struct biginteger_data
 * @brief The data structure for biginteger
 *
 */
template <typename Alloc>
class default_biginteger_vector_storage {
    using _Alty = typename std::allocator_traits<Alloc>::template rebind_alloc<uint64_t>;
    using _Alty_traits = std::allocator_traits<_Alty>;

public:
    using value_type = uint64_t;
    using pointer = typename _Alty_traits::pointer;
    using const_pointer = typename _Alty_traits::const_pointer;
    using size_type = uint32_t;
    using difference_type = int32_t;
    using allocator_type = Alloc;
    using storage_traits_type = vector_storage_traits<uint64_t, Alloc>;
    using is_reallocatable = std::true_type;

public:
    default_biginteger_vector_storage() noexcept = default;

    default_biginteger_vector_storage(const default_biginteger_vector_storage &) = delete;
    default_biginteger_vector_storage(default_biginteger_vector_storage &&) noexcept =
        delete;
    default_biginteger_vector_storage &
    operator=(const default_biginteger_vector_storage &) = delete;
    default_biginteger_vector_storage &
    operator=(default_biginteger_vector_storage &&) noexcept = delete;

    ~default_biginteger_vector_storage() noexcept = default;

    void destroy(_Alty &al) noexcept {
        if (WJR_BUILTIN_CONSTANT_P(data() == nullptr) && data() == nullptr) {
            return;
        }

        const size_type __size = size();

        if (WJR_BUILTIN_CONSTANT_P(__size == 0) && __size == 0) {
            return;
        }

        destroy_n_using_allocator(data(), __size, al);
    }

    void destroy_and_deallocate(_Alty &al) noexcept {
        if (WJR_BUILTIN_CONSTANT_P(capacity() == 0) && capacity() == 0) {
            return;
        }

        if (data()) {
            destroy(al);
            al.deallocate(data(), capacity());
        }
    }

    void uninitialized_construct(default_biginteger_vector_storage &other, size_type size,
                                 size_type capacity, _Alty &al) {
        auto &storage = other.m_storage;
        storage.m_data = al.allocate(capacity);
        storage.m_size = __fasts_negate_with<int32_t>(m_storage.m_size, size);
        storage.m_capacity = capacity;
    }

    void uninitialized_construct(size_type size, size_type capacity, _Alty &al) {
        m_storage.m_data = al.allocate(capacity);
        m_storage.m_size = size;
        m_storage.m_capacity = capacity;
    }

    void take_storage(default_biginteger_vector_storage &other, _Alty &) noexcept {
        auto &other_storage = other.m_storage;
        m_storage = other_storage;
        other_storage.m_data = nullptr;
        other_storage.m_size = other_storage.m_capacity = 0;
    }

    void swap_storage(default_biginteger_vector_storage &other, _Alty &) noexcept {
        std::swap(m_storage, other.m_storage);
    }

    WJR_PURE default_biginteger_size_reference size() noexcept {
        return default_biginteger_size_reference(m_storage.m_size);
    }

    WJR_PURE size_type size() const noexcept { return __fasts_abs(m_storage.m_size); }
    WJR_PURE size_type capacity() const noexcept { return m_storage.m_capacity; }

    WJR_PURE pointer data() noexcept { return m_storage.m_data; }
    WJR_PURE const_pointer data() const noexcept { return m_storage.m_data; }

    // extension

    WJR_PURE int32_t get_ssize() const noexcept { return m_storage.m_size; }
    template <typename T, WJR_REQUIRES(is_any_of_v<T, int32_t>)>
    void set_ssize(T size) noexcept {
        WJR_ASSERT_ASSUME(__fasts_abs(size) <= capacity());
        m_storage.m_size = size;
    }

    WJR_PURE const biginteger_data *__get_data() const noexcept { return &m_storage; }

private:
    biginteger_data m_storage;
};

template <typename Alloc>
struct is_biginteger_storage<default_biginteger_vector_storage<Alloc>> : std::true_type {
};

template <typename Storage>
class basic_biginteger;

namespace biginteger_details {

// const basic_biginteger<Storage>* don't need to get allocator
// use const Storage* instead of const basic_biginteger<Storage>*

/// @private
template <typename S>
from_chars_result<> __from_chars_impl(const char *first, const char *last,
                                      basic_biginteger<S> *dst, unsigned int base);

/// @private
inline int32_t __compare_impl(const biginteger_data *lhs, const biginteger_data *rhs);

/// @private
inline int32_t __compare_ui_impl(const biginteger_data *lhs, uint64_t rhs);

/// @private
inline int32_t __compare_si_impl(const biginteger_data *lhs, int64_t rhs);

/// @private
template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
int32_t __compare_impl(const biginteger_data *lhs, T rhs) {
    if (WJR_BUILTIN_CONSTANT_P(rhs == 0) && rhs == 0) {
        const int32_t ssize = lhs->get_ssize();
        return ssize == 0 ? 0 : ssize < 0 ? -1 : 1;
    }

    if constexpr (std::is_unsigned_v<T>) {
        return __compare_ui_impl(lhs, rhs);
    } else {
        if (WJR_BUILTIN_CONSTANT_P(rhs >= 0) && rhs >= 0) {
            return __compare_ui_impl(lhs, to_unsigned(rhs));
        }

        return __compare_si_impl(lhs, rhs);
    }
}

/// @private
template <bool xsign, typename S>
void __addsub_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, uint64_t rhs);

/// @private
template <typename S>
void __ui_sub_impl(basic_biginteger<S> *dst, uint64_t lhs, const biginteger_data *rhs);

/// @private
template <bool xsign, typename S>
void __addsub_impl(basic_biginteger<S> *dst, const biginteger_data *lhs,
                   const biginteger_data *rhs);

/// @private
template <typename S>
void __add_impl(basic_biginteger<S> *dst, const biginteger_data *lhs,
                const biginteger_data *rhs) {
    __addsub_impl<false>(dst, lhs, rhs);
}

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void __add_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, T rhs) {
    if constexpr (std::is_unsigned_v<T>) {
        __addsub_impl<false>(dst, lhs, rhs);
    } else {
        if (rhs < 0) {
            __addsub_impl<true>(dst, lhs, -to_unsigned(rhs));
        } else {
            __addsub_impl<false>(dst, lhs, to_unsigned(rhs));
        }
    }
}

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void __add_impl(basic_biginteger<S> *dst, T lhs, const biginteger_data *rhs) {
    __add_impl(dst, rhs, lhs);
}

/// @private
template <typename S>
void __sub_impl(basic_biginteger<S> *dst, const biginteger_data *lhs,
                const biginteger_data *rhs) {
    __addsub_impl<true>(dst, lhs, rhs);
}

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void __sub_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, T rhs) {
    if constexpr (std::is_unsigned_v<T>) {
        __addsub_impl<true>(dst, lhs, rhs);
    } else {
        if (rhs < 0) {
            __addsub_impl<false>(dst, lhs, -to_unsigned(rhs));
        } else {
            __addsub_impl<true>(dst, lhs, to_unsigned(rhs));
        }
    }
}

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void __sub_impl(basic_biginteger<S> *dst, T lhs, const biginteger_data *rhs) {
    if constexpr (std::is_unsigned_v<T>) {
        __ui_sub_impl(dst, lhs, rhs);
    } else {
        if (lhs < 0) {
            __addsub_impl<false>(dst, rhs, -to_unsigned(lhs));
            dst->negate();
        } else {
            __ui_sub_impl(dst, to_unsigned(lhs), rhs);
        }
    }
}

/// @private
template <typename S>
void __mul_ui_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, uint64_t rhs);

/// @private
template <typename S>
void __mul_impl(basic_biginteger<S> *dst, const biginteger_data *lhs,
                const biginteger_data *rhs);

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void __mul_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, T rhs) {
    if constexpr (std::is_unsigned_v<T>) {
        __mul_ui_impl(dst, lhs, rhs);
    } else {
        uint64_t value = to_unsigned(rhs);
        bool cond = false;

        if (rhs < 0) {
            value = -value;
            cond = true;
        }

        __mul_ui_impl(dst, lhs, value);
        dst->conditional_negate(cond);
    }
}

/// @private
template <typename S>
void __addsubmul_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, uint64_t rhs,
                      int32_t xmask);

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void __addmul_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, T rhs) {
    if constexpr (std::is_unsigned_v<T>) {
        __addsubmul_impl(dst, lhs, rhs, 0);
    } else {
        uint64_t rvalue = to_unsigned(rhs);
        int32_t xsign = 0;

        if (rhs < 0) {
            rvalue = -rvalue;
            xsign = -1;
        }

        __addsubmul_impl(dst, lhs, rvalue, xsign);
    }
}

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void __submul_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, T rhs) {
    if constexpr (std::is_unsigned_v<T>) {
        __addsubmul_impl(dst, lhs, rhs, -1);
    } else {
        uint64_t rvalue = to_unsigned(rhs);
        int32_t xsign = -1;

        if (rhs < 0) {
            rvalue = -rvalue;
            xsign = 0;
        }

        __addsubmul_impl(dst, lhs, rvalue, xsign);
    }
}

/// @private
template <typename S>
void __addsubmul_impl(basic_biginteger<S> *dst, const biginteger_data *lhs,
                      const biginteger_data *rhs, int32_t xmask);

/// @private
template <typename S>
void __addmul_impl(basic_biginteger<S> *dst, const biginteger_data *lhs,
                   const biginteger_data *rhs) {
    __addsubmul_impl(dst, lhs, rhs, 0);
}

/// @private
template <typename S>
void __submul_impl(basic_biginteger<S> *dst, const biginteger_data *lhs,
                   const biginteger_data *rhs) {
    __addsubmul_impl(dst, lhs, rhs, -1);
}

/// @private
template <typename S>
void __mul_2exp_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, size_t shift);

/// @private
template <typename S0, typename S1>
void __tdiv_qr_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                    const biginteger_data *num, const biginteger_data *div);

/// @private
template <typename S>
void __tdiv_q_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                   const biginteger_data *div);

/// @private
template <typename S>
void __tdiv_r_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                   const biginteger_data *div);

/// @private
template <typename S0, typename S1>
uint64_t __tdiv_qr_ui_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                           const biginteger_data *num, uint64_t div);

/// @private
template <typename S>
uint64_t __tdiv_q_ui_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                          uint64_t div);

/// @private
template <typename S>
uint64_t __tdiv_r_ui_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                          uint64_t div);

/// @private
template <typename S0, typename S1, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t __tdiv_qr_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                        const biginteger_data *num, T div);

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t __tdiv_q_impl(basic_biginteger<S> *quot, const biginteger_data *num, T div);

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t __tdiv_r_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                       uint64_t div);

/// @private
template <typename S0, typename S1>
void __fdiv_qr_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                    const biginteger_data *num, const biginteger_data *div);

/// @private
template <typename S>
void __fdiv_q_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                   const biginteger_data *div);

/// @private
template <typename S>
void __fdiv_r_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                   const biginteger_data *div);

/// @private
template <typename S0, typename S1, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t __fdiv_qr_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                        const biginteger_data *num, T div);

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t __fdiv_q_impl(basic_biginteger<S> *quot, const biginteger_data *num, T div);

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t __fdiv_r_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                       uint64_t div);

/// @private
template <typename S0, typename S1>
void __cdiv_qr_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                    const biginteger_data *num, const biginteger_data *div);

/// @private
template <typename S>
void __cdiv_q_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                   const biginteger_data *div);

/// @private
template <typename S>
void __cdiv_r_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                   const biginteger_data *div);

/// @private
template <typename S0, typename S1, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t __cdiv_qr_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                        const biginteger_data *num, T div);

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t __cdiv_q_impl(basic_biginteger<S> *quot, const biginteger_data *num, T div);

/// @private
template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t __cdiv_r_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                       uint64_t div);

/// @private
template <typename S>
void __tdiv_q_2exp_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                        size_t shift);

/// @private
template <typename S>
void __tdiv_r_2exp_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                        size_t shift);

/// @private
template <typename S>
void __cfdiv_q_2exp_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                         size_t shift, int32_t xdir);

/// @private
template <typename S>
void __cdiv_q_2exp_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                        size_t shift) {
    __cfdiv_q_2exp_impl(quot, num, shift, 1);
}

/// @private
template <typename S>
void __fdiv_q_2exp_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                        size_t shift) {
    __cfdiv_q_2exp_impl(rem, num, shift, -1);
}

/// @private
template <typename S, typename Engine,
          WJR_REQUIRES(biginteger_uniform_random_bit_generator_v<Engine>)>
void __urandom_bit_impl(basic_biginteger<S> *dst, size_t size, Engine &engine);

/// @private
template <typename S, typename Engine,
          WJR_REQUIRES(biginteger_uniform_random_bit_generator_v<Engine>)>
void __urandom_exact_bit_impl(basic_biginteger<S> *dst, size_t size, Engine &engine);

/// @private
template <typename S, typename Engine,
          WJR_REQUIRES(biginteger_uniform_random_bit_generator_v<Engine>)>
void __urandom_impl(basic_biginteger<S> *dst, const biginteger_data *limit,
                    Engine &engine);

/// @private
template <typename S, typename Engine,
          WJR_REQUIRES(biginteger_uniform_random_bit_generator_v<Engine>)>
void __urandom_exact_impl(basic_biginteger<S> *dst, const biginteger_data *limit,
                          Engine &engine);

} // namespace biginteger_details

template <typename S>
from_chars_result<> from_chars(const char *first, const char *last,
                               basic_biginteger<S> &dst, unsigned int base = 10);

template <typename Iter>
Iter to_chars_unchecked(Iter ptr, const biginteger_data &src, unsigned int base = 10) {
    if (src.empty()) {
        *ptr++ = '0';
        return ptr;
    }

    if (src.is_negate()) {
        *ptr++ = '-';
    }

    return biginteger_to_chars(ptr, src.data(), src.size(), base);
}

template <typename S>
std::istream &operator>>(std::istream &is, basic_biginteger<S> &dst);

template <typename Traits>
std::basic_ostream<char, Traits> &operator<<(std::basic_ostream<char, Traits> &os,
                                             const biginteger_data &src);

WJR_NODISCARD WJR_PURE inline int32_t compare(const biginteger_data &lhs,
                                              const biginteger_data &rhs) {
    return biginteger_details::__compare_impl(&lhs, &rhs);
}

template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
WJR_NODISCARD WJR_PURE int32_t compare(const biginteger_data &lhs, T rhs) {
    return biginteger_details::__compare_impl(&lhs, rhs);
}

template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
WJR_NODISCARD WJR_PURE int32_t compare(T lhs, const biginteger_data &rhs) {
    return -compare(rhs, lhs);
}

#define WJR_REGISTER_BIGINTEGER_COMPARE(op)                                              \
    WJR_NODISCARD WJR_PURE inline bool operator op(const biginteger_data &lhs,           \
                                                   const biginteger_data &rhs) {         \
        return compare(lhs, rhs) op 0;                                                   \
    }                                                                                    \
    template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>                        \
    WJR_NODISCARD WJR_PURE bool operator op(const biginteger_data &lhs, T rhs) {         \
        return compare(lhs, rhs) op 0;                                                   \
    }                                                                                    \
    template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>                        \
    WJR_NODISCARD WJR_PURE bool operator op(T lhs, const biginteger_data &rhs) {         \
        return compare(lhs, rhs) op 0;                                                   \
    }

WJR_REGISTER_BIGINTEGER_COMPARE(==)
WJR_REGISTER_BIGINTEGER_COMPARE(!=)
WJR_REGISTER_BIGINTEGER_COMPARE(<)
WJR_REGISTER_BIGINTEGER_COMPARE(>)
WJR_REGISTER_BIGINTEGER_COMPARE(<=)
WJR_REGISTER_BIGINTEGER_COMPARE(>=)

#undef WJR_REGISTER_BIGINTEGER_COMPARE

#define WJR_REGISTER_BIGINTEGER_ADDSUB(ADDSUB)                                           \
    template <typename S>                                                                \
    void ADDSUB(basic_biginteger<S> &dst, const biginteger_data &lhs,                    \
                const biginteger_data &rhs) {                                            \
        biginteger_details::WJR_PP_CONCAT(__, WJR_PP_CONCAT(ADDSUB, _impl))(&dst, &lhs,  \
                                                                            &rhs);       \
    }                                                                                    \
    template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>            \
    void ADDSUB(basic_biginteger<S> &dst, const biginteger_data &lhs, T rhs) {           \
        biginteger_details::WJR_PP_CONCAT(__, WJR_PP_CONCAT(ADDSUB, _impl))(&dst, &lhs,  \
                                                                            rhs);        \
    }                                                                                    \
    template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>            \
    void ADDSUB(basic_biginteger<S> &dst, T lhs, const biginteger_data &rhs) {           \
        biginteger_details::WJR_PP_CONCAT(__, WJR_PP_CONCAT(ADDSUB, _impl))(&dst, lhs,   \
                                                                            &rhs);       \
    }

WJR_REGISTER_BIGINTEGER_ADDSUB(add)
WJR_REGISTER_BIGINTEGER_ADDSUB(sub)

#undef WJR_REGISTER_BIGINTEGER_ADDSUB

template <typename S>
void increment(basic_biginteger<S> &dst) {
    add(dst, dst, 1u);
}

template <typename S>
void decrement(basic_biginteger<S> &dst) {
    sub(dst, dst, 1u);
}

template <typename S>
void negate(basic_biginteger<S> &dst);

template <typename S>
void absolute(basic_biginteger<S> &dst);

template <typename S>
void mul(basic_biginteger<S> &dst, const biginteger_data &lhs,
         const biginteger_data &rhs) {
    biginteger_details::__mul_impl(&dst, &lhs, &rhs);
}

template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void mul(basic_biginteger<S> &dst, const biginteger_data &lhs, T rhs) {
    biginteger_details::__mul_impl(&dst, &lhs, rhs);
}

template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void mul(basic_biginteger<S> &dst, T lhs, const biginteger_data &rhs) {
    biginteger_details::__mul_impl(&dst, &rhs, lhs);
}

template <typename S>
void addmul(basic_biginteger<S> &dst, const biginteger_data &lhs,
            const biginteger_data &rhs) {
    biginteger_details::__addmul_impl(&dst, &lhs, &rhs);
}

template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void addmul(basic_biginteger<S> &dst, const biginteger_data &lhs, T rhs) {
    biginteger_details::__addmul_impl(&dst, &lhs, rhs);
}

template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void addmul(basic_biginteger<S> &dst, T lhs, const biginteger_data &rhs) {
    biginteger_details::__addmul_impl(&dst, &rhs, lhs);
}

template <typename S>
void submul(basic_biginteger<S> &dst, const biginteger_data &lhs,
            const biginteger_data &rhs) {
    biginteger_details::__submul_impl(&dst, &lhs, &rhs);
}

template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void submul(basic_biginteger<S> &dst, const biginteger_data &lhs, T rhs) {
    biginteger_details::__submul_impl(&dst, &lhs, rhs);
}

template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
void submul(basic_biginteger<S> &dst, T lhs, const biginteger_data &rhs) {
    biginteger_details::__submul_impl(&dst, &rhs, lhs);
}

template <typename S>
void mul_2exp(basic_biginteger<S> &dst, const biginteger_data &lhs, size_t shift) {
    biginteger_details::__mul_2exp_impl(&dst, &lhs, shift);
}

template <typename S0, typename S1>
void tdiv_qr(basic_biginteger<S0> &quot, basic_biginteger<S1> &rem,
             const biginteger_data &num, const biginteger_data &div) {
    biginteger_details::__tdiv_qr_impl(&quot, &rem, &num, &div);
}

template <typename S>
void tdiv_q(basic_biginteger<S> &quot, const biginteger_data &num,
            const biginteger_data &div) {
    biginteger_details::__tdiv_q_impl(&quot, &num, &div);
}

template <typename S0>
void tdiv_r(basic_biginteger<S0> &rem, const biginteger_data &num,
            const biginteger_data &div) {
    biginteger_details::__tdiv_r_impl(&rem, &num, &div);
}

template <typename S0, typename S1, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t tdiv_qr(basic_biginteger<S0> &quot, basic_biginteger<S1> &rem,
                 const biginteger_data &num, T div) {
    return biginteger_details::__tdiv_qr_impl(&quot, &rem, &num, div);
}

template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t tdiv_q(basic_biginteger<S> &quot, const biginteger_data &num, T div) {
    return biginteger_details::__tdiv_q_impl(&quot, &num, div);
}

template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t tdiv_r(basic_biginteger<S> &rem, const biginteger_data &num, T div) {
    return biginteger_details::__tdiv_r_impl(&rem, &num, div);
}

template <typename S0, typename S1>
void fdiv_qr(basic_biginteger<S0> &quot, basic_biginteger<S1> &rem,
             const biginteger_data &num, const biginteger_data &div) {
    biginteger_details::__fdiv_qr_impl(&quot, &rem, &num, &div);
}

template <typename S>
void fdiv_q(basic_biginteger<S> &quot, const biginteger_data &num,
            const biginteger_data &div) {
    biginteger_details::__fdiv_q_impl(&quot, &num, &div);
}

template <typename S0>
void fdiv_r(basic_biginteger<S0> &rem, const biginteger_data &num,
            const biginteger_data &div) {
    biginteger_details::__fdiv_r_impl(&rem, &num, &div);
}

template <typename S0, typename S1, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t fdiv_qr(basic_biginteger<S0> &quot, basic_biginteger<S1> &rem,
                 const biginteger_data &num, T div) {
    return biginteger_details::__fdiv_qr_impl(&quot, &rem, &num, div);
}

template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t fdiv_q(basic_biginteger<S> &quot, const biginteger_data &num, T div) {
    return biginteger_details::__fdiv_q_impl(&quot, &num, div);
}

template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t fdiv_r(basic_biginteger<S> &rem, const biginteger_data &num, T div) {
    return biginteger_details::__fdiv_r_impl(&rem, &num, div);
}

template <typename S0, typename S1>
void cdiv_qr(basic_biginteger<S0> &quot, basic_biginteger<S1> &rem,
             const biginteger_data &num, const biginteger_data &div) {
    biginteger_details::__cdiv_qr_impl(&quot, &rem, &num, &div);
}

template <typename S>
void cdiv_q(basic_biginteger<S> &quot, const biginteger_data &num,
            const biginteger_data &div) {
    biginteger_details::__cdiv_q_impl(&quot, &num, &div);
}

template <typename S0>
void cdiv_r(basic_biginteger<S0> &rem, const biginteger_data &num,
            const biginteger_data &div) {
    biginteger_details::__cdiv_r_impl(&rem, &num, &div);
}

template <typename S0, typename S1, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t cdiv_qr(basic_biginteger<S0> &quot, basic_biginteger<S1> &rem,
                 const biginteger_data &num, T div) {
    return biginteger_details::__cdiv_qr_impl(&quot, &rem, &num, div);
}

template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t cdiv_q(basic_biginteger<S> &quot, const biginteger_data &num, T div) {
    return biginteger_details::__cdiv_q_impl(&quot, &num, div);
}

template <typename S, typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
uint64_t cdiv_r(basic_biginteger<S> &rem, const biginteger_data &num, T div) {
    return biginteger_details::__cdiv_r_impl(&rem, &num, div);
}

template <typename S>
void tdiv_q_2exp(basic_biginteger<S> &quot, const biginteger_data &num, size_t shift) {
    biginteger_details::__tdiv_q_2exp_impl(&quot, &num, shift);
}

template <typename S>
void tdiv_r_2exp(basic_biginteger<S> &rem, const biginteger_data &num, size_t shift) {
    biginteger_details::__tdiv_r_2exp_impl(&rem, &num, shift);
}

template <typename S>
void fdiv_q_2exp(basic_biginteger<S> &quot, const biginteger_data &num, size_t shift) {
    biginteger_details::__fdiv_q_2exp_impl(&quot, &num, shift);
}

template <typename S>
void cdiv_q_2exp(basic_biginteger<S> &quot, const biginteger_data &num, size_t shift) {
    biginteger_details::__cdiv_q_2exp_impl(&quot, &num, shift);
}

template <typename S, typename Engine,
          WJR_REQUIRES(biginteger_uniform_random_bit_generator_v<Engine>)>
void urandom_bit(basic_biginteger<S> &dst, size_t size, Engine &engine) {
    biginteger_details::__urandom_bit_impl(&dst, size, engine);
}

template <typename S, typename Engine,
          WJR_REQUIRES(biginteger_uniform_random_bit_generator_v<Engine>)>
void urandom_exact_bit(basic_biginteger<S> &dst, size_t size, Engine &engine) {
    biginteger_details::__urandom_exact_bit_impl(&dst, size, engine);
}

template <typename S, typename Engine,
          WJR_REQUIRES(biginteger_uniform_random_bit_generator_v<Engine>)>
void urandom(basic_biginteger<S> &dst, const biginteger_data &limit, Engine &engine) {
    biginteger_details::__urandom_impl(&dst, &limit, engine);
}

template <typename S, typename Engine,
          WJR_REQUIRES(biginteger_uniform_random_bit_generator_v<Engine>)>
void urandom_exact(basic_biginteger<S> &dst, const biginteger_data &limit,
                   Engine &engine) {
    biginteger_details::__urandom_exact_impl(&dst, &limit, engine);
}

template <typename Storage>
class basic_biginteger {

public:
    using storage_type = Storage;
    using vector_type = basic_vector<storage_type>;

    using value_type = typename vector_type::value_type;
    using size_type = typename vector_type::size_type;
    using difference_type = typename vector_type::difference_type;
    using reference = typename vector_type::reference;
    using const_reference = typename vector_type::const_reference;
    using pointer = typename vector_type::pointer;
    using const_pointer = typename vector_type::const_pointer;
    using iterator = typename vector_type::iterator;
    using const_iterator = typename vector_type::const_iterator;
    using reverse_iterator = typename vector_type::reverse_iterator;
    using const_reverse_iterator = typename vector_type::const_reverse_iterator;
    using allocator_type = typename vector_type::allocator_type;

    static_assert(std::is_same_v<value_type, uint64_t>, "value_type must be uint64_t");
    static_assert(std::is_same_v<pointer, uint64_t *>, "pointer must be uint64_t *");
    static_assert(std::is_same_v<const_pointer, const uint64_t *>,
                  "const_pointer must be const uint64_t *");
    static_assert(std::is_same_v<size_type, uint32_t>, "size_type must be uint32_t");
    static_assert(std::is_same_v<difference_type, int32_t>,
                  "difference_type must be int32_t");

    basic_biginteger() = default;

    basic_biginteger(const basic_biginteger &other,
                     const allocator_type &al = allocator_type())
        : m_vec(other.m_vec, al) {
        set_ssize(other.get_ssize());
    }

    basic_biginteger(size_type n, in_place_reserve_t,
                     const allocator_type &al = allocator_type())
        : m_vec(n, in_place_reserve, al) {}

    basic_biginteger(basic_biginteger &&other) = default;
    basic_biginteger &operator=(const basic_biginteger &other) {
        if (WJR_UNLIKELY(this == std::addressof(other))) {
            return *this;
        }

        m_vec = other.m_vec;
        set_ssize(other.get_ssize());
        return *this;
    }

    basic_biginteger &operator=(basic_biginteger &&other) = default;
    ~basic_biginteger() = default;

    explicit basic_biginteger(const allocator_type &al) : m_vec(al) {}

    basic_biginteger(basic_biginteger &&other, const allocator_type &al)
        : m_vec(std::move(other.m_vec), al) {}

    template <typename UnsignedValue,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    explicit basic_biginteger(UnsignedValue value,
                              const allocator_type &al = allocator_type())
        : m_vec(value != 0, value, al) {}

    template <typename SignedValue,
              WJR_REQUIRES(is_nonbool_signed_integral_v<SignedValue>)>
    explicit basic_biginteger(SignedValue value,
                              const allocator_type &al = allocator_type())
        : m_vec(al) {
        if (value != 0) {
            m_vec.emplace_back(value < 0 ? -to_unsigned(value) : to_unsigned(value));
            set_ssize(__fasts_conditional_negate<int32_t>(value < 0, 1));
        }
    }

    explicit basic_biginteger(span<const char> sp, unsigned int base = 10,
                              const allocator_type &al = allocator_type())
        : m_vec(al) {
        from_string(sp, base);
    }

    template <typename OthterStorage>
    explicit basic_biginteger(const basic_biginteger<OthterStorage> &other,
                              const allocator_type &al = allocator_type())
        : m_vec(other.begin(), other.end(), al) {
        set_ssize(other.get_ssize());
    }

    template <typename UnsignedValue,
              WJR_REQUIRES(is_nonbool_unsigned_integral_v<UnsignedValue>)>
    basic_biginteger &operator=(UnsignedValue value) {
        clear();
        if (value != 0) {
            m_vec.emplace_back(value);
        }
        return *this;
    }

    template <typename SignedValue,
              WJR_REQUIRES(is_nonbool_signed_integral_v<SignedValue>)>
    basic_biginteger &operator=(SignedValue value) {
        clear();
        if (value != 0) {
            m_vec.emplace_back(value < 0 ? -to_unsigned(value) : to_unsigned(value));
            set_ssize(__fasts_conditional_negate<int32_t>(value < 0, 1));
        }
        return *this;
    }

    basic_biginteger &operator=(span<const char> sp) { return from_string(sp); }

    template <typename OthterStorage>
    basic_biginteger &operator=(const basic_biginteger<OthterStorage> &other) {
        m_vec.assign(other.begin(), other.end());
        set_ssize(other.get_ssize());
        return *this;
    }

    template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
    explicit operator T() const noexcept {
        if (empty()) {
            return static_cast<T>(0);
        }

        if constexpr (std::is_unsigned_v<T>) {
            return static_cast<T>(front());
        } else {
            const auto ret = front();
            return is_negate() ? -ret : ret;
        }
    }

    basic_biginteger &from_string(span<const char> sp, unsigned int base = 10) {
        (void)from_chars(sp.data(), sp.data() + sp.size(), *this, base);
        return *this;
    }

    allocator_type &get_allocator() noexcept { return m_vec.get_allocator(); }
    const allocator_type &get_allocator() const noexcept { return m_vec.get_allocator(); }

    reference at(size_type pos) { return m_vec.at(pos); }
    const_reference at(size_type pos) const { return m_vec.at(pos); }

    reference operator[](size_type pos) { return m_vec[pos]; }
    const_reference operator[](size_type pos) const { return m_vec[pos]; }

    reference front() { return m_vec.front(); }
    const_reference front() const { return m_vec.front(); }

    reference back() { return m_vec.back(); }
    const_reference back() const { return m_vec.back(); }

    WJR_PURE pointer data() noexcept { return m_vec.data(); }
    WJR_PURE const_pointer data() const noexcept { return m_vec.data(); }

    WJR_PURE iterator begin() noexcept { return m_vec.begin(); }
    WJR_PURE const_iterator begin() const noexcept { return m_vec.begin(); }

    WJR_PURE iterator end() noexcept { return m_vec.end(); }
    WJR_PURE const_iterator end() const noexcept { return m_vec.end(); }

    WJR_PURE const_iterator cbegin() const noexcept { return m_vec.cbegin(); }

    WJR_PURE const_iterator cend() const noexcept { return m_vec.cend(); }

    WJR_PURE reverse_iterator rbegin() noexcept { return m_vec.rbegin(); }
    WJR_PURE const_reverse_iterator rbegin() const noexcept { return m_vec.rbegin(); }

    WJR_PURE reverse_iterator rend() noexcept { return m_vec.rend(); }
    WJR_PURE const_reverse_iterator rend() const noexcept { return m_vec.rend(); }

    WJR_PURE const_reverse_iterator crbegin() const noexcept { return m_vec.crbegin(); }
    WJR_PURE const_reverse_iterator crend() const noexcept { return m_vec.crend(); }

    WJR_PURE bool empty() const noexcept { return m_vec.empty(); }
    WJR_PURE size_type size() const noexcept { return m_vec.size(); }
    WJR_PURE size_type capacity() const noexcept { return m_vec.capacity(); }
    WJR_PURE bool zero() const noexcept { return empty(); }

    void reserve(size_type new_capacity) { m_vec.reserve(new_capacity); }

    void shrink_to_fit() { m_vec.shrink_to_fit(); }

    /// equal to set_ssize(0)
    void clear() { m_vec.clear(); }

    void swap(basic_biginteger &other) noexcept { m_vec.swap(other.m_vec); }

    void conditional_negate(bool condition) noexcept {
        set_ssize(__fasts_conditional_negate<int32_t>(condition, get_ssize()));
    }

    void negate() noexcept { conditional_negate(true); }

    WJR_PURE bool is_negate() const noexcept { return get_ssize() < 0; }

    void absolute() noexcept { set_ssize(__fasts_abs(get_ssize())); }

    basic_biginteger &operator++() {
        increment(*this);
        return *this;
    }

    basic_biginteger &operator--() {
        decrement(*this);
        return *this;
    }

    basic_biginteger &operator+=(const biginteger_data &rhs) {
        add(*this, *this, rhs);
        return *this;
    }

    basic_biginteger &operator-=(const biginteger_data &rhs) {
        sub(*this, *this, rhs);
        return *this;
    }

    basic_biginteger &operator*=(const biginteger_data &rhs) {
        mul(*this, *this, rhs);
        return *this;
    }

    template <typename T, WJR_REQUIRES(is_nonbool_integral_v<T>)>
    basic_biginteger &operator*=(T rhs) {
        mul(*this, *this, rhs);
        return *this;
    }

    basic_biginteger &operator<<=(size_t shift) {
        mul_2exp(*this, *this, shift);
        return *this;
    }

    // extension

    WJR_PURE int32_t get_ssize() const { return get_storage().get_ssize(); }
    template <typename T, WJR_REQUIRES(is_nonbool_unsigned_integral_v<T> ||
                                       std::is_same_v<T, int32_t>)>
    void set_ssize(T new_size) {
        if constexpr (std::is_unsigned_v<T>) {
            const auto u32size = static_cast<uint32_t>(new_size);
            WJR_ASSUME(u32size == new_size);
            get_storage().set_ssize(__fasts_from_unsigned(u32size));
        } else {
            get_storage().set_ssize(new_size);
        }
    }

    WJR_CONST static size_type get_growth_capacity(size_type old_capacity,
                                                   size_type new_size) noexcept {
        return vector_type::get_growth_capacity(old_capacity, new_size);
    }

    void take_storage(storage_type &other) noexcept { m_vec.take_storage(other); }

    void uninitialized_construct(storage_type &other, size_type siz,
                                 size_type cap) noexcept {
        m_vec.uninitialized_construct(other, siz, cap);
    }

    WJR_PURE storage_type &get_storage() noexcept { return m_vec.get_storage(); }
    WJR_PURE const storage_type &get_storage() const noexcept {
        return m_vec.get_storage();
    }

    WJR_PURE const biginteger_data *__get_data() const noexcept {
        return get_storage().__get_data();
    }

    WJR_PURE const biginteger_data &__get_ref() const noexcept { return *__get_data(); }
    WJR_PURE operator const biginteger_data &() const noexcept { return __get_ref(); }

private:
    void __check_high_bit() const {
        WJR_ASSERT(size() == 0 || back() != 0, "biginteger should not have leading zero");
    }

    vector_type m_vec;
};

template <typename Alloc>
using default_biginteger = basic_biginteger<default_biginteger_vector_storage<Alloc>>;

using biginteger = default_biginteger<memory_pool<uint64_t>>;
using stack_biginteger = default_biginteger<math_details::weak_stack_alloc<uint64_t>>;

template <typename Storage>
void swap(basic_biginteger<Storage> &lhs, basic_biginteger<Storage> &rhs) noexcept {
    lhs.swap(rhs);
}

namespace biginteger_details {

/// @private
template <typename S>
WJR_PURE bool __equal_pointer(const basic_biginteger<S> *lhs,
                              const basic_biginteger<S> *rhs) {
    return lhs == rhs;
}

/// @private
template <typename S0, typename S1>
WJR_PURE bool __equal_pointer(const basic_biginteger<S0> *,
                              const basic_biginteger<S1> *) {
    return false;
}

/// @private
template <typename S>
WJR_PURE bool __equal_pointer(const basic_biginteger<S> *lhs,
                              const biginteger_data *rhs) {
    return lhs->__get_data() == rhs;
}

/// @private
template <typename S>
WJR_PURE bool __equal_pointer(const biginteger_data *lhs,
                              const basic_biginteger<S> *rhs) {
    return lhs == rhs->__get_data();
}

/// @private
WJR_PURE inline bool __equal_pointer(const biginteger_data *lhs,
                                     const biginteger_data *rhs) {
    return lhs == rhs;
}

template <typename S>
from_chars_result<> __from_chars_impl(const char *first, const char *last,
                                      basic_biginteger<S> *dst, unsigned int base) {

    uint8_t ch;
    from_chars_result<> result{first, std::errc{}};

    auto invalid = [&dst, &result]() {
        dst->clear();
        result.ec = std::errc::invalid_argument;
        return result;
    };

    if (first == last) {
        return invalid();
    }

    while (convert_details::__isspace(ch = *first++) && first != last)
        ;

    int sign = 0;
    if (ch == '-') {
        sign = 1;

        if (WJR_UNLIKELY(first == last)) {
            return invalid();
        }

        ch = *first++;
    }

    if (base == 0) {
        base = 10;
        if (ch == '0') {
            base = 8;
            if (WJR_UNLIKELY(first == last)) {
                return invalid();
            }
            ch = *first++;
            if (ch == 'x' || ch == 'X') {
                base = 16;
                if (WJR_UNLIKELY(first == last)) {
                    return invalid();
                }
                ch = *first++;
            } else {
                if (ch == 'b' || ch == 'B') {
                    base = 2;
                    if (WJR_UNLIKELY(first == last)) {
                        return invalid();
                    }
                    ch = *first++;
                }
            }
        }
    }

    auto __first = first;

    while (ch == '0') {
        if (WJR_UNLIKELY(first == last)) {
            dst->clear();
            result.ptr = first;
            return result;
        }

        ch = *first++;
    }

    const auto start = first;
    if (base <= 10) {
        while (ch >= '0' && ch < '0' + base) {
            if (first == last) {
                ++first;
                break;
            }

            ch = *first++;
        }
    } else {
        ch = 0;
        while (ch < base) {
            if (first == last) {
                ++first;
                break;
            }

            ch = char_converter.from(*first++);
        }
    }

    if (WJR_UNLIKELY(first == __first)) {
        return invalid();
    }

    if (first == start) {
        dst->clear();
        result.ptr = first;
        return result;
    }

    const size_t str_size = first - start;
    size_t capacity;

    switch (base) {
    case 2: {
        capacity = __ceil_div(str_size, 64);
        break;
    }
    case 8: {
        capacity = __ceil_div(str_size * 3, 64);
        break;
    }
    case 16: {
        capacity = __ceil_div(str_size, 16);
        break;
    }
    case 4:
    case 32: {
        const int bits = base == 4 ? 2 : 5;
        capacity = __ceil_div(str_size * bits, 64);
        break;
    }
    case 10: {
        // capacity = (str_size * log2(10) + 63) / 64;
        capacity = __ceil_div(str_size * 10 / 3, 64);
        break;
    }
    default: {
        return invalid();
    }
    }

    dst->reserve(capacity);
    auto ptr = dst->data();
    int32_t dssize = biginteger_from_chars(start - 1, first - 1, ptr, base) - ptr;
    dssize = __fasts_conditional_negate<int32_t>(sign, dssize);
    dst->set_ssize(dssize);
    result.ptr = first;
    return result;
}

inline int32_t __compare_impl(const biginteger_data *lhs, const biginteger_data *rhs) {
    const auto lssize = lhs->get_ssize();
    const auto rssize = rhs->get_ssize();

    if (lssize != rssize) {
        return lssize < rssize ? -1 : 1;
    }

    const int32_t ans = reverse_compare_n(lhs->data(), rhs->data(), __fasts_abs(lssize));
    return lssize < 0 ? -ans : ans;
}

inline int32_t __compare_ui_impl(const biginteger_data *lhs, uint64_t rhs) {
    const int32_t lssize = lhs->get_ssize();

    if (lssize == 0) {
        return -(rhs != 0);
    }

    if (lssize == 1) {
        const uint64_t lvalue = lhs->data()[0];
        return (lvalue != rhs ? (lvalue < rhs ? -1 : 1) : 0);
    }

    return lssize;
}

inline int32_t __compare_si_impl(const biginteger_data *lhs, int64_t rhs) {
    const int32_t lssize = lhs->get_ssize();
    const int32_t rssize = rhs == 0 ? 0 : __fasts_conditional_negate<int32_t>(rhs < 0, 1);

    if (lssize != rssize) {
        return lssize - rssize;
    }

    if (lssize == 0) {
        return 0;
    }

    const uint64_t lvalue = lhs->data()[0];
    const uint64_t rvalue = rhs >= 0 ? to_unsigned(rhs) : -to_unsigned(rhs);

    if (lvalue == rvalue) {
        return 0;
    }

    if (lvalue > rvalue) {
        return lssize;
    }

    return -lssize;
}

template <bool xsign, typename S>
void __addsub_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, uint64_t rhs) {
    const int32_t lssize = lhs->get_ssize();
    if (lssize == 0) {
        dst->reserve(1);

        if (rhs == 0) {
            dst->set_ssize(0);
        } else {
            dst->front() = rhs;
            dst->set_ssize(__fasts_conditional_negate<int32_t>(xsign, 1));
        }

        return;
    }

    const uint32_t lusize = __fasts_abs(lssize);
    dst->reserve(lusize + 1);

    const auto dp = dst->data();
    const auto lp = lhs->data();

    using compare = std::conditional_t<xsign, std::less<>, std::greater<>>;
    int32_t dssize;

    if (compare{}(lssize, 0)) {
        const auto cf = addc_1(dp, lp, lusize, rhs);
        dssize = __fasts_conditional_negate<int32_t>(xsign, lusize + cf);
        if (cf) {
            dp[lusize] = 1;
        }
    } else {
        if (lusize == 1 && lp[0] < rhs) {
            dp[0] = rhs - lp[0];
            dssize = __fasts_conditional_negate<int32_t>(xsign, 1);
        } else {
            (void)subc_1(dp, lp, lusize, rhs);
            dssize = __fasts_conditional_negate<int32_t>(!xsign,
                                                         lusize - (dp[lusize - 1] == 0));
        }
    }

    dst->set_ssize(dssize);
}

template <typename S>
void __ui_sub_impl(basic_biginteger<S> *dst, uint64_t lhs, const biginteger_data *rhs) {
    const int32_t rssize = rhs->get_ssize();
    if (rssize == 0) {
        dst->reserve(1);

        if (lhs == 0) {
            dst->set_ssize(0);
        } else {
            dst->front() = lhs;
            dst->set_ssize(1);
        }

        return;
    }

    const uint32_t rusize = __fasts_abs(rssize);
    dst->reserve(rusize);

    const auto dp = dst->data();
    const auto rp = rhs->data();
    int32_t dssize;

    if (rssize < 0) {
        const auto cf = addc_1(dp, rp, rusize, lhs);
        dssize = rusize + cf;
        if (cf) {
            dp[rusize] = 1;
        }
    } else {
        // lhs >= rhs
        if (rusize == 1 && lhs >= rp[0]) {
            dp[0] = lhs - rp[0];
            dssize = dp[0] != 0;
        }
        // lhs < rhs
        else {
            (void)subc_1(dp, rp, rusize, lhs);
            dssize = __fasts_negate<int32_t>(rusize - (dp[rusize - 1] == 0));
        }
    }

    dst->set_ssize(dssize);
}

template <bool xsign, typename S>
void __addsub_impl(basic_biginteger<S> *dst, const biginteger_data *lhs,
                   const biginteger_data *rhs) {
    int32_t lssize = lhs->get_ssize();
    int32_t rssize = __fasts_conditional_negate<int32_t>(xsign, rhs->get_ssize());
    uint32_t lusize = __fasts_abs(lssize);
    uint32_t rusize = __fasts_abs(rssize);

    if (lusize < rusize) {
        std::swap(lhs, rhs);
        std::swap(lssize, rssize);
        std::swap(lusize, rusize);
    }

    dst->reserve(lusize + 1);

    const auto dp = dst->data();
    const auto lp = lhs->data();
    const auto rp = rhs->data();
    int32_t dssize;

    if (rusize == 0) {
        std::copy_n(lp, lusize, dp);
        dst->set_ssize(lssize);
        return;
    }

    // different sign
    if ((lssize ^ rssize) < 0) {
        const int32_t ans =
            static_cast<int32_t>(abs_subc_s_pos(dp, lp, lusize, rp, rusize));
        dssize = __fasts_negate_with<int32_t>(lssize, ans);
    } else {
        const auto cf = addc_s(dp, lp, lusize, rp, rusize);
        dssize = __fasts_negate_with<int32_t>(lssize, lusize + cf);
        if (cf) {
            dp[lusize] = 1;
        }
    }

    dst->set_ssize(dssize);
}

template <typename S>
void __mul_ui_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, uint64_t rhs) {
    const int32_t lssize = lhs->get_ssize();
    const uint32_t lusize = __fasts_abs(lssize);

    if (lusize == 0 || rhs == 0) {
        dst->set_ssize(0);
        return;
    }

    dst->reserve(lusize + 1);

    const auto dp = dst->data();
    const auto lp = lhs->data();

    const auto cf = mul_1(dp, lp, lusize, rhs);
    const uint32_t dusize = lusize + (cf != 0);
    if (cf != 0) {
        dp[lusize] = cf;
    }

    dst->set_ssize(__fasts_conditional_negate<int32_t>(lssize < 0, dusize));
}

template <typename S>
void __mul_impl(basic_biginteger<S> *dst, const biginteger_data *lhs,
                const biginteger_data *rhs) {
    int32_t lssize = lhs->get_ssize();
    int32_t rssize = rhs->get_ssize();
    uint32_t lusize = __fasts_abs(lssize);
    uint32_t rusize = __fasts_abs(rssize);

    if (lusize < rusize) {
        std::swap(lhs, rhs);
        std::swap(lusize, rusize);
    }

    if (WJR_UNLIKELY(rusize == 0)) {
        dst->set_ssize(0);
        return;
    }

    const int32_t mask = lssize ^ rssize;

    int32_t dssize;
    uint32_t dusize;

    if (rusize == 1) {
        dst->reserve(lusize + 1);
        const auto cf = mul_1(dst->data(), lhs->data(), lusize, rhs->data()[0]);
        dssize = __fasts_negate_with<int32_t>(mask, lusize + (cf != 0));
        if (cf != 0) {
            (*dst)[lusize] = cf;
        }
        dst->set_ssize(dssize);
        return;
    }

    dusize = lusize + rusize;

    using pointer = uint64_t *;

    auto dp = dst->data();
    auto lp = (pointer)(lhs->data());
    auto rp = (pointer)(rhs->data());

    std::optional<uninitialized<basic_biginteger<S>>> tmp;

    unique_stack_allocator stkal(math_details::stack_alloc);

    if (dst->capacity() < dusize) {
        tmp.emplace(dst->get_growth_capacity(dst->capacity(), dusize), in_place_reserve,
                    dst->get_allocator());
        dp = (**tmp).data();
    } else {
        if (dp == lp) {
            lp = (pointer)stkal.allocate(lusize * sizeof(uint64_t));
            if (dp == rp) {
                rp = lp;
            }
            std::copy_n(dp, lusize, lp);
        } else if (dp == rp) {
            rp = (pointer)stkal.allocate(rusize * sizeof(uint64_t));
            std::copy_n(dp, rusize, rp);
        }
    }

    if (WJR_UNLIKELY(lp == rp)) {
        sqr(dp, lp, lusize);
    } else {
        mul_s(dp, lp, lusize, rp, rusize);
    }

    const bool cf = dp[dusize - 1] == 0;
    dssize = __fasts_negate_with<int32_t>(mask, dusize - cf);

    if (tmp.has_value()) {
        *dst = **std::move(tmp);
        tmp->reset();
    }

    dst->set_ssize(dssize);
}

template <typename S>
void __addsubmul_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, uint64_t rhs,
                      int32_t xmask) {
    const int32_t lssize = lhs->get_ssize();

    if (lssize == 0 || rhs == 0) {
        return;
    }

    const uint32_t lusize = __fasts_abs(lssize);
    int32_t dssize = dst->get_ssize();

    if (dssize == 0) {
        dst->reserve(lusize + 1);
        const auto dp = dst->data();
        const auto cf = mul_1(dp, lhs->data(), lusize, rhs);
        dssize = lssize + (cf != 0);
        if (cf != 0) {
            dp[lusize] = cf;
        }

        dst->set_ssize(__fasts_negate_with(xmask, dssize));
        return;
    }

    xmask ^= lssize;
    xmask ^= dssize;

    const uint32_t dusize = __fasts_abs(dssize);

    uint32_t new_dusize = std::max(lusize, dusize);
    const uint32_t min_size = std::min(lusize, dusize);
    dst->reserve(new_dusize + 1);

    auto dp = dst->data();
    auto lp = lhs->data();

    uint64_t cf;

    // dst += abs(lhs) * abs(rhs)
    if (xmask >= 0) {
        cf = addmul_1(dp, lp, min_size, rhs);

        dp += min_size;
        lp += min_size;

        int32_t sdelta = lusize - dusize;

        if (sdelta != 0) {
            uint64_t cf2;
            if (sdelta > 0) {
                cf2 = mul_1(dp, lp, sdelta, rhs);
            } else {
                sdelta = -sdelta;
                cf2 = 0;
            }

            cf = cf2 + addc_1(dp, dp, sdelta, cf);
        }

        dp[sdelta] = cf;
        new_dusize += (cf != 0);
    }
    // dst -= abs(lhs) * abs(rhs)
    else {
        cf = submul_1(dp, lp, min_size, rhs);

        do {
            if (dusize >= lusize) {
                if (dusize != lusize) {
                    cf = subc_1(dp + lusize, dp + lusize, dusize - lusize, cf);
                }

                if (cf != 0) {
                    cf += negate_n(dp, dp, new_dusize) - 1;
                    dp[new_dusize] = cf;
                    ++new_dusize;
                    dssize = __fasts_negate(dssize);
                }
            } else {
                cf += negate_n(dp, dp, dusize) - 1;

                const auto cf2 = cf == (uint64_t)in_place_max;
                cf += cf2;

                const auto cf3 = mul_1(dp + dusize, lp + dusize, lusize - dusize, rhs);
                cf = cf3 + addc_1(dp + dusize, dp + dusize, lusize - dusize, cf);

                dp[new_dusize] = cf;
                new_dusize += (cf != 0);

                if (cf2) {
                    (void)subc_1(dp + dusize, dp + dusize, new_dusize - dusize, 1);
                }

                dssize = __fasts_negate(dssize);
            }

            new_dusize = normalize(dp, new_dusize);
        } while (0);
    }

    dst->set_ssize(__fasts_conditional_negate<int32_t>(dssize < 0, new_dusize));
}

template <typename S>
void __addsubmul_impl(basic_biginteger<S> *dst, const biginteger_data *lhs,
                      const biginteger_data *rhs, int32_t xmask) {
    int32_t lssize = lhs->get_ssize();
    int32_t rssize = rhs->get_ssize();

    if (lssize == 0 || rssize == 0) {
        return;
    }

    uint32_t lusize = __fasts_abs(lssize);
    uint32_t rusize = __fasts_abs(rssize);

    if (lusize < rusize) {
        std::swap(lhs, rhs);
        std::swap(lssize, rssize);
        std::swap(lusize, rusize);
    }

    xmask ^= rssize;

    if (rusize == 1) {
        __addsubmul_impl(dst, lhs, rhs->data()[0], xmask);
        return;
    }

    xmask ^= lssize;

    int32_t dssize = dst->get_ssize();
    xmask ^= dssize;
    uint32_t dusize = __fasts_abs(dssize);

    uint32_t tusize = lusize + rusize;
    dst->reserve(std::max(tusize, dusize) + 1);
    const auto dp = dst->data();

    if (dssize == 0) {
        mul_s(dp, lhs->data(), lusize, rhs->data(), rusize);
        tusize -= dp[tusize - 1] == 0;
        dst->set_ssize(__fasts_conditional_negate<int32_t>(xmask < 0, tusize));
        return;
    }

    unique_stack_allocator stkal(math_details::stack_alloc);
    auto tp = (uint64_t *)stkal.allocate(tusize * sizeof(uint64_t));

    mul_s(tp, lhs->data(), lusize, rhs->data(), rusize);
    tusize -= tp[tusize - 1] == 0;

    auto up = dp;
    uint32_t uusize = dusize;

    if (xmask >= 0) {
        if (uusize < tusize) {
            up = tp;
            uusize = tusize;
            tp = dp;
            tusize = dusize;

            dusize = uusize;
        }

        const auto cf = addc_s(dp, up, uusize, tp, tusize);
        dp[uusize] = cf;
        dusize = uusize + (cf != 0);
    } else {
        if (uusize < tusize ||
            (uusize == tusize && reverse_compare_n(up, tp, uusize) < 0)) {
            up = tp;
            uusize = tusize;
            tp = dp;
            tusize = dusize;

            dusize = uusize;

            dssize = __fasts_negate(dssize);
        }

        (void)subc_s(dp, up, uusize, tp, tusize);
        dssize = normalize(dp, dusize);
    }

    dst->set_ssize(__fasts_conditional_negate<int32_t>(dssize < 0, dusize));
}

template <typename S>
void __mul_2exp_impl(basic_biginteger<S> *dst, const biginteger_data *lhs, size_t shift) {
    int32_t lssize = lhs->get_ssize();

    if (lssize == 0) {
        dst->set_ssize(0);
        return;
    }

    uint32_t lusize = __fasts_abs(lssize);
    size_t offset = shift / 64;
    shift %= 64;
    uint32_t dusize = lusize + offset;

    dst->reserve(dusize + 1);
    const auto dp = dst->data();
    const auto lp = lhs->data();

    const auto cf = lshift_n(dp + offset, lp, lusize, shift);
    set_n(dp, 0, offset);

    dp[dusize] = cf;
    dusize += (cf != 0);

    dst->set_ssize(__fasts_conditional_negate<int32_t>(lssize < 0, dusize));
}

template <typename S0, typename S1>
void __tdiv_qr_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                    const biginteger_data *num, const biginteger_data *div) {
    WJR_ASSERT_ASSUME(!__equal_pointer(quot, rem), "quot should not be the same as rem");

    const auto nssize = num->get_ssize();
    const auto dssize = div->get_ssize();
    const auto nusize = __fasts_abs(nssize);
    auto dusize = __fasts_abs(dssize);
    int32_t qssize = nusize - dusize + 1;

    WJR_ASSERT(dusize != 0, "division by zero");

    rem->reserve(dusize);
    auto rp = rem->data();

    // num < div
    if (qssize <= 0) {
        auto np = num->data();
        if (np != rp) {
            std::copy_n(np, nusize, rp);
            rem->set_ssize(nssize);
        }

        quot->set_ssize(0);
        return;
    }

    using pointer = uint64_t *;

    quot->reserve(qssize);
    auto qp = quot->data();

    auto np = (pointer)num->data();
    auto dp = (pointer)div->data();

    unique_stack_allocator stkal(math_details::stack_alloc);

    if (dp == rp || dp == qp) {
        auto tp = (pointer)stkal.allocate(dusize * sizeof(uint64_t));
        std::copy_n(dp, dusize, tp);
        dp = tp;
    }

    if (np == rp || np == qp) {
        auto tp = (pointer)stkal.allocate(nusize * sizeof(uint64_t));
        std::copy_n(np, nusize, tp);
        np = tp;
    }

    div_qr_s(qp, rp, np, nusize, dp, dusize);

    qssize -= qp[qssize - 1] == 0;
    dusize = normalize(rp, dusize);

    quot->set_ssize(__fasts_conditional_negate<int32_t>((nssize ^ dssize) < 0, qssize));
    rem->set_ssize(__fasts_conditional_negate<int32_t>(nssize < 0, dusize));
}

template <typename S>
void __tdiv_q_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                   const biginteger_data *div) {
    const auto nssize = num->get_ssize();
    const auto dssize = div->get_ssize();
    const auto nusize = __fasts_abs(nssize);
    const auto dusize = __fasts_abs(dssize);
    int32_t qssize = nusize - dusize + 1;

    WJR_ASSERT(dusize != 0, "division by zero");

    // num < div
    if (qssize <= 0) {
        quot->set_ssize(0);
        return;
    }

    using pointer = uint64_t *;

    quot->reserve(qssize);
    auto qp = quot->data();

    auto np = (pointer)num->data();
    auto dp = (pointer)div->data();

    unique_stack_allocator stkal(math_details::stack_alloc);

    if (dp == qp) {
        auto tp = (pointer)stkal.allocate(dusize * sizeof(uint64_t));
        std::copy_n(dp, dusize, tp);
        dp = tp;
    }

    if (np == qp) {
        auto tp = (pointer)stkal.allocate(nusize * sizeof(uint64_t));
        std::copy_n(np, nusize, tp);
        np = tp;
    }

    const auto rp = (pointer)stkal.allocate(dusize * sizeof(uint64_t));

    div_qr_s(qp, rp, np, nusize, dp, dusize);

    qssize -= qp[qssize - 1] == 0;

    quot->set_ssize(__fasts_conditional_negate<int32_t>((nssize ^ dssize) < 0, qssize));
}

template <typename S>
void __tdiv_r_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                   const biginteger_data *div) {
    const auto nssize = num->get_ssize();
    const auto dssize = div->get_ssize();
    const auto nusize = __fasts_abs(nssize);
    auto dusize = __fasts_abs(dssize);
    const int32_t qssize = nusize - dusize + 1;

    WJR_ASSERT(dusize != 0, "division by zero");

    rem->reserve(dusize);
    auto rp = rem->data();

    // num < div
    if (qssize <= 0) {
        auto np = num->data();
        if (np != rp) {
            std::copy_n(np, nusize, rp);
            rem->set_ssize(nssize);
        }

        return;
    }

    using pointer = uint64_t *;

    auto np = (pointer)num->data();
    auto dp = (pointer)div->data();

    unique_stack_allocator stkal(math_details::stack_alloc);

    if (dp == rp) {
        const auto tp = (pointer)stkal.allocate(dusize * sizeof(uint64_t));
        std::copy_n(dp, dusize, tp);
        dp = tp;
    }

    if (np == rp) {
        const auto tp = (pointer)stkal.allocate(nusize * sizeof(uint64_t));
        std::copy_n(np, nusize, tp);
        np = tp;
    }

    const auto qp = (pointer)stkal.allocate(qssize * sizeof(uint64_t));

    div_qr_s(qp, rp, np, nusize, dp, dusize);

    dusize = normalize(rp, dusize);

    rem->set_ssize(__fasts_conditional_negate<int32_t>(nssize < 0, dusize));
}

template <typename S0, typename S1>
uint64_t __tdiv_qr_ui_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                           const biginteger_data *num, uint64_t div) {
    const auto nssize = num->get_ssize();

    WJR_ASSERT(div != 0, "division by zero");

    if (nssize == 0) {
        quot->set_ssize(0);
        rem->set_ssize(0);
        return 0;
    }

    const auto nusize = __fasts_abs(nssize);
    quot->reserve(nusize);
    const auto qp = quot->data();
    const auto np = num->data();

    uint64_t remv;
    div_qr_1(qp, remv, np, nusize, div);

    if (remv == 0) {
        rem->set_ssize(0);
    } else {
        rem->reserve(1);
        rem->front() = remv;
        rem->set_ssize(__fasts_conditional_negate<int32_t>(nssize < 0, 1));
    }

    const auto qusize = nusize - (qp[nusize - 1] == 0);

    quot->set_ssize(__fasts_conditional_negate<int32_t>(nssize < 0, qusize));
    return remv;
}

template <typename S>
uint64_t __tdiv_q_ui_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                          uint64_t div) {
    const auto nssize = num->get_ssize();

    WJR_ASSERT(div != 0, "division by zero");

    if (nssize == 0) {
        quot->set_ssize(0);
        return 0;
    }

    const auto nusize = __fasts_abs(nssize);
    quot->reserve(nusize);
    const auto qp = quot->data();
    const auto np = num->data();

    uint64_t remv;
    div_qr_1(qp, remv, np, nusize, div);

    const auto qusize = nusize - (qp[nusize - 1] == 0);

    quot->set_ssize(__fasts_conditional_negate<int32_t>(nssize < 0, qusize));
    return remv;
}

template <typename S>
uint64_t __tdiv_r_ui_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                          uint64_t div) {
    const auto nssize = num->get_ssize();

    WJR_ASSERT(div != 0, "division by zero");

    if (nssize == 0) {
        rem->set_ssize(0);
        return 0;
    }

    const auto nusize = __fasts_abs(nssize);
    const auto np = num->data();

    uint64_t remv;
    remv = mod_1(np, nusize, div);

    if (remv == 0) {
        rem->set_ssize(0);
    } else {
        rem->reserve(1);
        rem->front() = remv;
        rem->set_ssize(__fasts_conditional_negate<int32_t>(nssize < 0, 1));
    }

    return remv;
}

template <typename S0, typename S1, typename T, WJR_REQUIRES_I(is_nonbool_integral_v<T>)>
uint64_t __tdiv_qr_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                        const biginteger_data *num, T div) {
    if constexpr (std::is_unsigned_v<T>) {
        return __tdiv_qr_ui_impl(quot, rem, num, div);
    } else {
        uint64_t udiv = to_unsigned(div);
        bool xsign = false;
        if (div < 0) {
            udiv = -udiv;
            xsign = true;
        }

        uint64_t remv = __tdiv_qr_ui_impl(quot, rem, num, udiv);

        quot->conditional_negate(xsign);
        return remv;
    }
}

template <typename S, typename T, WJR_REQUIRES_I(is_nonbool_integral_v<T>)>
uint64_t __tdiv_q_impl(basic_biginteger<S> *quot, const biginteger_data *num, T div) {
    if constexpr (std::is_unsigned_v<T>) {
        return __tdiv_q_ui_impl(quot, num, div);
    } else {
        uint64_t udiv = to_unsigned(div);
        bool xsign = false;
        if (div < 0) {
            udiv = -udiv;
            xsign = true;
        }

        uint64_t remv = __tdiv_q_ui_impl(quot, num, udiv);

        quot->conditional_negate(xsign);
        return remv;
    }
}

template <typename S, typename T, WJR_REQUIRES_I(is_nonbool_integral_v<T>)>
uint64_t __tdiv_r_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                       uint64_t div) {
    if constexpr (std::is_unsigned_v<T>) {
        return __tdiv_q_ui_impl(rem, num, div);
    } else {
        uint64_t udiv = to_unsigned(div);
        if (div < 0) {
            udiv = -udiv;
        }

        return __tdiv_r_ui_impl(rem, num, udiv);
    }
}

template <typename S0, typename S1>
void __fdiv_qr_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                    const biginteger_data *num, const biginteger_data *div) {

    WJR_ASSERT_ASSUME(!__equal_pointer(quot, rem), "quot should not be the same as rem");

    unique_stack_allocator stkal(math_details::stack_alloc);

    biginteger_data tmp_div;

    const auto dssize = div->get_ssize();

    if (__equal_pointer(div, quot) || __equal_pointer(div, rem)) {
        const auto dusize = __fasts_abs(dssize);
        const auto ptr = (uint64_t *)stkal.allocate(dusize * sizeof(uint64_t));
        tmp_div = {ptr, dssize, 0};
        std::copy_n(div->data(), dusize, ptr);

        div = &tmp_div;
    }

    const auto xsize = num->get_ssize() ^ dssize;

    __tdiv_qr_impl(quot, rem, num, div);

    if (xsize < 0 && !rem->empty()) {
        __sub_impl(quot, quot->__get_data(), 1u);
        __add_impl(rem, rem->__get_data(), div);
    }
}

template <typename S>
void __fdiv_q_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                   const biginteger_data *div) {
    unique_stack_allocator stkal(math_details::stack_alloc);
    stack_biginteger rem(stkal);

    const auto xsize = num->get_ssize() ^ div->get_ssize();

    __tdiv_qr_impl(quot, &rem, num, div);

    if (xsize < 0 && !rem.empty()) {
        __sub_impl(quot, quot->__get_data(), 1u);
    }
}

template <typename S>
void __fdiv_r_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                   const biginteger_data *div) {
    unique_stack_allocator stkal(math_details::stack_alloc);

    biginteger_data tmp_div;

    const auto dssize = div->get_ssize();

    if (__equal_pointer(div, rem)) {
        const auto dusize = __fasts_abs(dssize);
        const auto ptr = (uint64_t *)stkal.allocate(dusize * sizeof(uint64_t));
        tmp_div = {ptr, dssize, 0};
        std::copy_n(div->data(), dusize, ptr);

        div = &tmp_div;
    }

    const auto xsize = num->get_ssize() ^ dssize;

    __tdiv_r_impl(rem, num, div);

    if (xsize < 0 && !rem->empty()) {
        __add_impl(rem, rem->__get_data(), div);
    }
}

template <typename S0, typename S1, typename T, WJR_REQUIRES_I(is_nonbool_integral_v<T>)>
uint64_t __fdiv_qr_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                        const biginteger_data *num, T div) {
    if constexpr (std::is_unsigned_v<T>) {
        const auto xssize = num->get_ssize();

        uint64_t remv = __tdiv_qr_ui_impl(quot, rem, num, div);

        if (xssize < 0 && remv != 0) {
            WJR_ASSERT(rem->is_negate());

            __sub_impl(quot, quot->__get_data(), 1u);
            rem->set_ssize(1);
            remv = div - remv;
            rem->front() = remv;
        }

        return remv;
    } else {
        uint64_t udiv = to_unsigned(div);
        int32_t xsign = 0;

        if (div < 0) {
            udiv = -udiv;
            xsign = -1;
        }

        const int32_t nssize = num->get_ssize();

        uint64_t remv = __tdiv_qr_ui_impl(quot, rem, num, udiv);

        quot->conditional_negate(xsign);

        if ((nssize ^ xsign) < 0 && remv != 0) {
            __sub_impl(quot, quot->__get_data(), 1u);
            rem->set_ssize(__fasts_conditional_negate(xsign < 0, 1));
            remv = div - remv;
            rem->front() = remv;
        }

        return remv;
    }
}

template <typename S, typename T, WJR_REQUIRES_I(is_nonbool_integral_v<T>)>
uint64_t __fdiv_q_impl(basic_biginteger<S> *quot, const biginteger_data *num, T div) {
    if constexpr (std::is_unsigned_v<T>) {
        const auto xssize = num->get_ssize();

        uint64_t remv = __tdiv_q_ui_impl(quot, num, div);

        if (xssize < 0 && remv != 0) {
            __sub_impl(quot, quot->__get_data(), 1u);
            remv = div - remv;
        }

        return remv;
    } else {
        uint64_t udiv = to_unsigned(div);
        int32_t xsign = 0;

        if (div < 0) {
            udiv = -udiv;
            xsign = -1;
        }

        const int32_t nssize = num->get_ssize();

        uint64_t remv = __tdiv_q_ui_impl(quot, num, udiv);

        quot->conditional_negate(xsign);

        if ((nssize ^ xsign) < 0 && remv != 0) {
            __sub_impl(quot, quot->__get_data(), 1u);
            remv = div - remv;
        }

        return remv;
    }
}

template <typename S, typename T, WJR_REQUIRES_I(is_nonbool_integral_v<T>)>
uint64_t __fdiv_r_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                       uint64_t div) {
    if constexpr (std::is_unsigned_v<T>) {
        const auto xssize = num->get_ssize();

        uint64_t remv = __tdiv_r_ui_impl(rem, num, div);

        if (xssize < 0 && remv != 0) {
            WJR_ASSERT(rem->is_negate());

            rem->set_ssize(1);
            remv = div - remv;
            rem->front() = remv;
        }

        return remv;
    } else {
        uint64_t udiv = to_unsigned(div);
        int32_t xsign = 0;

        if (div < 0) {
            udiv = -udiv;
            xsign = -1;
        }

        const int32_t nssize = num->get_ssize();

        uint64_t remv = __tdiv_r_ui_impl(rem, num, udiv);

        if ((nssize ^ xsign) < 0 && remv != 0) {
            rem->set_ssize(__fasts_conditional_negate(xsign < 0, 1));
            remv = div - remv;
            rem->front() = remv;
        }

        return remv;
    }
}

template <typename S0, typename S1>
void __cdiv_qr_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                    const biginteger_data *num, const biginteger_data *div) {

    WJR_ASSERT_ASSUME(!__equal_pointer(quot, rem), "quot should not be the same as rem");

    unique_stack_allocator stkal(math_details::stack_alloc);

    biginteger_data tmp_div;

    const auto dssize = div->get_ssize();

    if (__equal_pointer(div, quot) || __equal_pointer(div, rem)) {
        const auto dusize = __fasts_abs(dssize);
        const auto ptr = (uint64_t *)stkal.allocate(dusize * sizeof(uint64_t));
        tmp_div = {ptr, dssize, 0};
        std::copy_n(div->data(), dusize, ptr);

        div = &tmp_div;
    }

    const auto xsize = num->get_ssize() ^ dssize;

    __tdiv_qr_impl(quot, rem, num, div);

    if (xsize >= 0 && !rem->empty()) {
        __add_impl(quot, quot->__get_data(), 1u);
        __sub_impl(rem, rem->__get_data(), div);
    }
}

template <typename S>
void __cdiv_q_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                   const biginteger_data *div) {
    unique_stack_allocator stkal(math_details::stack_alloc);
    stack_biginteger rem(stkal);

    const auto xsize = num->get_ssize() ^ div->get_ssize();

    __tdiv_qr_impl(quot, &rem, num, div);

    if (xsize >= 0 && !rem.empty()) {
        __add_impl(quot, quot->__get_data(), 1u);
    }
}

template <typename S>
void __cdiv_r_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                   const biginteger_data *div) {
    unique_stack_allocator stkal(math_details::stack_alloc);

    biginteger_data tmp_div;

    const auto dssize = div->get_ssize();

    if (__equal_pointer(div, rem)) {
        const auto dusize = __fasts_abs(dssize);
        const auto ptr = (uint64_t *)stkal.allocate(dusize * sizeof(uint64_t));
        tmp_div = {ptr, dssize, 0};
        std::copy_n(div->data(), dusize, ptr);

        div = &tmp_div;
    }

    const auto xsize = num->get_ssize() ^ dssize;

    __tdiv_r_impl(rem, num, div);

    if (xsize >= 0 && !rem->empty()) {
        __sub_impl(rem, rem->__get_data(), div);
    }
}

template <typename S0, typename S1, typename T, WJR_REQUIRES_I(is_nonbool_integral_v<T>)>
uint64_t __cdiv_qr_impl(basic_biginteger<S0> *quot, basic_biginteger<S1> *rem,
                        const biginteger_data *num, T div) {
    if constexpr (std::is_unsigned_v<T>) {
        const auto xssize = num->get_ssize();

        uint64_t remv = __tdiv_qr_ui_impl(quot, rem, num, div);

        if (xssize >= 0 && remv != 0) {
            WJR_ASSERT(rem->is_negate());

            __add_impl(quot, quot->__get_data(), 1u);
            rem->set_ssize(-1);
            remv = div - remv;
            rem->front() = remv;
        }

        return remv;
    } else {
        uint64_t udiv = to_unsigned(div);
        int32_t xsign = 0;

        if (div < 0) {
            udiv = -udiv;
            xsign = -1;
        }

        const int32_t nssize = num->get_ssize();

        uint64_t remv = __tdiv_qr_ui_impl(quot, rem, num, udiv);

        quot->conditional_negate(xsign);

        if ((nssize ^ xsign) >= 0 && remv != 0) {
            __add_impl(quot, quot->__get_data(), 1u);
            rem->set_ssize(__fasts_conditional_negate(xsign >= 0, 1));
            remv = div - remv;
            rem->front() = remv;
        }

        return remv;
    }
}

template <typename S, typename T, WJR_REQUIRES_I(is_nonbool_integral_v<T>)>
uint64_t __cdiv_q_impl(basic_biginteger<S> *quot, const biginteger_data *num, T div) {
    if constexpr (std::is_unsigned_v<T>) {
        const auto xssize = num->get_ssize();

        uint64_t remv = __tdiv_q_ui_impl(quot, num, div);

        if (xssize >= 0 && remv != 0) {
            __add_impl(quot, quot->__get_data(), 1u);
            remv = div - remv;
        }

        return remv;
    } else {
        uint64_t udiv = to_unsigned(div);
        int32_t xsign = 0;

        if (div < 0) {
            udiv = -udiv;
            xsign = -1;
        }

        const int32_t nssize = num->get_ssize();

        uint64_t remv = __tdiv_q_ui_impl(quot, num, udiv);

        quot->conditional_negate(xsign);

        if ((nssize ^ xsign) >= 0 && remv != 0) {
            __add_impl(quot, quot->__get_data(), 1u);
            remv = div - remv;
        }

        return remv;
    }
}

template <typename S, typename T, WJR_REQUIRES_I(is_nonbool_integral_v<T>)>
uint64_t __cdiv_r_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                       uint64_t div) {
    if constexpr (std::is_unsigned_v<T>) {
        const auto xssize = num->get_ssize();

        uint64_t remv = __tdiv_r_ui_impl(rem, num, div);

        if (xssize >= 0 && remv != 0) {
            WJR_ASSERT(rem->is_negate());

            rem->set_ssize(-1);
            remv = div - remv;
            rem->front() = remv;
        }

        return remv;
    } else {
        uint64_t udiv = to_unsigned(div);
        int32_t xsign = 0;

        if (div < 0) {
            udiv = -udiv;
            xsign = -1;
        }

        const int32_t nssize = num->get_ssize();

        uint64_t remv = __tdiv_r_ui_impl(rem, num, udiv);

        if ((nssize ^ xsign) >= 0 && remv != 0) {
            rem->set_ssize(__fasts_conditional_negate(xsign >= 0, 1));
            remv = div - remv;
            rem->front() = remv;
        }

        return remv;
    }
}

template <typename S>
void __tdiv_q_2exp_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                        size_t shift) {
    int32_t nssize = num->get_ssize();
    uint32_t nusize = __fasts_abs(nssize);
    uint32_t offset = shift / 64;

    int32_t qssize = nusize - offset;

    if (qssize <= 0) {
        quot->set_ssize(0);
        return;
    }

    quot->reserve(qssize);
    const auto qp = quot->data();
    const auto np = num->data();

    (void)rshift_n(qp, np + offset, qssize, shift % 64);
    qssize -= qp[qssize - 1] == 0;

    quot->set_ssize(__fasts_conditional_negate<int32_t>(nssize < 0, qssize));
}

template <typename S>
void __tdiv_r_2exp_impl(basic_biginteger<S> *rem, const biginteger_data *num,
                        size_t shift) {
    int32_t nssize = num->get_ssize();
    uint32_t nusize = __fasts_abs(nssize);
    uint32_t offset = shift / 64;

    uint32_t rusize;

    if (nusize <= offset) {
        rusize = nusize;
        offset = nusize;
        rem->reserve(offset);
    } else {
        uint64_t high = num->data()[nusize] & (((uint64_t)(1) << (shift % 64)) - 1);
        if (high != 0) {
            rusize = offset + 1;
            rem->reserve(rusize);
            rem->data()[offset] = high;
        } else {
            offset = normalize(num->data(), offset);
            rusize = offset;
            rem->reserve(offset);
        }
    }

    if (!__equal_pointer(rem, num)) {
        std::copy_n(num->data(), rusize, rem->data());
    }

    rem->set_ssize(__fasts_conditional_negate<int32_t>(nssize < 0, rusize));
}

template <typename S>
void __cfdiv_q_2exp_impl(basic_biginteger<S> *quot, const biginteger_data *num,
                         size_t shift, int32_t xdir) {
    int32_t nssize = num->get_ssize();
    uint32_t nusize = __fasts_abs(nssize);
    uint32_t offset = shift / 64;

    int32_t qssize = nusize - offset;

    if (qssize <= 0) {
        if (nssize == 0) {
            quot->set_ssize(0);
            return;
        }

        quot->reserve(1);
        quot->front() = 1;

        quot->set_ssize((nssize ^ xdir) < 0 ? 0 : xdir);
        return;
    }

    quot->reserve(qssize + 1);
    const auto qp = quot->data();
    const auto np = num->data();

    uint64_t xmask = (nssize ^ xdir) < 0 ? 0 : (uint64_t)in_place_max;
    uint64_t round = 0;

    if (xmask) {
        // all is zero, then round is zero
        round = find_not_n(np, 0, offset) == offset ? 0 : 1;
    }

    round |= xmask & rshift_n(qp, np + offset, qssize, shift % 64);
    qssize -= qp[qssize - 1] == 0;

    if (WJR_LIKELY(round != 0)) {
        if (WJR_LIKELY(qssize != 0)) {
            const auto cf = addc_1(qp, qp, qssize, 1u);
            if (cf != 0) {
                qp[qssize] = cf;
                ++qssize;
            }
        } else {
            qp[0] = 1;
            qssize = 1;
        }
    }

    quot->set_ssize(__fasts_conditional_negate<int32_t>(nssize < 0, qssize));
}

template <typename S, typename Engine,
          WJR_REQUIRES_I(biginteger_uniform_random_bit_generator_v<Engine>)>
void __urandom_bit_impl(basic_biginteger<S> *dst, size_t size, Engine &engine) {
    const uint32_t dusize = size / 64;
    size %= 64;
    uint32_t dssize = dusize + (size != 0);

    dst->reserve(dssize);
    const auto dp = dst->data();

    do {
        for (uint64_t i = 0; i < dusize; ++i) {
            dp[i] = engine();
        }
    } while (0);

    if (size != 0) {
        dp[dusize] = engine() >> (64 - size);
    }

    dssize = normalize(dp, dssize);
    dst->set_ssize(dssize);
}

template <typename S, typename Engine,
          WJR_REQUIRES_I(biginteger_uniform_random_bit_generator_v<Engine>)>
void __urandom_exact_bit_impl(basic_biginteger<S> *dst, size_t size, Engine &engine) {
    if (WJR_UNLIKELY(size == 0)) {
        dst->set_ssize(0);
        return;
    }

    const uint32_t dusize = (size - 1) / 64;
    size = (size - 1) % 64;
    const uint32_t dssize = dusize + 1;

    dst->reserve(dssize);
    const auto dp = dst->data();

    do {
        for (uint64_t i = 0; i < dusize; ++i) {
            dp[i] = engine();
        }
    } while (0);

    do {
        uint64_t high = (uint64_t)(1) << size;

        if (size != 0) {
            high |= engine() >> (64 - size);
        }

        dp[dusize] = high;
    } while (0);

    dst->set_ssize(dssize);
}

template <typename S, typename Engine,
          WJR_REQUIRES_I(biginteger_uniform_random_bit_generator_v<Engine>)>
void __urandom_impl(basic_biginteger<S> *dst, const biginteger_data *limit,
                    Engine &engine) {
    std::optional<uninitialized<math_details::unique_stack_alloc>> stkal;
    uint32_t size = limit->size();

    WJR_ASSERT(size != 0);

    if (WJR_UNLIKELY(size == 1)) {
        std::uniform_int_distribution<uint64_t> head(0, limit->data()[0] - 1);
        const uint64_t gen = head(engine);

        dst->reserve(1);
        dst->front() = gen;
        dst->set_ssize(gen == 0 ? 0 : 1);
        return;
    }

    dst->reserve(size);

    const auto dp = dst->data();
    const uint64_t *lp = limit->data();

    if (__equal_pointer(dst, limit)) {
        stkal.emplace(math_details::stack_alloc);
        const auto tp = (uint64_t *)(*stkal)->allocate(size * sizeof(uint64_t));
        std::copy_n(lp, size, tp);
        lp = tp;
    }

    const size_t lst_pos = size - 1;
    const uint64_t lst_val = lp[lst_pos];
    std::uniform_int_distribution<uint64_t> head(0, lst_val);

    while (true) {
        uint64_t gen = head(engine);
        size_t pos = lst_pos;

        if (WJR_UNLIKELY(gen == lst_val)) {
            uint64_t now;
            do {
                dp[pos--] = gen;
                gen = engine();
                now = lp[pos];

                if (WJR_LIKELY(gen != now)) {
                    goto NEXT;
                }

            } while (pos);

            continue;

        NEXT:

            if (gen > now) {
                continue;
            }
        }

        dp[pos] = gen;
        while (pos) {
            dp[--pos] = engine();
        }

        break;
    }

    size = normalize(dp, size);
    dst->set_ssize(size);

    if (stkal.has_value()) {
        stkal->reset();
    }
}

template <typename S, typename Engine,
          WJR_REQUIRES_I(biginteger_uniform_random_bit_generator_v<Engine>)>
void __urandom_exact_impl(basic_biginteger<S> *dst, const biginteger_data *limit,
                          Engine &engine) {
    std::optional<uninitialized<math_details::unique_stack_alloc>> stkal;
    uint32_t size = limit->size();

    if (WJR_UNLIKELY(size <= 1)) {
        if (size == 0) {
            dst->set_ssize(0);
            return;
        }

        std::uniform_int_distribution<uint64_t> head(0, limit->data()[0]);
        const uint64_t gen = head(engine);

        dst->reserve(1);
        dst->front() = gen;
        dst->set_ssize(gen == 0 ? 0 : 1);
        return;
    }

    dst->reserve(size);

    const auto dp = dst->data();
    const uint64_t *lp = limit->data();

    if (__equal_pointer(dst, limit)) {
        stkal.emplace(math_details::stack_alloc);
        const auto tp = (uint64_t *)(*stkal)->allocate(size * sizeof(uint64_t));
        std::copy_n(lp, size, tp);
        lp = tp;
    }

    const size_t lst_pos = size - 1;
    const uint64_t lst_val = lp[lst_pos];
    std::uniform_int_distribution<uint64_t> head(0, lst_val);

    while (true) {
        uint64_t gen = head(engine);
        size_t pos = lst_pos;

        if (WJR_UNLIKELY(gen == lst_val)) {
            uint64_t now;
            do {
                dp[pos--] = gen;
                gen = engine();
                now = lp[pos];
            } while (gen == now && pos);

            if (gen > now) {
                continue;
            }
        }

        dp[pos] = gen;

        while (pos) {
            dp[--pos] = engine();
        }

        break;
    }

    size = normalize(dp, size);
    dst->set_ssize(size);

    if (stkal.has_value()) {
        stkal->reset();
    }
}

} // namespace biginteger_details

template <typename S>
from_chars_result<> from_chars(const char *first, const char *last,
                               basic_biginteger<S> &dst, unsigned int base) {
    return biginteger_details::__from_chars_impl(first, last, &dst, base);
}

template <typename S>
void negate(basic_biginteger<S> &dst) {
    dst.negate();
}

template <typename S>
void absolute(basic_biginteger<S> &dst) {
    dst.absolute();
}

template <typename S>
std::istream &operator>>(std::istream &is, basic_biginteger<S> &dst) {
    std::string str;
    is >> str;
    from_chars(str.data(), str.data() + str.size(), dst);
    return is;
}

template <typename Traits>
std::basic_ostream<char, Traits> &operator<<(std::basic_ostream<char, Traits> &os,
                                             const biginteger_data &src) {
    std::ios_base::iostate state = std::ios::goodbit;

    if (const std::ostream::sentry ok(os); ok) {
        unique_stack_allocator stkal(math_details::stack_alloc);

        vector<char, math_details::weak_stack_alloc<char>> buffer(stkal);
        buffer.reserve(128);

        const std::ios_base::fmtflags flags = os.flags();

        if ((flags & std::ios::showpos) && !src.is_negate()) {
            buffer.push_back('+');
        }

        int base = 10;

        if (const auto basefield = flags & std::ios::basefield; basefield != 0) {
            if (basefield == std::ios::oct) {
                base = 8;
                if (flags & std::ios::showbase) {
                    buffer.append({'0'});
                }
            } else if (basefield == std::ios::hex) {
                base = 16;
                if (flags & std::ios::showbase) {
                    buffer.append({'0', 'x'});
                }
            }
        }

        (void)to_chars_unchecked(std::back_inserter(buffer), src, base);

        if (!buffer.empty()) {
            __ostream_insert_unchecked(os, buffer.data(), buffer.size());
        }
    }

    os.setstate(state);
    return os;
}

} // namespace wjr

#endif

#endif // WJR_BIGINTEGER_HPP__
#ifndef WJR_BPLUS_TREE_HPP__
#define WJR_BPLUS_TREE_HPP__

#ifndef WJR_CONTAINER_GENERIC_BPLUS_TREE_HPP__
#define WJR_CONTAINER_GENERIC_BPLUS_TREE_HPP__

// Already included
// Already included
#ifndef WJR_CONTAINER_GENERIC_CONTAINER_TRAITS_HPP__
#define WJR_CONTAINER_GENERIC_CONTAINER_TRAITS_HPP__

#include <memory>
#include <type_traits>

// Already included

namespace wjr {

/**
 * @class container_fn<Alloc>
 * @brief The same characteristics and behavior of all allocator containers
 *
 * @details container must have the following member functions:
 * -# auto& __get_allocator() noexcept
 * -# void __destroy() noexcept
 * -# void __destroy_and_deallocate() noexcept
 * -# void __copy_element(const container& other)
 * -# void __take_storage(container&& other)
 * -# void __move_element(container&& other)
 * -# void __swap_storage(container& other)
 *
 * 1 : is used to manage the allocator of the container. \n
 * 2-3 : is used to destroy the container and deallocate the memory. \n
 * 4-7 : is used to assign the container data. Shouldn't change the allocator.
 *
 */
template <typename Alloc>
class container_fn {
private:
    using allocator_type = Alloc;
    using allocator_traits = std::allocator_traits<allocator_type>;
    using is_always_equal = typename allocator_traits::is_always_equal;
    using propagate_on_container_copy_assignment =
        typename allocator_traits::propagate_on_container_copy_assignment;
    using propagate_on_container_move_assignment =
        typename allocator_traits::propagate_on_container_move_assignment;
    using propagate_on_container_swap =
        typename allocator_traits::propagate_on_container_swap;

public:
    template <typename Container>
    WJR_CONSTEXPR20 static void
    copy_assign(Container &lhs, const Container &rhs) noexcept(
        noexcept(lhs.__copy_element(rhs)) &&
                !propagate_on_container_copy_assignment::value
            ? true
            : (noexcept(lhs.__get_allocator() = rhs.__get_allocator()) &&
                       is_always_equal::value
                   ? true
                   : noexcept(lhs.__destroy_and_deallocate()))) {
        if constexpr (propagate_on_container_copy_assignment::value) {
            auto &lhs_allocator = lhs.__get_allocator();
            auto &rhs_allocator = rhs.__get_allocator();
            if constexpr (!is_always_equal::value) {
                if (lhs_allocator != rhs_allocator) {
                    lhs.__destroy_and_deallocate();
                }
            }

            lhs_allocator = rhs_allocator;
        }

        lhs.__copy_element(rhs);
    }

    template <typename Container>
    WJR_CONSTEXPR20 static void move_assign(Container &lhs, Container &&rhs) noexcept(
        noexcept(lhs.__destroy_and_deallocate()) && noexcept(
            lhs.__take_storage(std::move(rhs))) &&
                std::disjunction_v<propagate_on_container_move_assignment,
                                   is_always_equal>
            ? (!propagate_on_container_move_assignment::value
                   ? true
                   : noexcept(lhs.__get_allocator() = std::move(rhs.__get_allocator())))
            : (noexcept(lhs.__destroy()) && noexcept(
                  lhs.__move_element(std::move(rhs))))) {
        if constexpr (std::disjunction_v<propagate_on_container_move_assignment,
                                         is_always_equal>) {
            lhs.__destroy_and_deallocate();
            if constexpr (propagate_on_container_move_assignment::value) {
                lhs.__get_allocator() = std::move(rhs.__get_allocator());
            }
            lhs.__take_storage(std::move(rhs));
        } else {
            if (lhs.__get_allocator() != rhs.__get_allocator()) {
                lhs.__destroy();
                lhs.__move_element(std::move(rhs));
            } else {
                lhs.__destroy_and_deallocate();
                lhs.__take_storage(std::move(rhs));
            }
        }
    }

    template <typename Container>
    WJR_CONSTEXPR20 static void swap(Container &lhs, Container &rhs) noexcept(
        noexcept(lhs.__swap_storage(rhs)) &&
                !std::conjunction_v<propagate_on_container_swap,
                                    std::negation<is_always_equal>>
            ? true
            : noexcept(std::swap(lhs.__get_allocator(), rhs.__get_allocator()))) {
        if constexpr (std::conjunction_v<propagate_on_container_swap,
                                         std::negation<is_always_equal>>) {
            auto &lhs_allocator = lhs.__get_allocator();
            auto &rhs_allocator = rhs.__get_allocator();
            if (lhs_allocator != rhs_allocator) {
                std::swap(lhs_allocator, rhs_allocator);
            }
        }

        lhs.__swap_storage(rhs);
    }
};

} // namespace wjr

#endif // WJR_CONTAINER_GENERIC_CONTAINER_TRAITS_HPP__
#ifndef WJR_CONTAINER_INTRUSIVE_LIST_HPP__
#define WJR_CONTAINER_INTRUSIVE_LIST_HPP__

#ifndef WJR_CONTAINER_INTRUSIVE_DETAILS_HPP__
#define WJR_CONTAINER_INTRUSIVE_DETAILS_HPP__

// Already included

namespace wjr::intrusive {} // namespace wjr::intrusive

#endif                      // WJR_CONTAINER_INTRUSIVE_DETAILS_HPP__

namespace wjr::intrusive {

template <typename Tag = void>
class list_node;

template <typename T>
constexpr void init(list_node<T> *node) noexcept;

template <typename T>
constexpr void insert(list_node<T> *prev, list_node<T> *next,
                      list_node<T> *node) noexcept;

template <typename T>
constexpr void push_back(list_node<T> *head, list_node<T> *node) noexcept;

template <typename T>
constexpr void push_front(list_node<T> *head, list_node<T> *node) noexcept;

template <typename T>
constexpr void remove_uninit(list_node<T> *node) noexcept;

template <typename T>
constexpr bool empty(const list_node<T> *node) noexcept;

template <typename T>
constexpr list_node<T> *next(list_node<T> *node) noexcept;

template <typename T>
constexpr const list_node<T> *next(const list_node<T> *node) noexcept;

template <typename T>
constexpr list_node<T> *prev(list_node<T> *node) noexcept;

template <typename T>
constexpr const list_node<T> *prev(const list_node<T> *node) noexcept;

template <typename T>
constexpr void replace_uninit(list_node<T> *from, list_node<T> *to) noexcept;

template <typename T>
class list_node_const_iterator {
    using ListNode = list_node<T>;

public:
    using iterator_category = std::bidirectional_iterator_tag;
    using value_type = ListNode;
    using reference = const ListNode &;
    using pointer = const ListNode *;
    using difference_type = std::ptrdiff_t;

    constexpr list_node_const_iterator() noexcept = default;
    constexpr list_node_const_iterator(const list_node_const_iterator &) noexcept =
        default;
    constexpr list_node_const_iterator(list_node_const_iterator &&) noexcept = default;
    constexpr list_node_const_iterator &
    operator=(const list_node_const_iterator &) noexcept = default;
    constexpr list_node_const_iterator &
    operator=(list_node_const_iterator &&) noexcept = default;
    ~list_node_const_iterator() = default;

    constexpr list_node_const_iterator(const ListNode *node) noexcept
        : m_node(const_cast<ListNode *>(node)) {}

    constexpr reference operator*() const noexcept { return *m_node; }
    constexpr pointer operator->() const noexcept { return m_node; }

    constexpr list_node_const_iterator &operator++() noexcept {
        m_node = next(m_node);
        return *this;
    }

    constexpr list_node_const_iterator operator++(int) noexcept {
        list_node_const_iterator tmp(*this);
        ++(*this);
        return tmp;
    }

    constexpr list_node_const_iterator &operator--() noexcept {
        m_node = prev(m_node);
        return *this;
    }

    constexpr list_node_const_iterator operator--(int) noexcept {
        list_node_const_iterator tmp(*this);
        --(*this);
        return tmp;
    }

    constexpr bool operator==(const list_node_const_iterator &other) const noexcept {
        return m_node == other.m_node;
    }

    constexpr bool operator!=(const list_node_const_iterator &other) const noexcept {
        return !(*this == other);
    }

    constexpr operator const ListNode *() const noexcept { return m_node; }

private:
    ListNode *m_node{};
};

template <typename T>
class list_node_iterator : public list_node_const_iterator<T> {
    using Mybase = list_node_const_iterator<T>;
    using ListNode = list_node<T>;

public:
    using iterator_category = typename Mybase::iterator_category;
    using value_type = typename Mybase::value_type;
    using reference = ListNode &;
    using pointer = ListNode *;
    using difference_type = typename Mybase::difference_type;

    constexpr list_node_iterator() noexcept = default;
    constexpr list_node_iterator(const list_node_iterator &) noexcept = default;
    constexpr list_node_iterator(list_node_iterator &&) noexcept = default;
    constexpr list_node_iterator &
    operator=(const list_node_iterator &) noexcept = default;
    constexpr list_node_iterator &operator=(list_node_iterator &&) noexcept = default;
    ~list_node_iterator() = default;

    constexpr reference operator*() const noexcept {
        return const_cast<reference>(Mybase::operator*());
    }

    constexpr pointer operator->() const noexcept {
        return const_cast<pointer>(Mybase::operator->());
    }

    constexpr list_node_iterator &operator++() noexcept {
        Mybase::operator++();
        return *this;
    }

    constexpr list_node_iterator operator++(int) noexcept {
        list_node_iterator tmp(*this);
        ++(*this);
        return tmp;
    }

    constexpr list_node_iterator &operator--() noexcept {
        Mybase::operator--();
        return *this;
    }

    constexpr list_node_iterator operator--(int) noexcept {
        list_node_iterator tmp(*this);
        --(*this);
        return tmp;
    }

    constexpr operator ListNode *() const noexcept {
        return const_cast<ListNode *>(static_cast<const ListNode *>(*this));
    }
};

template <typename Tag>
class list_node {
public:
    using iterator = list_node_iterator<Tag>;
    using const_iterator = list_node_const_iterator<Tag>;
    using reverse_iterator = std::reverse_iterator<iterator>;
    using const_reverse_iterator = std::reverse_iterator<const_iterator>;

    constexpr list_node() noexcept = default;
    list_node(const list_node &) = delete;
    list_node(list_node &&) = delete;
    list_node &operator=(const list_node &) = delete;
    list_node &operator=(list_node &&) = delete;
    ~list_node() = default;

    template <typename T>
    friend constexpr void init(list_node<T> *node) noexcept {
        node->m_prev = node;
        node->m_next = node;
    }

    template <typename T>
    friend constexpr void insert(list_node<T> *prev, list_node<T> *next,
                                 list_node<T> *node) noexcept {
        prev->m_next = node;
        node->m_prev = prev;
        next->m_prev = node;
        node->m_next = next;
    }

    template <typename T>
    friend constexpr void push_back(list_node<T> *head, list_node<T> *node) noexcept {
        insert(head, head->m_next, node);
    }

    template <typename T>
    friend constexpr void push_front(list_node<T> *head, list_node<T> *node) noexcept {
        insert(head->m_prev, head, node);
    }

    template <typename T>
    friend constexpr void remove_uninit(list_node<T> *node) noexcept {
        node->m_prev->m_next = node->m_next;
        node->m_next->m_prev = node->m_prev;
    }

    template <typename T>
    friend constexpr bool empty(const list_node<T> *node) noexcept {
        return node->m_next == node;
    }

    template <typename T>
    friend constexpr list_node<T> *next(list_node<T> *node) noexcept {
        return node->m_next;
    }

    template <typename T>
    friend constexpr const list_node<T> *next(const list_node<T> *node) noexcept {
        return node->m_next;
    }

    template <typename T>
    friend constexpr list_node<T> *prev(list_node<T> *node) noexcept {
        return node->m_prev;
    }

    template <typename T>
    friend constexpr const list_node<T> *prev(const list_node<T> *node) noexcept {
        return node->m_prev;
    }

    template <typename T>
    friend constexpr void replace_uninit(list_node<T> *from, list_node<T> *to) noexcept {
        to->m_prev = from->m_prev;
        to->m_next = from->m_next;
        from->m_prev->m_next = to;
        from->m_next->m_prev = to;
    }

    constexpr void init() noexcept { init(this); }

    constexpr void push_back(list_node *node) noexcept {
        intrusive::push_back(this, node);
    }

    constexpr void push_front(list_node *node) noexcept {
        intrusive::push_front(this, node);
    }

    constexpr void remove_uninit() noexcept { intrusive::remove_uninit(this); }

    constexpr bool empty() const noexcept { return intrusive::empty(this); }

    constexpr list_node *next() noexcept { return intrusive::next(this); }
    constexpr const list_node *next() const noexcept { return intrusive::next(this); }

    constexpr list_node *prev() noexcept { return intrusive::prev(this); }
    constexpr const list_node *prev() const noexcept { return intrusive::prev(this); }

    constexpr void replace_uninit(list_node *to) noexcept {
        intrusive::replace_uninit(this, to);
    }

    constexpr iterator begin() noexcept { return iterator(next()); }
    constexpr const_iterator begin() const noexcept { return const_iterator(next()); }
    constexpr const_iterator cbegin() const noexcept { return const_iterator(next()); }

    constexpr iterator end() noexcept { return iterator(this); }
    constexpr const_iterator end() const noexcept { return const_iterator(this); }
    constexpr const_iterator cend() const noexcept { return const_iterator(this); }

    constexpr reverse_iterator rbegin() noexcept { return reverse_iterator(end()); }
    constexpr const_reverse_iterator rbegin() const noexcept {
        return const_reverse_iterator(end());
    }
    constexpr const_reverse_iterator crbegin() const noexcept {
        return const_reverse_iterator(end());
    }

    constexpr reverse_iterator rend() noexcept { return reverse_iterator(begin()); }
    constexpr const_reverse_iterator rend() const noexcept {
        return const_reverse_iterator(begin());
    }
    constexpr const_reverse_iterator crend() const noexcept {
        return const_reverse_iterator(begin());
    }

private:
    list_node *m_prev;
    list_node *m_next;
};

} // namespace wjr::intrusive

#endif // WJR_CONTAINER_INTRUSIVE_LIST_HPP__
#ifndef WJR_INLINE_KEY_HPP__
#define WJR_INLINE_KEY_HPP__

// Already included

namespace wjr {

template <typename T, bool Inlined>
class inline_key {
public:
    constexpr inline_key() noexcept(std::is_nothrow_default_constructible_v<T>) = default;
    constexpr inline_key(const inline_key &other) noexcept(
        std::is_nothrow_copy_constructible_v<T>) = default;
    constexpr inline_key(inline_key &&other) noexcept(
        std::is_nothrow_move_constructible_v<T>) = default;
    constexpr inline_key &operator=(const inline_key &other) noexcept(
        std::is_nothrow_copy_assignable_v<T>) = default;
    constexpr inline_key &operator=(inline_key &&other) noexcept(
        std::is_nothrow_move_assignable_v<T>) = default;
    ~inline_key() noexcept(std::is_nothrow_destructible_v<T>) = default;

    constexpr inline_key(const T &value) noexcept(std::is_nothrow_copy_constructible_v<T>)
        : m_value(value) {}

    constexpr const T &get() const noexcept { return *m_value; }
    constexpr const T &operator*() const noexcept { return *m_value; }
    constexpr const T *operator->() const noexcept { return m_value.operator->(); }

private:
    lazy<T> m_value;
};

template <typename T>
class inline_key<T, false> {
public:
    constexpr inline_key() noexcept(std::is_nothrow_default_constructible_v<T>) = default;
    constexpr inline_key(const inline_key &other) noexcept(
        std::is_nothrow_copy_constructible_v<T>) = default;
    constexpr inline_key(inline_key &&other) noexcept(
        std::is_nothrow_move_constructible_v<T>) = default;
    constexpr inline_key &operator=(const inline_key &other) noexcept(
        std::is_nothrow_copy_assignable_v<T>) = default;
    constexpr inline_key &operator=(inline_key &&other) noexcept(
        std::is_nothrow_move_assignable_v<T>) = default;
    ~inline_key() noexcept(std::is_nothrow_destructible_v<T>) = default;

    constexpr inline_key(const T &value) noexcept(std::is_nothrow_copy_constructible_v<T>)
        : m_ptr(std::addressof(value)) {}

    constexpr const T &get() const noexcept { return *m_ptr; }
    constexpr const T &operator*() const noexcept { return *m_ptr; }
    constexpr const T *operator->() const noexcept { return m_ptr; }

private:
    const T *m_ptr;
};

template <typename T>
struct is_possible_inline_key : std::conjunction<std::is_trivially_copy_constructible<T>,
                                                 std::is_trivially_destructible<T>> {};

template <typename T>
inline constexpr bool is_possible_inline_key_v = is_possible_inline_key<T>::value;

template <typename T, size_t Threshold = sizeof(char *)>
using auto_key = inline_key<T, is_possible_inline_key_v<T> && sizeof(T) <= Threshold>;

} // namespace wjr

#endif // WJR_INLINE_KEY_HPP__
// Already included

namespace wjr {

template <typename Traits>
struct bplus_tree_node;

template <typename Traits>
struct bplus_tree_inner_node;

template <typename Traits, bool InlineKeys>
struct bplus_tree_leaf_node;

template <typename Key, typename T, typename Compare, size_t Size, bool Multi = true>
struct bplus_tree_traits {
    using key_type = Key;
    using mapped_type = T;
    using value_type = std::pair<const key_type, mapped_type>;
    using key_compare = Compare;

    constexpr static size_t node_size = Size;
    static constexpr size_t stack_size = node_size == 3   ? 48
                                         : node_size < 7  ? 32
                                         : node_size < 15 ? 24
                                                          : 16;
    static constexpr bool inline_keys =
        is_possible_inline_key_v<key_type> && sizeof(key_type) <= 8;
    using InlineKey = inline_key<Key, inline_keys>;
    using node_type = bplus_tree_node<bplus_tree_traits>;
    using inner_node_type = bplus_tree_inner_node<bplus_tree_traits>;
    using leaf_node_type = bplus_tree_leaf_node<bplus_tree_traits, inline_keys>;
    static constexpr bool multi = Multi;

    WJR_INTRINSIC_INLINE static const key_type &
    get_key(const value_type &value) noexcept {
        return value.first;
    }

private:
    template <typename Other>
    static void __native_copy(Other *WJR_RESTRICT first, Other *WJR_RESTRICT last,
                              Other *WJR_RESTRICT dest) noexcept {
        for (; first != last; ++first, ++dest) {
            *dest = *first;
            WJR_COMPILER_EMPTY_ASM();
        }
    }

    template <typename Other>
    static void __native_copy_backward(Other *first, Other *last, Other *dest) noexcept {
        for (; first != last;) {
            *--dest = *--last;
            WJR_COMPILER_EMPTY_ASM();
        }
    }

public:
    template <typename Other>
    static void copy(Other *WJR_RESTRICT first, Other *WJR_RESTRICT last,
                     Other *WJR_RESTRICT dest) noexcept {
        if constexpr (node_size <= 8) {
            return __native_copy(first, last, dest);
        } else {
            (void)std::copy(first, last, dest);
        }
    }

    template <typename Other>
    static void copy_backward(Other *first, Other *last, Other *dest) noexcept {
        if constexpr (node_size <= 8) {
            return __native_copy_backward(first, last, dest);
        } else {
            (void)std::copy_backward(first, last, dest);
        }
    }
};

template <typename Traits>
struct bplus_tree_node {
    using key_type = typename Traits::key_type;
    using value_type = typename Traits::value_type;
    constexpr static size_t node_size = Traits::node_size;
    using InlineKey = typename Traits::InlineKey;
    using inner_node_type = typename Traits::inner_node_type;
    using leaf_node_type = typename Traits::leaf_node_type;

    WJR_INTRINSIC_CONSTEXPR inner_node_type *as_inner() noexcept {
        return (inner_node_type *)(this);
    }

    WJR_INTRINSIC_CONSTEXPR const inner_node_type *as_inner() const noexcept {
        return (inner_node_type *)(this);
    }

    WJR_INTRINSIC_CONSTEXPR leaf_node_type *as_leaf() noexcept {
        return (leaf_node_type *)(this);
    }

    WJR_INTRINSIC_CONSTEXPR const leaf_node_type *as_leaf() const noexcept {
        return (leaf_node_type *)(this);
    }

    int m_size;
    unsigned int m_pos;
    bplus_tree_node *m_parent;
};

template <typename Traits>
struct bplus_tree_inner_node : bplus_tree_node<Traits> {
    using key_type = typename Traits::key_type;
    using value_type = typename Traits::value_type;
    constexpr static size_t node_size = Traits::node_size;
    using InlineKey = typename Traits::InlineKey;

    alignas(16) InlineKey m_keys[node_size];
    alignas(16) bplus_tree_node<Traits> *m_sons[node_size + 1];
};

template <typename Traits, bool InlineKeys>
struct bplus_tree_leaf_node : bplus_tree_node<Traits>, intrusive::list_node<> {
    using key_type = typename Traits::key_type;
    using value_type = typename Traits::value_type;
    constexpr static size_t node_size = Traits::node_size;
    using InlineKey = typename Traits::InlineKey;
    using ListNode = intrusive::list_node<>;

    const key_type &__get_key(unsigned int pos) const noexcept { return *m_keys[pos]; }

    WJR_INTRINSIC_INLINE void __copy(unsigned int start, unsigned int end,
                                     unsigned int dst_start, bplus_tree_leaf_node *dst) {
        Traits::copy(m_keys + start, m_keys + end, dst->m_keys + dst_start);
        Traits::copy(m_values + start, m_values + end, dst->m_values + dst_start);
    }

    WJR_INTRINSIC_INLINE void __copy_backward(unsigned int start, unsigned int end,
                                              unsigned int dst_end,
                                              bplus_tree_leaf_node *dst) {
        Traits::copy_backward(m_keys + start, m_keys + end, dst->m_keys + dst_end);
        Traits::copy_backward(m_values + start, m_values + end, dst->m_values + dst_end);
    }

    WJR_INTRINSIC_INLINE void __assign(unsigned int idx, value_type *const value) {
        m_keys[idx] = value->first;
        m_values[idx] = value;
    }

    WJR_INTRINSIC_CONSTEXPR ListNode *__get_list() noexcept { return this; }
    WJR_INTRINSIC_CONSTEXPR const ListNode *__get_list() const noexcept { return this; }

    alignas(16) InlineKey m_keys[node_size];
    alignas(16) value_type *m_values[node_size];
};

template <typename Traits>
struct bplus_tree_leaf_node<Traits, false> : bplus_tree_node<Traits>,
                                             intrusive::list_node<> {
    using key_type = typename Traits::key_type;
    using value_type = typename Traits::value_type;
    constexpr static size_t node_size = Traits::node_size;
    using ListNode = intrusive::list_node<>;

    const key_type &__get_key(unsigned int pos) const noexcept {
        return m_values[pos]->first;
    }

    WJR_INTRINSIC_INLINE void __copy(unsigned int start, unsigned int end,
                                     unsigned int dst_start, bplus_tree_leaf_node *dst) {
        Traits::copy(m_values + start, m_values + end, dst->m_values + dst_start);
    }

    WJR_INTRINSIC_INLINE void __copy_backward(unsigned int start, unsigned int end,
                                              unsigned int dst_end,
                                              bplus_tree_leaf_node *dst) {
        Traits::copy_backward(m_values + start, m_values + end, dst->m_values + dst_end);
    }

    WJR_INTRINSIC_INLINE void __assign(unsigned int idx, value_type *const value) {
        m_values[idx] = value;
    }

    WJR_INTRINSIC_CONSTEXPR ListNode *__get_list() noexcept { return this; }
    WJR_INTRINSIC_CONSTEXPR const ListNode *__get_list() const noexcept { return this; }

    alignas(16) value_type *m_values[node_size];
};

template <typename Traits, typename Alloc>
class basic_bplus_tree;

template <typename Traits>
class bplus_tree_const_iterator {
    using node_type = typename Traits::node_type;
    using inner_node_type = typename Traits::inner_node_type;
    using leaf_node_type = typename Traits::leaf_node_type;

    template <typename Other, typename Alloc>
    friend class basic_bplus_tree;

    using ListNode = intrusive::list_node<>;

public:
    using iterator_category = std::bidirectional_iterator_tag;
    using value_type = typename Traits::value_type;
    using difference_type = std::ptrdiff_t;
    using pointer = const value_type *;
    using reference = const value_type &;

    bplus_tree_const_iterator() noexcept = default;
    bplus_tree_const_iterator(const bplus_tree_const_iterator &) noexcept = default;
    bplus_tree_const_iterator(bplus_tree_const_iterator &&) noexcept = default;
    bplus_tree_const_iterator &
    operator=(const bplus_tree_const_iterator &) noexcept = default;
    bplus_tree_const_iterator &operator=(bplus_tree_const_iterator &&) noexcept = default;
    ~bplus_tree_const_iterator() noexcept = default;

    bplus_tree_const_iterator(const ListNode *list_node, unsigned int pos) noexcept
        : m_node(const_cast<ListNode *>(list_node)), m_pos(pos) {}

    reference operator*() const noexcept { return *get_leaf()->m_values[m_pos]; }

    pointer operator->() const noexcept { return get_leaf()->m_values[m_pos]; }

    bplus_tree_const_iterator &operator++() noexcept {
        ++m_pos;
        return __adjust_next();
    }

    bplus_tree_const_iterator operator++(int) noexcept {
        bplus_tree_const_iterator tmp = *this;
        ++*this;
        return tmp;
    }

    bplus_tree_const_iterator &operator--() noexcept {
        if (m_pos != 0) {
            --m_pos;
        } else {
            m_node = m_node->prev();
            m_pos = -get_leaf()->m_size - 1;
        }

        return *this;
    }

    bplus_tree_const_iterator operator--(int) noexcept {
        bplus_tree_const_iterator tmp = *this;
        --*this;
        return tmp;
    }

    bool operator==(const bplus_tree_const_iterator &other) const noexcept {
        return m_node == other.m_node && m_pos == other.m_pos;
    }

    bool operator!=(const bplus_tree_const_iterator &other) const noexcept {
        return !(*this == other);
    }

    leaf_node_type *get_leaf() const noexcept {
        return static_cast<leaf_node_type *>(m_node);
    }

    ListNode *get_node() const noexcept { return m_node; }

    unsigned int get_pos() const noexcept { return m_pos; }

protected:
    bplus_tree_const_iterator &__adjust_next() noexcept {
        if (m_pos == (unsigned int)(-get_leaf()->m_size)) {
            m_node = m_node->next();
            m_pos = 0;
        }

        return *this;
    }

private:
    ListNode *m_node = nullptr;
    unsigned int m_pos = 0;
};

template <typename Traits>
class bplus_tree_iterator : public bplus_tree_const_iterator<Traits> {
    using Mybase = bplus_tree_const_iterator<Traits>;

    template <typename Other, typename Alloc>
    friend class basic_bplus_tree;

public:
    using Mybase::Mybase;

    using iterator_category = typename Mybase::iterator_category;
    using value_type = typename Mybase::value_type;
    using difference_type = std::ptrdiff_t;
    using pointer = value_type *;
    using reference = value_type &;

    bplus_tree_iterator(const Mybase &other) noexcept : Mybase(other) {}

    value_type &operator*() const noexcept {
        return const_cast<value_type &>(Mybase::operator*());
    }

    value_type *operator->() const noexcept {
        return const_cast<value_type *>(Mybase::operator->());
    }

    bplus_tree_iterator &operator++() noexcept {
        Mybase::operator++();
        return *this;
    }

    bplus_tree_iterator operator++(int) noexcept {
        bplus_tree_iterator tmp = *this;
        ++*this;
        return tmp;
    }

    bplus_tree_iterator &operator--() noexcept {
        Mybase::operator--();
        return *this;
    }

    bplus_tree_iterator operator--(int) noexcept {
        bplus_tree_iterator tmp = *this;
        --*this;
        return tmp;
    }

    bool operator==(const bplus_tree_iterator &other) const noexcept {
        return Mybase::operator==(other);
    }

    bool operator!=(const bplus_tree_iterator &other) const noexcept {
        return Mybase::operator!=(other);
    }

protected:
    bplus_tree_iterator &__adjust_next() noexcept {
        Mybase::__adjust_next();
        return *this;
    }
};

template <typename Traits, typename Alloc>
class basic_bplus_tree {
    using _Alty = typename std::allocator_traits<Alloc>::template rebind_alloc<uint8_t>;
    using _Alty_traits = std::allocator_traits<_Alty>;
    using storage_fn_type = container_fn<_Alty>;

    friend class container_fn<_Alty>;

    using mapped_type = typename Traits::mapped_type;
    static constexpr size_t node_size = Traits::node_size;
    static constexpr size_t stack_size = Traits::stack_size;
    static constexpr bool inline_keys = Traits::inline_keys;
    using InlineKey = typename Traits::InlineKey;
    static constexpr size_t floor_half = node_size / 2;
    static constexpr size_t ceil_half = node_size - floor_half;

    using node_type = typename Traits::node_type;
    using inner_node_type = typename Traits::inner_node_type;
    using leaf_node_type = typename Traits::leaf_node_type;

    using ListNode = intrusive::list_node<>;

public:
    using key_type = typename Traits::key_type;
    using value_type = typename Traits::value_type;
    using key_compare = typename Traits::key_compare;
    using allocator_type = Alloc;
    using size_type = typename _Alty_traits::size_type;
    using difference_type = typename _Alty_traits::difference_type;
    using iterator = bplus_tree_iterator<Traits>;
    using const_iterator = bplus_tree_const_iterator<Traits>;
    using reverse_iterator = std::reverse_iterator<iterator>;
    using const_reverse_iterator = std::reverse_iterator<const_iterator>;

    static_assert(node_size >= 3, "node_size must be greater than or equal to 3.");
    static_assert(node_size <= 255, "node_size must be less than or equal to 255.");

    basic_bplus_tree() noexcept(std::is_nothrow_default_constructible_v<_Alty>) {
        init(&m_sentry);
    }

    // not implemented currently
    basic_bplus_tree(const basic_bplus_tree &) = delete;

    basic_bplus_tree(basic_bplus_tree &&other) noexcept(
        std::is_nothrow_move_constructible_v<key_compare>
            &&std::is_nothrow_move_constructible_v<_Alty>)
        : m_pair(std::piecewise_construct,
                 std::forward_as_tuple(std::move(other.key_comp())),
                 std::forward_as_tuple(
                     std::piecewise_construct,
                     std::forward_as_tuple(std::move(other.__get_allocator())),
                     std::forward_as_tuple())) {
        __take_tree(std::move(other));
    }

    ~basic_bplus_tree() { __destroy_and_deallocate(); }

    template <typename... Args>
    void emplace(Args &&...args) {
        auto &al = __get_allocator();
        value_type *const xval =
            (value_type *)_Alty_traits::allocate(al, sizeof(value_type));
        uninitialized_construct_using_allocator(xval, al, std::forward<Args>(args)...);

        __insert_value_ptr(xval);
    }

    void insert(const value_type &val) { emplace(val); }
    void insert(value_type &&val) { emplace(std::move(val)); }

    void erase(const key_type &key);

    iterator upper_bound(const key_type &key) {
        return iterator(__search<true>(key).__adjust_next());
    }

    const_iterator upper_bound(const key_type &key) const {
        return const_iterator(__search<true>(key).__adjust_next());
    }

    constexpr key_compare &key_comp() noexcept { return m_pair.first(); }
    constexpr const key_compare &key_comp() const noexcept { return m_pair.first(); }

    iterator begin() noexcept { return iterator(m_sentry.next(), 0); }
    const_iterator begin() const noexcept { return const_iterator(m_sentry.next(), 0); }
    const_iterator cbegin() const noexcept { return const_iterator(m_sentry.next(), 0); }

    iterator end() noexcept { return iterator(&m_sentry, 0); }
    const_iterator end() const noexcept { return const_iterator(&m_sentry, 0); }
    const_iterator cend() const noexcept { return const_iterator(&m_sentry, 0); }

    reverse_iterator rbegin() noexcept { return reverse_iterator(end()); }
    const_reverse_iterator rbegin() const noexcept {
        return const_reverse_iterator(end());
    }

    const_reverse_iterator crbegin() const noexcept {
        return const_reverse_iterator(cend());
    }

    reverse_iterator rend() noexcept { return reverse_iterator(begin()); }
    const_reverse_iterator rend() const noexcept {
        return const_reverse_iterator(begin());
    }

    const_reverse_iterator crend() const noexcept {
        return const_reverse_iterator(cbegin());
    }

    void erase(const_iterator iter) { __erase_iter(iter); }

    void __debug(bool print = true) {
        auto root = __get_root();
        if (root != nullptr) {
            (void)__debug(root, print);
        } else {
            if (print)
                printf("empty");
        }
        if (print)
            printf("\n");
    }

private:
    void __take_tree(basic_bplus_tree &&other) noexcept {
        const auto root = other.__get_root();
        if (root == nullptr) {
            init(&m_sentry);
            return;
        }

        __get_root() = root;
        other.__get_root() = nullptr;
        replace_uninit(&other.m_sentry, &m_sentry);
        init(&other.m_sentry);
    }

    // member function for container_fn (START)

    WJR_PURE WJR_INTRINSIC_CONSTEXPR _Alty &__get_allocator() noexcept {
        return m_pair.second().first();
    }

    WJR_PURE WJR_INTRINSIC_CONSTEXPR const _Alty &__get_allocator() const noexcept {
        return m_pair.second().first();
    }

    WJR_NOINLINE void __destroy_and_deallocate() noexcept {
        node_type *current = __get_root();

        // empty tree
        if (current == nullptr) {
            return;
        }

        auto &al = __get_allocator();
        int cur_size = current->m_size;

        // root is leaf
        if (cur_size < 0) {
            const auto leaf = current->as_leaf();
            const unsigned int cur_usize = -cur_size;

            for (unsigned int i = 0; i < cur_usize; ++i) {
                _Alty_traits::destroy(al, leaf->m_values[i]);
                _Alty_traits::deallocate(al, (uint8_t *)leaf->m_values[i],
                                         sizeof(value_type));
            }

            _Alty_traits::deallocate(al, (uint8_t *)leaf, sizeof(leaf_node_type));
            return;
        }

        // skip to the leftmost leaf
        current = begin().get_leaf();
        cur_size = -current->m_size;

        // cache of parent and parent's size
        node_type *parent = current->m_parent;
        unsigned int par_size = parent->m_size;

        // cache of `current' node's position in parent
        unsigned int pos = 0;

        do {
            const auto leaf = current->as_leaf();
            const unsigned int cur_usize = cur_size;

            for (unsigned int i = 0; i < cur_usize; ++i) {
                _Alty_traits::destroy(al, leaf->m_values[i]);
                _Alty_traits::deallocate(al, (uint8_t *)leaf->m_values[i],
                                         sizeof(value_type));
            }

            ListNode *next = leaf->next();
            _Alty_traits::deallocate(al, (uint8_t *)leaf, sizeof(leaf_node_type));

            // if `current' is not the last child of parent
            if (WJR_UNLIKELY(pos++ == par_size)) {
                do {
                    current = parent;
                    parent = current->m_parent;
                    pos = current->m_pos;
                    _Alty_traits::deallocate(al, (uint8_t *)current,
                                             sizeof(inner_node_type));
                    // if `current' is the rightmost leaf
                    if (parent == nullptr) {
                        return;
                    }
                    // if `current' is the last child of parent
                } while (pos == (unsigned int)parent->m_size);

                parent = static_cast<leaf_node_type *>(next)->m_parent;
                par_size = parent->m_size;
                pos = 0;
            }

            WJR_ASSERT(next != &m_sentry);

            current = static_cast<leaf_node_type *>(next);
            cur_size = -current->m_size;
        } while (true);
    }

    void __take_storage(basic_bplus_tree &&other) noexcept {
        key_comp() = std::move(other.key_comp());
        __take_tree(std::move(other));
    }

    // member function for container_fn (END)

    void __insert_root(value_type *xval) {
        const auto root = (leaf_node_type *)_Alty_traits::allocate(
            __get_allocator(), sizeof(leaf_node_type));

        __get_root() = root;

        root->m_size = -1;
        root->m_parent = nullptr;
        root->__assign(0, xval);
        m_sentry.push_back(root);
        return;
    }

    WJR_NOINLINE void __insert_iter(const_iterator iter, value_type *xval) {
        unsigned int pos;
        unsigned int cur_size;
        node_type *current;
        node_type *inst;

        do {
            leaf_node_type *leaf;
            do {
                ListNode *const node = iter.get_node();

                // empty
                if (node == &m_sentry) {
                    __insert_root(xval);
                    return;
                }

                leaf = static_cast<leaf_node_type *>(node);
            } while (0);

            pos = iter.get_pos();
            cur_size = -leaf->m_size;

            // non-full leaf
            if (WJR_LIKELY(cur_size != node_size)) {
                WJR_ASSERT_ASSUME(pos <= cur_size);

                leaf->__copy_backward(pos, cur_size, cur_size + 1, leaf);

                leaf->m_size = -(cur_size + 1);
                leaf->__assign(pos, xval);
                return;
            }

            const auto tmp_inst = (leaf_node_type *)_Alty_traits::allocate(
                __get_allocator(), sizeof(leaf_node_type));
            inst = tmp_inst;
            leaf->__get_list()->push_back(tmp_inst->__get_list());

            leaf->m_size = -(int)(floor_half + 1);
            tmp_inst->m_size = -(int)(node_size - floor_half);

            if (pos <= floor_half) {
                leaf->__copy(floor_half, node_size, 0, tmp_inst);
                leaf->__copy_backward(pos, floor_half, floor_half + 1, leaf);
                leaf->__assign(pos, xval);
            } else {
                // pos in tmp_inst
                const unsigned int rpos = pos - floor_half - 1;
                leaf->__copy(floor_half + 1, pos, 0, tmp_inst);
                leaf->__copy(pos, node_size, rpos + 1, tmp_inst);
                tmp_inst->__assign(rpos, xval);
            }

            current = leaf;
        } while (0);

        node_type *parent = current->m_parent;
        InlineKey key = inst->as_leaf()->__get_key(0);

        while (parent != nullptr) {
            inst->m_parent = parent;
            pos = current->m_pos + 1;
            current = parent;
            const auto inner = current->as_inner();

            cur_size = inner->m_size + 1;
            InlineKey *const keys = inner->m_keys;
            node_type **const sons = inner->m_sons;

            // non-full inner
            if (WJR_LIKELY(cur_size != node_size + 1)) {
                Traits::copy_backward(keys + pos - 1, keys + cur_size - 1,
                                      keys + cur_size);
                Traits::copy_backward(sons + pos, sons + cur_size, sons + cur_size + 1);

                inner->m_size = cur_size;
                keys[pos - 1] = key;
                sons[pos] = inst;

                inst->m_pos = pos;
                for (unsigned int i = pos + 1; i <= cur_size; ++i) {
                    sons[i]->m_pos = i;
                }

                return;
            }

            parent = inner->m_parent;

            const auto tmp_inst = (inner_node_type *)_Alty_traits::allocate(
                __get_allocator(), sizeof(inner_node_type));

            inner->m_size = (int)(ceil_half);
            tmp_inst->m_size = (int)(floor_half);

            InlineKey next_key;

            if (pos <= ceil_half) {
                next_key = keys[ceil_half - 1];

                Traits::copy(keys + ceil_half, keys + node_size, tmp_inst->m_keys);
                Traits::copy(sons + ceil_half, sons + node_size + 1, tmp_inst->m_sons);
                Traits::copy_backward(keys + pos - 1, keys + ceil_half - 1,
                                      keys + ceil_half);
                Traits::copy_backward(sons + pos, sons + ceil_half, sons + ceil_half + 1);

                keys[pos - 1] = key;
                sons[pos] = inst;

                inst->m_pos = pos;
                for (unsigned int i = pos + 1; i <= ceil_half; ++i) {
                    sons[i]->m_pos = i;
                }
            } else {
                if (pos == ceil_half + 1) {
                    next_key = key;

                    Traits::copy(keys + ceil_half, keys + node_size, tmp_inst->m_keys);
                    Traits::copy(sons + ceil_half + 1, sons + node_size + 1,
                                 tmp_inst->m_sons + 1);

                    tmp_inst->m_sons[0] = inst;
                } else {
                    next_key = keys[ceil_half];

                    Traits::copy(keys + ceil_half + 1, keys + pos - 1, tmp_inst->m_keys);
                    Traits::copy(sons + ceil_half + 1, sons + pos, tmp_inst->m_sons);

                    const unsigned int rpos = pos - ceil_half - 1;

                    Traits::copy(keys + pos - 1, keys + node_size,
                                 tmp_inst->m_keys + rpos);
                    Traits::copy(sons + pos, sons + node_size + 1,
                                 tmp_inst->m_sons + rpos + 1);

                    tmp_inst->m_keys[rpos - 1] = key;
                    tmp_inst->m_sons[rpos] = inst;
                }
            }

            for (unsigned int i = 0; i <= floor_half; ++i) {
                tmp_inst->m_sons[i]->m_parent = tmp_inst;
                tmp_inst->m_sons[i]->m_pos = i;
            }

            key = next_key;
            inst = tmp_inst;
        }

        const auto new_root = (inner_node_type *)_Alty_traits::allocate(
            __get_allocator(), sizeof(inner_node_type));
        new_root->m_size = 1;
        new_root->m_parent = nullptr;
        new_root->m_keys[0] = key;
        new_root->m_sons[0] = current;
        new_root->m_sons[1] = inst;
        current->m_pos = 0;
        inst->m_pos = 1;

        current->m_parent = new_root;
        inst->m_parent = new_root;

        __get_root() = new_root;
        return;
    }

    void __insert_value_ptr(value_type *xval) {
        __insert_iter(__search<false>(xval->first), xval);
    }

    template <bool Upper>
    WJR_PURE WJR_INTRINSIC_INLINE static bool
    __compare(const key_type &a, const key_type &key, const key_compare &comp) {
        if constexpr (Upper) {
            return comp(key, a);
        } else {
            return !comp(a, key);
        }
    }

    template <bool Upper>
    WJR_PURE WJR_NOINLINE const_iterator __search(const key_type &key) const {
        const node_type *current = __get_root();

        if (current == nullptr) {
            return cend();
        }

        unsigned int pos;

        int cur_size = current->m_size;
        const auto &comp = key_comp();

        // root search
        if (WJR_UNLIKELY(cur_size < 0)) {
            pos = __search<Upper, 1, node_size, 0>(current->as_leaf(), -cur_size, key,
                                                   comp);
            return const_iterator(current->as_leaf()->__get_list(), pos);
        }

        if (!__compare<Upper>(*current->as_inner()->m_keys[0], key, comp)) {
            goto NOT_LEFTMOST_AT_ROOT;
        }

        current = current->as_inner()->m_sons[0];
        cur_size = current->m_size;

        while (cur_size >= 0) {
            if (!__compare<Upper>(*current->as_inner()->m_keys[0], key, comp)) {
                goto NOT_LEFTMOST_AT_INNER;
            }

            current = current->as_inner()->m_sons[0];
            cur_size = current->m_size;
        }

        // leftmost leaf need to check first key
        if (__compare<Upper>(current->as_leaf()->__get_key(0), key, comp)) {
            return const_iterator(current->as_leaf()->__get_list(), 0);
        }

        goto LEAF;

    NOT_LEFTMOST_AT_ROOT:
        pos = __search<Upper, 1, node_size, 1>(current->as_inner(), cur_size, key, comp);
        current = current->as_inner()->m_sons[pos];
        cur_size = current->m_size;

        if (cur_size < 0) {
            goto LEAF;
        }

        goto INNER_LOOP;

    NOT_LEFTMOST_AT_INNER:
        pos = __search<Upper, floor_half, node_size, 1>(current->as_inner(), cur_size,
                                                        key, comp);

        current = current->as_inner()->m_sons[pos];
        cur_size = current->m_size;

        if (cur_size < 0) {
            goto LEAF;
        }

    INNER_LOOP:
        do {
            pos = __search<Upper, floor_half, node_size, 0>(current->as_inner(), cur_size,
                                                            key, comp);

            current = current->as_inner()->m_sons[pos];
            cur_size = current->m_size;
        } while (cur_size >= 0);

    LEAF:
        pos = __search<Upper, floor_half, node_size, 1>(current->as_leaf(), -cur_size,
                                                        key, comp);
        return const_iterator(current->as_leaf()->__get_list(), pos);
    }

    template <size_t Min, size_t Max, size_t Offset, typename Compare>
    WJR_PURE WJR_INTRINSIC_INLINE static unsigned int
    __search(const node_type *current, unsigned int size, Compare &&comp) {
        static_assert(Offset == 0 || Offset == 1, "");
        static_assert(Min != 0, "");

        WJR_ASSERT_ASSUME(size >= Min);
        WJR_ASSERT_ASSUME(size <= Max);

        if constexpr (Min == 1 && Offset == 1) {
            if (size == 1) {
                return 1;
            }
        }

        if constexpr (Max <= 16) {
            if constexpr (Offset == 0) {
                if (comp(current, 0)) {
                    return 0;
                }
            }

#define WJR_REGISTER_BLPUS_SEARCH_2(A, B, C)                                             \
    do {                                                                                 \
        if constexpr (A == Max) {                                                        \
            return A;                                                                    \
        } else if constexpr (B == Max) {                                                 \
            if (size == A || comp(current, A)) {                                         \
                return A;                                                                \
            }                                                                            \
            return B;                                                                    \
        } else if constexpr (C <= Max) {                                                 \
            if constexpr (Min < C) {                                                     \
                if (size < C) {                                                          \
                    if constexpr (Min <= A) {                                            \
                        if (size == A || comp(current, A)) {                             \
                            return A;                                                    \
                        }                                                                \
                    } else {                                                             \
                        if (comp(current, A)) {                                          \
                            return A;                                                    \
                        }                                                                \
                    }                                                                    \
                    return B;                                                            \
                }                                                                        \
            }                                                                            \
            if (comp(current, B)) {                                                      \
                if (comp(current, A)) {                                                  \
                    return A;                                                            \
                }                                                                        \
                return B;                                                                \
            }                                                                            \
        }                                                                                \
    } while (0)
#define WJR_REGISTER_BLPUS_SEARCH_4(A, B, C, D, E)                                       \
    do {                                                                                 \
        if constexpr (E > Max) {                                                         \
            WJR_REGISTER_BLPUS_SEARCH_2(A, B, C);                                        \
            WJR_REGISTER_BLPUS_SEARCH_2(C, D, E);                                        \
        } else {                                                                         \
            if constexpr (Min < E) {                                                     \
                if (size < E) {                                                          \
                    WJR_REGISTER_BLPUS_SEARCH_2(A, B, C);                                \
                    if constexpr (Min <= C) {                                            \
                        if (size == C || comp(current, C)) {                             \
                            return C;                                                    \
                        }                                                                \
                    } else {                                                             \
                        if (comp(current, C)) {                                          \
                            return C;                                                    \
                        }                                                                \
                    }                                                                    \
                    return D;                                                            \
                }                                                                        \
            }                                                                            \
            if (comp(current, D)) {                                                      \
                if (comp(current, B)) {                                                  \
                    if (comp(current, A)) {                                              \
                        return A;                                                        \
                    }                                                                    \
                    return B;                                                            \
                }                                                                        \
                if (comp(current, C)) {                                                  \
                    return C;                                                            \
                }                                                                        \
                return D;                                                                \
            }                                                                            \
        }                                                                                \
    } while (0)

            if constexpr (Max <= 8 || Min == 1) {
                WJR_REGISTER_BLPUS_SEARCH_2(1, 2, 3);
                WJR_REGISTER_BLPUS_SEARCH_2(3, 4, 5);
                WJR_REGISTER_BLPUS_SEARCH_2(5, 6, 7);
                WJR_REGISTER_BLPUS_SEARCH_2(7, 8, 9);
                WJR_REGISTER_BLPUS_SEARCH_2(9, 10, 11);
                WJR_REGISTER_BLPUS_SEARCH_2(11, 12, 13);
                WJR_REGISTER_BLPUS_SEARCH_2(13, 14, 15);

                if constexpr (Max == 15) {
                    return 15;
                } else if constexpr (Max == 16) {
                    if (size == 15 || comp(current, 15)) {
                        return 15;
                    }

                    return 16;
                }
            } else {
                WJR_REGISTER_BLPUS_SEARCH_2(1, 2, 3);
                WJR_REGISTER_BLPUS_SEARCH_4(3, 4, 5, 6, 7);
                WJR_REGISTER_BLPUS_SEARCH_4(7, 8, 9, 10, 11);
                WJR_REGISTER_BLPUS_SEARCH_4(11, 12, 13, 14, 15);

                if constexpr (Max == 15) {
                    return 15;
                } else if constexpr (Max == 16) {
                    if (size == 15 || comp(current, 15)) {
                        return 15;
                    }

                    return 16;
                }
            }

#undef WJR_REGISTER_BLPUS_SEARCH_4
#undef WJR_REGISTER_BLPUS_SEARCH_2
        } else {
            unsigned int pos = Offset;
            do {
                if (comp(current, pos)) {
                    break;
                }
            } while (++pos != size);
            return pos;
        }
    }

    template <bool Upper, size_t Min, size_t Max, size_t Offset>
    WJR_PURE WJR_INTRINSIC_INLINE static unsigned int
    __search(const inner_node_type *current, unsigned int size, const key_type &key,
             const key_compare &comp) {
        return __search<Min, Max, Offset>(
            current, size, [&key, &comp](const node_type *current, unsigned int pos) {
                return __compare<Upper>(*current->as_inner()->m_keys[pos], key, comp);
            });
    }

    template <bool Upper, size_t Min, size_t Max, size_t Offset>
    WJR_PURE WJR_INTRINSIC_INLINE static unsigned int
    __search(const leaf_node_type *current, unsigned int size, const key_type &key,
             const key_compare &comp) {
        return __search<Min, Max, Offset>(
            current, size, [&key, &comp](const node_type *current, unsigned int pos) {
                return __compare<Upper>(current->as_leaf()->__get_key(pos), key, comp);
            });
    }

    template <typename T>
    WJR_INTRINSIC_INLINE static unsigned int
    __init_remove_rotate(const inner_node_type *parent, unsigned int pos,
                         unsigned int par_size, T *&lhs, T *&rhs) {
        unsigned int size;

        do {
            if (pos != par_size) {
                const auto tmp = static_cast<T *>(parent->m_sons[pos + 1]);
                unsigned int tmp_size;

                if constexpr (std::is_same_v<T, leaf_node_type>) {
                    tmp_size = -tmp->m_size;
                } else {
                    tmp_size = tmp->m_size;
                }

                WJR_ASSERT_ASSUME(tmp_size >= floor_half);

                rhs = tmp;
                size = tmp_size;
            } else {
                auto tmp = static_cast<T *>(parent->m_sons[pos - 1]);
                lhs = tmp;

                if constexpr (std::is_same_v<T, leaf_node_type>) {
                    return -tmp->m_size;
                } else {
                    return tmp->m_size;
                }
            }
        } while (0);

        do {
            if (pos != 0) {
                const auto tmp = static_cast<T *>(parent->m_sons[pos - 1]);
                unsigned int tmp_size;

                if constexpr (std::is_same_v<T, leaf_node_type>) {
                    tmp_size = -tmp->m_size;
                } else {
                    tmp_size = tmp->m_size;
                }

                if (tmp_size >= size) {
                    lhs = tmp;
                    size = tmp_size;
                    break;
                }
            }

            lhs = nullptr;
        } while (0);

        return size;
    }

    static InlineKey __debug(node_type *cur, bool print) {
        auto n = cur->m_size;
        if (n < 0) {
            n = -n;

            if (print) {
                printf("[");
                for (unsigned int i = 0; i < n; ++i) {
                    if (i != 0)
                        printf(",");
                    printf("%d", cur->as_leaf()->__get_key(i));
                }
                printf("]");
            }

            return cur->as_leaf()->__get_key(0);
        }

        if (print) {
            printf("{");
        }
        InlineKey ret;
        for (unsigned int i = 0; i <= n; ++i) {
            auto x = cur->as_inner()->m_sons[i];
            if (x->m_pos != i) {
                printf("error!\n");
                exit(-1);
            }

            if (x->m_parent != cur) {
                printf("error 1!\n");
                exit(-1);
            }

            auto key = __debug(x, print);
            if (i != 0) {
                if (*cur->as_inner()->m_keys[i - 1] != *key) {
                    printf("error 2!\n");
                    exit(-1);
                }
            } else {
                ret = key;
            }
        }
        if (print) {
            printf("}");
        }
        return ret;
    }

    WJR_NOINLINE void __erase_iter(const_iterator iter) {
        constexpr unsigned int merge_size = floor_half * 2;

        unsigned int pos;
        unsigned int cur_size;
        node_type *current;
        node_type *parent;
        unsigned int par_pos;
        unsigned int par_size;

        do {
            leaf_node_type *leaf = iter.get_leaf();
            pos = iter.get_pos();
            cur_size = -leaf->m_size;
            parent = leaf->m_parent;

            _Alty_traits::destroy(__get_allocator(), leaf->m_values[pos]);
            _Alty_traits::deallocate(__get_allocator(), (uint8_t *)leaf->m_values[pos],
                                     sizeof(value_type));

            if (cur_size > floor_half) {
                leaf->__copy(pos + 1, cur_size, pos, leaf);
                leaf->m_size = -(cur_size - 1);

                // first key in leaf is changed
                if (pos == 0 && parent != nullptr) {
                    current = leaf;

                    do {
                        pos = current->m_pos;
                        current = parent;
                        parent = current->m_parent;
                    } while (pos == 0 && parent != nullptr);

                    if (pos != 0) {
                        current->as_inner()->m_keys[pos - 1] = leaf->__get_key(0);
                    }
                }

                return;
            }

            if (parent == nullptr) {
                if (cur_size == 1) {
                    _Alty_traits::deallocate(__get_allocator(), (uint8_t *)leaf,
                                             sizeof(leaf_node_type));
                    __get_root() = nullptr;
                    init(&m_sentry);
                    return;
                }

                leaf->__copy(pos + 1, cur_size, pos, leaf);
                leaf->m_size = -(cur_size - 1);
                return;
            }

            WJR_ASSERT_ASSUME(cur_size == floor_half);

            const auto inner = parent->as_inner();
            par_pos = leaf->m_pos;
            par_size = inner->m_size;
            leaf_node_type *lhs;
            leaf_node_type *rhs;

            unsigned int next_size =
                __init_remove_rotate(inner, par_pos, par_size, lhs, rhs);

            do {
                if (lhs != nullptr) {
                    rhs = leaf;

                    if (next_size == floor_half) {
                        leaf->__copy(0, pos, floor_half, lhs);
                        leaf->__copy(pos + 1, floor_half, pos + floor_half, lhs);
                        break;
                    }

                    const unsigned int moved_elements = (next_size - floor_half + 1) / 2;

                    if (moved_elements != 1) {
                        leaf->__copy_backward(pos + 1, floor_half,
                                              floor_half + moved_elements - 1, leaf);
                    }

                    leaf->__copy_backward(0, pos, pos + moved_elements, leaf);
                    lhs->__copy(next_size - moved_elements, next_size, 0, leaf);

                    lhs->m_size = -(next_size - moved_elements);
                    leaf->m_size = -(floor_half + moved_elements - 1);
                } else {
                    WJR_ASSERT_ASSUME(rhs != nullptr);

                    lhs = leaf;

                    leaf->__copy(pos + 1, floor_half, pos, leaf);

                    // merge rhs to leaf, and pos of iter is zero, then
                    // need to update key in parent
                    if (pos == 0) {
                        current = leaf;

                        unsigned int tmp_pos;
                        node_type *tmp_parent = parent;

                        do {
                            tmp_pos = current->m_pos;
                            current = tmp_parent;
                            tmp_parent = current->m_parent;
                        } while (tmp_pos == 0 && tmp_parent != nullptr);

                        if (tmp_pos != 0) {
                            current->as_inner()->m_keys[tmp_pos - 1] = leaf->__get_key(0);
                        }
                    }

                    if (next_size == floor_half) {
                        rhs->__copy(0, floor_half, floor_half - 1, leaf);

                        ++par_pos;
                        break;
                    }

                    const unsigned int moved_elements = (next_size - floor_half + 1) / 2;

                    rhs->__copy(0, moved_elements, floor_half - 1, leaf);
                    rhs->__copy(moved_elements, next_size, 0, rhs);

                    rhs->m_size = -(next_size - moved_elements);
                    leaf->m_size = -(floor_half + moved_elements - 1);
                    current = rhs;
                }

                current = rhs;

                pos = current->m_pos;
                current = parent;
                parent = current->m_parent;

                current->as_inner()->m_keys[pos - 1] = rhs->__get_key(0);

                return;
            } while (0);

            lhs->m_size = -(merge_size - 1);
            rhs->remove_uninit();
            _Alty_traits::deallocate(__get_allocator(), (uint8_t *)rhs,
                                     sizeof(leaf_node_type));

        } while (0);

        pos = par_pos;
        cur_size = par_size;
        current = parent;
        parent = current->m_parent;

        while (parent != nullptr) {
            WJR_ASSERT_ASSUME(pos > 0);

            const auto inner = current->as_inner();

            InlineKey *const keys = inner->m_keys;
            node_type **const sons = inner->m_sons;

            if (cur_size > floor_half) {
                Traits::copy(keys + pos, keys + cur_size, keys + pos - 1);
                Traits::copy(sons + pos + 1, sons + cur_size + 1, sons + pos);

                for (unsigned int i = pos; i < cur_size; ++i) {
                    sons[i]->m_pos = i;
                }

                inner->m_size = cur_size - 1;
                return;
            }

            WJR_ASSERT_ASSUME(cur_size == floor_half);

            const auto par_inner = parent->as_inner();
            par_pos = inner->m_pos;
            par_size = par_inner->m_size;
            inner_node_type *lhs;
            inner_node_type *rhs;

            unsigned int next_size =
                __init_remove_rotate(par_inner, par_pos, par_size, lhs, rhs);

            do {
                if (lhs != nullptr) {
                    rhs = inner;

                    if (next_size == floor_half) {
                        Traits::copy(keys, keys + pos - 1, lhs->m_keys + floor_half + 1);
                        Traits::copy(sons, sons + pos, lhs->m_sons + floor_half + 1);
                        Traits::copy(keys + pos, keys + floor_half,
                                     lhs->m_keys + floor_half + pos);
                        Traits::copy(sons + pos + 1, sons + floor_half + 1,
                                     lhs->m_sons + floor_half + pos + 1);

                        for (unsigned int i = floor_half; i <= merge_size; ++i) {
                            lhs->m_sons[i]->m_parent = lhs;
                            lhs->m_sons[i]->m_pos = i;
                        }

                        lhs->m_keys[floor_half] = par_inner->m_keys[par_pos - 1];
                        break;
                    }

                    const unsigned int moved_elements = (next_size - floor_half + 1) / 2;

                    InlineKey key = lhs->m_keys[next_size - moved_elements];

                    if (moved_elements != 1) {
                        Traits::copy_backward(keys + pos, keys + floor_half,
                                              keys + floor_half + moved_elements - 1);
                        Traits::copy_backward(sons + pos + 1, sons + floor_half + 1,
                                              sons + floor_half + moved_elements);
                        for (unsigned int i = pos + moved_elements;
                             i < floor_half + moved_elements; ++i) {
                            sons[i]->m_pos = i;
                        }
                    }

                    Traits::copy_backward(keys, keys + pos - 1,
                                          keys + pos + moved_elements - 1);
                    Traits::copy_backward(sons, sons + pos, sons + pos + moved_elements);
                    Traits::copy(lhs->m_keys + next_size - moved_elements + 1,
                                 lhs->m_keys + next_size, keys);
                    Traits::copy(lhs->m_sons + next_size - moved_elements + 1,
                                 lhs->m_sons + next_size + 1, sons);

                    keys[moved_elements - 1] = par_inner->m_keys[par_pos - 1];
                    par_inner->m_keys[par_pos - 1] = key;

                    for (unsigned int i = 0; i < moved_elements; ++i) {
                        sons[i]->m_parent = inner;
                        sons[i]->m_pos = i;
                    }

                    for (unsigned int i = moved_elements; i < pos + moved_elements; ++i) {
                        sons[i]->m_pos = i;
                    }

                    lhs->m_size = next_size - moved_elements;
                    inner->m_size = floor_half + moved_elements - 1;
                } else {
                    WJR_ASSERT_ASSUME(rhs != nullptr);

                    lhs = inner;

                    if (next_size == floor_half) {
                        Traits::copy(keys + pos, keys + floor_half, keys + pos - 1);
                        Traits::copy(sons + pos + 1, sons + floor_half + 1, sons + pos);
                        Traits::copy(rhs->m_keys, rhs->m_keys + floor_half,
                                     keys + floor_half);
                        Traits::copy(rhs->m_sons, rhs->m_sons + floor_half + 1,
                                     sons + floor_half);

                        for (unsigned int i = pos; i < floor_half; ++i) {
                            inner->m_sons[i]->m_pos = i;
                        }

                        for (unsigned int i = floor_half; i <= merge_size; ++i) {
                            inner->m_sons[i]->m_parent = inner;
                            inner->m_sons[i]->m_pos = i;
                        }

                        lhs->m_keys[floor_half - 1] = par_inner->m_keys[par_pos];
                        ++par_pos;
                        break;
                    }

                    const unsigned int moved_elements = (next_size - floor_half + 1) / 2;

                    InlineKey key = rhs->m_keys[moved_elements - 1];

                    Traits::copy(keys + pos, keys + floor_half, keys + pos - 1);
                    Traits::copy(sons + pos + 1, sons + floor_half + 1, sons + pos);
                    Traits::copy(rhs->m_keys, rhs->m_keys + moved_elements - 1,
                                 keys + floor_half);
                    Traits::copy(rhs->m_sons, rhs->m_sons + moved_elements,
                                 sons + floor_half);
                    Traits::copy(rhs->m_keys + moved_elements, rhs->m_keys + next_size,
                                 rhs->m_keys);
                    Traits::copy(rhs->m_sons + moved_elements,
                                 rhs->m_sons + next_size + 1, rhs->m_sons);

                    keys[floor_half - 1] = par_inner->m_keys[par_pos];
                    par_inner->m_keys[par_pos] = key;

                    for (unsigned int i = pos; i < floor_half; ++i) {
                        sons[i]->m_pos = i;
                    }

                    for (unsigned int i = floor_half; i < floor_half + moved_elements;
                         ++i) {
                        sons[i]->m_parent = inner;
                        sons[i]->m_pos = i;
                    }

                    for (unsigned int i = 0; i <= next_size - moved_elements; ++i) {
                        rhs->m_sons[i]->m_pos = i;
                    }

                    rhs->m_size = next_size - moved_elements;
                    inner->m_size = floor_half + moved_elements - 1;
                }

                return;
            } while (0);

            lhs->m_size = merge_size;
            _Alty_traits::deallocate(__get_allocator(), (uint8_t *)rhs,
                                     sizeof(inner_node_type));

            pos = par_pos;
            cur_size = par_size;
            current = parent;
            parent = current->m_parent;
        }

        const auto inner = current->as_inner();

        if (cur_size == 1) {
            _Alty_traits::deallocate(__get_allocator(), (uint8_t *)inner,
                                     sizeof(inner_node_type));
            node_type *root = inner->m_sons[0];
            __get_root() = root;
            root->m_parent = nullptr;
            return;
        }

        Traits::copy(inner->m_keys + pos, inner->m_keys + cur_size,
                     inner->m_keys + pos - 1);
        Traits::copy(inner->m_sons + pos + 1, inner->m_sons + cur_size + 1,
                     inner->m_sons + pos);

        for (unsigned int i = pos; i < cur_size; ++i) {
            inner->m_sons[i]->m_pos = i;
        }

        inner->m_size = cur_size - 1;
    }

    WJR_INTRINSIC_CONSTEXPR node_type *&__get_root() noexcept {
        return m_pair.second().second();
    }

    WJR_INTRINSIC_CONSTEXPR const node_type *__get_root() const noexcept {
        return m_pair.second().second();
    }

    compressed_pair<key_compare, compressed_pair<_Alty, node_type *>> m_pair;
    ListNode m_sentry;
}; // namespace wjr

} // namespace wjr

#endif // WJR_CONTAINER_GENERIC_BPLUS_TREE_HPP__

#endif // WJR_BPLUS_TREE_HPP__
#ifndef WJR_JSON_LEXER_HPP__
#define WJR_JSON_LEXER_HPP__

#ifndef WJR_JSON_LEXER_IMPL_HPP__
#define WJR_JSON_LEXER_IMPL_HPP__

// Already included

namespace wjr::json {

struct forward_lexer_storage {
    forward_lexer_storage(span<const char> input) noexcept
        : first(input.data()), last(input.data() + input.size()) {}

    const char *first;
    const char *last;

    uint64_t prev_in_string = 0;
    uint64_t prev_is_escape = 0;
    uint64_t prev_is_ws = ~0ull;

    uint32_t idx = 0;
};

struct dynamic_lexer_storage {
    dynamic_lexer_storage(span<const char> input) noexcept
        : first(input.data()), last(input.data() + input.size()) {}

    const char *first;
    const char *last;
};

template <uint32_t token_buf_size>
class basic_lexer {
    static_assert(((token_buf_size & (token_buf_size - 1)) == 0 &&
                   token_buf_size <= 65536) ||
                      token_buf_size == (uint32_t)in_place_max,
                  "token_buf_size must be a power of 2");

    constexpr static bool __is_dynamic = token_buf_size == (uint32_t)in_place_max;
    using storage_type =
        std::conditional_t<__is_dynamic, dynamic_lexer_storage, forward_lexer_storage>;

public:
    constexpr basic_lexer(span<const char> input) noexcept : m_storage(input) {}

    basic_lexer() = delete;
    constexpr basic_lexer(const basic_lexer &) = delete;
    constexpr basic_lexer(basic_lexer &&) = default;
    constexpr basic_lexer &operator=(const basic_lexer &) = delete;
    constexpr basic_lexer &operator=(basic_lexer &&) = default;
    ~basic_lexer() = default;

    /**
     * @brief read tokens
     *
     * @details Read at least token_buf_size tokens from the input.
     * token_buf' size must be at least token_buf_size * 2 - 1.
     *
     * @return return the number of tokens read.
     *
     */

    uint32_t read(uint32_t *token_buf) noexcept;

    WJR_PURE const char *end() const noexcept { return m_storage.last; }

private:
    storage_type m_storage;
};

using dynamic_lexer = basic_lexer<in_place_max>;

template <uint32_t token_buf_size>
using forward_lexer = basic_lexer<token_buf_size>;

namespace lexer_details {
inline uint64_t calc_backslash(uint64_t B) {
    uint64_t maybe_escaped = B << 1;

    uint64_t maybe_escaped_and_odd_bits = maybe_escaped | 0xAAAAAAAAAAAAAAAAULL;
    uint64_t even_series_codes_and_odd_bits = maybe_escaped_and_odd_bits - B;

    return even_series_codes_and_odd_bits ^ 0xAAAAAAAAAAAAAAAAULL;
}
} // namespace lexer_details

} // namespace wjr::json

#endif // WJR_JSON_LEXER_IMPL_HPP__
// Already included

#if defined(WJR_X86)
#ifndef WJR_X86_JSON_LEXER_HPP__
#define WJR_X86_JSON_LEXER_HPP__

// Already included
// Already included
// Already included

namespace wjr::json {

#if WJR_HAS_SIMD(SSSE3) && WJR_HAS_SIMD(X86_SIMD)
#define WJR_HAS_BUILTIN_JSON_LEXER_READER_READ_BUF WJR_HAS_DEF
#endif

#if WJR_HAS_BUILTIN(JSON_LEXER_READER_READ_BUF)

namespace lexer_details {

#if !WJR_HAS_SIMD(AVX2)
const static __m128i lh8_mask = sse::set1_epi8(0x0f);

const static __m128i lo8_lookup =
    sse::set_epi8(0, 0, 12, 1, 4, 10, 8, 0, 0, 0, 0, 0, 0, 0, 0, 16);
const static __m128i hi8_lookup =
    sse::set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 2, 17, 0, 8);
#else
const static __m256i lh8_mask = avx::set1_epi8(0x0f);
const static __m256i lo8_lookup =
    avx::set_epi8(0, 0, 12, 1, 4, 10, 8, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 12, 1, 4, 10,
                  8, 0, 0, 0, 0, 0, 0, 0, 0, 16);
const static __m256i hi8_lookup =
    avx::set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 2, 17, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0,
                  4, 0, 4, 0, 2, 17, 0, 8);
#endif

template <typename simd>
inline typename simd::int_type equal(typename simd::int_type x, uint8_t ch) {
    return simd::cmpeq_epi8(x, simd::set1_epi8(ch));
}

template <typename simd>
void load_simd(const char *first, typename simd::int_type *arr) {
    constexpr auto simd_width = simd::width();
    constexpr auto u8_width = simd_width / 8;
    constexpr auto u8_loop = 64 / u8_width;
    for (unsigned i = 0; i < u8_loop; ++i) {
        arr[i] = simd::loadu((typename simd::int_type *)(first + i * u8_width));
    }
}

} // namespace lexer_details

template <uint32_t token_buf_size>
uint32_t basic_lexer<token_buf_size>::read(uint32_t *token_buf) noexcept {
    using namespace lexer_details;

    constexpr bool is_avx = WJR_HAS_SIMD(AVX2);
    using simd = std::conditional_t<is_avx, avx, sse>;
    using simd_int = typename simd::int_type;
    constexpr auto simd_width = simd::width();
    constexpr auto u8_width = simd_width / 8;
    constexpr auto u8_loop = 64 / u8_width;

    constexpr uint32_t token_buf_mask = token_buf_size * 2;

    auto first = m_storage.first;
    const auto last = m_storage.last;

    if (WJR_UNLIKELY(first == last)) {
        return 0;
    }

    uint64_t prev_is_escape;
    uint64_t prev_in_string;
    uint64_t prev_is_ws;
    uint32_t idx;

    if constexpr (__is_dynamic) {
        prev_is_escape = prev_in_string = 0;
        prev_is_ws = ~0ull;
        idx = 0;
    } else {
        prev_is_escape = m_storage.prev_is_escape;
        prev_in_string = m_storage.prev_in_string;
        prev_is_ws = m_storage.prev_is_ws;
        idx = m_storage.idx;
    }

    WJR_ASSERT_ASSUME_L1(first < last);

    uint32_t count = 0;
    bool loop;

    do {
        simd_int x[u8_loop];

        const size_t diff = last - first;

        if (WJR_LIKELY(diff > 64)) {
            load_simd<simd>(first, x);
            first += 64;
        } else {
            if (diff == 64) {
                load_simd<simd>(first, x);
            } else {
                char buf[64];
                char ch;
                switch (last[-1]) {
                case ' ':
                case '\n':
                case '\r':
                case '\t':
                case '[':
                case ']':
                case '{':
                case '}': {
                    ch = ' ';
                    break;
                }
                default: {
                    ch = '\0';
                    break;
                }
                }

                std::memcpy(buf, first, diff);
                std::memset(buf + diff, ch, 64 - diff);
                load_simd<simd>(buf, x);
            }

            first = last;
            if constexpr (!__is_dynamic) {
                count |= token_buf_mask;
            }
        }

        uint64_t B = 0; // backslash

        for (unsigned i = 0; i < u8_loop; ++i) {
            const auto backslash = equal<simd>(x[i], '\\');
            B |= (uint64_t)simd::movemask_epi8(backslash) << (i * u8_width);
        }

        uint64_t Q = 0; // quote

        for (unsigned i = 0; i < u8_loop; ++i) {
            const auto quote = equal<simd>(x[i], '\"');
            Q |= (uint64_t)simd::movemask_epi8(quote) << (i * u8_width);
        }

        uint64_t S = 0; // brackets, comma , colon
        uint64_t W = 0; // whitespace

        for (unsigned i = 0; i < u8_loop; ++i) {
            const auto shuf_lo8 = simd::shuffle_epi8(lo8_lookup, x[i]);
            const auto shuf_hi8 = simd::shuffle_epi8(
                hi8_lookup, simd::And(simd::srli_epi16(x[i], 4), lh8_mask));

            const auto result = simd::And(shuf_lo8, shuf_hi8);
            // comma : 1
            // colon : 2
            // brackets : 4
            // whitespace : 8, 16

            uint32_t stu = simd::movemask_epi8(
                simd::cmpeq_epi8(simd::And(result, simd::set1_epi8(7)), simd::zeros()));
            uint32_t wsp = simd::movemask_epi8(
                simd::cmpeq_epi8(simd::And(result, simd::set1_epi8(24)), simd::zeros()));

            S |= (uint64_t)(stu) << (i * u8_width);
            W |= (uint64_t)(wsp) << (i * u8_width);
        }

        S = ~S;
        W = ~W;

        {
            if (!B) {
                B = prev_is_escape;
                Q &= ~B;
                prev_is_escape = 0;
            } else {
                const uint64_t codeB = calc_backslash(B & ~prev_is_escape);
                const auto escape = (codeB & B) >> 63;
                B = codeB ^ (B | prev_is_escape);
                Q &= ~B;
                prev_is_escape = escape;
            }
        }

        const uint64_t R = prefix_xor(Q) ^ prev_in_string;

        const auto WS = S | W;
        const auto WT = shld(WS, prev_is_ws, 1);
        const auto TW = shld(~WS, ~prev_is_ws, 1);
        prev_is_ws = WS;

        S &= ~R;
        prev_in_string = static_cast<uint64_t>(static_cast<int64_t>(R) >> 63);

        S |= Q;
        S |= ((TW & W) | (WT & ~W)) & ~R;
        S &= ~(Q & ~R);

        if (S) {
            const auto num = popcount(S);

            do {
                for (int i = 0; i < 4; ++i) {
                    token_buf[i] = idx + ctz(S);
                    S &= S - 1;
                }

                if (WJR_UNLIKELY(num <= 4)) {
                    break;
                }

                for (int i = 4; i < 8; ++i) {
                    token_buf[i] = idx + ctz(S);
                    S &= S - 1;
                }

                if (WJR_LIKELY(num <= 8)) {
                    break;
                }

                for (int i = 8; i < 12; ++i) {
                    token_buf[i] = idx + ctz(S);
                    S &= S - 1;
                }

                if (WJR_LIKELY(num <= 12)) {
                    break;
                }

                for (int i = 12; i < 16; ++i) {
                    token_buf[i] = idx + ctz(S);
                    S &= S - 1;
                }

                if (WJR_LIKELY(num <= 16)) {
                    break;
                }

                for (int i = 16; i < num; ++i) {
                    token_buf[i] = idx + ctz(S);
                    S &= S - 1;
                }
            } while (0);

            token_buf += num;
            count += num;
        }

        idx += 64;

        if constexpr (!__is_dynamic) {
            loop = (count <= token_buf_size);
        } else {
            loop = first != last;
        }

    } while (WJR_LIKELY(loop));

    if constexpr (!__is_dynamic) {
        m_storage.first = first;
        m_storage.prev_is_escape = prev_is_escape;
        m_storage.prev_in_string = prev_in_string;
        m_storage.prev_is_ws = prev_is_ws;
        m_storage.idx = idx;
    }

    if constexpr (!__is_dynamic) {
        return count & (token_buf_mask - 1);
    } else {
        return count;
    }
}

#endif

} // namespace wjr::json

#endif // WJR_X86_JSON_LEXER_HPP__
#endif

namespace wjr::json {

#if !WJR_HAS_BUILTIN(JSON_LEXER_READER_READ_BUF)

namespace lexer_details {

const static std::array<uint8_t, 256> code_table = {
    4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
    4, 4, 4, 3, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
    2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
    4, 4, 4, 4, 2, 0, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
    4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4};

}

template <uint32_t token_buf_size>
uint32_t basic_lexer<token_buf_size>::read(uint32_t *token_buf) noexcept {
    WJR_UNREACHABLE();
    if (WJR_UNLIKELY(m_storage.first == m_storage.last)) {
        return 0;
    }

    using namespace lexer_details;

    constexpr uint32_t token_buf_mask = token_buf_size * 2;

    auto first = m_storage.first;
    const auto last = m_storage.last;
    uint64_t prev_is_escape;
    uint64_t prev_in_string;
    uint64_t prev_is_ws;
    uint32_t idx;

    if constexpr (__is_dynamic) {
        prev_is_escape = prev_in_string = prev_is_ws = idx = 0;
    } else {
        prev_is_escape = m_storage.prev_is_escape;
        prev_in_string = m_storage.prev_in_string;
        prev_is_ws = m_storage.prev_is_ws;
        idx = m_storage.idx;
    }

    WJR_ASSERT_ASSUME_L1(first < last);

    uint32_t count = 0;
    bool loop;

    do {
        const char *ptr;
        char stk[64];
        const size_t diff = last - first;

        if (WJR_LIKELY(diff > 64)) {
            ptr = first;
            first += 64;
        } else {
            if (diff == 64) {
                ptr = first;
            } else {
                std::memcpy(stk, first, diff);
                std::memset(stk + diff, 0, 64 - diff);
                ptr = stk;
            }

            first = last;
            if constexpr (!__is_dynamic) {
                count |= token_buf_mask;
            }
        }

        uint64_t MASK[5] = {0, 0, 0, 0, 0};
        auto &[B, Q, S, W, UNUSED] = MASK;

        WJR_UNROLL(8)
        for (int i = 0; i < 64; ++i) {
            MASK[code_table[(uint8_t)ptr[i]]] |= 1ull << i;
        }

        {
            if (!B) {
                B = prev_is_escape;
                Q &= ~B;
                prev_is_escape = 0;
            } else {
                const uint64_t codeB = calc_backslash(B & ~prev_is_escape);
                const auto escape = (codeB & B) >> 63;
                B = codeB ^ (B | prev_is_escape);
                Q &= ~B;
                prev_is_escape = escape;
            }
        }

        const uint64_t R = prefix_xor(Q) ^ prev_in_string;
        S &= ~R;
        prev_in_string = static_cast<uint64_t>(static_cast<int64_t>(R) >> 63);

        S |= Q;
        const auto WS = S | W;
        const auto P = shld(WS, prev_is_ws, 1);
        prev_is_ws = WS;

        S |= (P ^ W) & ~R;
        S &= ~(Q & ~R);

        if (S) {
            const auto num = popcount(S);

            do {
                for (int i = 0; i < 4; ++i) {
                    token_buf[i] = idx + ctz(S);
                    S &= S - 1;
                }

                if (WJR_UNLIKELY(num <= 4)) {
                    break;
                }

                for (int i = 4; i < 8; ++i) {
                    token_buf[i] = idx + ctz(S);
                    S &= S - 1;
                }

                if (WJR_LIKELY(num <= 8)) {
                    break;
                }

                for (int i = 8; i < 12; ++i) {
                    token_buf[i] = idx + ctz(S);
                    S &= S - 1;
                }

                if (WJR_LIKELY(num <= 12)) {
                    break;
                }

                for (int i = 12; i < 16; ++i) {
                    token_buf[i] = idx + ctz(S);
                    S &= S - 1;
                }

                if (WJR_LIKELY(num <= 16)) {
                    break;
                }

                for (int i = 16; i < num; ++i) {
                    token_buf[i] = idx + ctz(S);
                    S &= S - 1;
                }
            } while (0);

            token_buf += num;
            count += num;
        }

        idx += 64;

        if constexpr (!__is_dynamic) {
            loop = (count <= token_buf_size);
        } else {
            loop = first != last;
        }

    } while (WJR_LIKELY(loop));

    if constexpr (!__is_dynamic) {
        m_storage.first = first;
        m_storage.prev_is_escape = prev_is_escape;
        m_storage.prev_in_string = prev_in_string;
        m_storage.prev_is_ws = prev_is_ws;
        m_storage.idx = idx;
    }

    if constexpr (!__is_dynamic) {
        return count & (token_buf_mask - 1);
    } else {
        return count;
    }
}
#endif

} // namespace wjr::json

#endif // WJR_JSON_LEXER_HPP__
#ifndef WJR_JSON_PARSER_HPP__
#define WJR_JSON_PARSER_HPP__

#include <bitset>
#include <charconv>

// Already included
// Already included

namespace wjr::json {

enum error_code : unsigned int {
    SUCCESS,
    FAILURE,
};

struct in_place_token_null_t {};
struct in_place_token_true_t {};
struct in_place_token_false_t {};
struct in_place_token_number_t {};
struct in_place_token_string_t {};
struct in_place_token_left_bracket_t {};
struct in_place_token_left_brace_t {};
struct in_place_token_left_brace_string_t {};
struct in_place_token_right_bracket_back_bracket_t {};
struct in_place_token_right_bracket_back_brace_t {};
struct in_place_token_right_bracket_back_root_t {};
struct in_place_token_right_brace_back_bracket_t {};
struct in_place_token_right_brace_back_brace_t {};
struct in_place_token_right_brace_back_root_t {};

struct in_place_token_success_t {};

template <error_code ec>
struct in_place_token_failure_t {};

struct in_place_token_repeat_t {};

inline constexpr in_place_token_null_t in_place_token_null{};
inline constexpr in_place_token_true_t in_place_token_true{};
inline constexpr in_place_token_false_t in_place_token_false{};
inline constexpr in_place_token_number_t in_place_token_number{};
inline constexpr in_place_token_string_t in_place_token_string{};
inline constexpr in_place_token_left_bracket_t in_place_token_left_bracket{};
inline constexpr in_place_token_left_brace_t in_place_token_left_brace{};
inline constexpr in_place_token_left_brace_string_t in_place_token_left_brace_string{};
inline constexpr in_place_token_right_bracket_back_bracket_t
    in_place_token_right_bracket_back_bracket{};
inline constexpr in_place_token_right_bracket_back_brace_t
    in_place_token_right_bracket_back_brace{};
inline constexpr in_place_token_right_bracket_back_root_t
    in_place_token_right_bracket_back_root{};
inline constexpr in_place_token_right_brace_back_bracket_t
    in_place_token_right_brace_back_bracket{};
inline constexpr in_place_token_right_brace_back_brace_t
    in_place_token_right_brace_back_brace{};
inline constexpr in_place_token_right_brace_back_root_t
    in_place_token_right_brace_back_root{};

inline constexpr in_place_token_success_t in_place_token_success{};

template <error_code ec>
inline constexpr in_place_token_failure_t<ec> in_place_token_failure{};

inline constexpr in_place_token_repeat_t in_place_token_repeat{};

template <uint32_t token_buf_size>
class forward_token_reader {
public:
    forward_token_reader(span<const char> sp, uint32_t *token_buf)
        : token_first(token_buf), token_last(token_buf), token_buf(token_buf),
          first(sp.data()), lexer(sp) {}

    WJR_INTRINSIC_INLINE bool operator()(uint32_t &token) {
        if (WJR_LIKELY(token_first != token_last)) {
            token = *token_first++;
            return true;
        }

        return read_more(token);
    }

    WJR_PURE const char *begin() const noexcept { return first; }
    WJR_PURE const char *end() const noexcept { return lexer.end(); }

private:
    WJR_NOINLINE bool read_more(uint32_t &token) {
        uint32_t count = lexer.read(token_buf);
        if (WJR_UNLIKELY(count == 0)) {
            return false;
        }

        token_first = token_buf;
        token_last = token_buf + count;
        token = *token_first++;
        return true;
    }

    uint32_t *token_first;
    uint32_t *token_last;
    uint32_t *token_buf;
    const char *first;
    basic_lexer<token_buf_size> lexer;
};

/**
 * @details Return true if parsing needs to be terminated.
 *
 */
struct empty_parser {
    bool operator()(in_place_token_null_t, const char *, const char *) { return false; }

    bool operator()(in_place_token_true_t, const char *, const char *) { return false; }

    bool operator()(in_place_token_false_t, const char *, const char *) { return false; }

    bool operator()(in_place_token_number_t, const char *, const char *) { return false; }

    bool operator()(in_place_token_string_t, const char *, const char *) { return false; }

    bool operator()(in_place_token_left_bracket_t) { return false; }

    bool operator()(in_place_token_left_brace_t) { return false; }

    bool operator()(in_place_token_left_brace_string_t, const char *, const char *) {
        return false;
    }

    bool operator()(in_place_token_right_bracket_back_bracket_t) { return false; }

    bool operator()(in_place_token_right_bracket_back_brace_t, const char *,
                    const char *) {
        return false;
    }

    bool operator()(in_place_token_right_bracket_back_root_t) { return false; }

    bool operator()(in_place_token_right_brace_back_bracket_t) { return false; }

    bool operator()(in_place_token_right_brace_back_brace_t, const char *, const char *) {
        return false;
    }

    bool operator()(in_place_token_right_brace_back_root_t) { return false; }

    void operator()(in_place_token_success_t) {}

    template <error_code E>
    void operator()(in_place_token_failure_t<E>) {}

    bool operator()(in_place_token_repeat_t) { return false; }
};

struct check_parser : empty_parser {
    using empty_parser::operator();

    bool operator()(in_place_token_null_t, const char *first, const char *last) {
        if (WJR_LIKELY(last - first == 4 && std::memcmp(first, "null", 4) == 0)) {
            return false;
        }

        ec = FAILURE;
        return true;
    }

    bool operator()(in_place_token_true_t, const char *first, const char *last) {
        if (WJR_LIKELY(last - first == 4 && std::memcmp(first, "true", 4) == 0)) {
            return false;
        }

        ec = FAILURE;
        return true;
    }

    bool operator()(in_place_token_false_t, const char *first, const char *last) {
        if (WJR_LIKELY(last - first == 5 && std::memcmp(first + 1, "alse", 4) == 0)) {
            return false;
        }

        ec = FAILURE;
        return true;
    }

    WJR_INTRINSIC_INLINE bool operator()(in_place_token_number_t, const char *first,
                                         const char *last) {
        constexpr auto __matches = [](uint8_t ch) { return '0' <= ch && ch <= '9'; };

        WJR_ASSERT_ASSUME_L1(first < last);

        if (*first == '-') {
            if (++first == last) {
                goto FAILED;
            }
        }

        if (*first++ == '0') {
            if (first == last) {
                return false;
            }
        } else {
            if (first == last) {
                return false;
            }

            do {
                if (WJR_UNLIKELY(!__matches(*first))) {
                    goto NEXT0;
                }
            } while (++first != last);
            return false;
        NEXT0 : {}
        }

        if (*first == '.') {
            if (++first == last) {
                goto FAILED;
            }

            if (WJR_UNLIKELY(!__matches(*first))) {
                goto FAILED;
            }

            while (++first != last) {
                if (WJR_UNLIKELY(!__matches(*first))) {
                    goto NEXT1;
                }
            }
            return false;
        NEXT1 : {}
        }

        switch (*first) {
        case 'e':
        case 'E': {
            break;
        }
        default: {
            goto FAILED;
        }
        }

        if (++first == last) {
            goto FAILED;
        }

        if (*first == '+' || *first == '-') {
            if (++first == last) {
                goto FAILED;
            }
        }

        if (WJR_UNLIKELY(!__matches(*first))) {
            goto FAILED;
        }

        while (++first != last) {
            if (WJR_UNLIKELY(!__matches(*first))) {
                goto FAILED;
            }
        }

        return false;
    FAILED : {
        ec = FAILURE;
        return true;
    }
    }

    WJR_INTRINSIC_INLINE bool operator()(in_place_token_string_t, const char *first,
                                         const char *last) {
        while (first != last) {
            uint8_t ch = *first++;
            if (WJR_UNLIKELY(ch < 32 || ch == 127)) {
                goto FAILED;
            }

            if (WJR_UNLIKELY(ch == '\\')) {
                if (WJR_UNLIKELY(first == last)) {
                    goto FAILED;
                }

                ch = *first++;
                switch (ch) {
                case '"':
                case '\\':
                case '/':
                case 'b':
                case 'f':
                case 'n':
                case 'r':
                case 't': {
                    break;
                }
                case 'u': {
                    if (WJR_UNLIKELY(first + 4 > last)) {
                        goto FAILED;
                    }

                    for (int i = 0; i < 4; ++i) {
                        ch = *first++;
                        if (WJR_UNLIKELY(!('0' <= ch && ch <= '9') &&
                                         !('a' <= ch && ch <= 'f') &&
                                         !('A' <= ch && ch <= 'F'))) {
                            goto FAILED;
                        }
                    }

                    break;
                }
                default: {
                    goto FAILED;
                }
                }
            }
        }

        return false;

    FAILED : {
        ec = FAILURE;
        return true;
    }
    }

    template <error_code E>
    void operator()(in_place_token_failure_t<E>) {
        ec = E;
    }

    error_code ec = SUCCESS;
};

/**
 * @param parser Return type of
 * success_t/failure_t/left_bracket_t/right_bracket_t/left_brace_t/right_brace_t must be
 * void. Otherwise, return type must be bool.
 *
 */
template <typename TokenReader, typename Parser>
WJR_NOINLINE void reader_parse(TokenReader &reader, Parser &parser) {
    unique_stack_allocator stkal(math_details::stack_alloc);

    struct stack {
        uint8_t type;
        const char *first;
        const char *last;
    };

    stack *stk = static_cast<stack *>(stkal.allocate(512 * sizeof(stack)));
    stack *current = stk;
    uint8_t type;

    uint32_t token, next_token;
    const char *const ptr = reader.begin();
    const char *const last = reader.end();
    const auto size = last - ptr;

    // empty json
    if (WJR_UNLIKELY(!reader(token))) {
        return parser(in_place_token_failure<FAILURE>);
    }

REPEAT_TOKEN : {
    switch (ptr[token]) {
    case 'n': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            next_token = size;
        }

        if (WJR_UNLIKELY(parser(in_place_token_null, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case 't': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            next_token = size;
        }

        if (WJR_UNLIKELY(parser(in_place_token_true, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case 'f': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            next_token = size;
        }

        if (WJR_UNLIKELY(parser(in_place_token_false, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case '0':
    case '1':
    case '2':
    case '3':
    case '4':
    case '5':
    case '6':
    case '7':
    case '8':
    case '9':
    case '-': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            next_token = size;
        }

        type = 0;
        goto NUMBER;
    }
    case '"': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            next_token = size;
        }

        type = 0;
        goto STRING;
    }
    case '[': {
        if (WJR_UNLIKELY(parser(in_place_token_left_bracket))) {
            return;
        }

        goto ARRAY;
    }
    case '{': {
        if (WJR_UNLIKELY(parser(in_place_token_left_brace))) {
            return;
        }

        goto OBJECT;
    }
    default: {
        return parser(in_place_token_failure<FAILURE>);
    }
    }

REPEAT_TOKEN_NEXT:

    if (next_token == size) {
        return parser(in_place_token_success);
    }

    // non-repeated callback
    if (parser(in_place_token_repeat)) {
        return parser(in_place_token_failure<FAILURE>);
    }

    // non-structural token, must be followed by space
    switch (ptr[next_token]) {
    case ' ':
    case '\n':
    case '\t':
    case '\r': {
        if (!reader(token)) {
            return parser(in_place_token_success);
        }
        break;
    }
    default: {
        return parser(in_place_token_failure<FAILURE>);
    }
    }

    goto REPEAT_TOKEN;
}

ARRAY : {
    if (WJR_UNLIKELY(!reader(token))) {
        return parser(in_place_token_failure<FAILURE>);
    }

    switch (ptr[token]) {
    case 'n': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        if (WJR_UNLIKELY(parser(in_place_token_null, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case 't': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        if (WJR_UNLIKELY(parser(in_place_token_true, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case 'f': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        if (WJR_UNLIKELY(parser(in_place_token_false, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case '0':
    case '1':
    case '2':
    case '3':
    case '4':
    case '5':
    case '6':
    case '7':
    case '8':
    case '9':
    case '-': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        type = 1;
        goto NUMBER;
    }
    case '"': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        type = 1;
        goto STRING;
    }
    case '[': {
        if (WJR_UNLIKELY(parser(in_place_token_left_bracket))) {
            return;
        }

        (current++)->type = 0;
        goto ARRAY;
    }
    case '{': {
        if (WJR_UNLIKELY(parser(in_place_token_left_brace))) {
            return;
        }

        (current++)->type = 0;
        goto OBJECT;
    }
    case ']': {
        goto ARRAY_BACK;
    }
    case '}': {
        return parser(in_place_token_failure<FAILURE>);
    }
    default: {
        return parser(in_place_token_failure<FAILURE>);
    }
    }
}

ARRAY_ELEMENT_SPACE:
    switch (ptr[next_token]) {
    case ' ':
    case '\n':
    case '\t':
    case '\r': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        break;
    }
    }

ARRAY_ELEMENT : {
    switch (ptr[next_token]) {
    case ',': {
        break;
    }
    case ']': {
    ARRAY_BACK:
        if (current == stk) {
            if (parser(in_place_token_right_bracket_back_root) ||
                parser(in_place_token_repeat)) {
                return;
            }

            if (reader(token)) {
                goto REPEAT_TOKEN;
            }

            return parser(in_place_token_success);
        }

        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        type = (--current)->type;

        if (type == 0) {
            if (WJR_UNLIKELY(parser(in_place_token_right_bracket_back_bracket))) {
                return;
            }

            goto ARRAY_ELEMENT;
        } else {
            if (WJR_UNLIKELY(parser(in_place_token_right_bracket_back_brace,
                                    current->first, current->last))) {
                return;
            }

            goto OBJECT_ELEMENT;
        }
    }
    default: {
        return parser(in_place_token_failure<FAILURE>);
    }
    }

    if (WJR_UNLIKELY(!reader(token))) {
        return parser(in_place_token_failure<FAILURE>);
    }

    switch (ptr[token]) {
    case 'n': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        if (WJR_UNLIKELY(parser(in_place_token_null, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case 't': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        if (WJR_UNLIKELY(parser(in_place_token_true, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case 'f': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        if (WJR_UNLIKELY(parser(in_place_token_false, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case '0':
    case '1':
    case '2':
    case '3':
    case '4':
    case '5':
    case '6':
    case '7':
    case '8':
    case '9':
    case '-': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        type = 1;
        goto NUMBER;
    }
    case '"': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        type = 1;
        goto STRING;
    }
    case '[': {
        if (WJR_UNLIKELY(parser(in_place_token_left_bracket))) {
            return;
        }

        (current++)->type = 0;
        goto ARRAY;
    }
    case '{': {
        if (WJR_UNLIKELY(parser(in_place_token_left_brace))) {
            return;
        }

        (current++)->type = 0;
        goto OBJECT;
    }
    case ']': {
        return parser(in_place_token_failure<FAILURE>);
    }
    case '}': {
        return parser(in_place_token_failure<FAILURE>);
    }
    default: {
        return parser(in_place_token_failure<FAILURE>);
    }
    }

    goto ARRAY_ELEMENT_SPACE;
}

OBJECT : {
    if (WJR_UNLIKELY(!reader(token))) {
        return parser(in_place_token_failure<FAILURE>);
    }

    switch (ptr[token]) {
    case '"': {
        break;
    }
    case '}': {
        goto OBJECT_BACK;
    }
    default: {
        return parser(in_place_token_failure<FAILURE>);
    }
    }

    if (WJR_UNLIKELY(!reader(next_token))) {
        return parser(in_place_token_failure<FAILURE>);
    }

    if (WJR_UNLIKELY(next_token - token < 2 ||
                     parser(in_place_token_left_brace_string, ptr + token + 1,
                            ptr + next_token - 1))) {
        return;
    }

    switch (ptr[next_token]) {
    case ' ':
    case '\n':
    case '\r':
    case '\t': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        break;
    }
    }

    if (WJR_UNLIKELY(ptr[next_token] != ':')) {
        return parser(in_place_token_failure<FAILURE>);
    }

    if (WJR_UNLIKELY(!reader(token))) {
        return parser(in_place_token_failure<FAILURE>);
    }

    switch (ptr[token]) {
    case 'n': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        if (WJR_UNLIKELY(parser(in_place_token_null, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case 't': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        if (WJR_UNLIKELY(parser(in_place_token_true, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case 'f': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        if (WJR_UNLIKELY(parser(in_place_token_false, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case '0':
    case '1':
    case '2':
    case '3':
    case '4':
    case '5':
    case '6':
    case '7':
    case '8':
    case '9':
    case '-': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        type = 2;
        goto NUMBER;
    }
    case '"': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        type = 2;
        goto STRING;
    }
    case '[': {
        if (WJR_UNLIKELY(parser(in_place_token_left_bracket))) {
            return;
        }

        (current++)->type = 1;
        goto ARRAY;
    }
    case '{': {
        if (WJR_UNLIKELY(parser(in_place_token_left_brace))) {
            return;
        }

        (current++)->type = 1;
        goto OBJECT;
    }
    case ']': {
        return parser(in_place_token_failure<FAILURE>);
    }
    case '}': {
        return parser(in_place_token_failure<FAILURE>);
    }
    default: {
        return parser(in_place_token_failure<FAILURE>);
    }
    }
}

OBJECT_ELEMENT_SPACE:
    switch (ptr[next_token]) {
    case ' ':
    case '\n':
    case '\t':
    case '\r': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        break;
    }
    }

OBJECT_ELEMENT : {
    switch (ptr[next_token]) {
    case ',': {
        break;
    }
    case '}': {
    OBJECT_BACK:
        if (WJR_UNLIKELY(current == stk)) {
            if (parser(in_place_token_right_brace_back_root) ||
                parser(in_place_token_repeat)) {
                return;
            }

            if (reader(token)) {
                goto REPEAT_TOKEN;
            }

            return parser(in_place_token_success);
        }

        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        type = (--current)->type;

        if (type == 0) {
            if (WJR_UNLIKELY(parser(in_place_token_right_brace_back_bracket))) {
                return;
            }

            goto ARRAY_ELEMENT;
        } else {
            if (WJR_UNLIKELY(parser(in_place_token_right_brace_back_brace, current->first,
                                    current->last))) {
                return;
            }

            goto OBJECT_ELEMENT;
        }
    }
    default: {
        return parser(in_place_token_failure<FAILURE>);
    }
    }

    if (WJR_UNLIKELY(!reader(token))) {
        return parser(in_place_token_failure<FAILURE>);
    }

    switch (ptr[token]) {
    case '"': {
        break;
    }
    default: {
        return parser(in_place_token_failure<FAILURE>);
    }
    }

    if (WJR_UNLIKELY(!reader(next_token))) {
        return parser(in_place_token_failure<FAILURE>);
    }

    if (WJR_UNLIKELY(next_token - token < 2 ||
                     parser(in_place_token_left_brace_string, ptr + token + 1,
                            ptr + next_token - 1))) {
        return;
    }

    switch (ptr[next_token]) {
    case ' ':
    case '\n':
    case '\r':
    case '\t': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        break;
    }
    }

    if (WJR_UNLIKELY(ptr[next_token] != ':')) {
        return parser(in_place_token_failure<FAILURE>);
    }

    if (WJR_UNLIKELY(!reader(token))) {
        return parser(in_place_token_failure<FAILURE>);
    }

    switch (ptr[token]) {
    case 'n': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        if (WJR_UNLIKELY(parser(in_place_token_null, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case 't': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        if (WJR_UNLIKELY(parser(in_place_token_true, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case 'f': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        if (WJR_UNLIKELY(parser(in_place_token_false, ptr + token, ptr + next_token))) {
            return;
        }

        break;
    }
    case '0':
    case '1':
    case '2':
    case '3':
    case '4':
    case '5':
    case '6':
    case '7':
    case '8':
    case '9':
    case '-': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        type = 2;
        goto NUMBER;
    }
    case '"': {
        if (WJR_UNLIKELY(!reader(next_token))) {
            return parser(in_place_token_failure<FAILURE>);
        }

        type = 2;
        goto STRING;
    }
    case '[': {
        if (WJR_UNLIKELY(parser(in_place_token_left_bracket))) {
            return;
        }

        (current++)->type = 1;
        goto ARRAY;
    }
    case '{': {
        if (WJR_UNLIKELY(parser(in_place_token_left_brace))) {
            return;
        }

        (current++)->type = 1;
        goto OBJECT;
    }
    case ']': {
        return parser(in_place_token_failure<FAILURE>);
    }
    case '}': {
        return parser(in_place_token_failure<FAILURE>);
    }
    default: {
        return parser(in_place_token_failure<FAILURE>);
    }
    }

    goto OBJECT_ELEMENT_SPACE;
}

NUMBER:
    if (WJR_UNLIKELY(parser(in_place_token_number, ptr + token, ptr + next_token))) {
        return;
    }

    if (type == 1) {
        goto ARRAY_ELEMENT_SPACE;
    } else if (type == 2) {
        goto OBJECT_ELEMENT_SPACE;
    } else {
        goto REPEAT_TOKEN_NEXT;
    }

STRING:
    if (WJR_UNLIKELY(
            next_token - token < 2 ||
            parser(in_place_token_string, ptr + token + 1, ptr + next_token - 1))) {
        return;
    }

    if (type == 1) {
        goto ARRAY_ELEMENT_SPACE;
    } else if (type == 2) {
        goto OBJECT_ELEMENT_SPACE;
    } else {
        goto REPEAT_TOKEN_NEXT;
    }
}

template <typename Parser>
WJR_INTRINSIC_INLINE void parse(span<const char> sp, Parser &parser) {
    constexpr uint32_t token_buf_size = 1024;

    unique_stack_allocator stkal(math_details::stack_alloc);

    uint32_t *token_buf = static_cast<uint32_t *>(
        stkal.allocate((token_buf_size * 2 - 1) * sizeof(uint32_t)));

    forward_token_reader<token_buf_size> reader(sp, token_buf);
    reader_parse(reader, parser);
}

} // namespace wjr::json

#endif // WJR_JSON_PARSER_HPP__
// Already included
// Already included
// Already included
